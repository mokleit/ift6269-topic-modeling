  Okay, recording started  Alright well. Hi, everyone. Welcome to ifta six to 69 privacy graphical models. So if this is the wrong zoom room, feel free to just leave. It's fine. Or you can just stick around and  Enjoy the discovery of a random class. So first topic as you notice I am  Speaking in English. So this is University Montreal class, but this is a grad level class in machine learning, where we have a very international audience, which explains why we teach in English.  There's also a scientific reason I teach discuss in English is that if you are doing grad level studies in  Machine Learning undergrad. That's a different thing. But, you know, now you're going either doing a master or PhD in machine learning at basically something affiliated with Mila  And in order to  Be able to interact with the scientific world. You also need to improve your English skills. So either practice. You're lucky in your native English speaker.  Or you're like me and most of the world who don't speak English as a first language, but it's very important, actually for participating in the scientific world in machine learning that conferences.  Like visiting you know my  Wife actually just got married.  I was being sent my girlfriend. But my wife. We just got married last week, so is from Germany, and we speak English with each other because I don't speak German.  Well, I mean, I speak now look more German but I don't speak as much German and English. So, so, English is very important for communicating. And so yeah, so that's also an important aspect of  Of, you know, learning, doing a PhD in machine learning, for example, is also to improve your English language. So if  So off how say see was evidence critic long layer.  Avoid away ma emailing your progress on the security.  Profile is that common Marco Polo cool etc etc.  Can you read the name for pleasure. A unless you live.  With a writer pool informational  Mayor we're collecting was C serum long lead for the facility PRC to be the the corner. So that was a I just say in French, that if you have trouble with English, we can try to find accommodations, given the diverse material is a French speaking University.  So perhaps another house rules. So I'm happy to have this class very interactive.  So I guess for now.  You could try to raise your hand. Can somebody just okay Jose. Can you raise your hand. See if it just pop out.  So normally, if you go oh yeah okay you pop up at the top of the participants. So yeah, so if you raise your hand. I can call you and you can ask your question. I think that will be better than writing in the chat. Because I see that something people are having fun in the chat. user avatar Jose Gallego  Thank you for some activity, just to make sure that people know how to do this. You click on participants and then you'll see a little window that shows up raise hand.  And that's what you do it. user avatar   Yeah so zoom one on one, I think in the interface. There's a button claw participant and there at the bottom you have a few buttons like  Yes. No. When I asked question go slower go faster. That's just a warning to tell me that you're totally lost or I'm totally bored. Feel free to use that you get him in clap.  Need a break or see your Hawaii. And I guess. Oh, I don't have the one because I'm the host, but I guess as a non host, you can raise the and I'm lowering the hand of course  And so you can click the this button to raise hand anybody as trouble finding this  So if you have trouble and you don't know how to raise a hand and you know no to say yes or no. You could the other button at the bottom is chat.  And in this way, you can also  Send a message to everyone or just to me.  I think you can also send message to individual people. If you want to talk behind my back.  Which you know use use sparingly.  That's fine. Um, so, no. So everybody is fine with the raise hand option, let's let's practice that. So let's do a yes. Click on the Yes button in the chat thing. Whoa. Impressive all these yes  So now if you click again. I guess it will just remove it, or I guess Kinane, wow.  Wow, this is cool.  The one. There's a zoo.  As the host can access all the transcript of personal chat after the end of the session. I was wondering about that. So the chat or when you do a private chat that's private to everybody else except the host. Is that correct,  Yeah. Do you want to  Take the mic and just tell us about this. user avatar Bhavya Patwa  Yeah, so I I read it somewhere like a few days ago on the internet that who's connected it after the session. And so I'm not very affordable. I read it somewhere. Yes. user avatar   Yeah, so don't say something you don't want to read  But yeah, that's good. So, by the way. So zoom has a bit of a security, privacy issues, blah, blah, blah. So I think my understanding is a lot of these have been fixed.  But I guess I was also trying to figure out to learn like again. What do people see in stuff and it's a bit hard to see. And actually, I was also able to hack into  Connecting to this meeting without user avatar Mojtaba F  Registering which I think it's a bit stupid but anyway. user avatar   But that's the main tool of us in Montreal, and I also heard that for people, for example, from Iran, it was easier to to connect to this tool than others. For example, bluejeans  Alright so alright so now we know how to raise hand. We know how to say yes and how to remove the yes of people user avatar   Guess I can clear all user avatar   Oh, I haven't been clear on. That's wonderful. user avatar   And even do Paul, but I guess I'll do politer okay so language is solve class.  So their website.  Is here at the top and an easy way to find a website, if your loss is to Google my name and then you find my homepage first hit on the top will be my homepage. And then I have a teaching section teaching section. And so then prophecy graphical models. Bring me back to this wonderful pitch.  So actually, that's what I do when I want to go to my page. Okay, so  Let's have fun. How many people have  already read the web page.  So yes, worry I have read it. No, I have not.  Wow. I love this quick poll.  Do you see the result.  Okay so 56 people see yes 12 people say no. Okay, good. So I'm glad that people have already looked at it. So basically, it's a very detail page. By the way, so  It sells everything about the evaluation.  The textbook will be using the homework logistics, the detailed outline etc etc. So I'll go quickly over a few of these items today as an introductory lecture but you know all the details, or are there.  So I guess the first thing is, what is this class about right so  Here the description. It says this course provides a unifying introduction to statistical modeling of multi dimensional data through the framework of prophecy graphical model.  Together with their associate and learning and in France learning. So, the key word here is multi dimensional data. So basically in this  Class will look at how to make modeling of data with many components.  And so basically we'll use the language of quality theory. So this will be random vectors.  Where they will be multiple components. And so a good example is that say, I have an image each pixel in the image could be one of the component of this random vector. So, the image could be million pixels. So we're talking about how to model of distribution over a million dimensional  Vector and and so this is fairly complex in the sense of  You know, traditional statistics that say a few  Decades ago people were more used to work with, say, five variables or seven variable. Let's uh, let's say, let's say we we studied population of people will look at their I don't know their age, their gender,  Their height or something. So you have a few of these variables and you could model them. But that's, you know, that's five or eight variables. So not million right  And so in order to go to a million. You will need to use or it's good to use tools from computer science tools like that that structure.  In order to be able to efficiently both  manipulate these complicated distribution as well as just to encode them. Okay, it will it will see already today in this lecture what I'm talking about, about how to encode them efficiently.  But a nice aspect of this is basically will  Will have tools like graph theory come from computer science which enables to efficiently have data structures which can  encode these distributions. So basically graphical model. And I'll go into much more details on the stuffing very soon but graphical model is a is a merging of property theory and graphs.  In order to encode family of distributions in a unified manner for that you can efficiently manipulate thanks to the graph properties. So you can use graph algorithms like this.  Message passing over them or you can use  You know the find the shortest path shortest path in in a in a graph, for example. So all these ideas come into handy. Okay. And so, so the nice thing about Africa ball. I think it's a nice mix between computer science and statistics and party theory. Okay, so that's the topic of this class.  It has a lot of topics in there are other classes which I will mention actually very soon.  But one key aspect.  Of this class to keep in mind. And let's look at the prerequisite. So the prerequisite is. Oops. How do I guess I cannot sit ups with my mess and I can see very quickly.  So I say previous course worse than linear algebra, there will be a lot of linear algebra.  You know, you have to know what the matrix is if you don't know what the matrix is you'll have trouble in this case multivariate calculus, you know, gradients partial derivative basic proteins statistics.  You know, what is a marginal distribution. What is a random variable, what's the expectation. So all these A, you should have a background in all these in order to take this class, otherwise you'll struggle. Okay, and the  There will be programming. So it's also a computer science class, so they will be programming.  Is so familiar with some matrix oriented programming language will be useful and will actually use Python with numbers. So if you don't know Python. That's fine if you know like if you do MATLAB or if you do other matrix oriented programming languages. It's totally fine.  You can learn Python. But, you know, as long as you know a bit of programming a fair amount of programming.  And then I have this morning. This class is quite mathematical. Okay, so I think that's also one differentiator with other machine learning class, which are around here.  So you might have heard the reputation of my class. It is quite math heavy  Both because of my interest and also I think because of interest of many students. So if you don't love math.  And if you're looking for a lot of applications and machine learning, but don't want to look at the fundamental theory behind it and  The beautiful mathematical applications or applications of math, then do not think discuss right. You don't have to think, discuss anyway. It's not a Sufi required class. And so let me go through other classes but  Before getting to the other tests. And I said, the amount of work is significant. So I write that. Because a lot of. So in the past. Students mentioned. Oh, it's a lot of work. It's another work.  And some people said it's a lot of work for three credits class will, by the way, it's not a three grids class. It's a fortress, guys. Okay, so it's at least eight hours of work per week. In addition to the lectures.  And so it's not an easy class. It's not a class where you can just like  breeze through on their, their hand in the feedback I got from previous years, people love this class, like they find it extremely rewarding, we find that the the  The concepts. The like that I go in with rigor three concepts, so I will really give present to the a lot of different concepts from protein statistics, but also optimization linear algebra, etc. In a more rigorous way which helps you to get to have a firm.  grounding in machine learning. So, so this class. It's a bit old fashion in the sense that this is stuff you know that, for example, I, I took a version of this class as a PhD student at UC Berkeley, which was like almost 17 years ago.  And and actually. So, by the way. So, so I did my PhD with with my Jordan and we will be using his book for this class, which I will distribute via email.  Because it's still not public. So the thing about the textbook, so that we go back to the textbook. Guess I'm  Moving around on different topics here. But so, the course will follow the unpublished district and introduction for significant models by Michael Jordan.  That will be made available to the students, but do not distribute. So that's here I email, Mike. I said, So I found your book online. Do you still want me to keep it private. Because now I found it online. He says, yes, please.  You know, keep it private. For now I'm on sabbatical. And I'm tending to finish the book this year.  So I emailed him again.  Yesterday, and I said, Oh, have you updated his book, and we'll see if he replies. But the point is, the book is not finished.  Beta nice three. Okay. So what I mean by a nice read. So Mike is actually a very great author writer.  And it's the kind of book where you can go on the side of the beach and just read it, you know, like with the palm trees and like  before going to sleep. It's, it's a nice casual pace and he's very like explaining things in a nice way. Okay.  And so it's not in the previous state because exercise are missing some references are missing. So, you know, you'll have to bear with it. But, but at least most of the concepts are there and we follow  This the basically kind of like it's kind of general approach. So the reason I was talking about that, as I was saying, well, this is old. Alright, so I have of course updated. A lot of this material with more modern perspective.  But still, because it's such fundamental concepts. A lot of these are almost like  The Immortal. But, you know, these, these are stuff which is still whoops still be useful in 20 years  Basic properties statistics will still be useful in 20 years  Yes, the latest of the Python library to us. You know, I don't know, like the implementation of empire torch of some algorithm will change in the next year, actually. But the math behind it. And a lot of these concepts are actually you know quite long term.  And so while I'm on the topic of textbooks. There's a few others. So if you want to detail and rigorous reference. So there's promising graphical models principal and things by definitely color.  So this book. Do not really on the side of the beach, unless you're a masochist or you just love very hardcore book.  So, I mean, it's not that bad, but it's it's it's much is the kind of book where you can kill somebody with so it's quite heavy. It's very thick.  But it's complete. So the nice thing is it's very thorough. So if you want the details of something that you do in understanding like Jordan book or you felt it was a bit a bit fuzzy. So you can go in this book.  There's the deeper learning book which is available online for free, which is very nice. So this can give you, you know, a very gentle review of applied math useful for this class, you know, an emphasis of the gentle review. That's a different style.  And I say, part one isn't useful chapter five continues, it's useful presentation of measuring basics. And so I will sometimes refer in the detail schedule as do in the planet, you know,  And then another classical book with the more Beijing perspective that Mike's book, but at least competed is pattern recognition and machine learning by Chris Fisher.  And here it is actually a cool anecdote, which is that Chris and Mike started to write their book together because Chris had written a  Christmas edition of this book, like in the 70s or 80s, and he wanted to make a new version in the beginning of 2000 or end of the 90s.  And then we were writing chapters with Mike. But Mike was much slower. And so at some point. Chris was like, well, this is not moving fast enough. I think I'll just finish the book by itself, you know,  Thank you and goodbye. And so basically split so Chris went on in finished writing this book. So, and Mike did his own book and never finished it, but perhaps it will be conditional. He had a sabbatical this here and  So you'll see that some of the chapters or fairly similar because, you know, they started from the same, but the whole thing is now different.  And in particular, the approach of Chris bishop is much more Bayesian. Okay, so are in one lecture, I'll explain to you more in details. This version Beijing philosophy perspective on priorities versus frequencies. So Mike is a mix between Beijing and frequent. This was Christmas pure Beijing.  Are right so that's for the textbooks.  Where was the prerequisites. What else didn't want to cover. Oh, yeah. The other Texas.  Let me find out the other classes. So if you go if you google Mila and you go for. There's a section like training courses.  You have a bunch of relevant classes for Mina  Which are mostly about machine learning and so whilst in French.  Look at the description is in English, but it gives the name is in French because this is diverse much. So this is my class with the link and then you see there's a few others. So there's  This class is 56 3690 for mom and pop session. I think this is his websites fundamental of machine learning. So this is a much more  introductory class then proceed graphical model. Okay, so there will be some topics overlap.  About, you know, perhaps linear regression. Let's look at the cost that the notion covered  So multivariate gushing density, there will be a bit of overlap maximum likelihood vision in France regression in your  Discussion. So we'll also cover some of that we won't cover neural network architectures decision trees or any of these things. So this is different. So the difference with my class is my class will go in a much more advanced level.  So if you have followed this class and you want to take my class, you know, it would be very helpful. It would be much easier.  If you want to follow my class as a first class and machine learning. I would advise against it, unless you have a good math background and you, you're fine to go with the rigor right from the start. Okay. But keep in mind that, you know,  That's the difference between those two classes. So this is more a required basic class that anybody should take my class it as a bit of a different  focus much more on the probability theory and the plus of graphical model of it monetization rather than just general machine learning.  And also it's it's a higher level. So this class as students. Today's session is 36 390. There's also an underground version. So it used to be the same.  Both undergrad and grad, which is also why it's a bit easier, though. Now they're trying to push it a bit harder because it's split, but still, it's not the same.  Difficulty as my class. And then I don't want I've seen  Is from  Which was from semak a new faculty at McGill University. It's called applied machine learning. This course will cover that topic and new development is that the mining and applying machine learning.  And so for example, if you're looking for applications of machine learning. I think that's that should be probably a good, a good test for you.  So in my class I will motivate the theory from applications, but I want to focus too much on applications. The main place where you will have the chance to also play with applications will be through the projects.  But the the assignment that kind of assignment to give you are much more. It's more like to understand the concepts.  So that's another class, you want to have a look. And then there's there's many, many, many classes, actually.  Like Sarath class. This is a technique class. I think it's probably a bit similar to mine. I don't actually know.  And then you have optimization class like it did your programming.  Oh, here. That's a cool new class from peeling back on reinforcement learning. It's more topic base Iran is teaching a class on an incredible class on self supervisor position learning. So that's basically like a very specific advanced topic class from from his days.  So yeah, so. So have a look at this at this website if you didn't have the chance to. I think it's a good you  Good idea of other classes which you might be interested in and and perhaps also more appropriate than Mike. Okay, I just want to make sure that if you stay in my class, you know, you know what you're signing up to and you can appreciate it.  Is there any questions so far about the topics and user avatar   What I've covered user avatar   Oh, somebody asked as anyone found the Jordan takes in a single PDF right and and separate into chapters. Yeah, I think that's what I distribute to you, is it one PDFs.  Any other question.  No.  No, of course, there's no cool actually. There's a fun stuff to make things interactive. So this was  I have not. Okay. So Jacob asked me, have you already distributed the text know and how can people who are editing acts as a PDF  Good point. So I am going to post a link. Actually, let me do it now.  Let's see, how do I do that.  So, user avatar   Good. user avatar   Alright so this I'll post it in the chat. This is  Class, this is a link which links to a class survey. So if I go there. I go to the email sign up for this class. So everybody here.  Who's remotely considering attending this class, even if you're not sure at all. Perhaps you're just shopping around or perhaps would just curious.  You should sign you should fill the survey, it takes like three minutes by tomorrow, and then I will add you to email this and that's basically what I'll use to manage  Them, it's originally to have the class. So some people are in studios. Some people did not have access to student yet.  Some people are getting into threat, so that then I can send you in particular that's what I'll use to send you the link to the pdf of my. Jordan Okay, and give you announcements, which are relevant to everybody and not just the people to do it.  So do other thing people have access to the homework assignment.  So the people were other thing I have access to do the homework, but they won't have their homework graded because there's already way many of you or registered and that's enough work for Jose.  So you will be grading the people who are just addicted.  Any other questions.  So something else in terms of format of the class. So it's a two hours class what i'll do normally is after 15 minutes or an hour, I'll take a 10 minutes break so you can, you know, stretch your legs, grab coffee review, something which was a bit fast and then reconvene okay  Will you discuss what topics are acceptable for the finals course project. Yeah, I'll go briefly I think today on this, or perhaps next class. But yes, I will.  Very soon.  Alright, so that's the email signup  Let's go back to the class website.  So I told you what this class was about  Okay, perhaps, uh, let's take a break on the logistics and tell you a bit about myself.  And then we'll go back on the logistics of the class and and I'll tell you more about the class. So I think I have some slides here. Let's see.  And then I do slideshow. user avatar   How do I present user avatar   Oh, it's not the right screen. user avatar   Okay, it's user avatar   Okay. user avatar   You go and Premier monitor  Boom. Here we go. Oh, that's shitty format, sir.  Okay. And so because some people said you wanted to learn a bit of French Dora. So the slides are in French.  But these are pretty easy slides. So yeah, just a bit a bit about myself.  So basically I'm a. Now take your faculty since two years ago, actually, I forgot at USC Montreal.  But I actually come from Montreal. So I did my undergrad at McGill University, and then I left.  Quebec for 13 years I did a little tour around the world. So I did my PhD at UC Berkeley with my Jordan.  Then I did a postdoc at University of Cambridge with zoom in Germany. You might have heard another Bayesian and then I went to Paris.  join forces backs group. This is where I did a little bit to position. So I'm kind of a specialist on complex organization. I was actually sharing there an office with  The Colorado who who's a Google brain researcher, you might know in Montreal and Mark Schmidt, who's a faculty in UBC  Which was a lot of fun. And then I became a research scientist in Paris and in your yes it was economists who and  Mixing things economist superior engineer. Yeah, I was there for a few years, starting my research group there enjoy life in Paris good food strikes. I had a view on the fly tower from my apartment. So it was pretty nice.  And then in 2016 there was an opening at university Montreal. Actually, I even had apply for my friend citizenship. I was planning to stay in France forever.  Was like I I'll become a French citizen, because I was tired after 13 years abroad. I was tired of not being a citizen, being a foreigner was kind of the idea  And you know my ancestors come from friends. So I have some French roots. Anyway, so there was an opening and he was in Montreal.  Which was looking for a faculty at the intersection of optimization and missionary, which is basically my expertise. So it was almost like my name was written.  On this faculty offering and I was meeting with your show Banjo at nervous about it and I was like, I'm not sure. You know, I'm pretty happy to Paris.  But yesterday I was very convincing saying that they would be amazing things happening in Montreal. They were trying to build critical mass. This was 2016 so it was like  When the deep learning was kind of like an exponential growth and starting to be super hot and having amazing successes and particularly neural machine translation.  Which came so some of this word came from in Montreal.  And so he convinced me to leave my my position in Paris and come back to my roots in Montreal and joined the faculty of years in Montreal.  And actually, I was very happy with this move because both. I love Montreal. My family's from here and this was the start of an explosion of investment in Montreal, like in fall 2016 is when you receive a trial got 100 million dollar grant  In safe. It's, it's called Safe for forgot was the name in English.  But this, these, these were these huge investment and this was for machine learning and optimization  In collaboration with the technique in order to do these people  And that was just a start of more investment than there was the Canadian Canadian AI initiative where a CFR invested 125 million their own Canada to create three center of excellence of AI. And so there was Mila which was created vector in Toronto and  Amy in Alberta and they're on the same time.  A lot of big companies started to  Create labs in Montreal to be close to Mila including  Google brand I think was first and there was deep mine. There was also Facebook. Yeah. Research Microsoft Research  Some, some  Both through there's two labs actually from Samsung there's eliminate the I, as a startup, it should register for this AI to all these are research lab in machine learning.  Which are quite connected with with Mina, and so actually doing machine learning in Montreal is quite fun. It's so very nice ecosystem.  Yeah. And so, so that's the story and my expertise of research is structured prediction.  Optimization. And also, I do applications and various domain computer vision that's running processing information retrieval completion biology. I'm not an expert on any of these some applications. I know a bit better than others.  But, you know, my main core expertise is in core machine learning. So what structure predictions, just so that you know as little parenthesis. It's quite related to purposely graphical. Oh, it's not the same thing. So structured prediction.  Basically the idea is  Well, first, before telling you. What's your prediction is that's just look why we care about search of addiction. Suppose you want to recognize words. Okay, so here's a character.  Oh, yeah. So here's a  Here's a character, so perhaps you can tell me in the chat which letter. Do you think it is.  Six. I love the 6611 is crazy.  Yeah okay but six. I love the six because this is a very strong m&s prior. Okay, so, because people who look at me as digits, all the time. They think everything is a digit was this was actually a character right so it's it's actually a letter, not a digit. So this was  It was a C and actually here. I also cheated, because the the I copied pasted the are twice, I think. So these are  Two ours but one is a seat because we basically look the same and another example application is word alignment in machine translation. It's a bit less use nowadays, but  Earlier it was a much more. It was an intermediate step use machine translation. Now it has been replaced by attention mechanism.  But the idea was, I have sentences which are transitioning to each other.  Like French and English. And I want to know which words are transitioning of each other. And why do I want to do that is because I want to build probabilistic dictionaries of translations. So I've been access to large corpus of data have  Translated text in French in English, then you can train your mission physician system. So, for example,  The French parliamentary procedure, sorry, the  Canadian Parliament through proceeding are all in English and French because it's the two official languages.  So you have this huge amount of text, which is both in French and English. And it's pretty easy to align the sentence at the which are transitioning each other.  At the sentence level it's harder at the word level. And that's the word I'm in  So, for example, Oh, are these two words, should we be translation and if you know English and French say, Yeah, sure, why not.  But if you look at the whole sentence you realize, actually this was not necessarily in this case in the context. This was not supposed to be aligned to man, it was supposed to be aligned to move it. Okay. And so the big  The big motivation here and the  There's the scope of the story is that context, help to make these classification decisions.  Intuition make them independent again so stretcher prediction. The problem is to have a structured object as input, like here a sequence of characters images.  And as output you want a whole structured object. So here a sequence of so it's a sequence of images and you want a sequence of character as the upload. Okay, so you don't want to classify each character independently, because the tasks.  Is actually a joint prediction and so  I there was word recognition. I gave an example. The other one was word alignment.  Where you have the Spirit sentences as input, which are concentrated each other and the output in this case is a set of edges which correspond to which word partnership each other. Okay.  And one aspect of structure prediction.  Is that the, the number of output is exponential like when you do m&s specification def 10 classes. It's easy to imagine that  You have 1000 classes or something like that. It's still pretty small. When you do structure prediction, the number of output is 10 to 23 right I will get those number it because  You know, yeah, you have to multiply each facility for each component and you have multiple components. Right. And so you need to find ways to handle this exponential blower.  And promising graphical model isn't is one way to do that when you will focus on having probability distribution on the app.  In structure prediction and traditional structure prediction. You don't care about having a policy decision of all you care as a decision. Okay. And then you would have a lot of  Techniques, which have nothing to do with priorities and which still perform very well like generalization of support vector machines. Yeah.  So other example is machine translation. So, the input is a sentence in one language and the output is a sentence in your language. You can see it as a sequence of word or a sequence of characters or there is this  3D object recognition. So this is like a little robot with a laser or the meter which is able to measure depth and so it goes around the world and it make measurements of depth and you want with this map of depth.  Oops. Interesting. You want to identify where the building are wider the tree. So to each pixel, which is a measurement of that is needs to be classified multiple  Classes like road building tree. And of course, you don't want to do it in an independent fashion here because you know if there's a specific piece of tree here. Usually there's a piece of tree just decided because  Trees and extended object, right. So that's why you can think of it also as a searcher prediction problem. Okay. And finally, and I last aspect of structure prediction, which is  Which is kind of like different just standard classification is this structured error function.  So, which tells you the cost of predicting white crime when the ground truth when a labeled example from a human would be why. Okay.  And so when you do, and this discussion, usually either your writer, Europe is it that you said seven months was a fan is like, okay, it's not them actually nine he said seven instead of nine.  No big deal. Like, it's like the cost you get is either zero or one. So that's the kind of customer should people use called the 01 their  Admission translation in the whole sentence is wrong. It's not the same thing is only one word is wrong or if one word was replaced by a synonym.  So in this case you want to encode these different errors in a much more  Rich way than just. Are you right or wrong. So you'll get the scriptwriter function which could count the number of mistakes or could even be something more fancy like initially natural language processing blue score is used  It's some kind of metric on distance of sentences which is supposed to correlate a bit with human translation, even though it creates a bit, but it's still not perfect either  But anyway, but the point is you want to have this structured arrow function and you want to take that in consideration and you're learning algorithm because it's important. Insert your prediction.  Alright, so I guess.  That's the end of the practices. So that's what certification. That's one of my specialty. So  In in this class I will cover it a bit using prognostic methods. So you could use a prospect graphical model to solve this problem, but  In a class which which I teach in the winter semester, which is a an advanced. It's called Advanced for sure prediction and optimization. I think if I go to my website. Here you can see the link in the teaching so advanced structure prediction and optimization  So there I cover such a prediction much more detail and particular journey ization of support vector machines to search your prediction.  And and and what happens in this case is you get very difficult optimization problems because of the complicated structure of the problem.  Like if you do as VM. You have aquatic program which is fairly simple to solve. But if you go structure. If you do the structure prediction version of  SDN, you get a credit program was an exponential number of variables. And so how do you handle  Such a big optimization problem right so then you need to do fancy optimization tricks, and so this, this is why the in the title is also an optimization in this class because it's a it's a good way to see applications optimization  So, just so that you know if you're interested in this class next semester.  So now I'll get back to unless there's a. Any question on this. user avatar   Question. user avatar   I love it when people say no.  Great. That's interactivity so  Let me now talk finish this thing where I want to go.  Oh, yeah. So, a bit of sexiness of  Data Science. So this, this is from the why is it okay I guess I didn't do the animation properly so yeah  Machine learning is a really hot field right now.  It's been a rough field for a while. This is from 2012 I didn't even know  So this was in the Harvard Business Review. It said that that scientists, the success job of the 21st century. So, indeed.  Combining computer science and you know protein statistics machine learning this kind of stuff. This is basically what the science is doing. So this kind of job is indeed extremely  In Demand nowadays.  And here, I'm not sure if I'll be able to do that. Let's see if I can do it. So I want to show you some cool application.  Which is a bit old but still interesting  So if I follow this link, let's go here and now, how do I share my sound it's share computer sound lucky destroy it. And now,  Let's try this, see if this works.  New people here my sound of the computer. Good. Okay. Melissa said yes.  Welcome to prices. So how many people have seen this devil  I agree.  So basically, this was, I think, in 2015. Let me see, what did I send my  Tech days doesn't 15. Yes. So this was  I think it was called the Star Trek translator, because in if you know about Star Trek.  You were able to just speak in one language, and then it would be automatically translate into the other language. Right.  So to do that you need to do first speech recognition. So I'm speaking  Let's say in English, then speech recognition is used to get the sequence of words in English that I've spoke then measuring pronunciation is used to translate that in French as a sequence of word then speech synthesis is used to alter  The sequence of French words in French. And so, and this was a this is implemented in Skype, which is owned by Microsoft. So you can use it. Now, if you talk to somebody in a different language. It's actually pretty cool.  And this was a live demo, which I think is pretty impressive. And in particular, 10 years ago. If you had asked me that this would be possible. Five years later, I would have not  Thought so. This was basically because of quantum leap in performance in speech recognition.  And also mentioned translation thanks to different  But let's see how how it works. Are you ready to start this demo. user avatar   Demonstration of striking translator using the language. user avatar   This guy on YouTube. He's on a single user avatar   Speak French. Actually, it was a pretty good translation and it was almost real time right so so that's very impressive. And so that's the kind of technology that now machine learning enables  So in this class you'll be able to see some of these components, in particular the speech recognition system use a structure call hidden Markov model. Hmm, which I will cover quickly today.  And now with a lot more bells and whistles, but that the hard, there was an HTML structure.  So you won't see in this class. The deep learning aspect because we can't cover everything, but at least you'll see the structure aspect of it.  Anyway, so that the slides will be posted online. The so you will be able also to find back this link.  Thing and let's stop sharing my sound.  Let's kill this thing.  And let's go back. I think I was done. Yes, was done with my presentation.  Let's just kill this but bye  Let's go back to the website. So I think  It's time for a break.  So can I go coffee break. user avatar   And we user avatar   Got  On coffee break. So let's take a 10 minute break. So it's 323 I'll be back at 333 so I'll just pause the recording. It's due to pause the recording.  I was muted. So 333 that's continue I'll still have my coffee.  So, somebody asked a question saying that in private to me saying the did 6135 last semester. So I think that's the deep learning class. Do you think my math level is appropriate for this class.  I think they don't really do that much math necessarily in these classes. It's not the same kind of math level that we're talking about, because they don't go in as much bigger but there's still a lot of math because the printing, you know, a lot of the inner entrepreneur.  So I would say that, you know, you should have a decent math level to do this class, but more importantly is. Do you love to go more in mathematical rigor.  That would be the question asking. So what you want for this class is a bit of mathematical maturity. What does it mean. It means you can learn new mathematical concept with no problem, like, or you're fine with it, you can  Say, Oh, you've never heard about singular value decomposition, you can just take a book or Wikipedia and read about singularity decomposition and now you're like okay. I'm fine. I don't have me the teacher to teach you something good about the composition  So that's the kind of of mass level that we're talking about.  Because the other thing which is important to keep in mind is that this is a grad level class. So I'm really teaching it as a actually I teach it as a PhD level class.  Even though the master take it through. But I teach this class at the PhD level in a sense of autonomy that you're expected to have it through your learning and so  There's a lot of assignments and and there's a bit of a hand holding in this in this learning with that which is good.  But on the other hand it's you know it's it's up to you to if you're interested in learning this topic and to do the work for that and to go, you know, read Mike's book or to go a bit deeper and some topics.  So that's what I want to say about that. And then there's more question. When you go for more recent research and Jim in this class to  Yes, so somebody asked about recent research and CGM in this test to  Yeah. So, for example,  I'll talk about version alone coders.  I'll talk about property also structure learning causality and these kind of topics which a bit more recent Mila  And somebody asked about having access to Stadium, Stadium, Stadium in studio that's universal Montreal Moodle.  You know, to speak with about this.  Actually Jose. You know who to speak with about students user avatar Jose Gallego  imagined that it would probably be Celine would be the person to contact but likely  If, if you already have your form. I don't know if you personally can add people to to the course from the, from the the websites so likely if they already have their registration form approved for for the course we could just ask them.  I don't know exactly. But that would be the two suggestions that have either go through the limb or we could figure out on our own. user avatar   Yeah so CNN is the grad contact for grad students affairs at, you know, some Montreal. So let's say you're registered from the gills through the enter university stuff at some point it will get to her will add you in the class.  And the thing is anybody who's registered for the class should be able to should be automatically added to studio at some point, it could take some time.  And so if you're a bit less about it. You could ask city not actually just other thing that class. I'm a bit wary to add you to studio because I like to keep the studio for the people are registered to make a distinction. Okay, so  I think I'm not sure if I can create a new group in studio to remember which one are we just started, which one are just auditing.  But yeah, so basically the all the material for the class is  Online on the website and also why I use the website. Where's my website.  And so, you know what, I'll like I posed a website in sorry the assignment. I also post them online tutor to drop the thing I don't post online or the solution, but I can send them by using the email list. And so in this way, people who are editing can also have them.  What is studio us for so studio is basically a university and platform for classes. It's kind of a big Moodle. And then if you know about Moodle.  And so it has been tailored for us in Montreal and so for doing online class management for doing class management online numerous material used to do. So anybody who's registered in this class should be added to studio automatically and  If you don't know how to login into studio. That's something else. You could also ask silicon about now while you should have instructions when you're created as a new students  And then in studio will have the interface to handle the assignment online. There's also a forum that you can use. So we can use the the  The stadium for discussion. So somebody asked can we use Piazza for discussion and no  Think that discussion will be either studio for a registered people's was about when you're looking for project partners or slack. I think slack is actually very good for  Discussion. So there's a general channel. People can also create new channels like wholesale normally create a channel per assignment.  So that you can ask questions about the assignment children, children.  And actually, while we're here, I would like to present Jose, who's our TA I have his, his website here will say is a third year PhD student at Mila supervised by me.  So perhaps will save you want to say a few words about yourself. user avatar Jose Gallego  Sure. Thanks. Hi. Welcome, everyone, and happy to see a lot of people registered for the course. It's going to be a lot of work.  But this is, I think, if you are interested in doing research in machine learning.  Even though maybe you might not be using the sun product algorithm, every day of your life. I think this is one of the courses that is useful to have a background knowledge.  Because it helps you solidify a lot of the understanding of the required for thinking about other stuff.  I will be your point of contact regarding the grading of the assignments. I will set up office hours that we will decide some time at some point this week in which you can come and ask questions either regarding the material, the course.  The, the assignment, the assignments and yeah, feel free to reach out to me on the on the slack. And yeah, good luck, it's gonna be a lot of work, but it will be worth it. user avatar   Thanks for say so.  Jose is extremely talented PhD student with me.  He is doing interesting work on a new version of information theory, which includes a notion of geometry between objects like similar to measure between objects. So we'll do a bit of information during this class, I think it is lecture and see on the schedule, blah, blah, blah.  Information Theory October 30 so  I'll probably do a bit of advertisement of closest work at that time as well. So, for example, having a kale divergence, which takes in consideration the the symmetry measure between objects will see why this is important.  But yeah, I'll say it was a TI last last year. So is now an expert on this class.  In particular, you also have revamped the assignments, so that it's much easier to submit them and grade them.  And he's he's really good at explaining things so I really encourage you to attend his office hours and also ask question on Slack. He also has been very good.  At hanging out there and answering questions and. And one thing I really appreciate about to say is that he has a very precise mathematical mind so it's perfect for this class.  Because I said like we are fairly rigorous. We're not too rigorous in the sense well too rigorous, what I mean is often I will  Like a good example is  To be rigorous about continuous probably distributions, you need to know about measure theory.  And we won't be covering measure theory in this class because it's not really needed and it just makes things more complicated. So we won't go at this level of rigor.  But at least you know making sure that the fear is or have the right conditions. This is the kind of rigor, we're talking about. And Jose is very good enough.  And we have somebody who's not muted. user avatar Rishabh Agarwal  You user avatar   Unless he has a question.  And then somebody asked a question. So I'm a researcher internet Mina, am I allowed to attend this class. Also, is there any kind of restriction  So there's no restriction to audit because auditing. Now, there's no physical space restriction is just, you know, space and zoom. So yeah, you're anybody can have it. That's fine.  Especially because as an auditor, you won't be having your assignment graded. So it's not more workforce.  But the project. Where are we supposed to discuss about it, forming a team topical so yeah  Good question. So the advantage of studio is that everybody is we just started will be there.  I'm not sure if everybody would go on Slack, so we'll see. So I think  It depends on how many people are on Slack. I think you can choose whichever is the most popular  Does the horse a minute auditor. Ask questions during office hours. Haha. user avatar Jose Gallego  Personally, I don't mind but just try to be mindful of the other people that are registered for the course.  Just because you're I appreciate your curiosity, but some people might be having some stressful moments because they need to submit an assignment.  Or they need to, I don't know. Prepare for the for the final exam or something like that. So feel free to come by just tried to keep in mind the other students as well. user avatar   Yeah, so the key, a key word here is mindful.  So be aware that there's already 96 people in the zoom session. Right.  And so if everybody asked a question in office hour. We're already overwhelmed. Right. So same thing like you've already sent me an email about the class.  gets crazy. So you want to be mindful with giving party to people are registered for getting support for the class because that's kind of the purpose of this if nobody's there and, you know, there you waited after the people who are just third install the fun  Recommended materials for measure theory.  I will follow up on this.  Either by email or on Slack. user avatar Jose Gallego  Quick this. There's this new reading group like a study group on measure theory at Mila so during the Mila slack lectures, look for some posts regarding measure theory.  Is something that is getting started. Now, so that could be a useful recommendation for those that are interested user avatar   In to say. So where are we, I told you about the textbooks.  I told you about the TA Jose.  It's a bit more about the logistics of the class.  So the content there is a detailed outline that I put  In the on the website. So basically this is a copy paste of the lectures from last year.  And you can see basically I actually write on my tablet PC during the lecture. Like, like a board because it's a very methodical class. I think it's the right pace, we can see, you know, you have the time to see things. I think it's actually, it's very nice to follow.  Us slides for the intro, but I was the exception. And so if you, you know, you can look for example to look at what it looks like.  And so these are from last year, lectures, right. So, so you can actually already have a look at them to help you follow  And then I update at the end of the day, usually within one day I usually upload the the the slides for the lecture. Sorry. Number slides, but the scribbles that I've done for every lecture there are scribe notes so scribe notes is basically a clean version of my scribbles  And I need to still clean up the one from previous year and post them online. So this so my to do list. So I might sometime asked for scribble volunteer scribe notes volunteer.  So what this means is you will basically do a later version of the lecture, I have a template for you and they tick.  And usually, you also have material, which was already tweeted from previous years, so it's not really that much work.  And this is a good way for you to really master the material and also learn how to use Natick, and is a way to  Help other people in the class and adding a paid version of the of the notes for the class. And when you do that, you get a bonus point at the end of the year. So that's kind of the incentive  Yeah, so you have the topics here and you have when the homework are do an out. So let's talk about the homework logistics. So basically, there will be so that yeah the evolution of the class. So there is 40% for the homework. There's five homework.  So 8% each there's a project which is 430 percent. I'll talk more about the project. Very soon, then the take home final exam 30%  Which is  In the last week of the class. So it's due on December 23 basically what happened is for the project. You have a few milestone.  I guess I could go right now quickly about it.  And so  So the, the, the milestone for the project is you will choose a group and us give your project choice on stadium by November 3  And then on November 24 you will send a one page update on what's up with your project that motivation for that. This can make sure that you don't wait to the last minute, because sometimes  People wait to the last minute and then nothing works, and then they have nothing to present and which is a bit sad. So I please you know this forces you to start something and then you will know what's not working. Early then on December 15 they will be a poster session.  So it used to be a physical poster session before coven. The idea there is to simulate kind of like a research presentation at conferences, because this is actually pretty cool. If you go to conferences.  you present your work to other peers and you get feedback on your work. It's a very nice experience actually and we kind of  Simulate that in this class. So you would present your project to the other people in the class, rather than doing a physical poster presentation this year will use gator town.  So it's basically a 2D world where you move around your little character and then you go to a poster board and you can see the poster and then you see the video feed of the person who's close to you can then talk to you about the poster so  I'll come back on the logistics of this interface or do I need to figure it out. But that's a kind of a nice way to have virtual poster sessions. Now,  And so the idea there is you will have to present your in six minutes to me your project and in anytime you want for the other people will pass by.  And then  The same day, I think I give you the take home final exam. So you have about eight days to take on final exam. And then on the 23rd, which is the last day of the term.  You can give it before, if you want. Like if you want to go on vacation or do Christmas I don't know scheme or something, you have your project report to heading on studio, as well as your take on final  And the project report is 48 eight pages I CML formats pretty short doesn't have to be long.  So what's the project about right. So, somebody asked me about the the question for for the project.  And so the idea of the project is just to go in greater depth on some concepts of the close of the course. Right. So the topic has to be linked with algorithm concepts or method presented in class. But beyond this requirement. The choices quite open  And particular, it may be and it should be tailored to interests. We encourage you to choose a paper that's closely fit your address, and any personal original contribution is value.  You don't have to have something novel in your research on your project. It doesn't have to be in your research, but you can if you want like you can say, hey, here's a research idea. I'm trying to work on. Could it be a good topic for this project with pasture.  And basically the standard test projects will have the following three components. One is that it will have some kind of like review on the topic from an article. Okay. So that's the idea that I want.  Unless exception. Sometimes there. You can also do it without. But usually I would like you to have your test project Robin, at least one article  And the reason for a research article. And the reason for that is so that you have a starting point you have something which makes sure that you won't go crazy or, for example, you could just try to reproduce some results in this paper or  You know, just understand the paper that's already interesting for the project was if you say, oh, I want to just do X, which has nothing to do with any papers and then you get lost in x, then it could be very challenging as a project.  And so normally you will have some kind of article or multiple articles and I give below a list of tentative projects.  But, you know, so there's a bunch of different topics here like  You know applications of graphical model implementation of algorithms. So I have I have kind of like, put them in different categories.  If interested in deep learning and gender models for example like D this kind of stuff. So these are give you example of papers, you might want to look but you can choose any other paper. It doesn't have to be chosen from this list, just to give you an idea.  And so then you will have an article review. You will then also normally have an implementation and then experiments. Okay, so depending on the nature of your project. You can have  Real data experiments or a synthetic data experiment, depending on if your project is more surgical or it's more applied. So, for example,  Let's say your project is more theoretical you're trying to understand some kind of  Extension of the concepts in class and perhaps you have an article and then you implement what happened in this article you just re implement it and then you just test it on  Sending the data and perhaps analyze these results. So that's also an important aspect of the project is to learn to teach you to to do research and or  Think critically and so analyzing the results and why does it not work, or what does it mean is kind of an important part of the project. And so that that could be the least efficient. You don't have to  Get new data sets, for example.  On the other hand, let's say your project is more applied, like you say, oh, I want to take the hmm and just apply it on on this like our real natural language processing tasked with some hardcore data. Well then, you know,  Your focus might be more on the implementation and just getting the results on this data, then on covering the concepts and everything, because you know that's where you put your focus. OK.  So the project will be in groups because there's a lot of you. I don't know how many are registered, but he's at. I think so. The groups are at least three or four people so that we don't have too many projects, because otherwise it's possible to grade.  So yeah, so go back over the website about the detail about the project that gives should give you any idea, any question about the project.  Okay, so somebody asked or normalizing flows. I personally consider as graphical models at all also acceptable for a project.  I think it's a bit similar to v. So v is nice because it has links with em that we see in this class normalizing flows, the main link with this class is the fact that you're defining fancy property fear probably is probably the distribution  What aspect is the usually and obviously have a graphical model component. It's just like, you know, here's the distribution on on listings  But if you really care about normalizing so shirt. You can have it as a project.  Any other question.  No question. So, so I've covered the projects. Now let's go back to the evaluation. So there's five, homework, homework, logistics below.  So the homework will be handled online in studio. So I think Jose will send you more detailed instructions as well because he makes a very nice like submission interface. So I don't want to go too much in details in this class because it's already made.  But a few key things. One is that  There's a late homework policy.  So that you know it's manageable to grade the work. We don't want to have homework like weeks after the deadline, because it's possible to greatest.  And so you have a budget of six late days that you can spend on the five homework and then you need to allocate them however you wish, so that you have some flexibility. Please read the website on how to do that.  No assignment accepted more than one we played and it was also we have a collaboration policy so so that's very important here is that we're happy for you to collaborate with other students in class. It's a very good way to learn like, by the way. So for me, you know,  The philosophy. I have for teaching and I think the philosophy, you should have a stake as a student is what's important is that you learn right it's not getting a new checkpoint on your site on you know on your degree or something, right.  Which means that  I'm happy for you to collaborate with other people. That's a good way to learn. On the other hand, you're going to just like copy on the other people that won't be helpful for your learning experience right  So the first thing is when you collaborate with other people. You need to mention it on your homework. Here are my it's like it's like  You know when you write a paper, you will have acknowledgments of, you know, other people and you also need to write your own independent writer because if you just copy the right of somebody else that's not super helpful for you to actually, you know, learn mature. Okay.  And  And that everything is virtual. By the way, I think it's important to to to stress, kind of a, of a horn or cool. Okay, so, so I Stanford, you have this thing called the Stanford hundred cold, basically, which is a reminder for you that you know  It's important for your own integrity in the and your own scientific carrier to uphold the highest standard and, you know, just not cheat and  We won't be there and actually it's not practical for us to put like crazy.  Traps everywhere to make sure you don't cheat and by the way science. It's really hard to  To also  Track. Cheers.  And it's not super feasible. So what I mean by that I mean that, let's say you made an experiment and you falsified your data, right, it can. It's super hard for other people to to  Realize it unless they have access to it and somebody tried to reproduce it and stuff like that. Right. And so  Science actually based on trust. It's based on peer review, but also trust and so fortunately people who tend to do this work, usually are trustworthy, though, there's a few bad apples and  Often they're caught, but sometimes are cut 10 years later. And so what I'm saying here is that, you know,  This is a ground level class. Now you're supposed to be mature enough to understand the importance of  Your own integrity and they won't help you. It will help you in the short term too cheap, but they won't help you in the long term.  And especially like okay you cheat. Now on the assignment and then you know you're a bit late on the paper deadline and, oh, well, why not just my experience on that would be working. Why not just  A few numbers there and, you know, the next step. You're also, you know, making up scientific results.  And  And if you're caught by the way, cheating. This will hurt your scientific carrier, like, you know, we're  For grad school applications for later. It's recommissioned letters. And so I would highly recommend you already lose some of the habits that undergrads had to kind of like perhaps cut corners and uphold the highest standards for for your scientific carrier okay so  Just a reminder. I mean, what I care about any ways that you learn  If I could not have if I mean, ideally, I would not even want to give you a great so then you would not have to worry about all these these like  hoops to jump through to get a grade. And then you can just focus on your learning. Fortunately, we have a system where we need to put grade. So  We'll but by the way we're usually pretty nice in this class like super hard grading, I need to coach Jose a bit closer, sometimes is a bit strict, I think.  We had feedback last year that he might be too strict will will perhaps relax this a bit so that also you can focus more on the learning aspect, rather than worrying about oh you know this. I spent all this time on this work and it's not recognized  So yeah, so that's the for the long rant. Sorry about that. It's also like I guess with the coven really key and everything is virtual like we can even check with somebody taking the exam.  Like it's it's getting tricky for these evaluation.  So that's why the homework logistics. What else do I need to cover  I think the last thing to cover is that  If you want to be to john Lieber I can receive my Montreal. So you're registered for the class, but you're not in a program like you know the master students or whatever.  You have to have my authorization, because I had bad experience in the past were some issues only  cause a lot of problems in my class. So I just want to make sure you know what you're getting into. And you belong to this class. So basically if if I have not already authorized you to be in the woods john label, send me an email saying why you're taking this class and you think you  You belong in this class. And then I could just say, Yes, fine. And then you get for district by city.  And last, logistics aspect in the schedule. Remember that October 20 and 23rd, there is no lecture. This is like the break for lectures in Montreal.  And then there's also the lectures during their ups and usually I'm traveling to conferences.  Like in Korea or something like that. So, usually there's two classes which are not there.  But this year because I'm not traveling, because everything is virtual so I have now two  Bonus lectures. So still we'll see how things go. During the year, I might create new lectures for that, or perhaps I'll decide that I'm just too lazy and I'm just more time for your projects. We'll see. But yeah, so that's the schedule. So any remaining questions about the logistics. user avatar Rishabh Agarwal  Are there, like to student. The ideal week user avatar   Yes, there will be, I mean I clear because this is not the new reps deadline. This is the new reps conference.  Okay, so, so I clear, there's a deadline as as as deadline, yet you distill lectures, because you can work around these things.  So you could use your late days for the homework during it. Here, for example, that's a good use of your they  Can one more team have the same project. Sure. It's usually happen. I wouldn't want everybody to have the same project, but let's say two or three, it's fine, because you know there's a lot of news. So I kind of expect everybody to have different user avatar   Projects. user avatar   Any other question.  Was something else I want to mention  Oh, yeah. So, um, these lectures are recorded  In particular, it's useful if you miss something and you want to come back. It's also useful if, for example, I have some students in Japan.  Which is in the middle of the night, right now, so you can't really watch live so then you can watch it later. I would hope that a lot of you would still come to the live presence, you know, you can ask questions. It's attractive. I think it's kind of nice.  But, you know, you have the flexibility to to do things.  By which they do recommend to have the projects team created by. Yeah, so I think by end of October, what I mentioned here in the milestones, I said.  Choose your group and give your own choose a project three or four students per project by end of October.  So right now it's always good to have your group people in mind. Perhaps your tour, you're hesitating between two or three projects, then yard field November 3 to choose your project.  But you'll have to interest to them, your project description, like a very short one saying, Well, here's the papers, we're  Decided to work with will be implementing this algorithm on this data set. That's kind of the idea and then the idea there is if you're in the wrong track. I can also tell you wrong track being when you're trying to do is impossible.  Perhaps you could try to do this instead. As a first step.  Are the video recordings week to be on the course website.  Good question. So I'm still I still need to figure out the logistics of getting this this cloud recording. I will send the link  Perhaps by email. First, perhaps on studio them. I'm not sure if I'll put it on the website at the beginning because it's a bit new to have all these like funny videos to the whole world to see.  But anybody is on the email. This will have the information on the website under recordings  So I think I covered most of the logistics. So perhaps it's time to get some material. What do you think  Where's my pen. user avatar   Outside user avatar   Good. I got a thumbs up. And yes,  The spirit of participation.  Alright, so  Let's start scribbles  So a reminder to email the signup  To feel this email signup  Survey by tomorrow to make sure you're on the  email list and then I'll send you the link to the Mike's books, my book. And also, I'll send you the link to the recording. So it's bit  If t  6269 F 20  And I'm not sure why this was  So let's talk about what promising graphical model are  Probabilistic  graphical models.  Right, find them this thing.  So,  I mentioned that this is to model.  Multivariate data.  And actually using property theory. Okay, so I said that graphical model.  Is a mix.  Between  Of graph theory. That's the graphical part of the name so graph here as nothing to do with the graphics. It's rough in the sense of graphs.  Graph theory. So this is like the CS part and probably the theory.  So the way I like to  To display it is this with this Venn diagram I would have CS here, I would have quality and statistics.  And I'll even put machine learning, by the way, and  What's the part of machine learning, which is not CS abroad in statistics. That's a good question. Perhaps signal processing Allah  Physics this super like machine learning is very multidisciplinary just kind of interesting aspect and graphical model is basically here.  And, um,  It's also also talk about probably T versus statistics. So basically,  Probably stick model.  Detail you stories about the world.  Okay, and using the. So for example, you can say, all I suppose that when I flip a coin. It's ideal random coin flip, with quality of one half, one half on each side. So that's a model of the world, middle of an experiment and then using the laws of property theory.  You can get answers to questions about the world and, in particular, you can also simulate possible observations that one could  View. If the would to perform the experiment. Right. So for example, you could say you could simulate flipping a coin six times and then you would see  The result of six coin flips and then you could repeat that multiple times to see what are the possible sequence of six conflicts, you can observe when you just flip a coin. Right.  And so using the laws are probably theory. You can simulate and you can also ask questions about the data, like for example.  The quality of seeing this data, right, you can say, what's the priority of having six six head in a road if I have a unbiased going to use property theory to answer that.  And the inverse process, which is to go from observations from data to a model.  Is what statistics is about  Statistics. Okay.  So I have observed thousands of coin flips.  And I want to know, is this the bias coin a are the coin flips independent perhaps I was actually not always flipping the coin, the same way, perhaps a, you know, I decided to just keep the result of the purpose time so they're not independent anymore.  And so then the question is, okay, what's the model for for this observation. Oh, that's the inverse problem.  You don't have to have a plastic models, but in this in this class we will mainly focus on models which are problematic.  What I mean by models in this case is just, you know, something which describes the world plastic models will use policy theory to describe the world. But there's other ways to describe the world which are not PC pro stick  And unfortunately this inverse problems is it'll be fine.  So we'll see when I go more in in in protein statistics, but  The direction from a probably stick model to data is very well defined. That's the beauty of property theory, it's just, you know, you pull the crank or for the theory. Sometimes it's intractable. You might need to compute  intractable integrals and you need to approximate things, but at least in theory you know how to do it.  Whereas, to go from data to prospect models. It's an it's what is called an inverse problem. And this is not well defined in the sense that there are many models which could give the same data. And so how do you know which one is the right one. Okay. And we'd come back when we talk about  Occam's Razor principle prior regularization these kind of tools to help you know Orient. What kind of models, we get from data, but  You also talk about no free lunch, which basically means that without more assumptions you can never be sure  From finite data. What's the model, which created it, because there's many models which are different and which would mean different prediction in the future, but which explain executive same the data.  The current observed data. And so without more assumptions you can under which one of these models, the correct  Omar asked what is the model that is not realistic deterministic. Yes, so  Let me give you something quite  Relevant or close to the computer science department. So the default is the pattern monochromatic either refresh or better soon as operational research.  So people in operational research often use optimization to model the world. So basically, they will have the observation, the constraints of a  You know, a phenomenon encoded in up to this problem. And if you want to make a prediction say what should be the, the, the value of something happening, you will solve this problem. And so there. There's no plastic model, necessarily, it's just using like  optimization tools and constraints to kind of like model the world. So that's an example of a model which has no problem teasing.  And let's say you want to do classification, like in machine learning, you were trying to classify input in like you're doing testify digits and all you care about is just having  An answer you don't care about the quality of answer. You could use a support vector machine to do that and there's no penalties there. So it's, that's a model, you know, that could be a pacifier model and there's no problem.  So usually, having policies is useful for  If you want to use this decision somewhere else and you want to propagate uncertainty, then it's good to know. Are you sure about the decision or, you know, this is just like a flip of a coin.  So that's an example of where the policies would be useful. Even if you classify things. So usually when you do decision theory in even international research you will include a bit of qualities, but sometimes needed  Okay, so that's just a high level perspective on property theory and statistics and we'll come back on these topics, of course, in much more detail, but I just wanted to give you a feel for prostate graphical and particularly to talk about applications. Right, that's  I mentioned earlier that  graphical models are applied in many different applications. And one of the beauty of machine learning is that the same graphical model, the same structure can be using very different applications.  And so let's talk about the hidden Markov model because that's kind of one of the most classical  graphical model.  And so in a market model is used to model sequential structure, right. And so let's have a look at what a graphical model could look like. So we'll have rounds, which are nodes in a graph arrows are the directed edges.  And what happened is I will have these random variable associated with notes. Okay.  So why  One way t y t plus one way to close to our different random variable. So if I delete them. It will be a random vector  And you're just index with time.  And the inner and a graphical model, the random variable or associated with a node. So a node is basically associated with a random variable.  So that's a semantic in a graphical model is that just another any grass. It's a graph where actually the know have the meaning of being representing some some properties of a random variable.  And  And the arrows that we have in the graph.  The meaning in a graphical model is the arrow gives you some potential statistical dependence. user avatar   Okay. user avatar   And the potential here is some kind of big caveat, because it's not because you have a narrow that there will necessarily be  Statistical independence, because you could be lucky and put the parameters in this in these distribution, such that it's still independent  But usually what it means when you have an arrow between two variable, it means that they are kind of correlated  So, so the distribution of white t plus one and white t plus two, as some kind of dependence between each each each parts.  The absence of arrows will usually mean some kind of independence statement and actually it will be conditional independence statements.  And I'll review next class conditional independence. Independence so that we're all on the same page for that. But basically, that's the semantic of a graphical model.  And in particular, here you can actually start to have fun here. So, this, this, this, this structure here in a graph basically means that  If I know the value of y t plus one and I condition on the value of white t plus one.  whitey plus two NYT gets separated, right, because the only path between those two variable go through white t plus one.  So, what, what it means is that why two plus two is conditionally dependent on whitey given white t plus one in particular, the future is independent on the past. Given the present. So that's the mark of assumption. Okay.  And welcome back and much more details of this again if you don't follow yet. That's just to give you a feel on these topics and another  Kind of like notation that we use in graphical models is the shading and so we could have other pieces of random variable.  And in this case, will use x so XD 60 plus 160 plus two and here the shading means that we have observed the value of these random variable in the data.  And so basically the hidden Markov model usually will have a backbone, which has this mark of assumption which are the wise here which are not observed. That's why it's called hidden  These might be not observed and then these will be observed that would be the data we have. And then we might want to infer these pieces to get more information about the test.  Though sometimes you also observe everything. So let me give you some examples.  Let's  So in speech recognition.  Basically the whitey pieces will be fundings  Which are basically phonetic pieces which makes words and the XT will be sound waves.  Right. And so the idea here is you will observe the sound waves.  And from the sound waves, you will want to infer what are the hidden for means, which are the things that people have said so by by solving this task by by finding people the most likely the most probable sequence of things you can do speech recognition from that. Okay.  And then this structure in this application would mean that oh if I knew which is actually not true fully true, but it's a good first order approximation to get a simplest model is you say, oh, Wi Fi know what I've said here.  Whatever I see here is independent of what I said there. Okay. So that would be the semantic of the mark of assumption.  But again, we'll go back in much more detail in future lectures on on these kind of conditional independence assumptions.  And so  Do I am a bit too slow today.  I won't have time to give you the punch.  So,  So perhaps let's see it for 25 minutes  Can I give you the punch.  I'll keep the punch for next semester. So for next class. It will make you look forward for  What's the point. So the punches. Basically, why do we need graphical model. Why do we need this this this graph structure. And the answer is to handle the exponential explosion of possibilities, because the number of full name sequence is exponential in the length of the sequence. Right.  And so if I want to define the distribution overall possible funny the sequence. I need to describe describe the distribution of an exponential number of objects and that's difficult and the graphical  Model will help you do that. Okay, so I'll answer how to do that in the next class. But let me just finish with three more applications.  So parts of speech tagging is another application. Hmm.  We're in this case the whitey will be sparks of peach. So I'll already write them like as possible and sanitation. So let's say determinant verb determinant adjective and noun. So the whities here will be parts of speech.  Or pause the abbreviation.  And the X. These  Are words in a sentence, right. So for example, I could have had the sentence. This is  A red box.  Then you can still model them as arrows. I was just too lazy to draw the arrows. That's why I just put straight lines.  So this would be x one, x two, blah blah blah, and then why one, two, blah, blah, blah. And DT VTT or just possible values for this random variable because we're in a variable can take multiple values that are these sensations. In this case, perhaps dtb to draw  So you could also use hmm model to model this  Identification of what are the parts of speech from a sentence.  So here's a third application. Totally different. Now we're commercial biology gene finding  And so in this case, you would have that the whities or binary variable which says whether  Pieces of the DNA or coding versus non coding user avatar   Region. user avatar   And you would have so so whitey basically belong to a binary variable. It's either 011 encoding zero means non coding  And then XD would be the DNA sequence.  So DNA pieces.  I think usually they're in triple but I'm lazy. Here, I'll just do some kind of I forgot what are the DNA.  Company components, but it's stuff like GTA CG, for example, but even sure if this makes sense. And so here the STS would be these possible. I even forgot the name  Of what a god or it is and then the whities would be this backbone.  hidden variable. And here you could have values like zero or one, which says that, oh, this is the piece which is actually does this make sense I think from a biological perspective, but I don't. I forgot these things so  Just to give you an idea. And so basically I want to know which piece of the DNA is quoting for a gene, because a lot of parts of the day. It has nothing to do with jeans. It's just like the support and he's kind of stuff.  But there are some pieces which are gene, and we want to identify them because it might be used for something in particular, we could then identify if somebody has a mutation and that the might have problems with this gene.  And it can also help you to start to do now synthetic biology and trying to fix it.  But again, you can model as a seat as hmm sequence. And you can use similar  Models that you use for part of speech tagging. But now with the DNA instead  And the last example I give because all these words discrete right so so the whities were always like discrete multiple choices. It's not like a continuous signal. So I can actually have also a continuous version of these application in control.  Where you could say, for example, white t plus one.  Is just some linear transformation of white t plus some control term the transform with the matrix be and some plus noise. Right. So you would have these would be matrices, the A and the B.  And this is the noise. Oops.  Okay, well I screwed up. It's just erase this  Let's just put in purple. So this is the noise, it will be see like a Gaussian random variable, and that will be the the the hidden states.  And then it would be the observation which could be obtained by a linear transformation of the hidden state plus more noise.  Okay. And so here  X and y te are continuous vectors right there. They're vectors in which are real numbers, which could be, for example, this whitey could model the position of airplane.  Like x, y, z position and X could be their radar reading the reading and on the radar right now. I'm trying to track where the plane is. And there's some noise because of the sensor has some nice  And if  epsilon t and Ypsilanti prime or independent gershenz  There gushing random variables. So, this model, which are linearly transformed. Gosh, and random variable and VT here was actually  Started this was a fixed input to the system. These are the controlled term, right. So you could say, perhaps we know how we were that kind of like forces we were applying to the system.  And so then the hmm becomes something called the cabin filter.  When it's continuous and you have these Goshen when a variable. The hmm becomes something called the Kalman filter.  Which is basically the simplest states constant state space model you can have. So it's a time series model with  Goshen model. Gosh, and random variable and linear transformation and the UT. The beautiful thing is that when you transform a gash in with our linear transformation, you get a gotcha. So everything is. Gotcha. That's why it's the simplest  Busy the gushing distribution is the simplest continuous distribution of continuous vectors. And that's why  And you can get it from this. Hmm.  Okay and. And then the question is, how do you do in France in the system. Right. I have observations of my radar and readings. I want to know where the plane is how do I do that well.  You will use message passing all over them, then we'll talk about in this class coming from the graphical model framework and  That's what it Kalman filter is basically doing  Filters is both the model is also called the the influence mechanism behind it. Sometimes the terminology is not super cute.  All right, so I'm already a bit over time. Is there any pressing question about what I've covered  So this Shang says he has a question. You have a question. This would mean by the yes  If you have a question, just ask him.  Clear repeat the mark of assumption. And so the the mark of assumption.  That it was from before. Okay, clear all let me clear things. So the mark of assumption.  So the mark of assumption actually is much more general and we'll see later on. But the in a time series context. When we say that there's a mark of assumption. It's only mean that  When I condition on  An element that's actually use a bit of a notation. So let's say I would condition on  This variable here. I know the value the hidden value of this variable like if it's coating on on coding. What happens if I have the mark of assumption is that the  All these variables here the future isn't is conditionally independent of the past, these ones, given this value. Okay, so it's a conditional independence statement. So I will revisit that next class when I will redefine what kind of shown depends me  Any other pressing question.  Okay. So, if not sorry I went a bit over time still getting a hang on this zoom thing. So, do not forget to fill the survey by tomorrow, and I will see you on Friday. user avatar   See user avatar   And I will stop the recording.
  Okay, so somebody asked a question already. Is it recording. Yeah.  He has the person has trouble with the zoom app on Linux. Anybody else here who's using Linux and zoom app is working.  Okay, so we have a few people made the zoom app work on Linux. And some people have trouble. So I'd recommend that if people can help each other on Slack, for example, to debug this  So the because the question was, can we use  The browser instead to do that. And the problem is that it's not compatible with the registration.  So it's a feature that Zoom is supposed to do in the future where you would be able to have both registration and use the browser. But for now, when you have a registration for the class, then you can use the browser as far as I know, so  Hopefully we can have with the help of other students, you can make it work on Linux. Otherwise, we can revisit it like next week.  So yeah, so perhaps on Slack like on general channels, say hey you know like how do I make it work on Linux or something started a thread on this.  Any other question about the class.  Or the logistics. I think I went through most of it last class. user avatar   Oh yeah, maybe, so I wish I could have asked you this last class, but I couldn't manage either  So I'm thinking in parallel to this if T 6390 which is kind of the intro class would you say it kind of makes sense to take these two classes in parallel or user avatar   So,  Do you have machine learning background already user avatar   I've played around a bit with graphical models. Previously, and I love math. I don't mind going into some unknown territory and learning on my own. So user avatar   Yeah, so I think if you're really motivated to learn these things you could do it. So the what you'll have is that the the 6360 whatever the number is.  Will give you a wider view of machine learning because you have a lot of things that we don't cover. So it's more superficial, but it's it's more breath.  And then  Some of these, you'll see it again in much more depth in this class. And so  You know, there will be a bit of repetition for you, but at the same time, it will just come consolidate the material. So,  I think it's, it's  Like this, this other classes, not a prerequisite for our class.  So for this class. So, so, yeah.  So, if you don't mind a bit of duplication. user avatar   I think it's doable. Okay, thank you. user avatar   Yeah, so a bit of background behind the class I'm teaching here. It was a like  This kind of class was  The, the graph. The first grad level machine learning class at Berkeley. That was when I was doing a PhD there, right. So, so basically the first main mission or interest. I took when I  When I was there and actually for the fun anecdote. I took the class first from Peter Bartlett.  And then I was a teaching assistant for Martin rain. Right. So I said, I get into class. And it was a very different way, the way it was thought  And then I said, again, just by curiosity, when Mike Jordan thought it because so in three different years. It goes through different teacher  It was, again, very different. So there was like a complimentary. And so what you'll get is a combination of those three things. Plus, also the pastas back version.  From France actually where we added a bit of stuff because in the US. It was three credits. Was this is for credits. Right. So there's a bit more material there.  Okay, so  Well,  Again, as a reminder, I will do like usually these class like 15 minutes an hour and then a 10 minute break. Then we finish. So what I'll do now is to continue  To present something I really wanted to present to you last time, but I didn't have time. Sorry, which is why are we talking about graphical models. Okay, so why do we need or why are we talking about graphical months.  Graphical so that was like the big punch line that I was so sad to  To not give you last time.  Let's see if it works. Yes, you see stuff and so  If you remember from from last lecture, I mentioned part of speech tagging with hmm as an application. So let's go back just to make things concrete. Let's say we wanted to solve but parts of speech tagging and we have as observation.  Actually this is we won't use the hmm model. But let's talk about the participants application, which is where we have a sequence of words. So that's our input.  And that mean make it more explicit. So I have x one, x two, blah, blah, blah. To x. So a vector of length t or t is the length of my sequence of words and I will use annotation.  X one collagen capital T.  In this class. So this is a bit like sci fi or MATLAB. So one column capital T could be seen as the set of numbers, one up to t.  And in this class will see often as a no nutritional shorthand will use set as index. Like, for example, I could say 135 if I wanted to. So x subscript 135  And basically this is to select component of a vector in a very, you know, shorthand way. So basically, this represents the, the vector x one, sorry.  With component, x one, x three x five and now there's a bit of a problem here because it's set is not ordered  Object a sentence just set of things. So there's no order, I could have also said 315 or 513 it's all the same set. And so when the victor is an order elements. So where does the order come from.  We will get back to that. But it doesn't really worry right now just think the order by default will be, you know, increasing index or something. But we'll get back to this notation all aspects and already. This gives you  A good example that there's a  There's a machine learning and especially in graphical model, there's a tremendous amount of notation that we use because we deal with complicated objects. And so you you need the patient to kind of talk about it.  And because these are complicated object it's takes a lot of  Stuff to describe them. And so often you'll use shorthand so often a lot of things will be implicit, it won't be explicit like here, there's a problem of, oh, what's the order they use for a vector. Well, it will be implicitly  Given. Okay. So one thing you'll develop in this class is also get this  This parsing skill of here's some equation or as an invitation and then you'll be able to make sense out of it, cookie, cookie by the beginning, it takes a bit of training. So feel free always to just, you know, raise your hand.  Asked me like I'm not sure what this means. And I can remind you what these things mean.  Alright, so we have a sequence.  Of words that will want to tag, for example, but let's say right now. We just think of it as a sequence of words and  We could represent every word. So I'll use x t here as one of the possible word as  Say discrete variable from with value one up to key right and so I let's say I have a fixed vocabulary. I could encode my vocabulary as the first word to the case word in my vocabulary and so  As an observation that could just tell you. Well, this is the first word in my dictionary. This is my third word in the dictionary instead of like spitting out the word you know with characters.  Okay, so that's just a nice mathematical encoding and let's say now we want to describe, we might describe a party distribution over these observations. Right. It's a  Good. This have it down so you can see more my hands do a bit of magic. So, so we want to model.  Let's say our goal is to define a distribution and later today. We'll do a lot of reviewing properties at distribution over x one up to it. So,  This is just a shorthand notation for this is a random vector X one up to 60 and I want to describe a distribution over these possible values. Okay.  So, so I said here just to be clear that K was the size of the vocabulary.  Which could be, I don't know 50,000  That's a standard vocabularies size. And so here the issue.  Is we want to describe their distribution over a lot of possibilities, right. So the present. The big issue here is that the state space, the number of possible values for my random variable is exponential in the size  Look at so you have an exponential size of state space.  Where the exponential is in  The length of the input.  Okay. Basically, the number of possible sequence when each word as K possible values is key, raise the capital T.  That's the number of possible sequence of key words. Right. And so if I want to describe the distribution over all these Cape race the tea possibilities. I need to say what's the priority of  The word this the sequence of words, let's say, da, da, da, da, da, da, da. And then I need to sell. So what's the probability of  That cat is in the bag or something. But you know, I have an expression number, so I need for each of them. I need to say was the property. The property since day one. So basically you need a race to the t minus one parameters.  In general, to describe this distribution.  To fully  Described user avatar   Distribution. user avatar   So that's easily 10 to the 23 and more right now like 10 to 23 I will get those number. It's a good number. That's a lot of things. So  Yes. How can you and so there's both the problem of how do you compute with a table with 10 to the 23 numbers. It's like, good luck, it doesn't fit in memory. You need to kind of do fancy stuff.  And there's also a question. If you remember from that, as I mentioned, statistics, if I have observation I can try to invert my process to find out what's the model with extremely observation.  So this, this statistic in particular here, it would be to estimate from observation and say I have a lot of observation of text that's basically by the way what GP.  GP D3 does is kind of train. It's called language model. So it's  Like, you know, billions and billions of sentences and it just tried to find a distribution of all these so that you can generate new sentences.  So in order to do that I need to estimate the parameters in my model. And here I have 10 to 23 parameters that there's no way with a even with a billion example I will be able just to make it to the 23 parameters, right. So that's the problem. And so  There's different way, you can work with that, in particular GP D3 use kind of like fancy neural networks structure to handle that. But  A much simpler approach and more. That's the older is to use  Factorization assumption and the distribution games. And that's what the graphical model ENCODE, so the trick. One trick to handle  This large number of possibilities is to make a factorization assumption about the distribution  About the distribution p  Okay, so for example.  We have, we want to define the distribution to join distribution on X one, two x t  And will say, Well, it's actually written as a product of functions. So, it will be there will be a term which only depend on the value of X one and then there will be a turn, which I index with two which depend on x two x one and I use the the the  Whatever it's called in English vertical bar notation.  Quite indicative of it's a conditional. And we'll, we'll see very soon, that it is a conditional. But right now, we'll just be a factor of two terms.  And then that will have f3, which is x three given X through  Blah, blah, blah. And let's say I have the last factor which is X t given X t minus one. user avatar   Okay. user avatar   And so  Rather than having an arbitrary this function dependence on my team variables I have that the distribution  That we want to evaluate on a specific input x one x two to 60 can be written as a product of factors, where each factor depends on that most two variables, right. So this, this is what we call this thing will be. Oops, I want a new  This thing here will be called a factor.  And this only has two variables.  In this case, and so that implies that choose fully specified this factor which depends on the on two variables which each can only take k values I need at most taste square parameters to specify  And so if I have T factors in my model.  That gives me roughly tee times case square parameters.  Then why do I say roughly well because if you suppose that these are distribution. It's actually k square minus one.  And, you know, there was the first node that I kind of like is a factor of this thing is a factor of only one variable. But, you know,  This is just to give you the the gist of the how it grows and so with this assumption about how the distribution behave. I went from K raise the capital T parameters to only case square times t. So, this is polynomial in cake.  And it was K raised the tea is exponential in this case.  Yeah. And so this basically is some kind of assumption.  And we'll see very soon in class what the assumption means but  If we'll review probabilities later today. Basically, when the distribution factorization specific way. You have some kind of independence.  assumption about the random variable, okay. So saying that the distribution factor is this way is equivalent to saying it as some kind of conditional independence assumption.  And where does the graph come into play. Well, these factor will actually be represented as clique.  In a graphical model. So it's basically, you will represent them as as as set of edges in a graph.  And that's where the graph would come from.  And one theme in this class that will come back to is representation of distribution. So,  Is how to represent distributions in terms of like the grass structure and also for each of these factors like or the arbitrary tables or they could be also some kind of like structured function. And we'll come back to that. Okay.  Let's so  Here, the idea was just to give you a gist of the motivation for graphical model so already. Now, from this assumption we get that we have a small number of parameters to estimate or distribution, rather than exponential number of pounds so with finite data that's a few  Hundred thousand observation I could estimate TK square parameters, let's say, to 10 and key. I'd say is 1000 just to make it more manageable or that's that's 10 million parameters, I could estimate 10 million parameters.  So now it's manageable. And so now what about computation. Right. How can we compute with such a representation. Okay.  So now let's talk about computation.  So say I want to compute  The marginal distribution.  Over x one.  So I would write p of x one. Okay, so p of x one to exceed that's called a joint and we'll get back to that very soon. When I redefine all these terminology, so make sure we're on the same page.  If I looked at only one component. I want the marginal over this component. How do you get the marginal. Well, you get the marginal by summing over all the other possibilities, right. So you get the so this is called a marginal dimension.  So we have that the marginal and variable one  Is by definition the summation over all possible values of the other variable, which I will write like this.  Of the joint.  Right.  Okay, it's already like more. In addition, right. So, this by definition, there's some x one, x two, x three, x for what it means. This is shorthand for summation  Of X through all the possible values, it can acids one to Kate. Right. And then there's also summation of x three belonging to blah, blah, blah. And then I also have summation of x t belonging to one up to  Because there's a product structure here each variable can take all its values independently of each other. So this is actually  And some over an exponential number of possibilities, right, this, this is all the possible values of x one to exceed there's an exponential number because of this community explosion right there.  Keep us with us for every some and I'm always rerunning the whole some each time. So I get basically an exponential son. This is this thing here is an exponential some  And so if you would do it in a dumb way. You will need to some over 10 to the 23 values and their computer that will take a lot of time, even if you can do tend to the tenant like a millisecond of something that's 10 to 13 minutes. Second, that's still a lot of many seconds.  Okay, so that's the big problem.  And so  That's where the factorization is used. So what we do is we use the factorization. So this is summation over x to blah blah t  Of F1 x one x two x two given x one, blah, blah, blah. And then I have F T X t given X t minus one.  OK, and now the magic is we will use the distributive et of product over plus, right. So that's a key property I have that  This to be activity. user avatar   We I'm not writing very well. user avatar   This tree. user avatar   This tree beauty. Beauty.  And what it means is that A times B plus C is the same thing as a time c A b plus a time see right  That's the magic of product and plus. And so what happened is, if I have a times b plus a time. See, I can factor is the A out and get A times B policy right  And when we have downs down  When we have  Here.  Is basically the generalized version where I have multiple products. So this is kind of the can think of it as  YOU KNOW, I COULD HAVE A times B times d or something and then plus blah, blah, blah. Plus, so I have a big son of products and what I can do is I can just take the thing which don't depend on  So, for example, like this thing I can factor is out of the sun, right by this to be because there's no x one in the sun.  And same thing this I can factor eyes for all the some which don't have extra next one in it. Okay. And so basically what you can do here.  Is you can push the some as far as possible. Inside, put the pen testers in there in the right way to kind of reordered the  The computation such that it's much more efficient in particular I said I could factor eyes out. If one of x one, because there was no x one in the sun, then I have summation over x two x two x two given x one.  And then there. There's the rest of the submission don't have extra next one so I can push the some inside so I can have summation over x three and then I have f of x three given access to  And again I will do that, pushing, pushing, pushing, until I get the end. And what I get at the end is summation over X, capital T of 5060 given X t minus one.  Let's see if I can close my parents. He says there's three that's parents, his parents, his parents.  Okay. So, this what I've done here was just more enterprise version of the simple distributed et la dimension.  And if you're not comfortable with these kind of manipulation, I recommend that you try to work it out at home by starting with like two elements than three elements and just convince yourself that this is correct.  And this is a central element of graphical model and why everything works.  Because what's happening now, once I put all these parenthesis, now I can start to compute this, this some and, in particular, what I can do is I can start by the inside element right so so this thing here for every value of X t minus one i some fit the factor over the value of x t  So you can think of this as a function of X t minus one.  Because it's T has been some doubt in disappeared. It's, it's not a variable anymore, but for every value of x minus one. I compute this.  I get some and then I get a value. And I do that for every possible value of x minus wanting to get a new factor, which we call a function of t minus one which is I call empty.  Okay. And why do I call it empty because this is kind of a message. You can think of it. That's a message that will pass in the computation graph right and so then what you'll get is  Here, I will have  You know, summation over X t minus one, I would have f t minus one which depends on x t minus one given X t minus two times my message.  Right, which was the result of the competition of the inside parentheses. And so this you could now call this, this only depends on AX t minus two, because I've sent out x two minus one. So, I'll call this empty minus one of x t minus two.  So it's a function with only depends on next two minutes.  And I would I would keep competing the sun.  Which by the way to compute each of the sun. So to compute this message is order k square compute right  For every value of X t minus one. I do have some over key elements. So that's order k i do that K time so I get a square compute this is case square compute time and use only K memory because I need to store the vector of values for every key elements, right.  And but each. So each time I do that it's a case square operation.  And so graphically where we can represent  Is we had  These variable x one, x two, x three, and actually the graphical model corresponding to this.  Example is like this. So the arrows are like this.  So that would be actually the graph for presenting this distribution.  And what we've done is  To compute the some user avatar   How do I go down. user avatar   To compute the some over all the values of these variable I started at the end at a leaf at the know which had no parent. So, you know, children.  And then what I did is I computed the message here empty which depend only on AX t minus one. And then I passed it back in the graph and then  At the next node I computed the message and t minus two which depends on the annex t minus two and then I pass it back in the graph. It's a threat. So you can think of the computation.  Ordering to compute the some store the value and then pass it back to the next known in the graph. And I do that from the end.  Up to the route where x one was  And so this is actually call a message passing or with them.  Because you can think of it as I have no one's in the graph. Each node do some kind of computation by looking at the values of message which come to them and then you pass them back to their neighbors.  But this is just a way to organize computation in a clever fashion and at the at the root. It was just these parentheses that I've used in distributed  Okay. But, and this enables you to compute  Efficiently. The marginal FP gently.  The marginal  P of X one.  So I replace a next summer word exponential number of terms by using the factorization property in a disability with still assembled Riddick special number of terms, but because I cleverly organize a computation. I only need here. This was t times case square operations.  So I went from Cape racing T to tee times case square so much more efficient. user avatar   So this is the time user avatar   And memory. Oh, and this case, actually, I could have done that in order came in.  The second message back is empty, minus one x two minus two.  So the question is, is  This empty. Oh yeah, it's empty, minus one, correct. Thank you. I love when I make mistakes with people stay awake. Skip.  So this is t minus one. Correct.  Thank you. Be 60  How do you say your name. user avatar Beheshteh Toloueirakhshan  Hello. user avatar   Ha. user avatar Beheshteh Toloueirakhshan  Yeah I correct when great user avatar   Thanks for correcting me. user avatar Beheshteh Toloueirakhshan  No problem. Thank you. user avatar   Alright, so  Yeah so. So here, this was just a very high  Level explanation for you can see both in terms of statistical complexity. We have less parameters to describe distribution as well as computational complexity, we can compute quantities, much more efficiently.  That's the whole motivation of graphical models. So the, the graph here was encoding the structure that we use and  The advantage of having a graph is that instead of us having to think really cleverly about oh, how will I organize this computation. So as efficient. There are actually systematic or rhythm to to do that for you.  And particularly, you can implement it in the computer. So that's done for you don't have to think and be clever about  Okay, so that's the  And basically that's that's what as the heart of graphical model is is just those two things. This the BT. BT and factorization.  Could I give the computational complexity for the second step of message passing the second step is still order case square  So basically,  Let's do a bit of an annotation. So, so this step here. I need to compute this some so for every value of X t minus one. I will some over  Oh, okay. So you're, you're, you're right. It's not a square. It's a cube.  Because this is a case square operation, but I do it.  Oh, this is order k. Yeah, correct. Okay. So for every value of this message.  I will do an order case on  And I do that k times because they keep us, it will not use which case  So it's order case square computation in order k memory because I will store these user avatar   You know, one at a time for every value of x two minutes. user avatar   So there's case square memory to encode the factor, but this is given to you. It's not extra memory, you need to use  Any other question about  Anchor you have a question, I see your head.  Hello. user avatar Ankur Agarwal  I'm not sure if I'm visible. Okay.  My question, so I just noticed that we  Have is to find the  Distribution of excellent right  So,  I look like, how lovely like Hollis every element of ahead of it being involved for the distribution of text. user avatar   Um, yeah, so. So if I understand your question. So you're saying how the other variables appear in the marginal. The next one. user avatar Ankur Agarwal  Yes, yes. user avatar   Yeah, so basically if I know, let's go. So if I go here, right, so let's go here. So if I know the value of x one.  Well, this influence the conditional of x two given x one. user avatar Ankur Agarwal  Very good. user avatar   But when I some over x two.  I will also have access to which appears in this term here, so they're all like they're all connected right user avatar Ankur Agarwal  Because in taking us include that issue forward rather than  Me.  Excellent. user avatar   Joint the joint of X one to capital X t is the things I'm trying to some  Because of this product structure there was all these these chain dependence. If I ask you, by the way, if I had asked you the marginal of  X ti  Na, na, na in both of these cases, you needed to some of everything.  Okay, because of this chain dependence.  And you can correct my pronunciation, by the way. user avatar   Yeah, it's user avatar   It's Joseph parts are you said it's closed. user avatar Vaibhav Adlakha  It's it's closed. It's closed.  So I wanted to ask if we don't have this assumption that it is only dependent on the US and then a graph would look something like X Men will have an ongoing has to every other excite an extra will have an ongoing us to everyone except expand and so on so forth. user avatar   Yes.  That's correct. It's called a complete directed graph. user avatar Vaibhav Adlakha  And I'm guessing which potential be able to move in this  Way should be able to compute that same complexity that exponential complexity using the similar approach that you described. user avatar   So if you have the complete graph like you just described, then there will be one factor.  Which depends on everything before you will have let's say it could be that x ti given x one x two extreme sport. So this, this is a huge table with TI X with k to the  T minus one entries so I'll have to some over this huge table. This is exponential. So at the end of the day, if you have just a complete graph, you don't gain anything in terms of computational complexity.  And we'll see that was to that very soon. Well, first of all, in general, doing in France in these distribution is NP hard. And so you need to have some kind of like assumption to kind of save the computation and one assumption is, indeed, like, okay. user avatar Vaibhav Adlakha  It's kind of be a sequence, for example. user avatar   And you user avatar Vaibhav Adlakha  Warm up user avatar Oumar Kaba  Yes. Hi.  So you're right. The mothers with arrows. When you draw them. And I'm wondering if it makes a difference or being directed graphs or not.  Because it seems like it shouldn't make a difference. user avatar   You intuition is correct. So first of all, we'll see in this fast. There's two type of graphical model at the basics. One is directed call the doctor graphical model.  Or Beijing network. The other one is undirected. They're called market network. And then there's no direction of the arrow and when the graph is a tree like this. This is a tree. The directed graphical model and the under the graphical model or the same. And so the arrows don't do anything. user avatar Oumar Kaba  Bigger user avatar   But we'll see that  When we talk about when we present things it will change something  In terms of competition. It doesn't change anything, but how to represent the distribution, it will change a bit user avatar   OK. user avatar   Cool. So I think you're all very motivated now to learn about this test, right.  That was it. That was a teaser of expansion computation. So we will, we won't go back actually to the exponential computation before some time.  Because the first few lectures, basically, now what we'll do is talk more about proteins statistics and and basically we'll talk about the simplest graphical model where you only have two nodes you have only two variable, so you don't have to think about  Exponential, the number of things because there's only two very use two variables. So it could run it, given the size of the input basically  But  Very you know when we get back to Africa mold and then the the graph aspect will become much model. Okay.  So let's say perhaps it's okay, perhaps before the break, I will talk about the theme of the class.  Then we'll take a break before we go to  production quality review. Alright, so let's talk about some themes in the class. user avatar   key themes user avatar   So,  Because we want to talk about distribution over you know multivariate data. And so one thing when we talk about this distribution is how we will represent the distribution. So the steam is representation  Okay, so how to present  Structure.  Structured  Property Distribution.  And so already, there's the  The graph come into play.  And that gives you some kind of factorization like I've just shown  So already, the factorization will help. But even if we talked about factorization. There's another aspect, which is the parameter ization  Which is how we will  encode the possible factors in my, in my  Distribution. Okay, so. So in the case above. I had a chain. And I said, I add factor which only depend to variable at a time. Okay. But then the question is, okay, well, what are these factors which depend on the two variables or the full table.  I eat any values.  Of for any pairs of input. I have a specific parameters. So I can have an arbitrary distribution on these two or I could use some kind of  Reduce pasteurization. So, and we'll see in class, something called the exponential family.  Which will use a more which will make an assumption about this distribution to make it more compact he represented. So for example, we could use  Features. So as an example, let's say I'm talking about a factor on on two words, rather than for every pair of words out of habit or I could say  Well that's defined some kind of like text features which depends on two words that it could be no  Dude, what are the letters, what, how many for every you could count the letters. That's one of the feature. So how many A's there. How many bees there.  You could look at pairs of letters, for example, that's another feature. So you could define a bunch of features like that. And each of these feature will have a parameter associated with it.  But, but if the number of features is smaller than the number of possible inputs. In this case, case square, then you would have a more efficient for presentation.  And the exponential family is a way to kind of define these pasteurization for wide set of distribution. And we also  When we talk about expansion of me. We'll talk about properties and it's much off me, because this is a way to talk about a lot of distribution at the same time in a unified fashion.  And will have interesting concepts there like maximum entropy, etc, etc. So we'll get back to that but but the first thing is, okay, how do we represent distributions.  The second theme, then, is how do we is estimate  These distribution or the parameters for distribution. So, given data.  How do we learn  Or estimate  So learning is kind of the machine learning terminology estimation is coming to statistics. Terminology The parameters of the distributions.  And as I mentioned in the last class. So this is statistics. This is an inverse problem. And unfortunately, as elbows. There is no can any call  That best method to estimate parameters. Unfortunately that's statistics is unfortunately a mess. Unless you're a business. If you're a Bayesian everything is simple. We'll get back to that. But if you're not evasion.  You just get a bunch of different principles that you can use. Okay. And these are you can think of them as learning principles.  And then you have different approaches. One is called maximum likelihood  And other one is called maximum entropy  And they're not the same. This is certainly the are the same for the special family will get back to that. But in general, they could give you a different answer. You could also have moment matching  So all these are different learning principles to estimate parameters in the distribution and we'll talk about them will talk about their properties and  Various aspects that the app.  That's the second team. It's got this one too. So how to represent the distributions, how to estimate the parameters in my distribution. The and I guess it's not estimation estimation purposes, more accurate.  Estimation and the third team will be in France.  In France.  That's basically the probabilistic inference is implicit. This is to answer questions about our data case of this. I guess I'll put probabilistic here because this is what we cover in this class probabilistic inference. This is to answer questions.  About the data.  And for example, we want to compute  The conditional of some variable given some other variable. So, for example, why here could be a query.  And X could be the observation. user avatar   Okay, so this is a conditional user avatar   Or we could want to compute the property of the observation itself. And this could be a marginal  And the conditional could be, for example, I have observed these symptoms in a patient, and I want to know what's the policy that it. Do you have a specific disease. Right. So it's conditional distribution.  And I might have estimated from data, a large graphical model encoding all the dependencies between symptoms and diseases. And now I want to know how to compute  Specific value and like, as I said, in general, and replica model, you need to some over an exponential number of objects. So it's intractable. So we need to use clever techniques. Right. So this brings the aspect of computation.  And there's different tricks that will see in this class. And so an example would be using message passing like I told you here like belief propagation  It's address address in  And that's one way to get exact answers. But in general, it won't be possible to always get exact because it's too expensive or it's too slow. And so we'll also see technique to do approximate and France, so we won't get exact answer will get an approximation.  And we'll see some different ways to do that. And so the example would be to use sampling like Monte Carlo.  Mc, Mc is an example of that Markov Chain Monte Carlo or another technique will cease called virginal method.  Which is basically to use optimization to up to approximate quantities.  So I'm just giving you some keywords right now and you're not supposed to understand all these these things. It's just so you have a few key words to understand the themes in the sense  And so keep in mind that will have these three themes and we'll have a lot of different topics which will cover aspects of these themes, but that's kind of like three big aspects of, you know, working with distribution over multivariate data.  Okay.  Any question about this.  No, I guess you're ready for your break. So let's take a 10 minute break and you have time to think about questions during the break. Also by the way. So it's 227 let's come back at 237  And I'll post the recording. user avatar   Recording user avatar   Recording  Welcome back.  So there was a question from Lauren  Lauren  Lauren. Yes. You want to ask your question again.  Or you want me to ask it.  So, ask away.  Okay, I was going to ask if  You go this way, we can hear you. user avatar   Have a human voice. user avatar Loren Lugosch  Can you hear me now. user avatar   Yes, we can. user avatar Loren Lugosch  Okay, yeah. So is there a difference between I've heard the phrase statistical inference and I didn't know if there's a difference between that and what you wrote probabilistic inference. user avatar   Question. So basically probabilistic and Francis  Fairly clear. It's basically competing priorities.  Statistical and friends is a bit ambiguous. It could mean estimating the parameters. It could mean computing the properties over the parameters. If you're a Bayesian, for example.  Okay, so the big difference is because when you're talking about statistics, you might be talking about the inverse problem rather than  The direct problem. So when I talk about probably it's easy. It's easy to go from the model to the  Answers you're looking for was when you talk about statistical inference. It might be that, oh, I have data and I'm trying to figure out answers to statistical  Statistical questions. And the question is this physical question probably stick question or it could be figuring out what is the maximum length parameter, for example, which is an estimation question. So I would say Cisco in France is a bit more ambiguous caustic in France is very clear.  Sometimes it can be the same. And sometimes they would mean slightly different things. Okay.  So another question was,  From Simon or small.  Should the stuff I presented about  Message passing on a chain be hundred percent clear right now. Definitely not. I didn't go enough details and we need to review a few concepts. So this is just to give you a few key words and the gist of motivation for graphical model.  I would say it's perhaps 30% clear right now.  Depending on your background.  Will first review now property theory definition and stuff like that. So that will already help you to understand the where the meaning of these factorization are and then we'll actually go to the graphical model aspect in more details later.  Okay, so. Oh la la is asking if my explanation of the statistical and probably stick in France have anything to do with discriminate of engineering models.  Yes and no. So this kind of in general model is usually in the context of classification  And I'll come back to that later. Then there's a question of are we talking about  probabilistic model for classification or just  Classification because I could do classification, which is, is there a cap or no, and I just tell you yes there's a cat. There was no there's no I don't have to tell you a policy about that. Okay, so in this case.  It has nothing to do with were talking about because everything we're talking here was was probably tease.  But if if you would talk. What's the quality of a cat being in the image. So then you would have a policy statement.  And then, indeed, there could be a descriptive direction in a generative direction and they're opposite  So it's kind of a bit creative, but we'll get back to that we will have a section where we talk about logistic regression  Which is mark this competitive than say naive Bayes which is generative and we'll make this distinction between gentlemen descriptive directions. By the way, I did my PhD thesis on this creative methods combining discovery methods and journey methods together like so. user avatar   I had a bit of expertise on this topic. user avatar   Alright, last question.  Is statistical inference mostly estimation for example parameters donation or what other part of us. That's gone friends have other than estimation  Well, I think people could use that as going in France when the also just want to  Define a conditional distribution. So that's like crossing in France, so people could still use this conference because they don't see  It Anglo it it's it's kind of like a query about the data that you want to insert so that also works, then if you're a Bayesian you could have something like  Rather than estimate the parameters we could define the distribution or parameters and that was successful in France with the different compute get the posterior over your parameter from the data.  And we'll talk about posterior later in the cast when we talk about Beijing versus frequent this approaches.  Cool. So something else I wanted to say before we go to door caustic review is I got this question.  In a Mila social by a student of our class where she asked  Art is proxy graphical model really used in application nowadays because she said what she sighed natural language processing was like lstms or these fancy neural network.  And that's a good question. And what I would say is two things. So the first thing is  A lot of similar models will use graphical model, and there can be still very effective. So it's true that for a lot application nowadays graphical model has been pure graphical model has been superseded by secure networks for a lot of tests to get better performance.  That's then the comeback by combining graphical model with neural networks and then you give, the more complicated model and but that's much more advanced so in applications, usually you don't have to go there yet.  And so if you would think about just adding a toolbox and you're in the industry and you just look at standard set of the art solutions nowadays. Often you want this and see process graphical model there because they've been superseded. On the other hand,  1015 years ago, the worst state of the art and D could still often match the performance of to the neural network is just people because they all use neural networks, they don't  Care about trying these other more simpler tools which already existed in the past. So that's one thing. Actually, I want to say three. So the second thing is  On the other hand, that you want to learn to walk before running and indeed the concepts we give you in this class.  Or are much more fundamental and and and and you know, long term understanding of policies and statistics is very important.  So even though for application to give the state of the art. Sometimes you will use a neural network neural networks are much harder to understand, analyze and give guarantees for, for example, and so  If you want to, you know, train your brain to understand how things work. It's good to use this positive graphical model.  Approach. And then third, is if you like math, what you get in this class is a lot of cool math applied on interesting problems, right. So we'll see information theory will see  Graph theory will see optimization like like Ranjan all these kind of cold math concepts and then you know they're in the context of applying to model the two variables data.  And so, so this class is also a good way to learn about other cool math concepts which is just useful for analytical perspective. user avatar   Okay. user avatar   So, any question about that. user avatar   Well, I finished my coffee. user avatar   Ah, and then the same person asked again in private is processing graphical model helpful to in studying because it definitely. So if you want to go to causal  Model, in particular, something called causal graphical model. The are based on graphical model concepts first so so already understanding graphical model will be super helpful to do causality data.  But be aware, like big disclaimer graphical model themselves are not causal, we are not talking about because I didn't need only talking about correlations about statistical dependence, as you might have heard Correlation does not imply causation.  I recommend you google this sentence and get a lot of examples like you'll get stuff like like beer consumption and  Like IQ.  Variation over the term or something. And actually, there's no they're correlated, but not the city causally TV.  Okay, so let's  Review probably TVs. Where is my pen, you  Know,  Quality review.  So I won't go in, in, in  Mostly this up today. This is next.  I won't go in in super gory details on properties because you should any any we have had a class on the topic in the past and you can review it on your own to make sure you're up to speed, but I will give you the important concepts which will be useful for this class and I set up the  The notation, basically.  But first, a bit of philosophy because that's fine. And it's cool. Why are we using qualities user avatar   Okay. user avatar   So the first thing is properties. This is actually a print principled  Framework.  To  Model uncertainty.  So property theory, it's a principle framework to model uncertainty.  So has been using a lot of successful applications like in statistical mechanics, quantum physics, other places.  And  But from a computer science perspective and an AI perspective. Let me identify some sources of uncertainty.  Because they're a bit different.  And this nomenclature is not super  clear separation between different sources. It's just to give you an idea, and we're thinking about more philosophy here. So there's not like not talking about clear mathematical decision this definition.  But just to give you some example of where uncertainties coming from. You can get actually something which I would call intrinsic uncertainty.  And that means that the system the way the system works by definition is random. Can you can. It's not because we're not. I mean, let's see the other sources.  Not because we're incompletely modeling things. It's just that we cannot have access to the truth. And an example of that is in quantum mechanics.  Where the physical laws themselves are defined using properties like you have the way function. And when you make a measurement, you get a random  Instance creation of the random distribution which is characterized by the the square the amplitude of the wave function. And so this is inherent prognostic system. Yeah. Then another source of uncertainty is when you have partial information about a phenomenon.  Or you don't have more specifically, you also you don't observe all the information you need to be able to get it deterministic. Right. So an example of that would be, say, a card games.  Right. So when you have cards. You don't know what's the order of the card. If I told you in advance what was the order of the card, then you will know what you would what you would pick right  If I told you exactly how I have shuffled the card. You know what the final order. Right. So there's no randomness in the card deck there randomness and the karmic IS COMING THAT YOU shuffled them in a way that you don't tell the other person.  Or rolling and die right rolling a di di n dice dice.  To die today one days. user avatar   Think so. user avatar   Yeah. And so, rolling a dice basically  If you made very precise and actually you can use robot to roll the dice in a deterministic fashion. So, do you really know the initial conditions.  Of like you, how you through the dice. You could actually do precise physics simulation to know what the trajectory in which side profile fall on  But you don't know the initial conditions right so you don't know the initial conditions because of that you don't know which side will fall on. And so it becomes random for you. user avatar   The initial user avatar   Conditions. user avatar   Alright, thank you. Some people are giving me English lessons. So I guess it's a die to dies.  I guess Jacob Lewis who are. That sounds like an English name, I will trust this person.  Rolling a dice and  No no no today.  Really so singular is die.  Okay. All right. Well, you'll find out like that. There's a few English words I have trouble with. And I always confuse them.  Alright.  Are intrinsic and pressure information, referring to the same phenomenon as lol were terrific and epistemic uncertainty.  Good question. Um, I think, not necessarily because this systemic uncertainty. Normally, I think, is has to do with the model uncertainty.  Brendan  Can you define for me. Elliot Eric and epistemic uncertainty or I'm not gonna cry. user avatar Breandan Considine  Well, my understanding was  Was that Palio terek uncertainty refers to  Something kind of intrinsic about the system and  epistemic kind of the first or knowledge about the system. So our understanding of how it works, rather than something that's, I guess.  Intrinsic system. user avatar   Okay, well,  Then the way you define and then yes, this would be the distinction  The problem is, I also there's a third, you know, for me, like I think partial information is not the same thing as  Not modeling the system properly. So for example. So for me, that's the third category I put because that's basically what happens in AI everywhere. It's incomplete modeling incomplete.  Modeling and as I said, these distinction are not super sharp and so partial information versus incomplete modeling. Sometimes you can definitely like mix them up of a complex phenomenon. Right.  Or actually, you don't know what's the correct model. And that's why I thought perhaps like epistemic had to do with also not knowing the correct model, but it's a bit different than not having the full information. So what I mean by incomplete modeling.  Well, so a good example like you do you do physics and you model, a system where you say, oh, let's neglect the friction, right, because it's just too complicated to consider the friction  Well, then you will have some approximation. And if you had perhaps a distribution over the effect of friction, you could actually model that with a distribution as well.  Right. Another example closer to AI is let's say you have a rule like most birds. user avatar   Can Fly user avatar   Right. And, okay, well, what does it mean, my most right. Perhaps you can quantify that. But the whole point is you would like to use simple rules.  They can be advantages to use simple rule.  But then it yields uncertainty.  Right. So, for example, instead of saying, Okay, I will say that I will assume that most birds can fly. Some of the birds don't fly actually  But then you can have a distribution over birds and those fly. Those don't fly. And then when you make a statement, you can have a distribution over like whether the statement is true or not.  If you want it to characterize exactly like okay well all the birds, which fly versus don't site and you have to specify the list of birds, which fly and the fly and it gets much more complicated room.  And another example is just, I'll put in quotes AI, artificial intelligence.  Like we're talking about, like, super complicated phenomenon here about like, you know, how does the brain works. How does intelligent behavior arise. This is super complex. So we'll make  A lot of like modeling approximation and simplification to work with that and this will give uncertainty about how things actually happen. And this can be modeled using this distribution.  From a Bayesian perspective where you put this division over everything. Also parameters, then it becomes very natural, actually, by the way.  And I guess. Here's something important is that you want simple rules not also sometimes because you don't know what's the perfect model, you don't actually have a model which is perfect.  So you don't have access to it. But also, even if you had the perfect model. You cannot compute with it. It's too complicated. There's, it's intractable to do anything with the model to make a prediction.  So you will do the approximation. And so in this class in particular will will see that compositional issues are very important.  So because of computational track stability requirement will also make a lot of approximation.  And we'll get also things which behave randomly when they could have been deterministic. If we had infinite computational power.  And let me give you a concrete example of like I want to compute the marginal distribution. Well, you could actually use sampling  To compute it now. So now even like the answer that will give will be a random quantity because it depends on your simulation. When do the marginal for from something  That is well defined is a is a not a random number. It's a deterministic number, but you could use randomized computation to actually approximate  Okay, now we have it.  I don't want to be pejorative, but this is a pedantic  Note that there is still debating whether do something quantum mechanics actually belong to the first category or any other indeed that's  I agree. They're still be based on whatever its intrinsic, you know, there was also these like in my time, which has been prepped superseded we talked about hidden worlds. I think perhaps this has been disproven. But there's other possibilities, probably, that is not intrinsic  Though I think my understanding at the time was that the  The majority viewpoint, was that it was intrinsic uncertainty.  Now, cool, cool, cool come  On. Yeah. So anyway, so these three sources is just, I don't want to make it a strict  Separation, but also it's just for you to keep in mind that there are multiple types of uncertainty which can arrive. But, you know, we can all use, you know, just politest properties to model these things and it's very, it has been a very useful tool.  Does the incomplete modeling also incorporate uncertainty. Uncertainty and parameter a model which was estimated or incomplete refers to ignore some terms like you said about friction. Friction.  To yeah I think I was more the incumbent modeling, I think, was more  I mean, this is also like, you know, just trying to  Put semantic on different categories, but my reflex would say incomplete modeling is that I don't want to model the whole thing, because I don't have access to it or I it's too complicated or it's a dresser up  When you're estimating a model from data, I would put it more like a partial information because you don't like if you had infinite data and you would have the distribution which characterize of data. So you would you would know the actual distribution.  But you know, it's an important distinction. Not to ship.  So two and three are often mixed user avatar   Up. user avatar   You want to say something. user avatar Dishank Bansal  Yeah, thanks and make sense. user avatar   Okay. user avatar   Yeah, but, by the way, you can do a lot of philosophy about these things.  My, my point here just to point them out. Is that so you keep them in mind. So in particular, often you will use properties properties to model something which is fully deterministic.  Is just that you don't have access to all the information to make the mistake or you will make approximation to make a difference in the two to because it's too complicated. And it can be very useful.  So let's talk about property.  And by the way, so probably tease.  Like  The model that people had is basically like  Areas.  Like so you could think of. I have a 2D  Plane, and let's say I have a square  Of unit area.  And you could assign properties to different event. And let's say, now I'm talking about the uniform distribution, then you know if I have a uniform distribution.  On the square, then the quality of falling in this sub region is just proportional to its area compared to the unit square right  And so for example if I asked was, the quality of falling  In either of these two squares, I would add them, I would add the two areas together. Right. And so the laws that will see very soon like  A protein submission satisfy the coma grub exams and the Komodo of axioms. Basically the mimic how area or volumes for that matter behave like so i. So, if I have distinct area and wanted to have the total area of this. I just some the area together right  An area of the positive number. So we'll, we'll see later.  These axioms. But intuitively kind of makes sense right sense. And that's this very simple model of properties that people have used it, which has been extremely successful in a lot of different applications.  Yeah, so let's do a bit of notation.  Usually in this class I will use capital letters to denote random variables.  So these will be random variables.  And or, you know, the might have indices, or I could use different letter. So that's actually a different letter. This is a y or z, for example.  And  Usually  These random variables or real valued.  And that will use lower case for their instance stations, but little x one little x two x three or four. These will be little x little y and little  Okay, so these will be there realizations.  Okay. And so what is a realization and what is a random variable. I'm, I'm using the the easy machine learning definition, rather than the standard  Measure theory definition but like if you take that like in a different book actually like it because it's sufficient for a purpose. So a random variable is just an uncertain quantity  A random variable.  Is it represents  And uncertain quantity  So, for example,  X could be  Resolved result.  Of a die. Thanks, in the chat. I can look back a die roll.  Okay, so  So acts as a capital letter is the random the uncertain output of a die roll and then the little value, the little x, which are the possible values that this random quantity can take. So for example, if I say capital X little x, this represents  The event, which is basically a possible  Set of possibilities.  In terms of result.  That x takes capital X ray takes the value little  Yeah, and and the space of possible values for the little x for the realizations would call the sample space. And often, I will use the  The letter capital omega for the sample space. So this is the sample space, which are the possible values of the random variable. And actually, it's called of elementary events.  Because formerly an event is actually a set of possibilities.  And the elements or events or the singleton which tells you what are the this the simple  Thing which can happen. And in this case are basically the possible values for my random variable. And by the way, a bit of notation. So, you know, this afternoon. I'll use purple for  Things to define for notation I use read for different names for the name of the definition and green for the definition. So this is possible.  Values.  Of my  Random variable. user avatar   Okay. user avatar   And so in the case of x being a die throw a rock die roll then omega here will be 123456 if it's a six sided die.  Yeah, it's equivalent to outcome space and academic events. Sure.  Yeah, and  There are two types of random variable that would consider this class.  Depending on what's the cardinality of the  sample space. So either they're discrete  And this means that there are simple space is comfortable  Are they are continuous  Actually, we'll see you later. We can have hybrid of the two. And that's a bit more complicated. When we talk about vectors, but it's focused now on just these simple types continuous  Where  Omega is uncomfortable.  Okay so comfortable standard math term means that you can put in by ejection with the natural numbers.  So you can count them. So the, the National numbers are comfortable. They're irrational number. All the fraction natural or unnatural is also comfortable, but the real number is not comfortable uncomfortable.  And so when we talked about either a finite possible sets. That's definitely comfortable or an infinite but comfortable set like the natural numbers, then we talked about the discrete random variable.  When we talk about say like measuring the temperature or measuring the height of someone in these all like real numbers with an internet possible  Precision then these are an uncomfortable set. Okay. And the part I won't go into this class, but in order to formally work with uncomfortable sets, you need to use measure theory that you need to work.  With stuff, which is a bit more complicated. In particular, so an event is a set of elements of things that I want to see was the quality of this event.  When the set is comfortable all possible subsets of your sample space can be events. But when the set is uncomfortable. You cannot define meaningful for the distribution of all subsets of the real life. For example, there's too many sets and you get into weird stuff like  Contradiction. You cannot satisfy the Coburg of exams over all the subsets of the real life. There's just too many sets you get weird stuff happening. That's where you need to find something called a sigma field.  And we'll get back to the important aspect of these things. But right now, we won't go into major theory, but be aware that it exists. It's used to really formalized rigorously distribution over uncomfortable objects and  Also something else that I have not done is the standard definition of a random variable. If you take a math class is it's a mapping between  The Omega space to the real number gets a random variable is a mapping and the good thing is you can have, you know, multiple random variables which are all different mapping, but from the same  Space and the distribution is only defined on the the Omega space where omega space and this induces a distribution on the values of the random variable.  But here we just skip this kind of like  Original space where everything is defined, because it's just complicate things a bit more and make things a bit more formal. We can just think of. I have a you know a random event.  And I have possible values. And then I have another one which is sorry different random variable, which are different values and and that can define the joint on them, depending on how I want to define them in. So we'll get back very soon. When we talk about during distribution. Okay.  So let's talk. Actually, let's make this more concrete for the discrete case and then  Let me know if you have any questions. user avatar   So, user avatar   Let's talk about the discrete random variable.  To do.  So now let's assume  That omega is comfortable  And thus we are in the discrete case.  And could Omega could be finite. Right. It could be like the role of a day.  And so far, discrete random variable.  X.  It's characterized by a probability mass function characterized  By a probability mass function A PMS.  Probability. user avatar   Mass. user avatar   Function. user avatar   Which will call in this class PMS for the abbreviation, and what the EMF does, it gives you the quality of every possible values of my rent environment even Sri event right so this is lowercase.  P  P have little x  Four x in my space. So this is my PMS.  Okay, and the property of my PMS, my little p will be that my PMS, whether it's after property. It's a P such that  It's positive  By means non negative sorry for all values.  And when I some overall my possible values in my simple space. I get one. user avatar   Right. user avatar   And so if you want to represent it graphically.  Let's say I have my the integer as my sample space.  I represent my PMS will give you, you get these like  Spike at every value is because there's no priority assigned to any non integer and, you know, these will have to some to one, so not sure if there's some to one, but make sure it's positive, and smiling one me something.  Okay, so that's so basically any PMS is uniquely determined by by sorry every discrete random variable is uniquely the domain by its piano  Which basically describe what it how it behaves and a property distribution, which is a bit different. And the probability mass function. So are probably the distribution  And I'll use capital P for the Property Distribution.  So I use big P  It's a mapping  From  The set of events which I'll use script. He to 01  Which satisfied the corner Grove accepts  The program exams, which I told you was basically an exam which mimics the properties of area.  So what are the Camorra exams. It says that the quality of any event. So, this the script. He is the set of all events. So the property of a specific event. He is non negative for all the events in my event. user avatar   Like this, I use user avatar   This currently user avatar   The property of my whole sample space, which is also an event is equal to one.  And the probability of the union which is comfortable. So this is a notation for a comfortable Union from equals one to infinity of the AI.  Is the summation of the polity  Of these individual event when you're destroyed.  I or user avatar   District user avatar   IE, when you take the intersection of any pairs. There's nothing in common.  So these are the Camargue of exams.  And this is basically what  Are the properties that we want to proceed to satisfy. And so the idea is, if I have a comfortable number of pieces which are all this joint  So you can think of areas. I could do the same thing. And by the way, measure a theory is like this p  Would also be called a measure and measure theory. So measure theory is just a formalization of these kind of axioms for very complicated set which can be uncomfortable, which can be very crazy  Okay, so in the case of a discrete random variable. I said the sample space is comfortable  And I said that the set of events can just be all subsets of my sample space. So the notation for that in math is to raise to the omega  Okay, this is just by definition the power set or the set of all subsets. user avatar   Of user avatar   Omega. Right. And so this is  So this currently he. This is a set of events.  And so what a protein distribution is it's a it's a mapping between events to to property values and it tells and what's the meaning of this is that, well, here's an event was opposed to this event. Well, my policy will tell me what it is.  And as I said earlier, is that this set of all events in measure theory is called a sigma field.  And it's needed.  When omega is uncomfortable.  So when omega is uncomfortable. You cannot take as your set of events all the subsets of omega, because you get too many sets. There's no way you can define actually that's a theorem. You cannot define any  Property Distribution or even the measure overall subset of the real line.  It's you get contradictions, you won't be able to set aside the camera zooms you can construct sets which are mutually this joint, but for which, when you take the measure of the two, you wouldn't get the some of the individual.  But when the set is comfortable, then you can use all the subsets. It's fine. You can meaningfully define in particular it's it's clear here from the PMs because  Here on allowing comfortable union and so any subset of the comfortable set is a comfortable sets. So then I can just use the eliminator element and just do a comfortable. Some which is totally fine from the PMs  Whereas when you have an uncomfortable set. If I have AI, which certainly if I have a set, which is a union of an uncomfortable. It's an uncountable union of of elements, then I can just split it in the lemon tree event. And then things will  Okay, so somebody asked about if this thing could be uncomfortable. Yes. Because if this is comfortable the power set is actually uncomfortable.  But this is fun. So, if omega is uncomfortable than the power set is even more uncomfortable. It's different. Cardinal and that's way too big. But the power set of the omega is still fine you can define the distribution, brother.  Alright, so, so that's what the distribution is and the notation is so so  The notation is that  When we talk about capital P and we talked about an event we use curly braces, because an event is a set of things.  And so I could say, for example, the quality of capital it x equal little x, that's the same thing as the quality of having the single Jen observation for capital X, the little x  Another set could be well capital X is equal x or capital X is equal to little perhaps on the x one. Now that's just used two different bags.  X or X prime. Okay, so what's the problem of having one or two different one of the two different values.  Well,  Because this can be written as the union of two single done which are just joined by the morgue have exams. This is the same thing as  And this is all we, by the way. So this is the same thing as p of little x the PMs right  Little p of x. And so this by the Camargue of exams is the probability of capital x equal to x plus the quality of capital X equals X prime which is just the PMs at x plus the PMs at expect  If x is not equal to x, because if it's the same, then, that wouldn't work because then they said x or x one, x equal x prime is the same thing. It's just a single to mix.  So here the event was the set X explain  And this thing here is the PF  Okay. And so for a discrete random variable. More generally,  We have that the probability of an event is just a summation over the element of my event p of x, because, because an event is a subset of the omega space and omega is comfortable, then any subset is comfortable. So then this is our comfortable summit, everything is  Okay, so let's talk about a continuous random variable because that will distinguish it from the discrete  Not going very fast today.  Open bedroom, please. Connect charger.  Alright, so let's look at the continuous random variable.  And so  Let's see. Omega space is the real line so I can  It's not cold. It's called  A continuous  Random variable instead of having a PMS, a party mass function. It's characterized by property density function.  Is characterized  By a  Probability density function.  PDF  And now we talked about integral. And I'll still use little p for the PDF  So this is a PDF  And what's the property of P. Well, it's a function from the sample space of the real number, like the PMs, but it's an integral function instead of being just discreet something so I have that p of little x for every element in my sample space is non negative and P is integral  And the integral over all my simple space of PR X the X is equal to  Okay, so instead of something. Now we can have some because we need to make uncomfortable, son. Well, then we need to use integral, right. So that's kind of the generalization.  And before we had our PMS were like the spikes because it wasn't discrete in this case. Now we have kind of like function which is continuous, an integral  part doesn't have to be continuous, but it isn't agreeable and then to find out what's the probability of being an interval, I will just integrate the density between the two values. Okay, so the property of an event.  How do you compute that. Well, in the case that I use the real numbers than the quality of an interval that's a set of real numbers is just the integral of a to b of P of X the X.  Okay, and  And how you get the density. Well, you can use the basically the the limits.  I mean first formally, you could use a random liquid him they removed, but basically you can think of the property density. The PDF as the limit  When you make of the, the probability of a ball.  Which includes x divided by the volume of this ball.  Where were be  Or balls.  In closing,  X.  And decreasing and rages  So you have, I have my ex. I have some radius here and this is my ball.  And as an increases, I get the ball which is narrower and narrower around x and in the limit. It's basically a range of zero. And when I looked at the rate, the ratio of. What's the quality of my  Ball over the volume of the ball, then I get the infinite simile basically density of my distribution. And that's why, to find the property of an object. I just integrate the density  Okay, so let me just  Make one recap. And then I have to stop because it's already over time.  But so basically the recap is  That if I have a discrete random variable.  X. What I need to talk about as a PM F p of x. And I can talk about event which are singleton  And these are just given my opinion if if I have a continuous random variable.  The  Quality of an event is given by an integral. Right. So I have a PDF, which is a density function. It's not something that you will submit something you will integrate  And then if I looked at an event. Well, because the integral over a single tenant is just zero because it has zero area.  So this is always zero. So that's a big problem right so every elementary event in this case a zero polity  But the thing is, I will summon uncomfortable number of them when I have a real interval, which is why I can go from zero to nine zero  But that's also why I cannot just use the quality of something, because if if it's go back. If this property here was true using uncomfortable unions and uncomfortable. Some then I would never get something different and zero for conscious random variable. So that's where the magic happens.  And so one interpretation is that when you think about p of x being in the infinity symbol interval around  X. So that's, that's why I use like the notation dx over to to convey this is roughly p of x times the right  It's not true. It's only true in the limit when the x goes to zero.  But you know as an approximation. I can think of at supposing that my quality density is constant in a very small liberal arts x and then the property of this being in this neighborhood is just p of x times the size of the neighborhood right but more formally, you have that  x in the interval plus or minus eight over to is just given by the integral. Right, so I will have x minus two x plus 2pm X the X.  And the last point is that  From measure theory or  So, by the way, this integral is deliberate measure not being integral. It's not the Riemann integral, but we won't go into this distinction  So the important side note is that four continents random variable, you can change.  The PDF  On the comfortable number of points.  without changing anything  So the property density function of a random variable is not uniquely define everywhere, right. So it's only a defined up to a set of measure zero which is actually a comfortable set basically without changing user avatar   Anything. user avatar   And the reason is because what's really important is the is not a PDF. What's important is the quality distribution, which is a quality over events. Oh yeah, and the events here by the way. So the events here.  Is the boreal sigma field.  For the real numbers, the set of events for which you can assign  For which you can assign probabilities or well behaved sets. It's not all subsets of the real line, but there will be a sets and it's called the boil sigma field.  And then alright so what I said is a kitchen to PDF uncountable number of points, without changing anything. And that's because when you integrate changing only a comfortable number of values doesn't change the integral. And this has basically, this has measure zero  According to the bag measure  If you don't know about the bag measure. It's not important. You can think of the next year.  This is just the formal definition of this integral.  Somebody commented something  Oh, lots of comments we equation fix me  Comfortable said being finished to such as I said of natural number. Sure, yeah.  Any other question.  So here the, you know, like the recap is kind of the important takeaway will for the discouragement unbearable in this class will talk about PMS.  PMS can be seen as less tables, right, what's that for any possible values of our random variable will give. What's the qualities  For continuous random variable, we cannot just give for every possible value or polarity. What we give is a density. And then how we get the quality of have a set of numbers is by integrating user avatar   And the other question. user avatar   I see simile. user avatar   Limited. Yep. So is there any reading you'd recommend for some of us who wants to just get an introduction to measure teary like I come from just the background of the stats and I understand like the continuous distributions and so on. But I'm intrigued to know more about the magic stuff. user avatar   Okay, I'll, I'll send them some in the slack. user avatar   Okay, thank you very much. user avatar   Anybody else user avatar   Brendan user avatar Breandan Considine  I yes I was curious if you just want to draw a sample from  A  Graph graph graph model.  Do you have to do all the integration or summation in the discrete case or can you  Maybe just evaluate the integral symbolically and then plugins and numbers. user avatar   So,  If you have Derek graphical model.  So there's already a difference between underwritten dark. See, you will do directed will see during the center's they're sampling  Lecture that you can do something called ancestral sampling, which is that you start at the root of the, the, you start with the nodes which have no parents  Then they will have a marginal distribution, you will sample their value from their distribution then given their value will simple their children. And then you have this condition.  So the question is, can you sample from an arbitrarily continuous distribution and this is non trivial. So we'll see when we talk about sampling that  We will assume that we have access to a recall which is gives you a uniform  Sample from the uniform distinction between zero and one which is already non trivial because this used pseudo random number generator unless you have access to physical system, but then you have pseudo random number generator  And then we will transform this to using the inverse ETFs trick, for example, to get a sample from your distribution.  But if you cannot compute the inverse ETF easily. You won't be able to do that analysis. So, so you already have like four number two distribution. It's non trivial to sample from it. So you will already need some kind of numerical techniques to orient just sample from an arbitrary institution. user avatar Breandan Considine  Will see nicely.  Thanks. user avatar   Oh, so we have some physics question here. I'm not having good units.  Probably tease or unit less. And then the volume as some kind of unit, I will have to think about that.  I think the, the answer is A, the units don't matter. The reason being that when you define your sample space.  You're telling already. What are the units which you care so so to be to be more specific, like if I have a distribution in 2D.  The density will be respected, a big measure into the  So if my set is not two dimensional like a line, it will have zero quality. So it will be zero, measure. So everything which has a meaningful. Probably it will be a two dimensional object anyway.  So there's always will be a dimensionality match between your the thing you say policy on the top and the thing which is the volume of the bar. They will both the both on  Two dimensional object or three dimensional object or one dimensional object. So there's no problem of like dimensionality mismatch in this case.  And so basically the volume of bn is also called a measure. So, and the measure is dimensional mess in in politics here, but that's because like I said, everything works in this case that like there's never a mismatch of dimensions.  Yeah, so like been new law says that p of the end is also volume. So p of the n is is like a weighted volume, right. It's an integrator volume integral with a density function which is uniform that also gives you something which, as  A bit the volume, but it's not a, not a uniform volume. So it's the same nature. So the both the numerator and denominator of distinct here. Oops. It's not working.  Both  These  Interested oh my pen is dead. Haha. Can my finger work anyway. So these thing here. Guess my mouse works. Yeah, p of bn as the same nature or dimensional UT are then volume of being so there's no problem.  But I like I like I like the physics intuition here, by the way, because, indeed, sometimes in protein. It's weird because we don't add the multi dimensional et or dimensional value to our quantities and everything is dimensional Ness and it's kind of weird.  And sometimes we will have weird models and practical  Okay. Any last question, I went way over time. Sorry for that. But if people had to leave, they can watch the recording for the next 12 minutes I'd say speaking times to they want  No question.  big thumbs up. Alright, so  Oh, if I can explain what a broad sigma field is right. So for the curious people a Burl sigma field is basically a well behaved set of sets on for which you can  Define a property measure again. And it's basically the way you work is we start with meaningful setting in the case these are balls and start with all balls like all balls.  Of all possible radius. These are assets, then what you can do is you can take union of bots.  You can take comfortable union of bots and comfortable union of dolls is permitted to find out what's the priority of that because it's, you can use the camera bag sense  And then you can use all possible comfortable using the balls and then you get basically the crossing my field, but you're not allowed to take uncomfortable union of boss. That's a set, which could be too complicated. It's not the city in your signature.  But you can think of the bra single field as generated from walls countable unit balls and a well behaved set of sets.  Cool. Alright. So. Have a nice weekend and I'll see you on Tuesday.
  Alright so recording started. So, today we will continue the review.  And then start to look at the parametric models today. Oh, I want zoom  And effect percent  Today,  We will finish the project to review.  And start to talk about  plastic model with parameters. So parametric models.  Okay, so, and  So perhaps just something quickly I mentioned last class. If I go back to last class I finished with the continuous random variables. Yeah, so the recap was that's a very important recap so that  When we have a discrete random variable. So the sample space will have the user avatar   Other questions over there. user avatar   For those who are auditing will get solutions to the assignments. Yes, for the people in the email list and it should be late tech format so  It's not required. So wholesale indications where you want to have that your solution is legible very legible.  And so if you can write very teeny with no ambiguity, then totally fine to just like take pictures as, as long as they are good quality pictures.  Otherwise, if you're somebody who writes, like a doctor, I can medical doctor, then it's better to do it in Natick, and by the way, doing also late tech is good practice because when you write papers, but you don't have to do it, then they take if you can, but you have to do it legible.  Alright, so  That was the questions which I had to notice. So let's go back to what I was saying. Yeah. So if I have a discrete random variable, the sample space will have  Either most comfortable number of possibilities. So it could be finite or it could be like the natural number or something like that.  And in this case, the distribution is characterized by a probability mass function which tell me what's the probability of each possible value right we call just will use the digital PR expert.  And so, as that before, as I told you the probability distribution is a is a function on sets right set of possibilities, right. So I use annotation here curly braces.  But if you just use a single 10 as reset. That's called an elementary event. And so then you get the PMs  Okay, but if if you would have multiple events. You could use the the love qualities to add  The individual priority of this joint possibility. So that's what I had mentioned earlier here, right. So if I have two possibilities. I could just have say x and x prime and I could just add the PMs of each individual  Elementary facilities. Okay, so, but for PMS for discrete everything is simple. And by the way, for simplifying our presentations, often in this class I will be assuming discrete  Distribution to avoid a lot of like the complications, though, I will explain how to generalize a lot of the results to continuous distribution.  But let's keep this nine. Now the tricky cases when you have a random variable, where the number of  Possible values is uncomfortable. Like if I measure the temperature I measure the height. This is a real number.  So there's an uncomfortable number of values and now you cannot just talk about a PDF. So instead what we use is we'll talk about the property density function.  A quality density function is always defined with respect to in measure theory to what it's called a base measure in this class. You can think of it as  This is just the big measure on the real life for a single simple continuous run a variable in one dimension or if it's in multiple dimension, it will be with respect to do the big picture in dimension D, but we won't go to more crazy stuff.  And so then  The, the, the meaning is I get the property of events of set of points by integrating the density over this, right. So, for example, the priority of being in an interval  Around X of length A over to is the integral from x minus eight OVER TO two x plus over to have the density, the x, right. And so in one dimension, this is a simple integral when we haven't built to various  Random variable. We could also have a density and and these will be these these integral would become volume integral, but we'll see that today when we talk about the distribution  Okay, and the side note, I mentioned at the end of the last class, which is very important is that the density function by itself does not at any point  Single point does not mean that much because everything is the final me using integrals. And so if I change a function at only one point, the integral doesn't change.  So that's the one of the mystery of  You know when you go to uncomfortable number of things. And so you can always change the density function at a finite number of points or a comfortable number of points and this one change the distribution  And so this terminologies call it's it's defined uniquely almost everywhere, almost everywhere mean everywhere except on a set of measures zero IE accountable number of points, for example.  Okay, but that's just a subtlety about continuous variable. So today, let's now talk about joint distribution. So the first thing is, let's talk about  The joint. The marginal etc. So what are we talking when we were talking about these objects. So this is in the context of a multivariate random variable routine. Very it  Or random vector. So I guess the V here was more like a victor. But there's a bit of ambiguity. When you talk about the random variable, the traditional definition of random variable is that the the outcome are real numbers. So it's a one dimensional thing.  But then, more generally, you can have that the outcome can be a vector. It could be a function. It could be any object you want. So that's a bit a generalized version of random variable.  More classically when you have a vector, you will talk about a random vector, rather than a random variable, but be mindful that people can use still random variable, even if it's not a escape.  So let's start. Let's say, for example, Z or random vector is formed of like two components x and y. user avatar   So, user avatar   And so tonight I have. It's a two dimensional random vector, and we could use as our sample space in this context.  For this random variable Z just the product of the sample space of our individual random variable x and y.  Okay, so every pair of values where I pick one and x in the sample space of x and one in the simple simple space of why is possible for for this sample space of z.  And so if I am in the discrete case.  Then we will talk about a PMS for this random variable.  So the PMs of z. So, in this case, this because it's a random vector we can call it about. We can call this probably the mass function, the joint PMS to be called a joint  Is joining multiple components. It's joined PMS.  And we can see on see on x and y.  So it's a reference on which are the components.  And in this case, so we can still use P will use P little p everywhere in this class so little p. Now we have to work too.  Far any possible outcome we need to specify the two components. So the joint will be  I need to tell what's expert. Why so the two key of x, y. That's the joint PMS evaluated as positive as possible outcome.  And in this case, it's just the probability that capital X is little x and capital Y is equal to little life okay and this big P here is the joint distribution by joined because it's a distribution on multiple variables. user avatar   I'm having issues here. Whoops. user avatar   Okay, yes, this is the joint distribution. user avatar   And there was a question, but incur can we see random vector can have made of multiple random variables. Yes, definitely. So that's what I've just done here.  The random vector z can be seen as the concatenation of the random variable X and the random verbal way.  Okay.  And so why don't we have discrete random variable, the number of possibilities, or a discrete  And we could represent and for a joint one, we could represent as a multi dimensional table. And in the case of two is just to the table. Right, so I could represent the elementary events.  Of z or the computation of x and y as a table.  And so, for example, I would have, what are the values of x.  And what are the values of why right  And so this entry of the table here would be x equals zero, n y equals zero.  And so now I could ride the PMs  All the entries of my PMS for each of these possible values in this table.  And to make a concrete example say that x is encoding whether oops I'm having issues with my pen whether a di result.  Is even  So 246 and let's say why is encoding whether the same die.  Is user avatar   OK. user avatar   So I roll the die and then I have one random variable, which says is this die result even another another random variable is saying the same diet. It's not a different diets, the same day. Is it  Right. So in some sense, these two random variable are definitely correlated, they're related to each other because they're talking about the same thing.  And so in this encoding the probability that  X equals zero to x equal one mean that the die is even and y equals one means the die is odd. And so if one is true. The other one is, is if one is one of the other one is zero. Okay, so basically  The probability of both being zero both being one is zero. So, this is the probability the PMs that each has value one, right.  And the property that one is zero and the other one is one is a policy that the die result is even. So it's one half. So this is one half. This is one  And so here is the joint PMS for this situation, you have to check that each the some of the entries is equal to one, because we need, we know that the sum of all possibilities has to be equal to one, too, because we have this correct distribution.  Any question about this.  So,  Now, what about the continuous case. So in the continuous case.  We don't have a PD PMS anymore. We have a PDF. Okay, so. Suppose that X and Y or continuous. And this, by the way, means n right  Is a shorthand for it. And I like to use this little  Curly whatever it's called, and  So in this case, the probability will be determine for for an event is determined by an integral right so let's say the probability of  Some bucks region.  Will be updated by integrating over the box. The pdf  And then we do in the case of to variable, it's a it's an area integral, the sky. And so this key here is the joint pdf  The giant quality didn't security functions.  Which could be updating the same way as I mentioned before, you could take the, the original priority.  Over balls or boxes, doesn't really matter. And then  Compute the ratio around the point x of these these these balls.  At the property of a ball give divided by the volume of the ball. In this case it's the area as the ball gets smaller and smaller. So that gives you the, the density  Okay, so now  What about the march. So these are the joint. So that's the joint.  Alright, so somebody asked a question, why is p one zero, not one as if x equal one p one  OK, so the person understood. So that's fine. Actually, now we'll talk about the marginal so because right now. These were to join. Right. So I was only talking about  Both assigning the value of x and the, the value of white together the marginal distribution is when I only look at components of my vector I don't look at the joint vector together. Right. So the marginal distribution.  The marginal only makes us the wrong.  Server. So the marginal only makes sense in the context of a joint distribution.  Mean, because if I only have one variable, normally we won't talk about marginal, we'll just talk about  The variable itself. And so this is always in the context of a joint  So this is basically the distribution  Of a  Component of the victim.  Of a random Victor  And so, in the context of a discrete random variable. For example, if I want to know what's the marginal  Distribution for  x being equal to us specifically view little x. Well, I just need to some overall possibilities of my other random variable, which is why after  X equal little x y equal to y.  So this is just a PMS. So this is the two p of x, y.  And this is called the some rule.  So you can obtain back the marginal by summing over all the possibilities of the other variables in the joint.  And this is what we mean when we talked about marginalizing out. Right. Well, we use the terminology marginalizing we need computing the marginal by sending out the values of the other random variables which make out the the joint. So in this case, you could you could see that you're marginalizing  Out.  Why the random variable. Why, because you're, you're not looking at you don't care. When you look at the marginal for x, you don't care about what's happening to why  Okay, so that's the marginal. We have the joint. We have the marginal. We don't have to be the marginal you basically some over all the possible values of the other components of the vector  So now let's talk about independence and feel free to interrupt at any point if you have questions, but given that this is all review Alice I'll goes relatively fast.  Now this talk about independence.  I having issues with my pen. user avatar   Dependence user avatar   Oh, I mean, I'm not connected. That's why. user avatar   We plug in my laptop. user avatar   Make sure and user avatar   maximum performance. user avatar   Okay. GOOD COPS THAT WILL BE BETTER. user avatar   Okay, independence of random variable.  We will say,  That x  Is independent the random variable X is independent of the random variable, why  If and only if  In the case of continuous or discrete that the the PMs or the joint that's already the joint PMS, or the joint PDF is factor rising as the product of the marginal  For all x, y in the user avatar   Simple sample space. user avatar   So for discrete random variable. This is fine because the emf is sufficient for continuous. It's a bit tricky because I could make this false for a bunch of points which are comfortable and it still works fine.  But let's not worry about these technical issues for companies so  Basically just if if the PDF factor is then they're independent, it could be independent, even if the PDF doesn't really factor is nicely, the right way. But we won't go into these technical details for the continuous case.  And the annotation in this case means that  We will use this notation that x is independent of why. So it's basically this upside down pie. That's the independence location.  And so the nice thing here is that  You can see here that the  In the pen dent. Sorry, I don't know how to write  And the rough meaning of when to random variable or independent means that  Knowing the value of one doesn't change anything for the for the other random variable, right.  Because  And we'll see, actually, when we do conditioning. So if I condition on one it doesn't change the the the posterior distribution on the other one. And so in some sense, I could think of them as, indeed, as  Two different things which have nothing to do with each other.  Alright, so let's be forgetting to conditioning. Let's just mentioned something important that if I have a lot of random variables.  X one up to x enter  We can talk about mutual independence which is stronger than just pairwise independence would say they are mutually independent  If and only if  The joint on all these variable.  Factor is as full product.  Of the margins.  And this here is the product. This is the direct product of sets. That's notation for direct product of sets here. Dear do simple space.  Somebody asked a question, is there a geometric interpretation of independence.  So yes, like if you would look at an in a plane, you would have things that are basically like the the  You would have stuff like more like rectangular like where where knowing the value of one should show an influence, so it's it's constant across lines for  Proper. Like if I change the value of one variable. Nothing changed for the other one. So you would have these kind of like rectangular style densities.  But I would need to think more carefully him to give you a more complete answer and then somebody does Mitchell independence also hold for conditions this mutual  Independence also hold for conditional independence.  Well, first of all, I haven't talked about conditional dependence yet. So let's get back to, let's get back to your question, one child will have defined conditional independence.  What's the difference between mature enough neutral. So the difference is more that we will see an example later on in the course, today.  Of  An example where you could have three random variables which are pairwise independent. So I could have for every pair, if I look at them by themselves. I don't care about the other value. I look at the at the marginal of those to  The are independent, but they're not mutually independent because if I look at the three together, then there's some dependence that happens. So that's the difference.  Somebody is asking about my notation. So why is  Oops. Okay, I want this.  So this is the product. And this is a product. So why am I using two different occasions, because this is actually a product of scanners.  Whereas, this is a product of sets. So it's a very different. I mean, it's both our product, but it's not the same kind of objects. Right. So there's actually a direct product of sets.  What is the best way of finding the joint distribution between a continuous and a random variable.  I do not understand the phrasing of the question.  please rephrase  Joins division between a continuous variable and the discrete random variable.  Hmm.  Well so so already, that's when you start to get in tricky situations because the first. First of all, when you talk about their countries random variable, you need to talk about their  PDF, the density function. And now if I have one component which is discrete in one component which is continuous. I won't be able to talk about the density function respect to the bag measure dimensions.  Okay, so it gets a bit trickier but you know okay so So somebody's asking about, let's see, I have  So let's say z.  Okay. Is x, y, where this is continuous and this is discrete, what do I do with that right well. So in this case, you could talk for example, about the probability that  Whoops. So an example of things you can talk about would be the probability that  X belongs to a box.  And y equals to some value aid for example. Okay, so actually I could put little by  So this is a meaningful this this will be kind of the, the sets for which you can assign value which one just be zero because if for X, capital X any specific value will have zero quality. Right, so I need  To have an extent to have non zero qualities. And in this case, you will both have a PDF and you will have  A PMS part. So, this will basically be the integral for x in the box of the joint of little x for at fixed value why  The next day, so basically this is kind of like a PMS part  Whereas this one is the PDF  So when you have hybrids like this. There's one component which behave like a PMS, ie, if I if I want to know the protein for multiple why I will just some over them. Right. It's a discreet some was the other part, the PMs the PDF part, I will need to integrate user avatar   So that, that's still feasible. user avatar   And in terms of terminology like somebody is asking. Oh, do you call it the PDF. PDF  Yeah.  Good question.  So I  I would call it like a hybrid PDF PMS, for example, that's an example. So when you do measure theory, this would still be called a PDF party density function where the base measure would be a not  A simple measure it will be a cross between the bag measure and the contact measure. So it would be a bag of X and counting on one so you can define these kind of like fancy based measure which are not just the bag or discreet and this enables you to easily handle these hybrid object.  Okay, so hopefully this clarifies that and I was talking about conditioning because somebody asked about mutual conditional independence and we need first to talk about conditioning and that's super important. That's one of the most important  Construction, we will our objects will talk in this class.  Because we talk about conditional independence everywhere so conditioning.  And the idea of conditioning is I am kind of like computing a new distribution, knowing that something already happened.  And so  First, let's talk about events and let's talk about random variable, okay. So suppose I have events A and B.  And so these are a set of outcomes. That's an event is a set of outcomes. Right. So it's a set of elements of mice and sample space.  And we will suppose that the quality of one event is non zero so that something can happen something in this event can happen.  Then the definition of the conditioning the conditional on the event a given even be so the vertical bar means given by definition will be the joint of A and B, or at the intersection when it says read normalize by the margin calls by the quality of user avatar   So this is basically a normalized user avatar   And so  Basically what's happening here is suppose that we have this is our sample space all the possible values right and then suppose that I have a set of possibilities which is B.  And now I have another set of possibilities which is a okay  And so, and suppose, in this case, for example, we had uniform distribution to simplify so that everything is proportional to the area I put on this.  On this to the plane here. And so the the the intersection of A and B is this little piece of area here.  Okay, and  Then I want to know what's the probability of being in a when i know that i mean be well then it will be basically proportional to this area but re normalized by the total area be right. And so that's the reality of being in a once i knew i mean  Shame shouts. So what is the equal sign with a triangle. That means definition. So it's a terminology to use. It's a  Notation to indicate definition.  And so  Basically  By the you have by the some rule that P B is equal to summation over all possible sets a  So we have that P of B is equal to the some over all set. A in the partition of my sample space.  Of the joint of a and right  And so that's why I call POV. The normal Iser is if you think of the conditional as a new distribution where I'm allowed to change what I will put as my first argument like this argument here could be different sets.  Well, I will want that this conditional is proportional to the joint of A and B.  And then I need to  Make sure that this is a distribution that need to be normalized by dividing by the sum of all these properties, which is this thing here, which is user avatar   OK. user avatar   OK, so somebody we. There's a lot of questions greener so mean definition in these notes so green is a convention I will use to to talk about definition, indeed.  In words. But usually I will use the equal with a triangle to means define when I when I use equations.  Is a cap be the same as a comma be  High enough so  So a  Intersection be means  Because he was a set of lemons and he was a set of elements. So when I think the intersection. I'm saying, Well, the thing which happened was, both in the end. Right.  Which also means that I am in a and I am in be right. So I could say that.  Something like  The simple  belongs to A and the sample belongs to be  That's why the comma here could be when when I have like x equals little x and y equals it away. That basically means that  The event.  That of what's happening is in the intersection of those two events of the event that exit was equal to little x in the event of white quote was equal to little more. So it's, that's why they're come on the intersection are very similar notation.  Alright. Finally, I'm a bit confused between the ratio between property functions. For example, a gas ocean and the area of planes, what would it look like in terms of area.  Okay, so when I'm talking about an area.  It's just, both because  If we have a uniform distribution, then this will be exactly how the probability distribution will behave when we have a uniform distribution.  And also because the axioms that proteins satisfy are similar to how area work I E. If I split into this joint event.  The area of those of the whole thing is the sum of the area of individual thing which is the same thing that the priority of a whole thing happening is some of the policy of each element thing.  But if it's not even from the submission. This will have nothing to do with the area right so a Gaussian, you could think of it that scene one d  Um, let's see, I have  Are  All right, and then I will have my PDF. This is my PDF  And I guess I'll put a zero here. And so this is whoops.  Is a Gaussian and well the nice bell shaped version of what I drew  And so  Here you can see that I don't have. If I would look at the length so area and when these length. So the length of an interval  Is not proportional to its, its property right because it's not a uniform distribution, right, in this case, though, when you add the PDF as a new access, you do have that the probability of being an interval is an area that's the area under the curve right  So this let's say this is a a b. And so this here would be the the area here under the curve which is the integral of px, the X from A to B. This is the probability of being  Of x being between AMD right  Okay, so that's so yeah so so you have to distinguish the area under the curve versus just area on on the rectangle, like I've done  Which would be appropriate for a uniform distribution like the bag measure like the bag measure is just saying that everything that the probability of finding  Somewhere is just proportional to the area of the space and then you need to read normalize to get it protein distribution, which is a new phone.  Okay, hopefully this answers the question.  So now I decided to define how to condition on sets. Now let's talk about random variable because I'm just talking about sets right now. So do now for this treatment and variable.  We will define a conditional PMS. Right. So when we talk about this variable. We talked about PMS. Now we want to talk about the conditional distribution of a random variable, so will actually use something called conditional PMS.  On the show no  PMS.  Which would be no by the two p of x given why  And by definition is a simple. It's the same thing as a event thing because that's the magic of adding  Discrete outcome. This is the event that capital X is equal to little x. That's a singleton elementary event, given that capital Y is equal to the right.  And so by the division of conditional priority. This is the intersection of the event, divided by the marginal right so this is the probability that  X equals little x and y equals y  Divided by the marginal of y equals little I  Saying it's right. And so this is  You take the joint PMS.  And then you realize by the marginal penis. user avatar   Okay. user avatar   And so we can see that the conditional PMS p of x given why we will often right or it's proportional to the joint p of x it away. What do we mean by by by this as well.  Why is fixed, right, because why is conditioner little Why is fixed. And so we can. This is a distribution over x. So, we will just make sure that it's read normalize to make sure it's some to one. And so the normalization constantly is what is the marginal. So the normalization.  Constant is the summation of x of the joint.  Which is equal to the marginal try and so that gives back the same formula as above.  So why do I mentioned that is because often the normalization constant is kind of a pain in the butt to compute. For example, you need to some or you need to integrate in the case of conscious when a variable.  And but it doesn't really matter too much. It's just a constant. And so what you can do is you can do a lot of computation, making sure we looking at only what is proportional to and you just we normalize idea.  And so, for example, you could notice while doing some computation that all this is a Gaussian, so like the joint is a Gaussian. And so then the normalization constant will be the normalized or by Joshua, but you don't need to explicitly computer because it doesn't really matter.  So we'll see when we do Bayesian updates. This is very  This is very  Useful as a trick. And what do you mean P of why is fixed.  Ah, yes. So like vicious, they said p of why is a constant in the, in the context of computing the conditional p of x given why when I will change x  Pure Why because, why is fixed and pure white is user avatar   20 user avatar   Alright, so finally, this is for discrete so then for continuous  For continuous random variable, instead of PMS will have a PDF. So the conditional  PDF  And it's defined similarly, as in the  In the discrete case, but I am not going to talk about events because it gives some issues. So I'm only working directly with the PDF. So by definition the conditional PDF of p of x given. Why is the joint pdf p of x, y divided by the marginal pdf P away. user avatar   Yeah. user avatar   And here, a subtle point  Is that the conditional p of x given. Why is undefined.  Which, in other words, you can put any value you want when the marginal and y is equal to zero. And I'm not talking about  The event capital Y equals it or y, which we know for continuous distribution, this is zero. I'm saying the density on for the marginal why you could have zero problem. You could have zero density on some region.  In this grace conditioning and it doesn't make any sense.  But  It doesn't really matter anyway you can just define to anything.  And you can define to anything because it will never happen anyway because the priority of adding this y is zero. So it won't become relevant somewhere.  Okay.  And so these are the definition  And be aware of that. The for continuous random variables conditioning is a much more rich concept than what I just wrote here. I gave you the, the, sorry, the simplest version by going directly with the PDF  But there's this there's a bunch of paradoxes which talk about conquering  counterintuitive.  Behavior of conditioning when you're not careful. Okay, so, so actually the true measure of theory definition of a conditional is is not just a value like this. It's actually a random variable. It's a random object.  Which is only defined up to a measure set of zero, a measure of set of measure zero  And basically normal you you when you condition, you have to talk about how you will split the events and depending on how you will spend the events, you could get  Different results and I forgot. There's a name for this paradox, which I forgot the name, but I'll put it back on the notes later after I find it.  How does kind of shooting works when one variable is continuous out very transparent X know. Is it the one with the circle. I'm talking about the one with the circle if me google it quickly.  So he says it's Bertrand.  Russell Bertrand Russell's paramedics user avatar   King I'm having issues. I don't see anything. user avatar   So he says, Bertrand. user avatar   Now this is any kind of mix. user avatar   Ah, interesting. So it's a different, it's the same paradox. I was talking about, but it has a different name.  Okay, so perhaps it's transparent.  So the products on I'm referring to is basically that if you have a uniform distribution on the sphere. And now you talked about  Conditioning on a great circle, depending on how you manipulate this conditioning, you will get two different answers, even though it should be the intuitively you think it should be the same because it's just a great circle, who cares. And now I lust. Where are the people. Oh, can I see  Okay. Well, anyway, see myself over there.  So last thing before the break.  It's not Simpson's paradox. And it's not the Buffon needle. But anyway, I'll find it. So yeah. So two things.  First is, as I mentioned earlier. So note that if I have independent random variable.  So if I have x independent of why  Then if I computed conditional of X given why  This is, by definition, the joint of x and y divided by the marginal wine but because of independence. I have that the joint is the product, right. So I have p of x times y by independence.  By independence divided by poi and so I have a pair of white consoles and I get that the conditional of X given. Why is the same as the marginal so knowing why doesn't change anything. It's like if I didn't know anything about why  So that's kind of like a nice  Kind of consequence of when you haven't been variable which kind of just to find a meme of independence. Oh, actually, I find it there there there below that my group chat. user avatar   Okay, cool. user avatar   And  You know, an example of conditioning.  Where it could be very useful. And, you know, we'll, we'll see it all over the place in this class is  You know, let's say I have a model of symptoms and diseases. Well, I would want to compute the probability of having cancer.  Given that I made some for example tumor measurement  Which is equal to Italy.  Right.  And so probably temporary measurement could be the screen when there's multiple types or it could be, say, a size, in which case it will be a continuous value. And so then we will be talking in this in this conditional will be talking about the PDF. Right.  Well actually having cancer is just an early but we could still, I guess that's the question. So somebody had this conditioning works when when the variable is continuous. This one is discrete  Well, you can just reuse. Look at these thing and it still makes sense. So, for example,  Let's see why is continuous, an X is discrete, like my cancer example.  So x is whether you have whoops, blah, blah.  Sorry, I'm just trying to get my pen. Alright, so let's say why is the measurement of my tumor, which is continuous and x is a binary variable which says I have cancer or not.  Well, then this joint here will be a hybrid PM F on X PDF on why and that's fine. Right, so I can still just do that here. user avatar   Okay. user avatar   So somebody is asking if you have a mix continuous discreet distribution. I have seen a direct delta function uses it appropriate  Yes, it's kind of a device like basically a very good affection is actually, it's called a distribution in mass. That's the proper theory of it. It's an object that  When you integrate you get value so so point wise. It doesn't make any sense because it's everywhere, everywhere. It's infinite that the point. But then we integrated, you will get the, you know, you will select value at the point where it was supposed to be infinite.  And this actually comes from the fact that, you know, I said we could define PDF respective based measure if the base measure is discrete, then it's basically you get these direct delta for integrals at the value, which are the the discrete  The discrete possible values of your random variable. And so you can think of it as a PDF for discrete random variable is something that will integrate  But where you will have the direct delta function at every possible values of the random variable, times the actual PMS of this random variable. It's a when you integrate, then you just get the song like like in the standard discreet  So if you don't want to distinguish between some an integral and just having two goals everywhere, then this direct is very useful. And actually, I think we might use this device at some point in this comes  In wouldn't a calculation of poi the injectable often poi is indeed and tractable. It's an integral or it could be an infinite some or etc. So, indeed. That's why in this class will talk about approximate  Techniques to compute marginals, and finally, can a distribution and self be treated as a random variable, and if so, how would you describe independent distributions.  So a distribution itself is  For example, let's talk about  Let's say we have a discrete random variables, right, a discrete random variable is represented by its PM, if suppose, for example, we have  Key possible values. So we need key numbers which are positive and which some to one. And so to describe this random  Distribution on key values. I just need to specify. What's the quality of each of the key values. So it's a vector of dimension key which is  Actually called the property simplex. So they're all positive. There's some to one. Okay, so if I did find a distribution over the property simplex. Now I have a distribution distribution and sometimes because these are the parameters of distribution. So, you can still talk about that.  And then the question is, how do you define independent submission. Well, you can say that, let's say I have two different  Also, this doesn't really make much sense as is what it means to be independent. This case but  You could have, let's say, Okay. So a good example would be, I have  Two different  Distributions.  Of x and y.  Are these it or actually I define a joint distribution of x and y. And is this distribution in as independent components right  And I could put a distribution over this distribution and I could say, what's the probability that when I sample from this distribution. I have that x and y are independent.  So that could be an example of question which would make sense in this in this setup. And I think the answer would be zero because  If you sample from most joints. Usually, you will have some dependence and to get independence. Perhaps you will have only zero mess, but it would be this is getting a bit beyond the scope of this class.  Alright, so it's 332. So we'll take a 10 minutes break  And be back at 342. And in the meantime, I'm going to grab some coffee and put some music.  Okay, so we have a few questions. First question in the chat a private question, whereas in the assignment I use a TI upside down instead of a pie upside down.  Is this the same thing. Yeah, kind of its. This is latex late tech laziness. Normally, the, the T upside down might mean uncorrelated rather than independent like perpendicular. It's actually kind of like a translation of perpendicular, but  We will in the assignment. I mean, independent. Okay, so this is just a latex laziness.  Alright, so then  Another question which was asked in private is  The protein distribution for random variable describe how the priorities are distributed over devalue of the random variable. How are the distribution falling practice. So that's a really good question.  And we will get back to that later in class. So this is basically a statistical questions. It is that, oh, I am a phenomenon, what should be  The description of this phenomena. So I observations of phenomenon, what should be the distribution describing this spin amount and  And that's a statistical question which is not easy. You could use maximum likelihood you could use your prior information prior knowledge. If you're Beijing this kind of stuff. So we'll see different tools to do that later in class.  Right now we're focusing more on properties, which is that suppose I know what the distribution is how to compute some quantities.  That's kind of the framework and Omar says in of course we're not going over measure theory details can be considered, it doesn't really matter for practical applications or is it mainly because the go over the scope of the class.  Well,  So I think for a lot of practical applications. They don't really matter, but not all. So that's why I won't say it's it's just  So the main reason is because it's over the scope of the class because you can get an entire class just one major theory was here, we're covering other things.  It's also more than that. So what happened is when you talk about distribution over much more complicated object like something call stochastic processes.  So we were you have a discussion over an infinite number of elements like a function  Or something or distribution over distribution where this distribution is for a consistent variable, so that's  Starting to get very hairy object and then the measure theory starts to become more relevant to make sure you don't say CD things. So, for example, we'll see when we talk about  Non parametric Bayesian methods, towards the end of the class. So basically non parametric mean an infinite number of parameters, basically, that's kind of what it means, doesn't mean there's no parameters just means there's a lot of parameters and includes an infinite number of them.  So the existence of these objects often are very  non trivial to prove. Like sometimes we can define things which don't make any sense. If you don't know enough about Misha theory.  So there's this thing called like the coma Grove extension theorem, I think, which says that, oh, if I have  Defined all the marginals of my joint there is exist and the satisfy some rigor. The condition then exists a joint, but this is already like some very powerful theorem coming from that major theory.  And so, so the short story is  Beyond the scope of the class. And for a lot of simpler applications. You don't have to worry too much about it.  It's always good at some point to, you know, learn about it to make sure you so if you go more events more advanced topics you need to learn about it to make sure that you don't see something see  Is there a course on stochastic processes at Mina, not sure, actually think in the math department. There might be some as Parker think Dr Zhou I was teaching something with dynamical systems. I'm not sure if he does the classic processes as well. user avatar   So, yeah. user avatar   Okay, any other question.  So for these kind of questions. I also recommend perhaps like there's a lot of you from different universities. So you can just ask it on the slack and like we already have a few questions, answers like apparently there's one technique and somebody mentioned the math department.  Sector so so we can use our collective intelligence to answer these questions. When I should have put my pen here so that it's charge suit.  Okay, so we saw conditioning.  Now let's talk about base rule, which is kind of related  And then we'll see parametric models, very soon.  Alright, so now we have base rule.  What's based rule. Well, it's just a simple manipulation of the definition of conditional property to invert the conditioning. So this enables to invert.  Conditioning  And so by definition I have that  P of X given. Why is the joint divided by the marginal so I can multiply the marginal on one side and so  Then I have that I could use this to join to just define the conditional in the other direction P of why given x divided by PX and so  The base rule is simply  P of X given why  Is the same thing as p of why given x times p of x divided by POS  So basically,  If I know what's psi given x  I can obtain p of x given why so reverse the conditioning by multiplying by p of x and dividing my PR  And so basically I had that  By definition,  Of the p of why given X was p of x, y divided by p of x.  And so this implies something called the product rule.  Which is that the joint is the conditional times the marginal so pure why given x times p of x, just the same thing as p of x given why fans.  So this is called the product rule.  And then you can see easily how I can obtain the bays rule by just  Taking this piece and sending it on this. Like, that's how I get, please.  Alright, so now  There is something very powerful and very important in this class called the chain rule.  Which is just  Applying the product rule multiple things.  The chain rule. What you do is you do a successive  Replication  Of the product rule.  On multiple random variable.  To get that the joint.  On x one, two, x n.  We can say, well, this is by the product rule. It's the conditional of accent, given everything before  Times the marginal on one up to minus one, right, and then I can expand this joint here again, using the product rule. This is the conditional n minus one, given one up to n minus two times the marginal x one to n minus two. Well, then they get the same thing here. And so you keep doing this.  And you get that the joint can always be written as the product from icons n to the conditional of x i, given everything before  So I have that the joint can be written as this product of conditions.  So this is always true.  And moreover, here the one up to end was a bit arbitrary. You can also choose  Any of the n factorial  Permutation  And I could have  Instead of starting at accent I could have decided to start exploring and then go x seven or whatever. So you can just per minute, all the variables and just reapply the same kind of  Statement statements that and you get just a different factorization.  And so here, know that if I apply if I look at if I looked at this quantity here when I equals one, I get one up to zero.  Right, so the convention here. So that's removed this so the convention here my limitation is that if I have one up to zero or tend to zero. Whatever is some  On the right, it's smaller than the left. This is the empty set right this empty set of index and I have that P of X one given X the empty set is just the same thing as p of x, right. It's like if I didn't condition on anything. So this is just like the station convention.  In order to not have to have a specific term separately with different notation. So that's kind of to go faster.  And so now when a key aspect is that these conditional here. The can be simplified.  When making  Conditional independence assumptions.  And this is where, when we talk about a directed graphical model that's what we'll do.  In a  In a directed graph Kemal that will see later in a few lectures. So not not that soon. We basically see that will have that p of x i given x one up to i minus one will be equal to p of x i given x where these are the parents in a graph.  In particular, if I have a chain. It could just be x minus one. Okay, so when I have that the the conditional  Of a lot of variables reduces to a smaller conditional. This is basically conditional independence assumption assumption, right. So let's define it formally  But before I defined formally, just to be clear that  This is true for all distribution and I can. But the problem is, here I have a lot of variables. So, these are very complicated factors and particular the I will have the XM given  Everything before. And so that's a huge, huge, huge table with an exponential number of entries. So it's super complicated  And so when we talk about that a graphical model, we will make assumption which will centrify these factors too much smaller factor, where the parents here could have one or two nodes are not a very small number. So that's basically the assumptions woman. user avatar   Times p of x one. user avatar   There's some question. user avatar   Is there a connection here to the chain rule in differentiating function composition  I mean in math, often you have a decrease the level abstraction. You can make links with a lot of things right, so here  The idea of a chain rule in general is just you chain a bunch of operations that's I think the biggest link so you know there's a train rule for derivatives. There's a chain rule for  Conditioning. There's a few different channels. So here  This is a chain rule for  Basically expanding a joint into a product of terms. I'm not sure if it's directly related related to  Differentiating a function positions. But I think at the some abstract level, it could be  Alright so conditional independence. That's where I was going to get conditional. What's the definition of conditional independence.  It's just a generalization of independence, where now we look at conditional statements. So we'll say that x is the random variable X is conditionally independent  Of the random variable why given Z. So it's the difference between independence and I will talk about given and the notation is like independence. But with a little twist. So, this is x conditionally dependent of why given Z. So I add the little vertical bar to talk about conditioning.  And this is if and only if there's multiple equivalent definition. So here I just think that the joint of x, y given Z factor is so it's basically like the the independence, but it's just that I will replace with conditional statements I have p of x, given z times P of why given  This, this has to be true for all x, and their sample space. Why in their sample space and Z. It's simple space and I need to condition on something which is not which is possible. So, such that the marginal is not here.  Okay, so that's the definition of conditional dependence. It's very similar to independence, but now I'm making these statements conditioned on a variables.  And now you can as an exercise to the reader.  To the reader.  You can try to prove by using the definition of conditional dependence that if I have x conditional independent of Y given Z.  Then I also have that when I condition on y AMP z x given y and z is actually the same thing as just p of x, given z. So, knowing why doesn't change anything on my conditional x, given z.  So this is the direct analog. So this is the conditional  Analog  Of x independent of why implies that the conditional of X given. Why is the same thing as the margin politics.  But here I added conditioning Wednesday.  Oh, somebody is paying attention, such that p AMP z is not equal to zero.  Thanks.  Man.  Okay, so let me give you an example to make this more concrete would have  Something which would make sense to talk about conditional independence.  So let's have that Z is the indicator  Of whether some mother.  carry genetic disease.  We take disease.  And then I have that x is whether son one has the same disease.  And  And perhaps it's in the  Lab. It's not too crazy biology. So  Someone has disease. And then, why will be son to has disease. Okay, and a typical assumption is that whether once I know that the mother has a disease, there's a certain probability that the that she transmitted to the sun, but  The quality that  One son has it versus the sun is independent, you could you could think that  Once I know I have the disease when I transmitted. There's, there's, it's, it's like a flip of a die each time whether you get it or not, or a flip of a coin Tobias queen.  And but they're independent. Okay, so this would be an example where you'd say that x given x will be independent of way given Z. user avatar   Okay. user avatar   But  It's not true here that x is independent of why marginally. Okay, so here you have that x is not  marginally  Independent  Of why  Is he  It's not the case that x that's we've been an annotation. So it's not that x x or not just and here we say marginally to just say, well, we have marginalize opposite. So we don't care about it but just look at a joint of x and y.  And in this case, they're not independent, which would mean that there exists x and y, such that the marginal and x and y is not equal to the product.  And the reason here from just a modeling perspective is is because  When I know that X has a disease. It gives me information that the mother had the disease.  And if I know that the mother has a disease. It also gave me information about the other son, having the disease.  So for example, if the mother doesn't have the disease, then the property of the sun, having a disease is basically zero whereas if  The sun has a disease, then it increase the probability that the mother had the disease. I mean, actually, it's pretty high, which increases probably that the other son has the disease. So there's a relationship here.  And so in graphical model notation. This condition depends statement will be like this, I will have that x y here and z.  Z will be the parent and I will have two children.  And we will see when we talk about graphical model that this means that p of x, y, z factories in a way which is just that p of x, given z times p of y given Z and then times the marginal z.  And so from this factorization, it is clear that once I condition and z z is just a constant, then the marginal the p of x, y given Z factories as p of x, given z times PA y given  Which is the conditional  Alright, so now let me give you an example of pairwise independence, which is not maturing the bend and  That's what I told you earlier.  Of pairwise  Independent  Random variables.  That are not  mutually independent  And so I could define X to be a coin flip.  And then I could have wide to be another independent  coin flip.  And so by definition here I have the X is marginal independent of why  And I could define  Z to be x four x or y  Okay.  So x or. So these are binary values. So, x or have to binary values. It's one when only one of the two is one. So, one, zero, and gives you one but one gives you zero  And exercise to the reader, you can show that Zed, in this case is marginally independent of x, y and Zed is marginally independent of why  But you don't have that said is jointly independent of x and y. This is not true, because if I know x and y. I know z by because it's a it's a deterministic function of  X and Y.  Okay.  And so here when I only knew two of the three variables.  The looked like independent, but if I knew three variable. If I know if I like to i can i can actually predict the third value of the third, the value of the third variable which is why they're not all Mitchell be independent.  That's what's that, so that's the difference between pairwise independence and which one the pennies.  Okay.  So let's talk about other annotation.  So that we all have the same terminology  Will talk about  Expectation mean  Of a random variable.  Expectation of mean of a random variable. So the notation is this little bar capital e mail us bracket.  It's like a sexually linear operator acting on random variables. So I put x in the bracket. And so by definition, this thing is either if it's a discrete random variable, it will be the sum over my simple space.  Of x px  This for discrete random variable.  And it will be the enter goal or my space X P of X the X four continents.  And the expectation operator.  Which is basically an integral  Is a linear operator.  Is a linear operator.  Which means that if I take expectation of a capital X plus capital Y. Where is a Skinner, and X and the Y or random variables.  I have this is the same thing as a time expectation of X plus expectation of. That's what we mean by hitting your perimeter. So it looks like a  Well, it behaves linearly on these vectors. If you think of x and y as vectors.  And that's basically a property of some or integrals.  And then somebody asked a question.  In private should we make the leap from conditional statements to graft surgery. For example, in the assignment, not necessarily in this case.  This is more for illustrating the concepts that I'm showing it right now. But in the assignment you should use directly the definition of conditional independence and you can just use the graph.  Because we will cover the properties of graph later in the class. But, you know, use them for the first time.  Alright so expectation. We also have the variants of parameter  The variance. I will use VA and bracket. So by definition this is the expectation of X minus expectation of X squared. Yes, there's a bracket here.  So it's the the expectation of the deviation from the mean this quick  And if you expand that and you use the linearity of the expectation, you get that this is the same thing as expectation  Of x square minus expectation of X.  Square.  And this is basically a measure of dispersion  Right it tell us how spread out your distribution is  And so for example if I have some kind of like bell shaped random variable, this would be the expectation of my random variable.  And I would have  Some standard deviation  Which basically tells you, like how wide the bump is and this standard deviation, which we could use sigma will be the square root of the variants of x.  That's the standard deviation  So the variance is square and then to get the standard deviation, you take the square  To give you the scale of the dispersion of the distribution.  Okay. So that covers that probably T concepts. I wanted to review.  So let's shift gear. Oh, why  We are not normalizing E x  Alright, so somebody is asking about normalization. So the thing is, then the polity distribution is normalized, so this is already rescaling things properly. user avatar   Okay. user avatar   And then why do we call expectation invariance and operator.  So this is me can call them that, because  You could think of the expectation as operator, which takes as input a random variable or its distribution that see on a fixed space and as output as killer. Okay.  And why is it useful to talk. Think of it as an operator. Well, because then there's some properties of this operator, which you can talk about like linearity, which I just described. Right. So I can take  Linear combination of my random variable than the expectation of the linear combination is the same as the linear combination of expectation. And so this is some kind of properties. You can talk about  So it's a useful mathematical abstraction.  Is an expectation and variance or properties of our distribution. I feel confused cutting it operator.  So if I fixed my distribution, then it's expectation variance or fixed yes I talked about them as an upgrade or because I can now consider how the expectation varies when I change its input in terms of distributions are random variable.  That's, that's all.  Okay, so let's talk about parametric models I have 13 minutes  Oh, I guess I'm answering a lot of questions this year. Well, that's, that's good. But I'm going a bit slower. That's okay.  Parametric  Models.  Alright, so now we're talking about instead of a fixed distribution. We'll talk about a family of distribution and so apparently model is a family.  Of distributions.  And I will use the annotation curly p  As a set of distributions and I will use  My set of parameters as its index.  For example, I mean this is just notation.  And so this is a set of distribution.  And basically, I will have to mention the PM effort, a PDF of my distribution. And so these would be either a PDF or PMS and I will each of these will have a parameter of theta and parameter theta will vary in my set of parameters. They were called capital theta.  And so these are the possible  PMS.  Or PDF, depending on whether I'm talking about a parametric is apparently family of distributions, which are continuous or discreet  Depending on parameter a sale.  So the, the, the column semi colon notation is just  Come out or semi colon don't really mean the city that much different is just often I use semi colon to because the two elements will have very different nature. Right. So the first argument here.  This is like the, the, this is where I will put the variable which present my outcome was this is a parameter. So it's kind of a very different  Nature, so we'll see that a few times in this class. So depending on parameter data.  And  When we talk about a family of distributions will always have implicitly from the context.  What is it on so from the context.  Will have what is the support of the distribution.  He. What is the sample space for my random variable.  And it's usually fixed  For all the panelists.  So for example, if I talk about a model for coin flips my sample space would be 01  And then  I would have different protein distribution on 01 which will be presented by. What's the quality of ahead, for example, so I will have one sinker scale or parameter which did represent my distribution.  My quality of seeing ahead.  Or another example is if I have a gamma distribution my support will be the positive number.  And then the gamma distribution will have two  Parameters describing the shape of the distribution. user avatar   Funding user avatar   What's this support means in math, usually the support of a function is where it is non zero  And so depends on the context, but so in this case the support of random variable is where its quality is not zero, which depends also. Okay, are we talking about the density about the PMs etc etc.  But so in this case.  There's already a bit of ambiguity, because you could say that the support which is the sample space.  And then, normally you could also have that the density zero at some element of the support if you wanted, because you put a parameter which tells you that the policy zero. In which case, they're not normally excluded from the support of the distribution.  Then somebody's asking about the notation.  Yeah, so the this the semi colon, as I mentioned, is, is basically like a comma. But it's a comma between things which are perhaps quite different. And I want to highlight it, so I'll use a semi covenants that  So often used it.  By the way, there's an abuse of notation here that I will often use so abuse of notation.  I could  describe this family as p of little x theta theta, you know, getting to my set of parameters. So this is not correct notation, because if I have a fixed x here.  So if this access fix this is a scheduler. So this would be a set of skills. Right. I don't want to set of skills. What I want is basically that this x berries.  I want to represent the whole PMS for the PDF for all x right just why here. I use a duck. The duck means I consider the whole PMS as a function, not at a specific point  And so a better notation which is still not ideal but  Is to use, which I think I'll use often.  Is to use the capital value instead. So we use capital.  Data  And basically the semantic here is that I'm talking about the distribution for random variable capital X, so the the  Letter here is just telling me what's the name of my random variable. What's the letter I will use for it.  And the fact that I use a capital letter here means that I'm not looking at this value for a fixed little X. I'm thinking of it as you know all. It's not us, but it's still a bit ambiguous. Yep. So this is really a better notation, because now it's clear and I'm looking at the whole function.  Is capital he possesses space. I don't see oh the so there's no capital H here. It's this is capital feta. Alright, so this thing here is a circle with an agent at this is capital data.  And capital theta is the set of parameters.  So there's no age anywhere.  Okay, so now let's talk about some example of these parametric model.  So the notation that will use to talk about these Patrick model, I could say capital X. Is there any of our x is distributed according to Bruno he  With parameter theta, right. So this means. So the little squiggly line here means that the random variable X is distributed  So the squiggly mean distributed as a  Bernie.  distribution with parameter theta. user avatar   That's basically what it means. user avatar   And then I could use annotation that the PMs  I guess let's use this  Again, more notation. So I could use that the PMs on X with parameter theta.  For my random variable is a bird new EP and f of x with birthday.  Okay. And so this is to indicate the variable for my PF  And this will be the parameter  Of my distribution.  And this is what this is data to the X one minus data to the one minus x. So we'll see it very soon. So let's define it Bernie.  It's the quality of a coin flip.  So the sample space is just 01  And the probability of X equals little x that, sorry.  Policy that my conflict is one and I'm using here Bayesian notation. This notation.  Will come back to this later.  Is just data. user avatar   Right. user avatar   And so the parameter of a Bernie is just what's the probability of having head or having the result to be one and this is given by theta. And so because the property has to be between zero and one. I have that my possible parameters is just the interval between zero and one.  And I can quickly and because I have that the probability of having zero is one minus the quality of having one I will have that the priority of x equals zero is just one minus data.  That so quick way to write that is that the PMs for my brother. He is data rates of the x times one minus beta one minus x right  Which is where we where we mean by Bernie on X data. So, so when I write Bernie X semicolon data. This is a shorthand for this expression.  And so if the data if x equals one.  Only this will be one. So, I will just be left with data and if x equals zero, this will become one this will become one. And so we'll get left with the one nice data. So this works for both specific. So it's just a fancy way to write a very simple thing.  Okay, so I guess a lot of people are talking about notation. So indeed, you know, I use the new unique so I use for example like this notation here.  As a shorthand to talk about the PMs or PDF of a distribution. So another another short term that it will use later. So another example.  That we use later is I will use the notation normal on X given parameter new and sigma square right so this is a Gaussian with me new  And Pat and variance sigma square and then I'm talking about the pdf of this Goshen on the variable x. And that's basically what I mean by this thing when we get back to that.  And then when talking about decision function do we use small p or big P so small, P is for P MF PDFs.  Big P is for the actual quality of the event. And so that's why I use curly brace. Right. So when I when I use big P i use curly brace here when I use when I talk about PMS or PDF. I use it.  Okay, so  Just before we finish.  So guess next class. I'll talk about the Goshen, but  Just here quickly the expectation of  A bird new ye, it's just data. It's variance  Is data times one minus data you computed  And you know, that's what it looks like so. So if I put data and I write variants of x here.  Or I could use actually to kind of like highlight the dependence on data. I could put a semi colon data. So this way I can see how it varies as a function of data. And so the maximum value is one quarter, which is reached at one half. And so I get this kind of quadratic function here.  So the maximum variance of the brewery is when you have an unbiased coin flip.  And and finally the last example, the gosh, and I mentioned to you.  Was  In this case it as a PDF  The parameter or demeaning, the variance  And so I will say this is just a PDF of a gash in with  Me and Mew and variants sigma square  Or I could use. In addition, X given using my square that's like a more Bayesian way.  Either way, that's the same meaning. And so this means the pdf of a gash in which is minus x minus new square divided by two sigma square square  one over square to pies sigma square. So, that's the density of aggression. And so when I don't want to write this whole expression with a lot of things I could just say, oh, well,  This thing I could write it in a full equation and just right. Oh, this is the cash the pdf of. Gotcha.  And somebody asked, Why is this equal to data because I told you that by definition in a bird new ye the probability of x equal being one is just data.  And this notation here was just saying, well I when I know the parameter of theta of my distribution that's the  That's the PMA for my burden.  Okay, so I have to run. It's a time's up.  And so great. So  Don't forget the assignment is up, but you know you have time to do it. They have two weeks.  And I'm going to see you on Friday.  You do.
  Actually didn't have to stop the recording.  Okay, this is recording so hi everyone  Welcome to yet another chapter of this wonderful journey into the land of multivariate probabilities. So today.  Move my notes today today.  What are we going to do today. Now, first of all, zoom, I didn't think about that. user avatar   See 125 good user avatar   Alright so today.  The plan is to  Look at statistical  I guess we'll finish.  Some quality distribution notion  And look at  Frequently just  Vs Bayesian approach.  Will do a bit of statistics.  We talked about also  The philosophy of frequencies versus Beijing ism.  And give an example of Asian  Okay. And, and so, just so that we are  on the same wavelength. So basically, for the current lectures like there's another two or three lectures, where we talked about  Basically basic statistical concepts decision theory maximum like you how to analyze estimators etc. Okay. And so that's to get the  The fundamental concept or polities statistics just set up and then after that, we'll start to do lectures of first like we go like classification and regression  So I talked about them as example of two node graphical models of specific graphic come over. We have two variables. And so I talked about journey was a descriptive approach in this case. And we'll talk about logistic regression versus, say,  Discriminate analysis, this kind of approaches.  And  Then later on in class to structure quality distribution with like  Andre and and directed graph. So that's kind of the, the plan.  So last class I talked about already, the Bernie distribution and Gaussian distribution. So, let's talk now about the binomial distribution, just as an example of a  How to build another distribution, because that's a fairly standard one  And so this is us this binomial distribution. It's normally it can be used for example to model.  The, the result of an independent  coin flips. Right. So the burner. He distributions, as I mentioned as fast. So let's see.  Let's, let's close so brilliantly. You can think of it as modeling one single coin flip, because there's only two possible values, heads or tails.  head or tail and now the binomial will be that. Okay. Well, now I will flip anytime a coin and you can define the binomial distribution as the son of an independent  Burden he random variable.  With the same pattern random room right  And know that usually the the the parameter for a birdie distribution can be you know that P  Just convention but will you stay that just because so that we're using it. This is the parameter or distribution.  And so in terms of notation. Suppose that I will have n random variable indexed by i. And so the notation we often use is you see x I or II ds, which is user avatar   Me right it's currently so user avatar   There. user avatar   I guess we don't even put the dots. Yeah, I don't even put the dots in this class. Yes, I just put I the okay so I say  Burden with burner you theta.  And  So, so this ID means that the random variable or independent  With their mutual independent. It's not just pairwise independent Mitchell independent. So I guess it would surely let's write it down to make sure user avatar Oumar Kaba  I hear you. This person. Here we go. user avatar   Unless you had a question. You had a question.  Well, I forgot who I muted because it's already moved out to the way  So Mitchell independence and identically be and they typically distributed. So that's what the, the second identically distributed  So in this case, each run a verbal has the same distribution as the other one.  In terms of marginal so they're all Bernie's with the same time we're feta.  If I wanted to have different conflicts with different bias, then it could still be independent, if I want, but I would not be identically distributed  So I need this notation will appear a lot, especially in the context of machine learning. Usually we assume that  The observations or ID when we do learning, though it doesn't have to always be the case. Let me just do a user avatar jacob louis hoover  Sorry, I'm just changing the screen display here, may I ask a question.  Yeah, so when you when you when you return right here are the x i a set of random variables or is x, I just, I'm just, I just like to have clarification on what is x i. user avatar   Yeah, that's a very good question.  So that's lazy notation.  So,  What you could have written normally like to be more more precise, you would have written for example like x one, blah, blah, blah. To x n.  Or ID. So that's another way to be a bit more specific. But this is fairly common statistics, where you just put the index and you put the squiggly ID. And that means that when you range over the index each of these variable or. So this basically implicitly  Is talking about  This set of of random variable, okay.  But yeah, but that's so that's one thing to keep in mind, by the way. So in in in protein statistics. There was a lot of very implicit notation often  And in machine learning, there's there's even another layer. So because you lose. Often the bid the formality of math.  Like you're not perhaps as rigorous that people are in mathematics. And you also have much more complicated object to talk about all the times. So you cannot just  Spell out all the details, each time. So, so one has to get trained to have this implicit parsing, but it's definitely the first time you see that it's not here. So you need to ask about it to make it on the biggest  Alright, so we have these n random variables and actually already and was implicit here like so. You know, so  Normally, it would be from the context, you would also like it's ID, ranging from one to something in this case and  So now, let a new random variable be defined as the some of these individual burner you random variable. Okay, so this x will have a binomial distribution, then we have  That X, capital X is distribute according to binomial and you will have the number of flips and the parameter of the flips so binomial. Okay, so you call this binomial distribution.  With parameter and and data.  Which, by the way, and is not the same kind of type of parameter for these distribution usually  I mean, it's still a pattern in the sense of, I can change channels to get a different distribution, but so it has a much bigger effect and data because it changed the output space, right. So the values of x. So the sample space of x is  02 n, right, because basically X is counting the number of times I had one as my result which could be, for example, counting the number of times I had had  And so we've actually n times the max is n. And so in some sense in this case and controls the support of my distribution or it can control what are the possible outcome.  And if you remember when I talked about parametric family. Usually the support will be fixed. And then when you change the parameter you want don't want to change your support. That's why, usually in this in this  From a parametric feminity perspective, say, that will be the partner and and will be fixed from the phenomena and, in particular, usually what would happen is from observation you would like to estimate fair but envelope. Yes, you will be fixed.  A right so and you can derive the distribution of the distribution of the binomial. So this is the PMs of the binomial, which I write  With a comma here. So the PMs here.  Is actually coming from. It's the product of an independent burglaries so I can actually look. So if I have observed X time head. I can actually use the product of the PMs of the brewery because they're independent distribution. Right. So you have that the joint of  The joint factories is a product of marginal in this case.  The joint over all possible outcomes of my head or tail with have, for example, the probability of head race to the number of seen head.  And then I will have the probability of Tail racing number five seed tail which if the x is the number of times we'll see head, then n minus x is the number of signups detail.  So what I'm just road there is that would be the joint distribution where I forgot about the individual conflicts. I don't care about with the indigo conflict and now I missing a term which is that  What I wrote here is the joint. If I would have told you the first coin flip is not a tail. The second contract is a head is it is a tail.  I'm missing, actually. So I'm cutting the heads. So the first and fifth is the head. The second is not a head or whatever, that's a possible outcome. But here, all I'm saying is that I have x head. I'm not saying what's the order  And so you need to some over all the possible ordering which can could have given you X heads and so that given by the binomial coefficient, basically. So it's x choose n  Or n choose x sorry that's always the problem. I don't know how to say this thing. So this is the number of ways.  To choose  X elements.  Out of n choices, right. So this is call user avatar   And choose x user avatar   Very standard  Community element. So I have that and choose x by definition is n factorial divided by x factorial n minus x factor.  Okay.  And of course this PMS is valid for x in the sample space, so x belongs to omega typical x  And what I told you is that this thing here was basically could be seen as theta. Some over i X i.  One minus data. Some over i one minus x where x i are the individual observations of heads or tails. Right.  And this is the product.  Of my brewery.  Payments on X i with Banner theta.  Which was a joint on all x one to accent.  Right. So I started so just give me a bit confusing here. So I started with these independent random variable. So for every i add a capital X i and now each the could take little excited  And the density of this joint because they're independent is just the product, sorry, the PMs of the joint. This is a product of the PMs of each individual  PMS. And so, so that's what I wrote here. So that's this product. Now it happens that for the binomial. I don't care about the individual trips I only Sunday. So the ordering of how the quick fix happened. Doesn't matter.  Only the account matters which is why you get this extra binomial coefficients. And I'm not sure why this things. It's kind of user avatar   Showing in TV. user avatar   Alright, so any question about the binomial.  Door asked if n is sort of like a hyper parameter user avatar   Hmm. user avatar   So we'll see hyper parameter later.  In the often come  In the context of  So usually you talk more about hyper parameter in the context of Bayesian distribution. So you have a because we put distribution of our distribution and these distribution will have parameters which will call hyper parameters.  And then you could put also distribution over these hyper parameters which then you have hyper hyper parameters.  Hmm, so  Right under the hyper parameter here I would call it more like a description of the contextual description of what I'm modeling.  Right. user avatar   Yeah. user avatar   It's a bit like saying, what is the problem solving, so that you could say, oh, I have a choice but to prom between some problem of five, eight ID or time. See, and I'm saying okay and problem solving a pipe and then I have some model for typing. So here it's like  I need to say how many coin flips. I am using my model.  But indeed, if you would like to model something where somebody  An unknown number of points.  And then you can also put a distribution and on how many times he will flip a coin, then  You could put a distribution over and as well. And then it becomes a parameter and and you can have a hyper parameter for the distribution which will be put on the value of it will see that that again when we talk about Beijing minutes  Any other question.  If not,  Let me now. Alright, so this is the binomial. And so some properties of the binomial, will that mean about a binomial because it's the sum  Over these burglaries, I can easily use the linearity of the mean to say that, well, the mean of X is by very tedious. Some over I have the mean of the exercise.  And so we know that the meaning of a burglary is data. The property of the burglary. So this is an times data. And so that's easy to compute. And similarly,  We can have that the variance of some of x i. So, it turns out that when you compute the variants of so the variants of A. It's not a linear period or in general.  So if I have the variants of a somewhat random variable. It's not the sum of the random, but it's not the some of the violence but if they're very they're in a variable or independent  So by independence, you will have that it is the case that it's just a some of the variants of the excise so this is true when the excise are independent.  And so this is just n times the variants of a brand new, which is theta one mistake.  Alright so this allowed us here to just to play with standard  Properties of distribution. And so I will mention now other distributions, just so that you have the names. I encourage you to look at them on Wikipedia, for example.  If you have never seen them to look at their properties. So there's a parasol distribution which has pander lambda and lambda is the mean of the distribution.  And so this is a distinction us on natural number. Alright, so the sample space here is 012 blah, blah, blah. Just, just the natural number. And so this is often used to model count data. user avatar   Right, so user avatar   For example, user avatar   How many times a detector has detected some some academic particle. So, the physics happens all the time. The person distribution.  Or it could be number of accidents at intersection, so could use a parcel.  So the Goshen, we already mentioned it, but I will write it again. So in one d  I will use the notation and and then we have the mean and the variance sigma square. So that's the mean  And the distant variance and here the sample space is the all the real numbers.  And somebody pointed out that I screwed up.  Ah. Alright, so this person says, I should have  A  So here I should have some of n minus x side.  That's the claim. And the answer is no, because here.  I had the, the distribution for a burden, yet I will remove this right so the distribution for a burden. He is just a theta race to the x i one minus data race to one minus x, because that's one coin flip.  And and what happened is when I will some this n times I will have you know that's why here. This is the same thing as n minus summation over i X i.  And this is x, right. So, everything works.  OK, so that was a little clarification, we go back to our distributions.  Are put space for Yashin is the real number. There's the gamma distribution. That's another standard distribution. So, gamma  Is actually  A distribution on the positive number. So, r plus  And it has two parameters the scale and the sorry the shape and the inverse scale. So this is the shape parameter  And this is the inverse  And verse scale.  Also known as the rate editor.  And you know this distribution looks something like like this.  So it's not like gosh it's like ish shifted think it's not symmetric  And then able to draw it.  Is it some kind of gamma distribution.  As a mean  I think alpha divided by beta square  But I'm not sure if I put it in the right place. So perhaps I won't put it  Mean of alpha divided by A square, but you can see all the nice properties of that in the  In the  In Wikipedia.  On the core any place where you describe us distribution.  So we'll see the gamma later they're pretty important a gamma with shape parameter. One is an exponential  Variable. It's called an exponential that special case and you can combine independent games together and we normalize them to get a dish a distribution.  And the version distribution is a distribution over the property simplex. And so it's super important when we talk about distribution over parameters of the distribution. I can move on. So we'll get back to that.  So, somebody asked what's the difference between a gamma and a better distribution. So,  Gamma is this mission on positive real number, a better distribution.  Do I have it here. Yeah. So let's talk about it. I'll just write down other distribution and then I'll answer your question at the same time. So other distribution or the Laplace distribution, which is kind of like a Gaussian, but instead of a  X squared dependence. It has a absolute minus x dependence or perhaps this is cushy and kind of mixing up now.  cushy distribution exponential i said this is the special case of a gamma and exponential decision not to be confused with the exponential explosion of family is much more general than exponential distribution.  The gamma distribution is a special case of the exponential family a Gaussian is an extension of me. So he all these that I mentioned here are all exponential families was the exponential distribution is a very specific type of gamma. So  So this is basically a gamma with shape one and then rate better  And then now we talked about the better distribution. So, somebody asked about the better description. What's the difference with a gallon. Well, a better is actually a dairy sugar distribution.  In dimension to okay so and so this is  This is dairy Schleyer  On two elements.  And the sample space for the beta is the unit interval 01  Okay, so that's the big difference. So the sample space for a gamma is all positive numbers. The simple space for a Becca is a number between zero and one. Okay.  And. And basically, if you have a coin flip. You want a parameter for your conflict, like a brewery. Right. So, and this parameter has to be between zero and one.  So now if I want to put a distribution amateur I wanted distribution on Joel one and a bit dies the natural  Now the direction is the same thing instead of adding parameter for one conflict. It's actually a parameter for key sided  Die. And so you need a proxy for every of the key outcomes and you have into some of these properties has to be equal to one. So it's a pretty simple. It's and to this day is actually a distribution on the participants.  Sense. If you have only two possibilities. You don't need to put the party of one and the other one because you know that it's data for one possibility has to be one minus data for the other one. So that's why you only need to put the property on the 01 interval for the better.  Alright, so somebody says, I screwed up my mean of my gamma selecting it's just check that. Because that's kind of important. So user avatar   You should user avatar   I think you might be quite right, actually.  Yes, yes, yes, that's right. It's kind of normalize the scale, the rate  Very good. So this is actually the mean good thing that people know their Gemma or parties is alpha or beta and the variance is  Alpha, beta square  Yeah, thank you.  What would be the input space of the Irish layer. It's the property simplex. Indeed, so it's it's it's so that the Irish layer.  Will come back when we talk about the multi normal in more detail. So I will formalize it but for now I think of it as the participants.  Which are key elements which seems to one, there are positive.  Okay, so these are some distribution. Let's talk about now frequencies invasion.  And let's talk about statistics.  And okay so somebody asked an important question question related to that statistical concepts.  So the question was, how do you know which decision to use to model a given problem.  And that's what will have hint of answering in this class.  And but it's, you know, this is a very difficult question to answer. In general, because this is basically this is like I have a  Problem that I'm trying to model. And I want to know what's their good distribution tomorrow this problem. Okay.  And I send it was a ill defined problem. So so the so the, the short answer is that it's hard and statistics are trained for it and they will use various  Knowledge. We have about modeling things to kind of, like, for example, like a good starting point is, oh, what's the range of the thing I'm trying to model if it's  On the interval 01 well then you need to have a distribution with range on 01 so you couldn't use a Gaussian, or you can say, well, you could not use a normal guy because this is all real numbers. So  That's a starting point, or if it is a discrete thing. I'm trying to move. So that's the first step. And then there's other properties of the thing you're observing that you could use.  But this is basically a statistical question and that's what we're going to cover now also. So, a reminder, the cartoon that I had mentioned at the beginning of class was that  Let's say I have a model and I will use data to characterize a specific choice of model because, for example, I could have seen a movie with Pandora theta is an example model and then I can use property theory.  To  Answer a question about data. And so I'll use capital X or random variable. And if I have instead of observations of a random variable like the output of n coin flips inverting this process is that this site. user avatar   Statistics. user avatar   So this is ill defined  Define inverse problems.  Was this is very well defined because you gave the models so  You just use rules of properties to get whatever policies, you want to compute the problem is that these might be intractable to compute to you need approximation, but at least philosophically, there's, there's no problem with that.  And I mentioned here by the way that you know this inverse calm of doing statistics. I have observation I want to figure out the model which explained his operation that's at the heart of epistemology of science, right.  System ology is the study of what what do we know the piece de mala G, we don't know how to write rewrite that  The piece de mala G, the study of what we know and how do we know what we know, in particular, right. So,  I have I made a bunch of innovation. I figure out a model how good my model is do I know that this is the right model. Like how do I know that right  And so that's really super fundamental because how do we know that we have quote a good model of the world. And actually, you know, you never know. You can never be sure unless you have an infinite number of observations which you never have so  That's why we cannot guarantee that tomorrow the earth won't explode. For example, and we have very good knowledge that it probably won't be the case, given the current scientific knowledge and prediction, but we cannot be sure with 1700 percent  Okay.  So now let's give a very concrete example. That's why I talked about the New Year. Yes. The binomial. So let's say I want a model and independent coin flips. user avatar   Okay. user avatar   And actually I model them as, oh, I only look at I know in this case actually model them as an independent conflict so i don't i looked at how I get it.  And so from the theory.  If I say the are the same coin flip that I'm keeping all the time and I say, it's, it's, I have  The same coin. So it's a Bernie like a Bernie theta model is pretty good for that. And so then I can compute, what's the probability of k heads in a row. user avatar   Right. user avatar   And that would be actually theta race to the key that's the reality of observing K reds and the role, but that's actually not the priority of observing them among and flips. So I want to put the the end flip, because I don't want to compute it, but you could compute it.  What's the property of if I did envelopes of observing a sequence of k heads in a row.  And then statistics instead will answer question like, Okay, I have observed  k heads.  And n minus k tails. So in this case I removed the order information notice before I talked about order, but now there's no order. And now what is data, right, what is the correct parameter modeling this user avatar   Okay. user avatar   And  Now let's look at the frequent is versus the Bayesian answer to that. Okay, so, so the short story is a frequent. This will estimate data with various tools.  Whereas a Bayesian will not estimate data, they will quantify their uncertainty about the parameter using a distribution. Okay. But let's before going to this formalization of this approach. Let's just talk about the meaning or probabilities for a frequent is resuscitation.  And by the way, so  Or La La asked what amount of data points. Do we need to sufficiently model the underlying distribution. So this is already a successful question.  And it's a question of as a lot of things you need to answer before being able to answer period this question. So what does it mean to sufficiently model.  And so, in, in particular when we talk about statistical decision theory, we will talk about that. So if we formalize what we mean by by  Evaluating the, the quality of our model if a loss, and then you can talk about things which are called sample complexity, which is how many samples. Do I need to guarantee a specific error. And this is according to my mind my surgical procedure.  But it takes a bit of  It takes a bit of setup before answering  And says, probably theory deals with in France. Yes, whereas statistics is about model fitting. If I were to independence with mission terminology  Sure.  That's, that's, you know, these are correct key words.  Yet  From his vision. Alright, so what is a priority.  What is a property. What does it mean, okay.  So,  IE. What I mean by semantic so semantic in language is talking about meeting. So what is the meaning  Of a property. So now we're a bit more in philosophy again. But if you're a traditional  I mean, so the frequent this perspective is more classical let's put it this way. So the frequent his perspective.  On Property is basically that the property. When we say the property of capital X is little x  This represents  The limiting frequency  Oh, am I sharing my song. user avatar   Yes, it's not sure user avatar   The limiting  Frequency  Observing  Capital X equals little x if and that's a huge if  I could repeat  An infinite number of times.  The ID experiments.  Which were characterized by this red number  Right, in some sense, so so capital X equals little x, it's it's modeling some phenomenon like I flipped in coins independently or I could actually flip  Coins in a way that are dependent, if I want. But let's say I have a distribution and on this capital X.  Well, this character is a phenomenon. Now I say, oh, I will make observation in this phenomenon.  repeating this experiments such that it's always the same phenomenon. So it's always the same distribution and the results are all independent. So, because if there are dependencies. Then of course there's there's no repetition.  So for example, I could flip a coin and times.  And then I start again flip a coin and times then start again 50 times and all these are independent. So that's what the ID. So that works. And then I looked at the fraction of time I observed that I fit Kate heads.  And when I looked at the proportion of time I observed key heads. This will be the policy of observing tickets  Okay, so that's what the meaning of her. That's why it's called for this perspective, because it's looking at frequencies of observations over infinite repetition of a fundamental  And we'll see later in this class that this can be justified from the law of large number and central limit theorem day. But so, but first  Let's talk about. By contrast, the Bayesian  Perspective.  And so as a Bayesian and this is actually the subjective Bayesian version.  So the probability is not a limiting frequency. The property is actually encoding or subjective belief about something.  In codes and you know if I'm getting a bit AI ish artificial intelligence, like we talked about in agent. Because actually in artificial intelligence often use will have artificial agent and so it's the belief  That capital X is equal to little ticks from  And basically, so why are we carrying about qualities. When revision. Well, what happens is if you pick any Bayesian textbook, it will start about  How to act rationally what it means to combine information about events in a rational way and they will say that the laws of probability, like the combo grab axioms. user avatar   would characterize user avatar   A rational  Way to combine beliefs.  Beliefs and evidence user avatar   Which are the observations. user avatar   Okay and this, this notion of rational way to combine beliefs using the laws of properties. So it has motivation.  In terms of  Gambling  Utility Theory.  So utility  Or decision theory.  Etc. And basically, when you read these details treatment they will show that, oh, if you don't combine your so there's always an operational ization I have these belief and then I will act according to this belief. And if I don't use a lot of properties to combine them.  Then you get a quote screwed up somewhere like you you somebody could win money against you if you gamble using these beliefs or  You know you won't act optimally according to this.  To the framework of utility theory. So, so there's so you would put a lot of like properties that you want to what it means to act rationally and to do things and then a good example is, okay, well if if I if I gamble.  In a phenomenon where there's uncertainty and if I use it as a properties in my candle well on the long term, I won't be silly newsman I will be even  Game, but you won't lose money. Whereas if if you don't use the laws of priorities and somebody else is then they will win against you on the long term. So they will make money on it, right. So that's the justification.  Alright, but the bottom line is, in this case for them.  Then where you get this distribution comes from encoding your belief and then how to combine them in a in a  In a good way. And let me make this a bit more concrete now.  By saying that operationally what happens. Okay. So in practice.  So the Bayesian approach.  What they do.  Is  It's actually very simple philosophy. So Bayesian statistics is actually trivial from a philosophical perspective, just very neat. And that's why, by the way, Asians often are very opinionated in their viewpoint, because they have such an elegant theory.  Feel us physically user avatar   Very simple. That's simple philosophically. user avatar   So you treat all uncertain quantities.  With as random Berber  IE, you put a distribution of them and the distribution represent your beliefs right as random variable.  And so the idea. So the way you go about the world is you encode  All knowledge.  About the system.  Which are basically your beliefs.  As a prior  On probabilistic models.  And then  You use the law, probably to  Rule, which is a bit where the name come from.  To get  Updated beliefs.  And answers.  You're flipping a coin. I don't know what data is well then you put a distribution of everything.  You don't know what the distribution of data should be well then you put a distribution over this distribution that you could ever say that so you know you encode anything which you're not sure about anything which is uncertain. You need to put a distribution.  Above it, and then you can, once you have distribution, you can just use laws or policies to actually  Update your information. So, for example, we'll see later that I will have my prior theta i have my observation Model X given data. Now I have observed x when they can update my  Distribution or theta, which was we call the first year. So I'll show that in a few minutes. Yeah, I'll get back to that.  But that's here, just the general kind of like philosophical approach.  Then let's see if there's some questions. I don't want to go too much into details of this yet, because I want to illustrate it with an example because it will become much care right now. It's a bit abstract,  Let's see. So, so as a Bayesian subjective based on who the agent is while frequent. This is absolute objective truth for everyone. So the subjective viewpoint is differently.  Subjective I mean the beige viewpoint is subjective. So it depends on who it is. And that's what makes it kind of problematic when you try to do science with it.  The frequent. This is not to see that objective is just that.  It's objective in the sense of, we think there's a quality of something and the meaning of that is if I take your limit of repeating the same experiments, it will give me the the correct frequency  But there's a question there, which is, well, how do you make sure that their experiments is really properly repeated. And so there's a bit of uncertainty about the experiments itself. And so it's hard to kind of like characterize that. But I guess the role the the the frequent this  Goal is indeed to be a bit more objective.  Okay, so how do we update  Or or belief about data will sit very soon it's using the bedroom, but just before I get there.  Let's go back to the to the frequent is justification.  So,  I think. Let me just see.  Yeah, so  So if we go back to the we're doing statistics. Right. So I said,  So we want to do statistics on a bit annoyed disappear so fast. Perhaps there's it's sitting for that.  It's kind of weird. So  Basically, if you have observation as a Bayesian all you do to do statistics is just compute conditional priorities over your data because data is also model by a distribution. So there's  Because everything is in the prostate model you can just use the probably the last property to do statistics.  If you're a frequent this you need to use went from observation to estimate the parameters you need to use some principles. So it will be maximum likelihood  method of moments maximum entropy will see a few of those. Okay. But there's no unique clinical way to get your, your method. Okay, so that's the frequencies approach, but now a bit of justification for the frequencies semantic of problem D.  Because I said so justification. user avatar   For frequent just user avatar   Semantic user avatar   So,  If I have four discrete  Oh yeah, I'm not able to write what's happening.  For a discrete  Random variable capital X. Suppose that the probability that x equals little x is equal to theta, right.  So by the laws of morality, it means that the probability that X is not equal to x. This the compliment event is just one minus state.  And so now if I define a new random variable capital B which is the indicator of the event X equals little x user avatar   So this is the indicator function. user avatar   indicator function. user avatar   So, normally the notation. Is this funny one with us set as subscript. And then it's a function of  So I guess this is the preps. To be clear, this set should be a subscript  Okay. And so this thing is equal to one if you belongs to set and it's equal to zero. Other words,  Right and so capital B will be equal to one when what I've observed is that capital X is equal to little x and it will be equal to zero otherwise. And so then be as possibilities, zero and one. And that means that be is actually model. According to a movie.  With parameter theta, right, because that's the quality of capital X equal equal to x. So it's a very random variable.  And so now if I repeat my experiment that I said, so if I repeat, I ID experiments.  I he  I will have that. Now I have big eyes which are ID Bernie's with parameters data.  There's this thing called the law of large number in property theory. The law of large number  Which says that when you look at empirical frequencies of some of independent variable.  Of large numbers that converse to the expectation  Of large numbers.  So in this case you have that one over n summation over i.  Have BI. So, this  Frequencies of observing bi equals one converge. And this is actually almost sure convergence to the expectation of any BI. So let's just be one and this is a brilliant use of equal to theta.  So this is by the law of large numbers and this is basically what I said was the limiting frequency  Okay, so this is what we said we said that the cementing reality is that if I observed this thing and if it number of time that this limiting frequency will be equal to quality because data was a problem.  And there's another thing which is quite useful in property theory is the central limit theorem.  And then in this case will have that if I re scale, the deviation of the summation of bi, which is basically one over n a binomial and data. So this is basically what this is.  And I, and I guess I'll put  I will subtract the mean, which is data. Well, this converge in distribution to a normal with means zero and the variance of my random variable, which is this case is theta one minus data.  And if you notice here I have scaled by square with. And so basically it means that if I divide both sides by score and I will divide by n, the variance. And so I get  So we'll divide here by and if I remove this thing here. And so I get that as an increases, I get a very concentrated normal around  So not only I have that the user avatar   This user avatar   Yeah, so this also tells you how fast it converges to to this deterministic universe. Okay. But so  Some I mentioned these just to give you useful to remind you, or to put pointers on useful tools from standard property theory like law of large numbers in central  We theorem are very important concepts in terms of synthetic behavior of independent random variables. And here it's also to justify the  The meaning of frequencies property. Now there's some questions and see  Are there any message that involve updating beyond the parents.  OK, so somebody asked  The deep question. So the sign says all the Bayesian approach and codes more about pattern because it gives a whole distribution was a frequent this approach only good point estimates, which is true the frequent is only tried to give you evaluate for theta.  What would be reasons where one would want to use frequently instead of vision.  So I will go back on this question and more details. Once we see properties of estimates. Okay, but the gist of it is if you have a good idea about what is the phenomenon that you're trying to describe. So for example, I have, you know, I have  flipped a coin and times. What is your prior about the quality of this coin like what the parameter of this coin. Right.  Have you observed a lot of time coins in the world in the past and and so well in my experience, most coins are close to be unbiased and so I should you put a distribution which is fairly concentrated around one half.  Well, if this is a good information using this in your estimating procedure will give you usually better performance than just  A pure frequent this approach, which is a bit more agnostic about this prior beliefs. Okay, so if you have good knowledge about the world, being a Bayesian usually will be  Will give you much better result if your, if your knowledge is wrong or if you have not included it correctly, then you will do worse than  A method, which was a bit safer and it's put it this way, but we'll come back like usually Beijing methods, even when you have wrong beliefs are  So good properties was the frequent this methods like maximum likelihood is actually extremely bad in  Some perspective in particular we'll see we'll talk about admissibility the procedure later and the maximum likelihood estimator is not accessible. So that's pretty pretty bad, but we get back to that.  Okay, now there's a lot of questions.  Oh what is in the parents assists of. Yes. Okay. So, this is  feta one mistake that  Perhaps I'll just put here.  Okay.  These are the buyer system either here.  Well, if you want to use the to estimate the probability of x equal little X. Yes, it's I'm biased because an expectation. It has stayed out which is the truth.  Okay, so  The plan now is we'll take a break a break and then I will illustrate the Bayesian approach in this coin flip example.  But just before we get there, is there any other burning question on this so so  I will get. I think it's important to see a few examples to kind of be able to to answer.  These questions that you have. I think all these a lot of the questions you're asking right now are extremely natural and they're important questions. It's just that also you have to be a bit patient because we need a bit of setup to be able to to  To be able to formalize the answers to this question.  And be as a random variable. Yes, capital B zero numbers.  Okay, so let's do a break. It is 232. So let's come back at 242 me  The zoom recording  Welcome back.  So there were some questions during the break.  So there's shank Darshan sorry says that if you're Asian and there's a wreck or recall that you trust. Hundred percent and give you the value of the quantity for which you were uncertain. How would you update  As well. So this if like basically the Oracle would have to be encoded in a way that it updates your belief, according to the laws of properties in a meaningful way. So either. I know record could be I have observed that number of times something, and that will  Basically concentrate your posterior to the correct value. So, and we'll see that very soon. But basically the idea is as a Bayesian you could tell you have a posterior which is contracted  On a value when you're searching, because that just includes your belief. Now that you're searching about something, just like  In a command for that guy who mentioned about can trigger prayers. You don't have to use to get prayers for as a patient. So it's more a  It's a. How could I see  A convenience or it's a simplification tool, but  You could only be a Bayesian and still not use conjugate prayers. That's fine.  Okay, so let's put this into practice. Let's be put our Beijing hat and how does that work. Okay.  So, oh, I don't want to put a yes here.  Let's delete. Oops. Okay, let's not do that.  Oh, yeah, yeah. What did they do.  Okay, well, I'll have to fix that later.  Alright so coin flips, let's say, now we're going through this conflict example.  And we are be a big  Alright.  So suppose that we're basically modeling a bias coin flips.  And so we believe that  There are an ID coin flips.  Already that's an assumption. Right. That's a belief in some sense.  So busy are likely model is already some kind of belief as evasion and so will say, oh, well,  Capital X is model as a binomial with n conflicts and Pamela feta. The problem is  And we'll assume to be known, but theta is unknown. Right. That's where we have uncertainty. So I observed my confess I don't know what's the correct time. And so then, as a Bayesian if this is a noun, you need to model it  As a random verb.  Eight random variable. And so we need to define a distribution on data. So we need to we need a distinction of repeat data will use P of data that's a density because data here is a continuous value and this thing is called a prior distribution.  Usually when we have parameters and models will talk about the distribution we have on these parameters which is basically our belief about this.  Parameters will be our prior belief because it's before any observations. So the sample space of this random variable, which I'll use a capital version of theta, which is capital theta is the interval 01  So already, the beta that we talked about earlier becomes convenient for them.  So now the question is, okay, if I observed something. How do I update my belief. Right. That was the question of doing statistics. So suppose we observe  Capital X equal equal x which is the result of an flips.  Then we can update  Our belief about data.  Using Israel.  Right. So originally marginally I might believe about what's the bias of my coin is p of theta. Now I have observed capital X equal little x  And so I can use the joint to update my belief. So what I would compute is I have observed capital X equal intellects, so I would compute the posterior, which is, what's the probability that  Theta as a value capital theta is equal to little theta. When I have conditioning that I've observed capital X that's a conditional positive  And so by based rule. This is the same as the probability that typical X equals X given data. So I can invert a conditional and then times the prior  And then you to read normalized by the margin all over x just  All right, and so this is where we call the posterior believe. So this is the belief. After observing something  Which is basically or updated belief.  Which could be, by the way, us as a prior for further experiment because this is our updated prior because we, you know, see more stuff.  This is call our prior belief.  And this thing here is called the observation model.  For the like you  Like you're observing capital X equals X.  And this thing, the little p of x here.  This is basically a normalization factor to make sure we have a distribution, but it's in Beijing terminology. It's also called the marginal like you, because you can obtain p of x by integrating the joint.  respect to theta and marginalizing out basically theta and just be left with x. So, this is called the marginal like to  So,  Theta was a continuous variable. So, a bit of notes. Right. So I already mentioned that in the past but P of X given data. In this case, this is a PMS because f x was the screen but PA feta. That was a PDF because  Theta is a continuous  Random variable in this case I kids values in their, in their continuum on 01 right  And so the joint. I can still define the joint on x and data by multiplying these two together, by definition, and so this is actually a mixed distribution. Right. So it's, it has a PMS part and it has a PDF part but that's fine. I told you earlier that this is totally fine. Just  Okay.  So that's the mechanism of updating our beliefs. And so let's do an example.  So if we use a uniform prior  Suppose that we choose as our prior it's uniform  On zero  Okay, because let's say after Yuri. We have no idea what should be the bias of our coin. So we just say, Are we just choose uniform passivity zones around. So this is kind of like, quote, no specific preference.  But, you know, why is really uniform no specific, it is a bit of that. Then the question of when you don't have any information about something, what should be the uniform  Version of it is actually not that clear. And we'll see that when we do maximum entropy later in the class.  maximum entropy is another way to have no specific prior about something, but it's still some kind of pride.  But we'll get back to that. So this is a very subtle point anyway. But this thing at least very natural. At first sight. I don't know what's the value that's put the uniform distribution of 01  Okay, so now we have that the posterior of data given x, this will be proportional to the joint which will be delighted. So what's the likelihood for binomial, we said it was feta race to the x.  One minus data race to the n minus x. There's a constant, but which is the binomial put coefficient. But this doesn't depend on theta, it only depends on an x, so I don't need to put it. And so this is basically my life to P of X given data. This is cheap, up to a constant.  That's very important.  Because here. That's something I told you, like, one quick way to do  Posterior or Bay's rule computation is you don't care about the constant because at the end of the day you will read normalize to make sure it's some to one. And so that would tell you what the value of that constant. So that's a way to do quick  Quick computation. So recall that this is proportional to  An X here is fixed, so X doesn't vary so anything which has x in it is a constant for the, the, the purpose of this competition beta, we want to compute the distribution of our data. And so now we want to see how those data that are used with x fixed  And then we have the prior and the prior is basically  The indicator on the intervals are one.  Because it's basically one when  Data belongs to 01 and zero otherwise. It's just a  constant density and zero.  Okay, so that's our that's our distribution for a given x. And so that we can compute what the scaling, you know, what's the normalization right so let's do that. So the scaling. I need to integrate  This thing on all possible values of theta. So that's the integral on 01 of the joint data rest of the X one minus data and minus six, and then the indicator disappear because I only integrate between seven one d theta.  And what is this. This is actually the beta function. So this is a this is an integral, which doesn't have any clear, simple value. So this is actually, it's a special function is called the beta function and it has to  Argument the beta function. So it was an issue. This is called beta of x plus one and and minus x plus one.  Okay. And so this is the normalization constant  So that the integral of pure theta. user avatar   You know, user avatar   P theta to have failed I given x the data equals one. Right. And I don't need to put it. So because we have this problem.  Somebody asked a question.  And minus x plus one.  No, it's n minus x plus one.  Because basically, you know, you have a minus one in the in this game.  And so  The so the better function in origin or also, by the way, the better function. So be  Capital be a little a big. This is the better function, by definition, you can actually obtain it from the gamma function which is gonna have a schema be divided by gamma of A plus B. user avatar   So this is called a bit of function. user avatar   Oops. user avatar   Just, you know, giving you a bit of a crash course on special functions. This is the beta function. And that's the gamma function.  And other special function. And by definition the gamma function of a is in the integral from zero to infinity of you a minus one e minus you be  And turns out that when you put a natural number. In this case, you get two factorial. So, gamma, and I think is an n plus one factorial, one of these two I forgot which one  Okay. And so that's or normalization folks constant. And so it turns out that this distribution PFA like given x that I wrote above is actually called a beta distribution. So, that's the density of a bit that distribution that I  Mentioned earlier, it's a distribution on 01  And what's the beta distribution. Well, it has a density. So beta on theta. So the density on theta with parameter alpha and beta of my beta distribution is defined as data race to the alpha minus one one minus data raised the beta minus one. I have the  Indicator on 01 because it has to be between zero and one and it's normalized by beta, the beta function.  Right so and so now you see why I had user avatar   The user avatar   Plus one here, right. So, because I have this minus one here. So I take my my coefficients. And I have to to get minus way to add one on each of these  So the alpha and the beta or the actual parameters of the beta distribution.  Okay and so  By the way, so that's it. So, so if I had a uniform prior and I have a binomial likelihood model.  And then I just use the base rule to update my prior to the posterior, I get that. My posterior is now not a uniform. It's a beta with some parameters which are given by the number of times I've observed heads.  And to make these relationship, a bit more clean. So note that the uniform distribution.  On 01  Is obtained by using a beta with parameter one in one, right. So if I put one in my  If I put alpha equals one, I get one minus one which is you right so I don't. So I get. And if I put one for beta i get that these disappear. I just get one.  Turns out of the beta function one one is also one. And so I just left with the uniform distribution. So the uniform distribution is a special case of the beta distribution.  And the post. So the prior was a beta distribution and in this case here to put steer is also a beta distribution. So that's why you also you call them conjugate  And as  So the posterior  Is a beta with observation x and then I added one in an n minus x, which was my observation plus one. And so now exercise to the reader exercise to the reader.  And actually, I think this is a special case of your assignment. By the way, which would you do with the direction. So if you use instead of beta one  beta alpha zero, beta zero as the prior  Because the uniform was beta one one, right. So,  Then the posterior  Will be  Beta with x plus alpha zero  And n minus x plus beta zero  Right, so the parameters of your bed on your prior or like prior counts. And so when I observe X, which are the count of time I see heads. I will now have a theater with new counts in my editor, which will be the prior counts, plus the observe counts. It's actually pretty cool.  Intuitive updates.  So, somebody asked  How would you update the prior based on further results.  Well then you could take this posterior as your new prior and let's say now, we observed another bunch of flips and then  You would  You know, just get a new post here and here by the way you do notice that because each time you observe  A head you add one to the parameter of your beta, you can think of. I've done and flips and I will observe X ends and that gives me the update I just wrote, or you could have done it.  And updates. Right, so I observed one fit and then I get one and zero. So then I update as my new posterior, I would just have  Plus one on the on either the alpha parameter or the beta Pamela depending if it's a head or tail and then. I repeat,  And then I do the End Times. I would get the exact same posterior and then if I just do it in one shot. Like, I've done so.  That makes sense, because why should it matter how you split things up. And yeah, that's one of the beauty of the, of the, probably stick framework which is fairly consistent in this regard.  Any question about this.  Okay, so, um,  So that now I have my posterior okay and as a Bayesian the posterior  So the posterior  P of Theta, given my observation. I'll use annotation little capital X little, little x just to emphasize what was my  My random variable that I observed  This thing actually contains all the info.  From the data.  That we need  As a Bayesian to answer.  New queries.  About data.  Right, so basically a Bayesian had some prior information about data was uncertain, but, you know, it had some prior information, then you made an experiment you have observations about X and then you update your  belief about data, then you can just get discard this data because you're all all the information from the data is already coded into position.  And so now, what kind of queries that could ask. So, for example, a question, I could ask  A question that I could ask is what is the probability  Of head.  On the next clip.  And it's called an F equal one.  Random variables. So let's say the F is the flip outcome. So it's a brilliant year in a variable on the next  I'd say I've observed a bunch of flips from my  From my coin and I say, Okay, now what should be the probability of getting head on the next clip. Okay.  And so as a frequent this if you're a frequent is  You would basically say, well, the quality that f equals one, given my data.  Is just data that we're theta is some data hat is an estimate of failure. So, so this is basically notation.  To mean  Estimate and so we'll see. We'll see that very soon. When we talk about estimator and theory of estimators but basically the Hatton location is often used for an estimate of a very of quantity  But as a Bayesian  You don't do point estimate as a Bayesian  You only use probabilities. Right. So here I'm asking about it. Probably piece or you're asking actually about uncertainty of something. So now you just look  OK, well, the quality of capital F equals one given capital X equal little x. This is just a problem statement, this is just something I can use allows the qualities to compute a while so then I will just  Obtain that by integrating that out the uncertainty about data, right, so this is by definition of laws or policies. This is the joint of ethical one and  Theta.  Given  X equal little x the theta.  And so then by the product rule.  Then this is just p of f equals one given data and capital X equals little x times be a theta.  given x equals x the theta. So now the magic is this is the posterior that's something I already computed  And this by our model.  We said that all the coin flips. We're in I. D. Right. So, so what's the semantic of this. Well, the property of my next coin flip. When I know that the parameter of my coin flip is data and I've observed  Stuff in the past. Well, the thing is, when I know my parameter feta, what I observed in the past doesn't matter anymore. I know. Was the parameter, right, because that's by the model that we had, we had that this is just equal to say that by or model. That's how we modeled the data. user avatar   Okay. user avatar   And so as a Bayesian  We would compute this marginal probably to hear marginal Overwatch lover theta and this turns out that this is also just the integral of our data of Seta the posterior  And so it's actually the hysteria me. Okay. So, by definition, this will be the conditional mean of feta, given my observation.  This is called the posterior mean of data.  And so  So this is just to tell you if I want to compute the property of the next coin flip, using the laws of properties. That's what I get. Right. And so, but because the property of a conflict is represented by feta. So you could say that a meaningful.  Bayesian estimator.  If I force you to give me a point value for theta.  And I'll use the annotation data base and then it's a function of the observation, so that all x would be the posterior me  Good questions.  They will be partially answered with what I'm seeing. Next, so I'll first say, the next thing and it will answer these questions.  So to make these things concrete in our current example.  Oh yeah, so the notation.  So the notation.  Is data hat is a function of the observation to the parameter space.  So here, capital theta represents a parameter space, not the random variable, but I'm using the same notation, or two different things. Again context that will happen a lot in this class. So in our coin example.  And  We had that the posterior  Was a beta distribution over a theta get with parameter  Alpha is x plus one and beta is n minus x plus one, right. These were the parameters. When we use the uniform prior  And so the mean of a beta random variable.  Turns out is alpha divide by alpha plus beta. So it's the normalization proportional to alpha and you normalize by the two parameters.  And thus,  The base estimator.  The posterior mean estimator of our random variable in this case is  If I have x plus one.  Divided by x plus one plus and minus x plus one x cancels out. And so I get  N plus two. They have two plus one. user avatar   Plus two user avatar   So in this framework with a uniform prior. This is my estimator of the property.  And so  The first thing that  You notice is that this is actually something called a biased estimator.  And usually Bayesian estimator will always be biased because they come from your prior bias in some sense.  Meaning that when I think the expectation over the possible observation.  Of the estimate based on this observation. So, theta hat is a mapping, which they observation to a possible value of the parameter. And so now if capital X is my random observation and I look in average what is  My estimate value of the parameter. So that's what I just wrote, and this is not equal to the true value of the pattern because  The expectation  Of  So the expectation of over x because this thing here.  So here, this is equal to expectation of capital X plus one divided by n plus two. So by linearity. This is just expectation of X plus one divided by n plus two.  Plus, and we set the expectation of a binomial is n times data. So it's n theta plus one little by n plus two.  Okay and so  This is not equal to theta. So there's, it's a bit. It's a bit smaller than data, but it is a sensibly as synthetically unbiased as sent particularly  Unbiased  Because this converge as n goes to infinity to just data right so so when you have more and more observations in expectation your estimator will be the correct value.  And so you can compare  Or and contrast  To with the maximum likelihood estimator, which will we read arrive later. But it turns out that the the maximum likelihood estimator is a very nice frequent just count. So it's just x divided by n. And so this is actually unbiased right an expectation  I get that expectation of X divided by n. This is just equal to end data divided by n, which is equal to Kate right so the maximum I could estimator here is unbiased.  Okay, so  Sorry. So there's a few questions. So if two people have different priors. It is the case that a lot observation there posterior will converge to the same if their prior are consistent with a phenomenon. I need to put nonzero probably t  On the correct parameter of the phenomenon, then the posterior will contract towards the true value and so  Yes, whatever the prior you had if it's a meaningful prior when you have an infinite number of observation you converse the right value. So you get to the same conclusion as the other person, which is the right I get a peek was there on the correct that.  So somebody is asking about uncertainty about the property of f equals one estimate  So in this case,  I  Yeah, so that's a good question. So basically,  There's a question of estimating the property versus what's your belief about the probably team with our. What's the probability  Let me say so, the outcome is I will flip a coin on the next point I will do another confit. Okay. And so then it will either be one or zero.  But I have on something about that. So I need to encode this uncertainty with a property.  So as a Bayesian that it's clear. You just put probably to for that. And you can use a lot of property with is what I've just done to compute that. Okay, so  Here, there's no estimation or property because the property is already included by your belief.  Nice. You say instead of saying, Oh, well, I will flip a coin and this actually has a property. And now I want to estimate this property. So that's a different question intubation question. I just answered.  And actually what I've done here doesn't tell you that right because then you will need.  Probably t over this property and this is included by the posterior, in some sense, because the posterior or tell you what's the posterior over theta.  Which is actually what the property of the connects conflict is and  It's a distribution. It's not actually not a point estimate right so so if some data has very high quality will you're more certain about that. So, so the posterior is encoding this uncertainty about the quality of the next clip.  Is just a quick overview or we will revisit this technique in more depth. I'm a bit less in the calculation terminology here.  Alright, so now why is there an n  So what happened there is that we were doing n coin flips. That was the observation and I'm asking something else which is what the property of the next concept. So it's not the same thing as what's the priority of having  Some result for n conflicts. So that's why there was an end and observation, but when I look at capital F. There was no no anymore because it's just one conflict.  And whether we will review these in more depth. Yes, we will review it more in more depth in the Bayesian lecture which will come much later in class.  And so right now. This is considered to be, you know, like already giving them the basics of the Bayesian operational approach, but we'll see a bit more information when we talk about properties of estimator.  About what are the properties of Asian methods.  Does a better distribution always have to be the posterior case of concepts.  No.  It depends, of  The know because if you put a really, really weird prior on feta, then the posterior or want this to be a beta distribution. Right, so  So it turns out that the the bangle y'all. It's called an IQ of model and the beta prior or conjugate, which means the posterior will also be a beta, but if I use a different prior like I don't know a Gaussian normalize on 01 well then the procedure is definitely not a bit a bit of distribution.  So there's a bit of a mix of terminology here in this last question, which is, if we start with a uniform prior and we choose data had to be the maximum likelihood and austere, then the estimate would be unbiased so  What he means it's not maximum likely what what you mean is if I take my posterior which has a Bayesian and cold all the information I want about the also the data.  But then there's a frequency, which comes with you with a gun and says, Hey, give me an estimate a point estimate  It says, No, no, I have a whole distribution. It says, No, I want to point us to and that is a question of what's the point estimate to us given up austere and the post to your mean is actually  One of the very meaningful one and we'll see when we talk about sensical decision theory. Why is it meaningful.  But another one that people often do is, oh well, give me the most likely value, according to my post here, which is the map the maximum AP story you value.  And so if you do the map in a poster where the prior was uniform, then it's the same thing as a maximum likelihood. And does the answer will be the same as makes makes you and does it would be biased because makes I'll make it wasn't based so that can answer this question.  Okay, so let's  Wrap up here and I'll start. So the maximum liquid principle and then I guess we'll have to finish next class.  And where was I so to recap, to summarize, to summarize,  All these concept.  As a Bayesian  All you care about is to get a posterior  And then from the posterior you will use pen battery no plus US law. I need to charge my pen between things use law of probabilities  Whereas  In frequent is statistics.  Which is basically more the traditional statistics.  What you do is you consider multiple possible estimator.  An example would be a maximum likelihood estimator, you can use the moment matching principle which could give you a different value, you could use a Bayesian austere mean that's another point estimator. Right.  Which will depend on your prior. So for each party will have a different estimator, you could do map, you could do regularize Emily. So that's something you've seen admission earning you add LT realization to your maximum likelihood estimator.  And many more. All these are different estimation procedure. So then the question is, well, which one to choose.  Right. And so in order to get a bit more order the frequent is statistic has spent a lot of energy to analyze the properties  Of these estimated their statistical properties.  So in particular, what's the biased or is it unbiased.  What's the variance of the estimator. How does it vary when when you change the data.  Or D, consistent it when you have infinite data does it convert to the right thing different.  So we'll talk about these  In the next lecture when we talk about statistical decision theory was formalized what we're trying to do and then properties.  And so this with this tool bag of like another sis then statisticians builds kind of a kind of a sensor.  A library of oil in this situation, it makes more sense to use this procedure and this intuition. It makes more sense but  It's kind of an arbitrary. There's no absolute certainty of there's no notion of a best method in frequent the statistics was as evasion.  If you can correctly encode your belief, then the best method is simple use bedroom. The big problem is, well, how do you include your belief. Where did he come from and it's super hard, so that's  Probably one and problem two is also computing this posterior or using polities usually you get these intellectual intractable integral  And so you need that proximate methods and so everything has approximate anyway. So even though they have this optimal procedure.  First their beliefs come from a bit random places. And second, they use a lot of approximation. Anyway, so, so it's even though, in, in theory, it's very elegant and practice, it's really hard to make it work very well.  Okay. I think people have inserted their own questions. So let me briefly.  Website it to  What does it mean for an estimator to be consistent. I will formally defined that again in the future. But it basically mean the intuition. It means that as n ghosts and VT the estimator convert to the right quantity  So it's a symbiotic result. So let's talk about the maximum likelihood principal  Maximum likelihood  Principal  An elite.  Why do I I heard some sound of God.  Come from  So the setup is  I'm given a parametric family.  P of X data so that could be a PMA for a PDF, depending on the setup or a mixed. If you want to go more complicated. And then I have some parameters in some set of parameters and then we want the goal is, we want to estimate  Or learn. That's the machine learning terminology theta from observation.  From x x will be the observation.  And by definition the maximum likelihood estimator as a function of the observation, the data sufficient of X is simply the value of data which maximize the property of the observation. So it's a max overall my parameters of the probability of X given data.  And this p of x given theta.  We call this a function of data because right now X Division is fixed. So given us observation, what should the data. So it's a function of data. This is called the likelihood function. So I will use capital L for the likelihood function. This is the likelihood user avatar   Function. user avatar   And it's a function of data. It's not a function of x. Well, I mean, some sense you could think of it is also a function of x, but  Normally access fix. And so in other words we have that the maximum likelihood estimator.  Maximizes  The property of the data.  It's, it's the solution of a situation.  You know, so it's fairly intuitive. It's like, okay, I'm on all these parameters which one, give me the problem parameters, such that the quality of whatever I'm observing is maximum and makes sense because I observed it  And in the case of n conflicts, I said it was x divided by n.  But we will derive it formally next week. And so basically, now we're starting to do optimization right to solve the maximum likelihood problem. I need to optimize a function  So then we will. This will give us an excuse to review optimization theory. Like, how can we maximize function. Look at the derivative. Look at the stationary points, it should  And so we will first look at unconstrained optimization when there's no constraint of my parameter and then we'll look at constrained optimization. For example, in the case of them will turn on it. So that's the menu for the next lecture.  Any question before I end up this  Now, so  Semicolon will  Oh, good question.  So not all these semi colon is  It's not a joint normally I don't use semi colon for joint. But note that the joint and the conditional a proportional and but here, this is given. Yeah, so they are proportional and  And so  Max, by the way. So if I had p of x theta.  Oh yeah, no, no. So, this is this V, the conditional this case.  Okay, sorry. So somebody asked p of x comma theta, this is this is a if I was a Bayesian, this would be p of x given them.  Because as soon as you had a prior well if unless the power is uniform, then it will change the destination.  And some of our X given data equals one. Correct, yes. user avatar   That's a good question. user avatar   All right. I think that's also one note is that for the assignment for the last question when I talk about on bias and bias in the variance and consistent etc will will see that next week so so so right now you don't haven't seen for me enough information to DO do the last  Assignment. But after next week in the middle of next week you'll, you'll have enough information to do most of it.  Okay, so enjoy your weekend and I'll see you next week.
  The cloud that's when I would enter. Okay, so this is recording. So today, we're going to look at maximum likelihood estimation  And statistical decision theory. Oops. Oh, are you serious, this is kind of like having issues.  Statistical user avatar   Decision. user avatar   Theory.  And that's when we can talk about formal analysis tools for different estimators like if we want to compare maximum latitude estimators versus, say map estimate. Right. And we talked about which one is better for example.  And already today will be a good example of reviewing some interesting mathematical techniques and particularly how to compute the how to optimize a function, right. So, because that's important when we do maximum likelihood  And so that will be one of the first one example, among a long series of interesting examples of like math techniques that you learn in class. Okay.  So,  Yeah. So just a little review of last class at the end of last class I talked about the maximum likelihood principle which is  One way to estimate the the primer. The parameter for  From data to model data. Right. So I have a parametric family of distributions and I want to know what is the best parameter for my distribution which  Describe my data. So the idea is to find the perimeter of the distribution which maximize the probability of the observation. So that will be x  And so that's why we call the, the maximum likelihood estimator. So no, no notion often only talk about an estimator will be will have a hack notation and it's a function of the observations of the data.  Okay, so let's do an example.  In today. Let's start with an example.  Ml he  Example,  One.  So let's do the binomial example. Okay.  So suppose that  I have n coin flips.  So we know that  If I flip always the same coin. I can count the number of heads versus tails and this random variable capital X will be distributed according to binomial.  With parameter theta.  Theta is the unknown quality of head.  Here the sample space is 02 N, okay.  And we had that the PMs  For the binomial was the binomial coefficient  Excuse n know entries X always say the runway and then data race the x and one minus data and minus six. So that's the PMS. And so now, suppose I observe X I want us to make theta. Okay.  So I need to maximize over theta, the probability of X given data or proteomics.  So the PMs year is a funny function of failure because it's data race to the x. So it's kind of pointed me a function of data.  And so there's a trick that we use all over the place in priorities this sticks, which is that instead of maximizing the likelihood. We will maximize the log likelihood  Okay, so we will maximize the log of the likelihood instead  Of the likelihood  And the log of the likelihood. I will use annotation little l of theta for that. And this is called basically to log like you  And  First of all, why do we do that well because when we have. So when you have independent friend of variable you multiply the densities together the PMs together. So you often have these product.  Of simple functions. So here you have failed I raised the X that came from a product of theta, theta theta.  And so when you hit it with a log, then you get the x becoming a coefficient in front and then you just get log of failure. So it's a bit better behave as a function of state and we'll see when we try to optimize  Whites much nicer. But before looking at that, why can we do that well the justification for district is because the lug is a strictly monotone  Function. So the log function is strictly monotone in this case strictly increasing  And so what this means is that if I have some number which is smaller than other one that implies that log of A is smaller than log of be supposing that these numbers are in the domain of the function for for this is for a be positive.  And so because i. So, basically, which means that if  I have if I maximize  The lug of p of x data.  So the arg max notation means the maximizers have a function. So there could be many doesn't have to be unique. So it's a set  But in this case, if some feta star is maximizing log it means that log of p of x data star.  Is bigger or equal than all the other ones. And so by the minute. The city of lug. It also implies that p of x data stars is bigger than all the other one. So it's also maximizers you have the same arguments are max.  Of p of x.  So that's a cool trick. So it's not only the log, you can do that with any strictly monotone function.  But the log is convenient when you have products.  And so now let's look at the log of the PMs of a binomial. So the log of the binomial.  What I get is,  So, the product of terms become a son, right. So that's the nice thing. So I've logged of the binomial coefficients.  And this is just a constant for  Theta right respect to data. So with respect to theta.  And then I have x log of theta because the log of data to race to the x, the exponent goes in front of the log. So I just get excellent data plus and minus x.  log of one minus data.  That was the last time. Okay, so this is my my leg leg to you and for the  As a function data for the binomial distribution.  OK, so now I want to maximize this  This function over the  Correct parameter  So a bit of review so have have some function. If I have a end it's differentiable, what happens is if I have a local max inside the domain of my function or inside my constraints, then a local max will be a point where the derivative is equal to zero.  And then what happened is in one dimension is that the derivative before will be positive and then  And then here the derivative will be negative. Right.  So if you have a local max. You know when you come from the left, you will first increase  And I'm not sure if I'm doing in the right direction. So you will first increase then reach a plateau and and decrease right so that's a local max.  And so  You want to look at the zero of the derivative. So you want to look  For theta, such that the derivative of the log like you to respect to theta is equal to zero.  Okay.  So let's take the rate of of LLC. So the first term will give me the derivative of log, which is one of our theta and x is just a constant. So I get x of theta.  And. And then the second term, I will get the derivative of  Let's see. So the derivative of this thing here is one divided by one minus data and then I have the constant in front of it. Right, so I get. Oh, and then I need the derivative inside the parentheses, which is minus one. So I get minus  Minus x divided by one minus data.  Okay. And so this is the derivative of the likelihood. So we want this expression.  To be equal to zero. So you want to solve for theta so that it's equal to zero. So what I can do is just  Multiply everywhere by theta and one minus data, we assume that these are non zero for now. So we have x times y minus data minus n minus x times theater. So I've just multiply both sides by theta and women's data and then zero doesn't do anything. So this is equal to zero.  And so now I can  regroup things together. So I have  Minus x data.  And then here I have plus x data.  And so then I'm only left with x and minus and data. OK, so now I can solve it. So I get basically that say the star is equal to x divided by user avatar   Okay. user avatar   So just solving for theta. So that's the only statuary points here.  I eat only point where the derivative is equal to zero, which is not surprising, because the lug this function here as a function of theta is actually strictly concave  Would come back to these properties of functions later on.  Note, by the way, from a notation perspective, I will often use the star to indicate  That it's the solution to an optimization problem. It was often as  Solution.  In optimization  So the hat. I use often to say, oh, it's a  Misty mater and the star because it's the solution to an optimization problem.  Okay. And so from that. I mean, to be formal. Now we need to prove that this is the correct global max.  I will come back very soon on this. But for now, just trust me that this is the and, in particular, here it's it's it's a global max because the function that actually strictly concave, which means that it as a unique stationary point and this district point is a global max if it exists.  And so here, and we have that our maximum likelihood estimator for the binomial is just a relative frequency  Okay, so what I already had mentioned before, it's the relative frequency  Of the time I've observed heads.  That's the reality of it which kind of makes sense. It's pretty natural  OK, so  Somebody asked about first versus second order. So first order. So you can think of the order. It's coming. The terminology is a bit coming from a tailor expansion.  So if you do at Turner expansion of a function, the first term initiative expansion is called zero order. It's a constant and the second therm will have the derivative and linear term.  And so it's it's the call. And that's the first order part. The second order will have the second derivative and it could Radek term and the third are there will be third derivative which is Richard. So that's kind of the idea  What is the interpretation of it, of a million conflict. I didn't get it clearly.  OK, so somebody asked if the arguments of fed I had should be x and n  Map up up up up soul basically  Yes and no. So I would say more know the reason being that end is kind of like from the context, it's kind of in the context of the problem.  So all this. The only thing which matters here is, is what's the possible output in this experiment, the possible input and experiment. The observation.  And that's x was n is a fixed object given from the content.  But in general, yes. When we we start to look also about the behavior of an estimator when the the sample size very then we'll have a sequence of estimator.  And then we can actually put a subscribe to the estimate usually would. We won't put it as an input would just put like say that at little subscript and for example. So with so that when we talk about consistency, for example.  And now, somebody's asking about the interpretation of this. So to be clear, so the setup here is I have observed a bunch of coin flips.  And I'm asking the question, what would be a good distribution to model. My observation and as I said in the last class, there is no universal principle for that.  And so actually we never know what's the correct answer to that. Okay.  But there is a bunch of different principle that frequent this consider and one is the maximum likelihood principle. And that's what I'm describing here, which means that you will choose the parameter or redistribution.  In a family. So first you have to have chosen your family. And then in this family you'll choose the parameter which maximize the probability of the observation.  And so here are Patrick family, which was pretty natural. It's the binomial.  And that's basically coming from assuming that each coin flips are independent, and also that the probability of every flip is the same in terms of head or tail right so from that actually you get the binomial distribution and then deriving the maximum likelihood  Parameter in the context of the observation of see x heads. We got that the value of the property of the coin flip. Basically, which is given by feta is just the proportion of time we saw heads among and flips, which kind of makes sense.  So Allah will a ask about different estimated four different sample size will get back to that when we do statistical decision theory.  And Jacob. Yes, you're correct. If you are flipping for you two coins and you observe 13 heads, then your estimate of the probably to head as a maximum likelihood estimator will be 13 over 42  Okay, so  Let's  Talk more about optimization and  So basically, that the plan right now is we'll talk more about optimization. I'll give you yet another example on multinational  Distribution and sentence out. Now we can talk about Lagrange multipliers, which is kind of cool.  And then we'll, we'll talk about statistical decision theory and statistical decision theory will really clarify the setup of  How do we evaluate an estimator. What is the goal of an estimator. And how do we know something is good or bad. And also, and Sir Allah. Allah is question about like estimator for different simple sense. Okay, so that's the program.  So step one, let's talk about optimization, because we need to  Have the correct tools so  A bit of a review.  And we'll go in the mid more rigorous details about optimization. When I will review like Ranjan duality in the context of a super interesting and amazing equivalence between maximum entropy  And maximum likelihood in the exponential family, and we'll see actually these two problems are  Like orange and do all of each other, which is really cool. But that's a lot of fancy words. I just said, and you'll see that in a few lectures, I forgot. What's the lecture number, but it's a bit later in class and then I will you like orange do it and then I'll talk a bit more about  Fancy optimization, but right now already get the basics.  Alright, so let's say I want to minimize a function over some  set of parameters. Right, so, so, so that's constrained optimization ever theta.  So we've been doing like okay I was maximum missing the likelihood, usually in music in convicts optimization we talked about unionization because but you can just replace maximization with musician by putting a negative sign in front of the objective  So the important aspect is that when your function is differentiable, the derivative or if it's a multi variant function, the gradient a theta equal to zero.  And this is called a stationary points of f.  So when we talk about stationary points of a function  We're talking about in the case, it's differentiable. We talking about zero gradient point. So this is a necessary condition. So,  If f is differentiable  Then  This is a necessary condition. Let's put perhaps a star here.  This is Sir re  Conditions.  For  theta star being  A local men.  When  theta star is in the interior of the constant set  Okay.  And so  Very important to distinguish necessary and sufficient condition so sufficient mean that if you have that, then it implies whatever you want to imply, here's the three points is necessary, which means that  Any local men will have to have this property, but there's some  Points which will have this property which are not local men, right. So that's the big difference here.  And to make sure that it is a local men.  So to have the sufficiency. You can look at the second derivative, right, so you can check  The second derivative which in the case of multivariate will be the history and  The SDN of f.  At theta star to be strictly positive definite for a local min. So this would be sufficient.  So if you both have the zero gradient and the husband, which is tricky positive in a neighborhood.  Then  You get a local  Okay. And so this is basically just a generalization of the, the second derivative is positive.  It because now we're talking about minimum. So, second derivative positive mean that the slope start  Was negative or because there was a decreasing function, then it's zero. And then it's positive. And so the slope is increasing. Right.  Because if I go the other direction. That would be a local maximum level comment, or it could be especially point that sorry so subtle point. Okay, so basically the little  You know, here's a function  And so that's a local men. That's a local max. That's a local men. So that's local men local max.  Local men.  Local max. That's the global men.  And this is a settlement.  And all these were especially points all these had zero diverted. Right. And so what distinguish between local men or local max or a subtle point was the second derivative. So the second derivative was a positive at a local men negative at a local max and zero at a subtle point user avatar   In this case, user avatar   And just so that we're on the same  Page. The  When we say that the SDN is tricky positive definite. So we say a matrix is tricky. Positive definite if and only if, so that's one characterization. If I make a quadratic form out of it. I get positive number for all you have equal to zero in Rd  Okay, see if there are questions now.  Do we want to local maximum. This case. Well, I'm talking here about minimizing a function. And so I'm we're not talking about. So come next year, but sure. I mean,  Here I described both local max and local men for local men. You want the hesitant to be strictly positive PS tricky positive definite. And for a local max. It's just you want it to be negative different  I feel like I've already answered a question for any disagreement is not a distribution distribution.  I'm not sure I understand your question, so perhaps you can rephrase  Yeah, I still don't understand the question. So  So welcome to rephrase this question if he still has it.  Is there any other question about optimization  Oh, I have more comments. So let me give you more comments and then I'll see if there's more questions. So  So here I gave you necessary condition for  local optimum, which is that you need to have the degree is equal to zero, and I give you sufficient condition for a local max or men by looking at the, the question or the secondary right  So what about global result because this is kind of annoying. These are just local result again. So in general, all you have our local result on the local if you only look at one point, and it's there. If you can only get local results.  So in order to get a global results, you need to look at properties of the function everywhere, right. And so for example if the Halcyon of the function  Is a semi positive definite.  Are which is the same thing as  This definition here, but instead of a of a could put an equality on both sides. Right. That's what it means, then  If I have that the his in his PSD for all theta in my parameter space.  Then the function is said to be  Convex user avatar   Whoops. user avatar   These are nice functions.  Basically their function which which are Ball, ball shape. Because the derivative is always positive in any direction. So it's always increasing. So you get like this kind of bowl behavior. And so in this case.  Having the gradient of f data equals zero.  Is sufficient  For  feta star.  To be global.  A global minimum.  So that's the beauty of context function if you find a saturation point, then you know the discussion point is a global  Min, max depending if it's convex concave  Okay, so let me find the question of  Kinetic for Emily, if we're trying to find the light to you the view.  Of a Gaussian  Yes. So, so the likelihood. So if you have, if you. Okay, so here I had that  So the binomial here could also be have been seen as the result of n coin flips and then just looking at the likelihood of the end conflicts, right.  Of the observer income as a function of the parameter theta. So for the next question is, okay, well, instead of having and conflicts. Let's say I have an observation from a Gaussian  And then what's the likelihood of  These an observation of a gotcha. And so then what you would do, because you're suppose their ID, it would take the product of the densities and then when you take a look like to you get the sun. So, and will actually see that I think of it later when we do linear regression  Okay.  Yes. No problem.  Ah,  So in order to get a global result either you you have a context function and then you're you're happy, or I mean this is not exhaustive and you can also have non smooth.  Function which is like non differentiable function and it gets a bit more hairy. I don't think we've covered them in this class, but in my advanced search or prediction method. I'll talk about some gradient and generalization of derivative  But in this fast. We'll talk about smooth function, and in particular in the assignment. You'll also I think at some point have to deal with that, but  Basically  If you have a smooth function.  IE differentiable continuity differentiable  I mean an optimization usually smooth means that the derivative is as Lifshitz gradient, but here I'm a bit  Let's talk about  continuously differentiable, then what you want is you want to look  At the point with zero gradient  And  The boundary points.  So you want both the story points and the boundary points.  To  Will basically give you  enough information to find the global max or global man.  I'll just call them the global optimum.  Okay.  Um, so let's look at an example.  And for example,  Here's a function  Here's a co log likelihood function.  Just actually concave, but it doesn't have any history books. I mean, let's keep  This and the point here is that, let's say this is my constant set right the interval 01 was my constraints.  And so theta star, the max. In this case, so the max.  And so feta star in this case is equal to zero. That's the arg max.  Of theta in the constraint set of theta, even though that f prime of theta star here is not equal to zero, right, because it appears at the boundary  So let's say you in one day. It's actually pretty simple, like you look at all this sensory point of the function, even if it wiggles around like this.  Look at all the century points and you look at the value of the boundary. And then you take the max over these finite number of points, hopefully, if it's a will be a function and I will have a finite number of story points.  Though there are crazy function which could have an infinite number of them anyway let's  Keep it on the simple case where you have a finite number of them, you can just take then the max over all these tertiary points and the boundary. And then you find the global max and same thing for the global me  Okay. Now, when you're in in multivariate it's much more complicated because then the boundary will be a an infinite set  And so then you might also need to solve in the problem of the boundary. So, but often by inspection. You can show, for example, that when the norm of the parameter goes to infinity. So it's anywhere. For example, let's say I'm doing an unconstrained position of a function  Over all the art to the D. Okay, so then you look at the zero gradient of the function and then you also look at the value of the infinity.  And often, you can say, well, the value of infinity smaller than the associate point, I found so I found a global max. That's another way to find a global, even if the function is concave  OK, but so you have to be careful about the boundary cases. Right. So you have to be careful.  With  Boundary  Cases.  IE when the data star belongs to the boundary of your constraints, it  And that's why, by the way, I was  Specific here to say that  The fishery point condition is necessary, only when the point is in the interior of the constraints that because  You could have local men or local max at the boundary. Just because you're not allowed to go farther outside of the boundary. So this becomes a local me. Right. And so in this example, I could still  It could still increase the function by going this way, but I'm stuck because I need to stay in the constraints and that's why you didn't have a stationary point user avatar   Even though it was an optimal. Okay. user avatar   And I know the example of this would be if the boundaries that infinity. So another example.  Is  Let's say  You know, here's an example of like likelihood  And let's say it keeps increasing, and then never reaching this assembly.  And so statuary points here would be this one and this one, so their local max local men, but no global  That would be one of the boundary. And then the other boundary would be at infinity. And that's where the the point will be maximized. But this is not a point. There's no like point at infinity. And so in this example, you would say that the  The art max doesn't exist. So this example.  Is an example where the Emily.  Does not exist.  I mean, you could argue that oil. The plus infinity extended real number is there are the LD. In this case, but usually that's consider as not  That can get some statistics, often by convention, they will talk about the existence of an elite when you have a finite value, not these extended real number, especially because in the case of a  real number, it's fine, because you can just have one. But then if you have your in multiple dimension, then the plus infinity one is a bit weird, because it depends on the direction  So, yeah.  So, so that's the simple notes about Emily and so that highlights some properties of the Emily. So let's give some notes about me, Emily.  So it does not always exist.  You need to rigorously condition to ensure its existence.  Of the likelihood function of the construct set  And so example where it does not always exist example where it doesn't exist is when the  The solution would be at the boundary of your constraint set but  The constraints that is open.  So, for example, suppose you decided that you would consider only these parameter which are 01. So this is a French notation of internal so the  This  open bracket like this this bracket here that the the wrong direction is means opening in French rotation know in English use parenthesis, but I don't like parenthesis. I think it's confusing with brackets. So I use the French notation for intervals. That's the open interval 01  And so that's one example. Or another example. When say that star.  Is basically plus infinity. I put it in quotes, and that's the example about. Okay, so these are two examples where we would say that the maximum likelihood estimator does not exist. An example of that is, let's say you have a Gaussian  Patrick family and you only have one observation or all your observation fall or equal then and you estimate the variance, you would say that the variance is actually  In this case will be zero, which exists.  So that's fine though. You might perhaps decide that a variants of zero is not good because then the gushing does not have a density. So you would have excluded the zero variance case in your parameter set for example. And in this case, and it will not exist.  Okay, interesting question from the shunt  If the optimum does not exist. Couldn't we just take the second best optimum. Okay.  The question here is, when you have a continuum of possibilities. The second best or usually not existing either, right. So for example, let's look at  The case here, right, so the the best is basically at infinity, the second best is basically the point just beside infinity, which is also infinity. So, so there's no second best or third best here. Right, so it doesn't really help.  So that's the match. So a lot of protein statistics, by the way, is analysis. So, it's called analysis and math sense. So I know this. This is the branch is the formal branch of calculus.  Like limits and real numbers and epsilon delta eyes and sequences of it and yeah and the you know the rigor, there is there's a lot of these you know boundary cases when you talk about infinite number of things.  And then somebody asked. Oh, well, the maximum feeling is an example, a bad mix.  Okay. So Jacob is being a bit formal now. So for you, he says that the max that infinity is also the boundary of open set.  Because the real numbers is open set, but it's also a tool sets. It's a bit weird.  So,  The boundary of a set is the intersection of it's closer with it's  Open its interior us. I don't think that works.  Because all the real numbers is in the closure so yeah so I don't think you could say that the person field is is is a boundary in the usual sense of bunk.  But now we're getting into fancy math.  Are you happy with that Jacob.  Are there necessary sufficient conditions for theta stars to exist. Sure. I mean, so basically there. What's happening is  It's similar to conditions of  Existence of  Global max to function and then condition function and so example of that is if the  Salt put in a min because I'm more used to min. But if the function goes to infinity at the, let's say it's unconstrained and the function goes to infinity in all directions. So it's called courses, then there exists a global min  And so in this case there will always be user avatar   minimizer user avatar   So yeah, so that's an example. And so basically these are regarding the conditions for making sure that they exist a solution to an institution problem.  Okay, so other properties their properties of the MLS so it doesn't always exist, though. That's a bit like  I mean, I could argue that, you know, data star plus infinity is sufficient. And we can see that in practice. Also, this does happen sometimes. And that's fine for us. We'll get back to that.  Particular like depends how you prioritize things so you so plus infinity could mean one divided by plus and CD, which is zero and zero is a nice finite number. So depending on how you paraphrase your distribution.  You could say that that well the maximum likelihood founder is just zero. And that's fine. So I think this is more like  A technicality, but something else which is my brand is that is it's not necessarily unique  Right, so you can have multiple maximum like you parameter  Can have multiple global max.  And this happens in particular, when you look at mixture models.  I can mixture of gosselin's and particular you can start to have permutations of your labels which all have the same maximum likelihood value, but we'll get back to that.  And more another interesting properties that it is not admissible in general.  And to formally defined what isn't admissible estimator, I will have to do statistical decision theory, which is what we do.  At the end of this class. So that may not be formal. But I'll just give you the intuition of what this means that it means yes I say see later. But what it means is that there exists strictly  Better estimators  And that from a specific perspective that will formalize which is basically with the risk of an estimator, there exists. Other estimator, which have better risk everywhere.  And so there's no one better or equal everywhere and strictly better somewhere and which means that they're dominating the maximum likelihood estimator, which means that, well, this is kind of a stupid estimator it from this perspective. Okay.  Which is a bit surprising and actually the an example of that is the James Stein estimator that will see later, I'm just giving you the keywords. Now, just to satisfy your curiosity. But that gives you an example which dominates.  The maximum effectiveness debater and then you say, well, if it's dominated. Why then, it's still common and standard well  There's a few things. One is that in  It has a very natural interpretation, like the the the relative frequency for the binomial. So that's why it's often mention first just for pedagogical reason and  But in practice, I guess you should never really you escaped. So, and then standard statistics, you will need to to  regularize in order to avoid the bad properties of maximum likelihood estimator, but there's a question of how you regularize and that's non trivial and so  I guess, even though it can be strictly the minute. Sometimes people still use it. And then there's a third part which is much more advanced topic is we realized that in modern machine learning.  Even though maximum likelihood estimator will overfit that's also why it's not that great.  Because of other reasons, often, it does not overfit and so just trying to maximize the likely you can still give good results. But that's because there's many in this case there was actually many maxima and you will pick the correct Mac, the best good maximum in terms of performance.  But we'll get back to that.  In particular, when we talk about physical destructive.  Okay, so I then had manner is asking a good question. It says, well, how it can be nothing unique don't we just have one global max for any function.  And depends what we're talking about right. So the value of the global max in terms of f is always unique because it's it's a global max of a of a, you know, set of real numbers.  But the art Max, the maximum the parameter which will maximize the function. These could be non unique. Right. So for example, if I have a function like this. user avatar   Oops. Okay. It's making them. user avatar   Here's one global next. Here's another global next. So I have say that one star.  And I have stayed a two star. I have through global I have to maximizing parameter, even though the value here of the global Max's unique right  Okay, so that's some thoughts on the Emily.  Now let's jump to  The multi Naomi muting new meal example will step our optimization tools, another notch.  So me that is saying we should not use Emily and practice.  I would say, unless you know what you're doing. Yes. Do not use a million practice.  Think that would be a wise advice.  But we'll get back to this notion, when we talk about properties of estimators and we'll talk about overfitting. And these kind of things but but the maximum likelihood estimator can overfit the data that's one of this problem.  Alright so example.  Multi normal  Distribution.  Perhaps I should take a break. Now, what do you think  I think a break now or after putting the setup of demands normal  Now, you say yes.  Oh yeah, there's a fight between the yes and no. So 1113 yes, it knows  Oh, then those are winning are going up. That's funny.  So yes means I am giving I'm taking a break and no means I putting the setup.  And, well, that's funny. Okay, well I choose I put it, I already started to do the setup so I'll do the setup. Oh yeah, it's too. It's too close. Oh, yeah, yeah.  Oh. Why are there no is being keen. Okay. Well, yeah. So let's let's let's do a 10 minute setup and then I take a break and you can think about a setup.  So,  Suppose  That x i is a discrete  Random variable.  On K choices.  Okay, and will often call this in  This class, but this is also a terminology which is used. Now it's called a mutiny. So when you have capabilities you call this a move to New York.  And it's a basic. It's the generalization of the new year. That's why it's called booty new a new year's two possibilities capabilities like I'm rolling a die with case sides.  Some new new  And so we could choose in this case as simple space.  The number one, two k, which are the capabilities. That's just an encoding.  Of the capabilities. So for example, let's say you're modeling words and you have a vocabulary, you could have that the first word in your vocabulary will be, I don't know. It could be not sabbatical normal in dictionary. Right. And you could just have the index encoding the possibilities.  But  We will use a different encoding now because it's very convenient and we see why.  But it's just a different way to encode the same object. Right, so it is convenient.  To encode the capabilities.  As a vector in our to the key. Okay, so we're basically map each passivity as the unit basis vector in our to the key. user avatar   Okay. user avatar   This is basically called the one hot encoding.  in machine learning term.  And so, or simple space in this case will be  k vectors.  Whoops. So I'll use AI for the  Basic vector  In our to the case. So I have  Cave actors.  Were EJ belongs to our to the gay and what's EJ. Well, it's the eighth basic vector. What is that it's just a vector.  Where you have zeros everywhere and you have a one in the eighth coordinate, which is why it's called a one hot encoding is because it has zeros everywhere. And there's only one  One which is weird indicate in the position of where the one appear is is encoding. What's the bus. What's the choice.  Okay, so that's your discrete number of choices.  And  We want to describe probably t  Over these k choice. So I need to have k numbers which are a non negative and which some to one. Okay, that's, that's the possible. That's all the possible PMS for this. And so we'll use just a vector in our to the k as well to describe the parameter. So the parameter  For an arbitrary discrete  Random variable on key choices. It's just out, we'll use pie instead of data, I guess. Not sure why. user avatar   Standard user avatar   Unit nation.  And it will belong to the property simplex. Right. So once it probably simplex that's a useful set that will appear a lot  So by definition, and I use as index the number of objects that they use to define a property on or the number of components. So this is a set of vector in Arcade. So by belongs to arcane  Such that  The pages are positive for all  Coordinates and the summation of my coordinates is equal to one, right.  This way, this then the page is really encode valued properties for all the option.  Which are possible. So this is called the property simplex  On key choices. user avatar   I guess in dimension. user avatar   And so here are parameter space for the multi new he user avatar Oumar Kaba  Will be the priority siblings. user avatar   Right. These are all valid. user avatar   Probably these user avatar   somebody's asking if we cannot instead  encode the parameter using only k minus one number instead of key numbers with some to one. And the answer is yes. And we'll see why this is useful somewhere else. But for now, we'll just use this power position.  And so  Let's display just a little drawing to just see what it looks like, let's say in dimension three. So in dimension three  It's actually the convex hall of the three bases vector. So it looks like that.  That would be data, right, so these are all numbers which are all in the positive orphaned because they have to be negative.  In the some of them is equal to one. That's why there's a plane constraints, right. So, and then more dimension, you get a hyper plane constraint. So it's the intersection of the some to one hyper plane and the positive. Awesome. That's what the property sentences.  And I'm not able to drive in dimension for because unfortunately I'm only able to do dimension three  Okay, and so we will write  The notation, we will use for a multimedia will be x i is distributed  Accordingly, according to a multi Neue with parameter pay  So that's the parameter of my Latino you  And this is called him into new district distribution.  Generalization of the Bernie.  OK, so now let's do the Milton Romeo, which is a generalization of the binomial.  So,  Now, if we consider like the binomial, we consider  And  roll dice dice roll.  Or dice for them. So I have x i, which are ID.  Routine UI with parameter pay  And then I define X to be the sum  Of all my mood to New Year in a variable. And here, this only works because of my encoding right if I had us one to the k and coding summing  The possibilities doesn't make any sense because something for somebody to with something possibly three to five. That doesn't make any sense. But because I have encoded  Each possibility within dimension k, then when I some the vector together. I just add the count of how many times I've observed each  Possibility, right, because I have a one for every time I've chosen this unit vector right  So this really tells you  The component of the vector x here tells you how many times I've chosen every possibility among and  Viral case sided dice roll right  And this is the severe according to the multi normal so like the binomial. You see how many throws I've done which is n and you you give a parameter pie, instead of just p. So, this is called the multi normal distribution.  So if you put n equals one. It's like a multi new you, by the way.  Is the multi normal  Distribution.  And it's PMS is a generalization of the binomial. So first of all, I have that x in this case.  Papa, so x we is a vector in our to the k  Actually, it's even natural number the k because there is our accounts and the sample space.  Of x is all integer vector and one blah blah blah and key such that NJ belongs to the natural number and the summation over Jay  And NJ is equal to n. Right, so  Basically the  I have observed n di role, and I'm counting how many times I've observed each possibilities and so because I have n di role, each person, the sum of all possibilities has to be right. So that's what the NJ represents  And the PDF  For x  So the probability of X given pie. I could also use semi colon by the same is I use the  Mucho mucho coefficient. So that's a generalization of the binomial coefficients are using station here, x one, blah blah blah, x n ke sorry  X x is a vector here. So using the parent is this subscript is busy to get the components.  And then I have the product.  Of the page G raised to the x.  And x was in one blah blah blah to NK  And Mitch nominal coefficient. So, this thing here is called mu t normal  Coefficient.  Just basically counting the number of ways I can  Per mutes things around when they have like in one, two NT such that the have a  Specific signature. So, okay. So, what I mean is,  Like the binomial coefficient was counting the number of ways I can choose  Cake heads among n possibilities here I it's I need to choose any one possibilities for the first choice among an  End to for the second choice. I'm on an end three to try and and I can compute all these together, right. So that's counting this, it turns out that it's actually a simple computer or coefficient, you have that the  Not sure if it's n one blah blah blah and k choose n you call this, but perhaps this is n factorial divided by and one factorial  Blah, blah, blah. And factorial. And so you can see that the binomial coefficient is the material coefficient when you only have two possibilities.  And you have that this you have k and n minus k because they have to some to him.  So that's the multi normal any question about it.  And so you can see the usefulness of the encoding is that now I can simply, you know, raise  The, the, probably t to the value the components of my vector right so it was if I hadn't going in a different way. It would have been difficult to express the PMs  Okay, so I'll let you think about this over the coffee break. So I'll take a 10 minute break. So it's 337 I'll be back at  347  Put some music. Oh, and that's posed the recording.  Okay. Is there any question in the break about the multi normal distribution.  Also, if you think I'm speaking too fast or too slow. You can go in the go slower go faster feedback. You can write in the chat.  You can  Attend I think sometimes to speak too fast, but I'm  Gonna give me the feedback. So that's the Mr normal  And alright so now let's do the maximum like you in this case. So let's do  Like the binomial, but  No, he  So,  The login so we want to. Suppose x is our observation and we want to maximize the likelihood of x.  Well, the likelihood of pipe. Right. So we want to find the pie which maximize and quality of the data. So let's construct the log likelihood as before the law like you'd  Like  So as a function of by little by  Oops. This is the log of p of x given pie.  So this is basically as before the log of a constant. The material coefficient in one blah blah blah to NK  Suppose again that I use annotation and one to n k for the components. The key components of my vector x right  This is a constant.  So I don't care about it when I optimize respect to pi. And then they have summation from G equals one, two k of NJ log of pages.  So, this we can ignore  When computing the enemy.  So what's the MLS here. Well, the Emily pie hat.  Of x  Is the arg max for pie in the property simplex. So I would write it like this, I will have buy in RK such that pie belong to the property simplex  Of the login so so this is so probably simplex here is capital theta is the set of parameters. I'm just using pie for some reason.  And so now what's separating is we have constraints which are  fairly important to take in consideration. So this is really a constrained optimization problem.  How do we handle that. Okay, so that's so this is kind of an excuse for me to talk about constraint of position.  And so there's actually two options at least  The first option, which was already mentioned by somebody, a while ago, or for  A video and actually send that file to Vivian.  Is to re prioritize the model in a full dimensional parent parameter space. So we read parameter eyes.  The problem.  To be  So that basically so that theta is full dimension.  What I mean a full dimensional okay so so basically here.  I played again my property simplex  That was data three. So I'm in three dimension because I my vector on dimension three  But it's actually a two dimensional object of privacy centric, so it's it's so, so the the property simplex is not full dimensional it's it's a two dimensional object in the sense that they can repel tries it with only two parameters.  Even though I mean dimension.  And so what I can do is I can say, well, I don't care about PK because I can just say pi key is one minus the summation from Jay equals one.  Two K user avatar   Minus one of by Jay user avatar   And I will just use  As parameters of my model k minus one number pie one, two pi k minus one.  Which have to be in 01  And with the constraint.  Which is now inequality constraint.  summation over j  K minus one by Jay  To be smaller than one. So if the summation of the key minus one coordinates small equal to one.  Then I can define the last coordinate as a valued probably t because then one minus that will be positive. If it's bigger than one, then that's not a valid. There's no way I can get the correct point of the policy kicks. Okay.  And so when we do that in some sense where like projecting the policy simplex  It's smaller equal  Because  These are close it smaller or equal  So what I've sold. So, this  Here would be the projection on the to the plane. So, this set here are pie one, pie to  Such that  Pie j is in 01  And summation of by Jay is small equal to one. Okay, so that's a full dimensional version of the property simplex, in particular in dimension to this is the set of numbers, right.  So this is 01  And you have this is basically pie one plus by two equals one. user avatar   Okay. user avatar   So now you don't need to worry about this like annoying any equality constraint which reduce the effective number of degrees of freedom by one  And then the magic, you still have constraints, you said have boundary constraint of 01 but here the magic is there's actually some magic.  Is that log of by  Jay,  X. user avatar   I mean issues. user avatar   Acts user avatar   As a barrier function.  Away.  From pie J equals zero.  Okay, so what I mean by that is if we talk about sort of maximizing that he will will will minimize the negative log like you  So if I look at minus log of pi j  It looks like this. And so close to zero.  In this region here, I get a huge penalty. And so if I'm trying to minimize  Some of these kind of function like weighted some of these kind of function. This will make sure that I will never put page equals zero. It's something that will make sure that the pages stay positive.  And so because of that, it turns out that if you just do  Unconstrained optimization. So you can try.  Unconstrained  Optimization.  On by one  Two pi k minus one of  The log like to have as a function of these numbers.  Hoping  That's the solution.  Is in the interior  Of the constraints that  Will  Okay, so we'll  When we solve the problem. I'll show you why it's actually the case. But that's, by the way, that's what we've done already.  In the bird new you kicks in the brand new case. I only have one parameter, even though there was two options. So you can just think  Through New Years and machinery with cake was too. And so I already use the full dimensional pasteurization could have used that I wanted to such that the some to one, but I just use  And then this data had to be between zero and one. So let's go back to the Bernie, blah blah blah, or the sorry the binomial, but it's same with some liberties. Where was my  Maximum like you parameter. Here we go. So  Oh, I forgot here that I didn't want to have these hoops. The way he rates. user avatar   Issues now. user avatar   Ah, user avatar   Let's try something. user avatar   Oh, Alex.  Okay.  So you raise  Success. Success.  So,  Oops.  So this ratio here is always  In 01 right. So X is between is a bit between zero and and so the ratio of X Men is always between zero and one and so  Even though I forgot about the constraint that data has to belong to 01 I just treated this as an unconstrained optimization problem.  It turns out that when I said the derivative equals zero. And I looked at the solution, it was still fine was always feasible.  So, and that's because of the lug magic of the barrier thing. And so, so that's the binomial version of what's happening for the Middle East. And so it turns out that you know one way to do that would be indeed to just re penetration in a full dimensional set  Forget about these positivity constraints and boundary constraints, because you had you were lucky you had these function which push you away from the boundary. And when you do unconstrained optimization and you get a global men.  Adding constraint and if there's noblemen is feasible adding the constraint won't change anything, right, the constraint only change something when the solution will be outside of the constraints set and then the boundary will like clap things in  What's the word on the line before solution hoping. Good question. user avatar   Right, this user avatar   So, hoping  And the solution is in the interior  And that's okay so and so. So this, this approach is what we actually done for the binomial.  And it will work. And that's fine just said the great equals zero, you'll see it's working, but because I want to be pedagogical I wanted to show you to teach you about Lagrange multiplier. Let's do the second approach, which is to deal directly with the equality concerns.  Okay, so you use the method which is of Lagrange multiplier.  Approach.  To handle the equality concerns.  On the project simplex  And we will still ignore  The boundary  The positivity constraint like that the payday belongs to 01 constraint.  Because the the long term will take care of them.  Okay, then I don't think in this class I will talk about  The I forgot. I think when I will talk about maximum entropy to its maximum likelihood in the expansion of me duality.  I'm not sure I will talk about inequality constraints handling with Lagrangian approach. But anyway, I will formalize the language and method later here. We'll just give you the intuition.  But what's the setup the setup is that we want to maximize some function of pie.  Such that we have an equity constraint on pay, which is equal to zero.  And so in this case.  You know, I had summation over j KJ equals one. So I can translate it to one minus summation over j  Of by J equals zero, right. So that's the, that's the equity constraint that we will  So this is what we call  So,  That's the one we were trying to use for the maximum likelihood estimator.  In the multi normal case.  And so what's the Lagrangian multiplier approach. Well the idea is to define this language and function which is both a function of the original parameter and the Lagrange multiplier.  And normally, you would have one leverage multiplier for constraint here does only ones which will be a skater.  This is the Lagrange multiplier.  And will define the luggage and function as the original function where maximizing or minimizing  And the sign. Doesn't matter for equality construct. So I'll just use. Plus, if you look at the Wikipedia article usage minus, though I think they might be minimizing  But you'll see that in the case of equal he constraint, the sign doesn't matter at all. So you can just put a plus or minus, doesn't matter. And then you put the constraint, you  And the method is to look  At discretionary points.  stationary points.  Which are to zero gradient  Of this leveraging function.  Okay. And these points will be necessary conditions.  These will be necessary.  Supposing that everything is differential  For local optimum.  And then you can check user avatar   You user avatar   Can check  The something called the bordered Hession I won't go into detail of that.  To  Get  Local men.  Are max information.  The same way as before you check stationary points that these aren't necessary and then if you want to know if it's a local mean our local Max, you need to look at the second derivative in general.  Unless you know stuff about your function like if if f is concave and we're maximizing it. Well, then the story points here, supposing G, for example, is linear, like it is here, then the point of the of the Lagrangian function that I just define will indeed be the global max.  And so what are the century point of that. So if I take the derivative of J respect to pi.  R Us this notation here. So the gradient of g respect to buy  This has to be equal to zero and the derivative of Jay respect to lambda  Has to be equal to zero. This is just  G of pie. When you take the derivative of Jay respect to lambda, all you get a j phi. And so, all you're saying is that Jeff by equals zero, which all it means is that pie is a positive is it satisfies the constraint, right, because these are the feasible.  Okay, there's a few questions from the past. Do we need to push the pages away from one as well. I'm  In a pop up.  No big. Well, yes, you need to. But this is done by the the reprivatisation right so if you look at the log likelihood here.  So when I looked at  lug of pie K. This is one minus summation of OJ of pages, right. So I have loved of this. And so if any pages close to one, this will be  basically giving zero in the argument of the login, will it will be pushed away. OK, so the so the case term of my log likelihood will actually handle the pushing away from one  So start back, ask the question, if you can tell in advance whether solving the unconstrained version.  should satisfy the constraint or you need to solve it. And in general, you need to solve it.  And actually, that's fine, right, that usually the unconstrained version is simpler. So it's I think it's a if you're if it's kind of a nice approach to just  Forget some constraints solve it, see what it does. And that gives already some insights on the problem. And then if you're lucky.  It satisfies the constraint and you're like, Okay, I'm done. If you're unlucky will then you need to go through like Grinch and approach to handle the constraints or other constraint of tradition.  Now if you know something about the function like in this case there are barrier function, then you could have concluded that  The unconstrained optimization will likely satisfy the constraint, because here. What happens is, you know, you know the the objective blows up at the boundary. And so, you know, you'll you'll stay away from the boundary from just a look like you shake  Check. So somebody is asking about what is this, this is  Something called the bordered  Has him. So it's a test yet have mentored with the constraints.  Which actually I learned today. By the way, because I was curious, like, okay, what are the second order tests in these equality constrained by grunge and stuff and Wikipedia told me or check the board edition.  But as you notice, if I didn't know about that before. This is not like this is outside the scope of the class.  But it's just for curiosity.  And somebody asked are there constraint on the constraint that can be handled by the leveraging multiply, multiply method.  Yes. So first of all, like,  It has to be so the one I'm talking about here. I'm taking the story points. I'm thinking the gradient. So it has to be differential  And then if you only care about local men and local Max, then that's fine as long. I mean, as long as you're able to find a string points, it might be intractable to find the stitching points because you might have to solve.  A transcendental equation for example. And so then you need numerical techniques which you know will come back later. Right now I'm doing analytical techniques, basically, that's what I'm doing.  But yeah, so there's no other constraints. If you want to have easily global men are global, Max, then you need more regulated conditions. For example,  You need the the equality constrained to be linear. Basically, if it's, if it's a nonlinear function. Usually you you get trouble because you basically get a non complex problem to work with. user avatar   So we'll come back to that. Q. user avatar   Okay, well let's just do it in practice, to see how it works.  So I have my leg leg. Good.  elif pie. But it's summation over j  Of NJ log of ag  Right, I forgot the constant  Remove my mug away.  And know that is so this is actually a concave  Function.  Is in pi j. So, if you compute the person  For feasible pie.  Basically pies, which are  In the interior of your property simplex  This will be negative different. So this is why it's a 68 strictly crew came to me it's tricky could give  Which means that if it's strictly concave, it will have unique  stationary point  So I think that the partial derivative of Jay, we expect to pay J.  Remember that Jay is my augment my leg arrangement, the language and function. Right. So I want to find the pies, such that this is equal to zero.  So, so I take the derivative of l with respect to pi. And I will just get  NJ divided by Jay  So there's, there was a some of key terms. So here I had to some of our key terms now I'm looking at pi j. So, there's only the N G that that will look at  And I also have this constraint to to consider.  And  The constraint was written here. So it's one minus some of the pages. So if I think the derivative of this respect to pay. Jay, I just get a minus one.  And this was times, London, so I get  Minus lambda. And I want this to be cool to see  That pretty simple equation to solve. So, this implies that page A star is just NJ divided by lambda  So that's pretty standard in Lagrange multiplier method. You'll often have that this lambda is some scaling.  Such that the constraint is satisfied. Right. And so in particular.  Now I want to solve the other part of the especially constraint which is that g of Python star is equal to zero.  IE, that the summation over j of by Jay  Is equal to one.  And so if I some  Of the pages start I wrote above that imply that this is summation over j of NJ divided by lambda I want this to be equal to one.  Which implies that my optimal Lagrange multiplier is just a summation over j of the engine.  Which is equal to 10  Okay, so we get that the optimal  Parameter value for the multi normal is just also, it's the relative frequency of the observation. So it's NJ, divide by. So this is the Emily.  For a multi month for a multinational also very natural.  Okay, so notice  So notice  That pages star because it's ratio of energy divided by n. So it's always between zero and one. So, it is like I said, this is  Belong to 01 so it satisfy the constraint already  Been by the constraint on the observation set that the sum of the energy divided by n. This that some of the energy is equal to end. So then I know that the some of the pages is equal to one.  OK, so now somebody is asking about, oh, what about the boundary cases. So if the page j is equal to so if NJ is equal to zero for some  J. Then I get that page, the stars to zero which is up the boundary, but that's fine. So this formula is still valid. Even though.  You know, you need a bit more work to kind of prove that everything is fine.  Because  Because basically, the well I guess what happens here is in the if you look at the log lacunae which is here.  When this is equal to zero hoops.  When this is equal to zero here, then this any value of pay doesn't enter in the objective  But what will happen.  Is that  The other terms will basically set all their value to be the ratio and which implies that the remaining terms will be zero, that still works.  And okay I guess everybody is fine. So perhaps  A little drawing to illustrate the semantics of the language with the buyer. So why does works. So there's actually a nice subtle point interpretation that I will talk about later, when I talk about the ingenuity, but there's a picture behind this approach.  To player.  Technique.  So the idea is, I have some objective that I'm trying to, let's say, maximize. Okay, so these are the sub level set of  So these are the  level sets.  Of f of x. Okay, so IE.  It's the set of points which all have the same value. So  Set Point.  X such that f of x is equal to some by. You see, and as I barely see I get different levels.  And  Let's say, now I have my  Set the feasible point which is determined by the curve, such that g of x equal to zero. user avatar   Okay. user avatar   Which basically means that these points are on the level set  Of g of x at values you. So this is also a level set right so I could you know probably like if I could.  Perhaps I'll I can complete the curve. Let's compete curve, this is g of x equals zero, and perhaps it looks like an ellipse as well. It's, it's a quadratic  Right, and so I could have  And now the magic is you know that in calculus that the gradient  The gradient is perpendicular to the level set so graph of f is perpendicular to the level set at any point when the function is smooth. You have that the gradient is the direction of maximum increase and it's perpendicular to the constant value of the function  And and so  Same thing here, the gradient of Jay will be  Perpendicular  At  Any point  And it turns out that that the optimum point you will have that  The gradient of f.  Should be parallel should be pampered the color to the possible variation  Of  In the physical set because the directional derivative is the duck product between the gradient and the direction  And so if the duck product between the direction and the greatest zero. That means that basically I have a special report so so moving in any direction. Any feasible direction from x is not increasing or decreasing my my function and does. I have a status report.  And so it's critical to level set but the grid of jays also preparing to go to the level set. And so it turns out that they're parallel. And that's what the leveraging  Is basically telling you because I have here that grant of Jake is perpendicular to the level set and granted this is perpendicular to the level set as well.  And so I have that, you know, this is a star. And so I have that graph of f of x star.  Is basically parallel to ground up Jay star. And so I put some factor when they're palo you put a factor. It could be negative. By the way, it would be positive. Doesn't matter. Once you have j of extra day I eat you have graph of f and g or parallel  So basically this this is German Kiki what's happening. And that's what this arrangement supplier approaches.  I guess here my star was a bit  Any quality constraint, you will have a similar picture, but instead of having just level set, you'll have some level set and you'll just want to make sure that the  The gradient of Jay points in the right direction, compared to the visibility set of the sub level set of the of the of the inequality constraints. So then that just means that you'll have constraints on the  You'll have constraints on the sign of the Lagrangian multiplier to make sure it's it works.  But we'll get back to that when we talk about any coffee culture.  All right. Any question about this.  Isn't beautiful  I like this. I mean, this is kind of ugly picture, but I think it's  I think it's cool that bridge with the player. Cool.  Marvelous. Oh yeah, like that. Alright. So let me start statistical decision theory because it's  It's important for the assignment as well, I guess. Let's talk about the square less lifetime in six minutes, hopefully.  Statistical  Decision.  Theory.  And I will. I'll go through the false formalization next class right now. I'll just give you the bias variance, the composition, which is what you will need to do  So the bias variance  The composition  For the squares.  Squared loss.  an estimator.  Is a function  From the data.  I Ed observation.  Two parameters that we're trying to estimate  And more generally, you could think of a parameter as a complicated object parameter could be the parameter of a function. So it could be a whole function. But right now, let's think about say deployment of a Gaussian. So it's a finite dimensional vectors of simple  And example of estimator, or the estimator, which I already mentioned. So this is  The arg max.  Over my parameters of the likelihood  And we also talked about the map estimator.  The map estimator maximum epic story theory is instead the art max.  Or my parameter of the posterior be afraid. I give an x.  Which is the same thing, removing the constant marginal look like you'd have the joint p of x given data times P of theater. Right. So I had both the  The likelihood  Like the hood.  And the prior  It's kind of a weighted like version.  Okay. And so now let's say I want to compare this to estimator. So how do we evaluate an estimate. So how  Do we  Evaluate  And for these estimators  And so as I said it estimator is a function  From the observation to the parameter. So I'll use delta for this function. That's pretty standard and statistics to use delta as the estimator. So it's a estimator. It's a function from the observation. So omega to the parameter space. Okay.  And in terms of notation, we could say theta hat. So the value of our estimate is just the mapping  With delta of and using or random variable capital X as so theta hat is a random verb because it depends on the random observation capital X. So think of, you know, x could be the observation of a binomial or multi normal or a gash in random variable.  And so in the next class I will formalize all this framework, but now let's just look at the most standard tool.  To evaluate  It's called the frequent this risk.  Of an estimator.  And so it's the noted as our. It depends on the value of the true parameter theta.  And on the estimator that I'm considering so delta and by definition, it will be the expectation over the random observation of something which is called the loss which tells me  When the true value of the parameter is data and I have predicted  Data hat or delta x.  how bad this is and I think the expectation over the  The possible data. Okay, so, so this L.  Here is called the statistical loss function.  And we'll go more into details next class.  And so what we do here is an average over possible data. So, so this is basically tell you mean you this estimator, how well does it do an average over all possible observation from  Okay, so that's the frequencies risk.  And so now if the statistical last that we use is a square loss.  Just want to talking about the square less. So the square last will use when we estimate theta and we predicted a prime  It's just the LT not so that's the multivariate generalization of the one dimensional squared us  So it's the norm square  Yeah.  And so  Now there's a square less. So if I compute the frequent this risk of the square less. Let's do that expectation over X of norm of theta minus  And I'll use data hat for my let's use that a hat, remember, is the decision we make  Don't have X. So I have stayed on my in a state of hat. They'll to Nome square. I'm all Google slightly over time, but I'll try to wrap up every like two or three minutes so  If you have to go he will have the recording. Sorry about that.  So yes, I want to take the expectation of X of data miner state. I had to norm squared. So what I do is I do something which has done a lot in math is at zero. So this is theta minus expectation of Theta hat.  Plus expectation of Theta hat minus beta hat sell to an arm square right so this thing is just, I've just added zero. So it doesn't do anything. But it's very convenient.  And now.  I can expand the Sturm. So this is basically a quadratic term. So I can have the first term square. So, this is the expectation of the norm of theta minus expectation theta.  Square.  Plus, so that's basically  This first term that I did a squared, that then I have the other term squared. So expectation of  norm of theta hat minus expectation of data hat.  Yes.  Squared.  And then I have the cross term so plus two.  And then expectation and then I use the because it's these are vectors. I'm using the inner product validation. So we'll have the first term theta minus expectation of Theta hat.  And the other term which is expectation of data hat minus data.  Okay, so these were the cross term. They had two of them.  And so now  I can use the linearity of the expectation on  On the product because this is linear. So, so this thing here. So theta is a constant data is there is a true parameter which that constant expectation of data is also constant because I do the expectation. So this is a constant. And so I can move this inside right because it's a linear  That's the unity of expectation. So what I get then is this becomes to expectation and not the expedition inside this becomes  Data minus expectation of data hat.  And then expectation of feta hat.  minus expectation of data.  And so this is so basically the crust term vanishes.  So I'm left with two terms.  And these two terms are the bias and the variance read. That's why it's called the bias against the competition. So I have that the risk  Of a the squared, the frequent this risk of an estimator. When I use the square loss which is the expectation over x of the data minus data hat.  Square this split into pieces.  It's the norm of theta minus expectation of beta hat.  Square.  Plus the variance  beta hat minus expectation of beta hat. user avatar   Square. user avatar   Okay, so, and here, by the way, there was a there was an expectation, but this is a constant. This is a constant. So, this expectation doesn't do anything. That's why there's no expectation  Here. Okay. And so this first term.  Is called the bias square  And this term is called the variance  And so and so bias here.  By definition, will be the norm.  Of theta minus expectation of data hat. So you look at what's the average value of your parameter and is an average over a possible observation is your estimate the correct value. That's what the bias means  And so the  So the, the risk. Oops.  Four Square loss.  Is equal to buy a square plus variance and all these are positive value. And that's why there's a. It's called the bias variance decomposition  Just very fundamental and statistics.  And I think I'll come in more in this next class. And I'll put in quotes trade off.  Because usually the trade off. But now, the important thing is in the assignment I talked about is the estimator consistent. So let's talk about this so consistency.  Informally, what it means.  Is that you do the right thing.  As n goes to infinity.  Where n  Is the training set size.  So you can think here now x will be  Some idea observation from one up to n and theta can be indexed by data and which basically mean  What's the estimate using data of size in  Okay. And so in the assignment. We want to know when is the Emily estimator consists of the money, but I give you example of estimators and to tell you is this estimate or consistent, which  Formally would normally mean that in a theta and a hat in quality will converge to the right value, but in the assignment is just sufficient to look at the bias in the variance  So if the bias of theta and hat.  Goes to zero as n goes to infinity and the variants  Of theta and a hat goes to zero, then that implies by the band's variance decomposition, that the risk of theta, theta and hat.  Goes to zero as n goes to infinity. And we would call this, then that data and hat is consistent  Okay, because this also implies that data and hat converge to in probably two theta, which is a more the standard standard notion of consistency.  And then to show that it's not consistent. Well, you need to show that it does not converge to theta in property or that these biases and parents will always be positive. So I think  In general, like the bias in the variance not going to zero is not sufficient to show that it's not consistent, but in the assignments. If it's sufficient, you can just use that  Okay, so sorry for going over time. I just want to make sure I show you that so that you can do the last part of the assignment.  next class I will  formalize more what statistical decision theory is and give you an example of estimators and properties of estimates.  So is there any lingering important questions.  So somebody is asking question about the assignment.  So I'm thinking the assignment. I'm talking about random variable. So if I have x condition independent  Why W given Z. The y comma W means to join factor.  Is the conscience of the property respected by the Beijing distribution and parameters.  So map will be a constant estimator, as long as your prior satisfy some regard he condition. The Emily usually will be a consistent estimator as well on the record the conditions.  And somebody asked a question whether  If you don't use the log and you try to maximize the non log version.  Well, you just get very messy derivative, basically because you get like  Theta x. So you have a product of terms right so then when you take the derivative product, you need to have like the  First term product all the other terms, they would have second term times all the terms. So you get a very messy derivative. And so just looking at certain conditions will be a bit hairy. That's why the adventure of the log is to make everything linear  Okay, so I think I went way over time. So I think I will wrap it up here so well.  See you all on Friday.  Oh, and importantly, there is a deadline on Thursday, which is to drop the class. If you want to drop the class. The deadline is to  Do it on Thursday. So don't forget
  Okay, it is recording  So today did a zoom  Today we are going to go over some very fundamental stuff the statistical decision theory.  Decision.  Theory and talk about properties of this to me.  So perhaps to step back a bit. So, so far we I've done a bit of a review of policy and statistics talked about the maximum likelihood estimator.  Which was a one of the most that's a natural way to estimate parameters from data. So we're doing statistics and it will also an excuse to review interesting math with in particular, using Lagrangian multipliers for the, the method of leverage with the players to handle constraint.  So, and then at the end of last class I talked about the bias variance decomposition for the squared error term.  So that was a bit of a quick preview, which was useful for the assignment, but  The quick preview of what we will do today, which is a statistical this show that decision theory, which is the much more general framework of how to evaluate statistical procedure.  And so, in particular, if I tell you I have a learning algorithm. A and learning algorithm be which one is better. And usually, you cannot just answer this question in absolute  There are, in theory of statistics and machine learning. There are no free lunch theorems, which basically say, you can always find a data set.  For which a procedure learning algorithm. A will be better than our learning are going to be and then a different data set where it will be the other way around. So usually  You know, just make these statements in absolute but you can say for you can try to qualify. These statements in order to make sense of comparing different techniques. And so today, we'll see the the framework to do that. Okay, and  And that will help us to kind of like have a strong basis for just the theory of the class. And then in in later lectures will start to  To go with more concrete examples of statistical tests like linear regression or by notification. So, which are more centered machinery task. Okay.  So that's the plan. And if you take any if you read any statistics textbook, they will always start with statistical decision theory because that's kind of like the basic  formalization of what we're trying to do.  So what's the formal setup.  Welcome. First, I'm talking about. This is called the string theory. So  Statistical  Statistical and it's called decision theory because the idea is you need to make some  Decision with uncertain information about the world.  From say some random observation that you had  And  How can you evaluate these decision based decision theory.  Alright, so the formal setup for that.  And as usual, feel free to interrupt me at any point with questions.  And I'll put a bit of notation. So, we will have a random observation.  Which I'll use the for data.  So the is now a random variable, and it will be distributed according to some distribution peak, which we don't know. Okay, so, so this p here. I guess I use a big piece. So it's not a PMS. Now it's just a distribution.  This is an unknown distribution.  Which characterize the thing we're trying to model.  And the thing we could be trying to model is the result of a bunch of coin flips. Right. And so the will be the observation I made from a bunch of coin flips and P would be the distribution of the coin flips, which I don't  And so I say unknown distribution which models the world.  which models the world. What I mean by the world is. I mean, the world of interest. Right. The phenomenon we care about like the world the phenomenon.  And given that usually we'll think about parametric family often this p  Would be say  Identity or PMS and I will use as a subscription failure to highlight that you know it's defined by a parameters data. Okay.  But it doesn't have to be from a perfect family could be anything. I need distribution. Alright, so we have. And so that's that's the part which is random. So, so d is coming from a distribution that we don't know, and D is random.  Now, we also have an action space.  Which is why it's called decision theory. Right. So we need after seeing the observation D will need to make an action in the action space.  Which basically is our statistical decision. Okay. And when you do estimation from data, your action is basically a parameter  Okay, so that's what I had observed a bunch of coin flips. I use maximum likelihood estimation, to get a parameter for my coin flip. That's my action. Okay. And then we need to evaluate this action that's why we'll have a loss.  Which would tell us how good our action is so the last use capital L and it has two arguments. It has an argument. The unknown distribution which characterize the world.  And an action. Right. So this is the last. This is the statistical us just to use statistical just to make clear that we're in the context of a statistical problem.  And it's to distinguish to some losses we use machine learning later. So that's also why use that. So this is the statistical loss of doing  Action little A in our action space.  When the world.  Is capital P. Okay.  And I've put in quotes the world because  We're talking about the world with just a distribution, but that's kind of the idea  And  If we have a pet treadmill and, you know, we will often  Write this loss as theta a,  If  You know,  We have a parametric model.  IE, the big P was had a PDF or a PF  P little data for some  Data  In some parameter space. Okay. So the idea is  Either as my first argument they take this abstract distribution or if I have in in my this context of the task of trying to solve. I think of possible distribution which all has some parameter describing them, then it could just be a function of the parameter itself.  It's kind of the same idea.  And the important thing is that this loss is basically describing the goal, it does it's tell us what are we trying to accomplish, because it tells us how how do we evaluate our decisions describe the goal.  Or the task. user avatar   Okay. user avatar   So those are the three main ingredients for statistical decision theory, we have a random observation we have some action space which is basically the decision we need to make. And we have a loss which tells us, given the unknown world and this action, how well this is  And then the last part is, well, we will, we want to make a decision. So normally will have a decision rule. And that's what we try to evaluate, so I'll use delta for a decision rule. These are mapping from the set of observations to the set of actions. Right.  And so this script D here is just the sample space for the random variable capital, right. So this is the observed possible observed data observation. The festival daily observation I can have  And so this is called the decision.  Okay, and that's it so super abstract  Oh, there's already some questions so super abstract and that will make it more concrete with some examples that that would become much clearer what would be the difference between the and a random variable.  Okay so capital D is a random variable. And I use the notation, the squiggly  capital P. So, this thing here just to specify that this random variable was add the distribution capital P. So that's the distribution of my random variable like the same way. I said, capital X is a binomial Ed P, for example, that just specifying the distribution  Oh, and I should perhaps read all the chat so Umar I'd already had answered a question.  Yeah, but yes. Now I have inserted for everyone.  Thanks for more. That was the correct answer.  So let's do examples. Because right now, it's, it's kind of abstract  Ah, yes, I will give a concrete example now.  And the decision rule could also be penetrates. Sure. So the same way that I have capital P which is distribution and I have delta for this general  But I could decide that I will penetrate my decision rule, I would have a parametric family of decision rules and I will just use a parameter. Instead, same way I could have a Patrick family of distribution, but  Notation. Alright, so that's the examples because right now. And this is super abstract  Let's do example so  First example is a parameter estimation, but we have already talked about when we talked about maximum likelihood estimator Power Meter estimation  So,  Our action space in this case will be the set of parameters for a parametric  Family  Of distribution that I'll use a script P data for that.  And then  Delta.  Is a parameter  Estimator  From data.  Right, so an example of very standard setup is I will have that d  You don't have to have that right but that's, you know, the standard estimation problem, you will typically have that D or ID observation.  I would have, like, say, an observation where so that's like the training set where usually  You would have that x eyes or ID.  From some PII data.  And data is unknown. Right.  Because we don't know what generated data.  Okay and so  So in this setup.  I have my I defined. Now the distribution and my data capital D, which is just ID samples from us fix distribution p of little theta. That's my world.  And now my action is from this data, guess what should be the correct parameter which generated the data.  Okay, which we could say instead of prime  And then I need to define a loss to say, How well am I doing. So a typical us that people will use is a square loss. And so the last when the true distribution is p of data and my action was a here is a family. Right. So you can think of data prime  This is good, you know, a here belongs to the action space, which in this case is just the same set as the parameters.  And so I could use the LT norm. So I will say, well, how well I'm doing will be measured by the Ultra distance between my guests parameter and the true parameter is A squared loss.  But it's not the only one. So another example of last you could use when you try to estimate densities or parameters could be the KL divergence. So other loss.  Could be used  And so, for example, you could use the kale divergence between P of theta and your guests distribution. That's another  Way to measure how well you're doing. Does not the same thing of the L to error, but if  So, so if you don't know what the kale divergence. Don't worry, we will go over them much more detail with me to information theory and this kind of stuff.  For now, you can just think of it as a measure of the this similarity between two distributions. But if the two distributions, our oceans.  And if they have the same variance and they just have different means. Then the kale divergence between them will also be  Something like a scaled L to error so that the LT norm between the parameter will be very similar than the kale divergence when discussion, but if it's not gushing and it's, it might be different.  Okay, so that's parameter estimation  What about hypothesis testing. So that's a different statistical task.  In hypothesis testing.  Oh, before I go to that. But it's testing. So at the end of last class I did the, the, I looked at the expectation of the square there for a general estimator.  Data hat. Right, so that I had for presented the so you know that notation normally would be theta hat is applying my decision rule on the observer.  Of their observation, right. So, delta right so that's what we had and so last time we looked at the expectation of the squared error for that.  And we side it was decomposing as two terms the bias square and the violence of the data. Okay, so that was can be they. So this was the the completion of the expectation of the, the loss when the expectation is over the random training data.  And we come back later on this very soon. When we talk about summarizing the the the property administrator  Okay, so, Mr is asking  Okay, so if we use the kale divergence, the action spaces different  So if we  Start. So the assumption here is the action space is a parameter, but in the company. If I want to be able to use the kale divergence it these parameters have to be associated with a  Property Distribution. So we need it will be a Patrick family if I don't have a parametric family in mind doesn't make sense. Talk about the killed average. So, indeed.  There's a bit of an assumption there to use a key leverage  Was the norm.  You could. You could even have different parameters set for the action space. So you could have that  This set of parameter here is different than the parameters which are used to describe the real data as long as the but they have to be in the same space, though, to make sense of Dale to so  I'll, could you redefine the ok so somebody says that you could if you remember tries things the square, this would be a different values that the problem. Yes, that is a problem.  Indeed, the square less is not invariant to remember translation. So if I just multiply my parameter, bye bye skater. For example, it doesn't change normally  I could just decide that sort of prioritizing the mean I could prioritize by the mean divide by to write a Gaussian. I will just use the media by my  And that's, that's my new parameters and then still use the correct meaning that. Gotcha. And then the LT norm will be different. Yeah.  But the kale wouldn't be different. So that's the nice adventure of the kill divergences of this is in various to how you prioritize thing because it only depends on the distribution, not our penetration  But the alto norm is basically measuring how well you've estimated a parameter. And if you care about these precise pressurization then it makes sense if you think this participation is is  is arbitrary. Well, then it doesn't make much sense.  So yes, in the setting action space and parameter space are the same thing.  In the one I just presented  This is called the also the well specified setting in the sense that the the thing where you know that the true parameter which describe the  Real world distribution is actually belonging in the family that you're considering of actions. So in particular, if you get the right parameter, you could have a zero L to norm.  But in margin are all sending you could assume that you, you might not even have the right to correct Patrick family and in this case the could be different.  And then somebody asked a question of how well the data. The idea assumption work in the real world.  And how does the estimation method change when the data is an ID. Okay, so that's that's a bit  We'll come back to this later because that's a complicated question to answer. So a when you do independent experiments like I do have been to find coin flip. It is really good. Okay. If I have a industry and. And so if we play. Let's say you're modeling a factory and you're looking at  failures of the machine. Right. Well, sometimes these failure might be correlated, right, because there might be. It's not like each time you make an observation of all as it failed or not.  And it's not like it's  flipping a new coin. So in this case, the independence assumption might not be a good one. So you want to introduce some correlation and the graphical model terminology is one way to model to represent these correlations. Like you could have a chain coronation will see that later.  But the idea is, is kind of like the the  It's like thinking doing physics with assuming friction is not present right so it's a nice idealization which work in some setting. Very well.  Alright, last question from Jacob. We don't know theta. And we're guessing a that is close to theta as we can get. Right.  That's the idea. So the idea is  I mean, an example of a parameter estimator is the maximum necklace.  Okay. And so now we're saying is the maximum and accurate estimate close to the true value. And that's the square, which measure that  And and the assignment, you're basically showing that for some examples. The Emily might be  And actually we will show at the end of this class that the Emily is consistent usually under a rigorous conditions, which means that the bias and aspirants will go to zero as NGOs and feet. So the base will reduce in the various reduce as angles.  And actually usually there are often on base as well, but not always.  Okay, so  This is testing. So there was parameters Commission second example is I will just have two actions either zero or one. Okay. And this is often call hypothesis testing.  In statistics,  Because one might be that the hypothesis is false and zero is true, or the other way around, I guess. One is the opposite is true and zero. It's false. And so you're testing whether the hypothesis should be true or false.  And delta. In this case it is. This is what we call the statistical disk.  That this is that is the call test.  Okay, like for example like  An example of statistical test is you want to know if  Two random variable or distributed the same or they're not a dispute the same or  That's a bit complicated as well. So let's say you suppose that you have to gushing random variable. And you want to know whether the mean is the same or not.  Okay. And an example of that would be all i modeling the, the, the result of of how can the outcome of a medic medication.  And perhaps the mean represent like the the concentration of something in the blood and you want to know whether  Doing medication. A versus medication be changed anything I either concentration will be different in average or it doesn't do anything. So there are the same.  So it's a test of is a different or is it the same and so normally what you would have is, you will have some  You will compute some statistics from the data. And if the statistics as a bigger value than something you'll say, oh I reject the hypothesis or accept the hypothesis. So that's basically the two decision of zero and one.  And then the last in this case will be  It's usually the 001 last so you'll see  I have theta.  Have a  So fade out basically represent like was the hypothesis is true or false. And he is, you know, I suppose it is true or false. And you just say are your right. Right. So it's you'll get an error of one  When they're not equal.  And so when they die is not equal to a and you'll get an error of zero. If you're right. Right. Okay.  And and if you're familiar with the terminology of statistical tests. There's type one error and type two errors which are different type of errors.  But they're basically related to, you know, the expectation of this last because if I take the expectation of that respect the priority of saying the correct things, then you'll get the you know the the proportion of time you will you will screw up.  Here, but we want, you know, this is quite statistical thinking we won't go through tests in this class, but it's just to give you another example of how sensical decision theory work. user avatar   Okay. user avatar   And the third example. And that's the most important for us because that's very related well as also important. But see, this is what we do in machine learning.  And so when you do prediction.  I mean, there's many destination learning as well. By the way, but let's do basically supervised learning things prediction in machine learning. Okay.  So what I mean my prediction. I mean just classification. So this is learning  A prediction function.  In supervised learning  So, or position function or classifier or regression function depends if you do regression or classification and you could think of this as doing estimation. So you're trying to estimate a parameter, but the parameter is actually a function, right. So this is like function estimation  Okay, so what's the setup so that the setup would be that the training set would be  Input, output pairs.  You know, an observation of x and y. Right, x could be an image. Why the label or X could be an individual and why it could be their height right so that's, you know, the supervisor data.  And so x i will belong to some input space. I'll call this script x. So that's the input space and why i belongs to some output space.  Is here and when you you're doing machine learning in this provides context. You're trying to learn the input output relationship right  And so, if y is 01 or discrete. This is called classification  Is actually binary classification and if why for example is the real number, then this is called regression, you're trying to  Find a continuous value associated with the input.  Alright, so that's my observation.  And sold the distribution  On  X, y, we will call it P of failure. So, for example, like PA theta. So here I'm I'm abusing a bit the notation and you'll see why. So, but it's called PA theta, the joint.  On x, y is it tells you how x and y are related. OK, now the distribution characterizing my my training set.  Which I use P is a if I assume that the data is ID, then P is just a product of POS data, right, so this will be p of theta product PA theta blah blah blah PF data enzymes.  Because that's all I'm saying here is that the joint on x one, x, x one white one, two, x and y n which is characterizing capital D is just by independence is just a product of the joint on every x  Ray construction.  But this is also to clarify the terminology and the notation that capital P is really characterizing the whole observation and it could be made of multiple pieces in terms of distribution.  Okay, so that's my the what characterize the data set and now in the classification or regression, while we want to learn is a mapping function. So it's a set of function. So, it will be  The action space could be the set of function or you could have a subset of that right. If you want to prioritize your, your functions, but in general, you could have the function from X to Y and in mathematical notation. It's script why race to do script x. This is the set of functions.  From script x to y.  So that's my actions. Okay.  And now.  Papa, Papa. Now, what's the statistical last that we use in machine learning.  Machine learning  Machine learning  What we use as a no should available elation of our classifier or aggressor is basically the prediction air right  So what do we mean by the prediction era. So the last supposing that may true world with PR theta and for a prediction function, I will use f  Is usually  The generalization air and prediction air was that the expectation of the on the true distribution that I don't know, have some costs that they use little L for the cost of predicting f of x. When the truth West capital Y.  Si. That's the expectation is, that's the transition Arabic  So this thing here.  In mission earning. This is call the generalization error.  Now, or if you do castigation, it could be called the classification air.  Etc. Etc. There's multiple names for that.  And alright so a bit of notation.  So what we mean by expectation with little pure feta means that we think the expectation over a random variable. When the come from POS data. So the expectation is on capital X and given away.  And this little l  Here is called the prediction loss.  Not to be confused with the statistical us the statistical less is the capital L. The prediction last is just how we evaluate individual predictions. Like, here's an image.  Of something and then you say it's a cat. But actually it was a tiger. Okay, then how do we, how bad is that right, then you put a number for that. Okay. And then for an example of classification US could be the 011 error. If you do  Was upon classification. A typical  Prediction last that you will use his little l of capital Y F of capital X is just this one error, right, is why not equal to capital. The prediction, you made.  Is that zoo. One was the one there.  But let's say you do machine translation perhaps  So so so you're trying to you predict a set of words a sequence of words and you had a gold translation.  Which was provided to you, perhaps with your use instead of just are the equal you'll just say we'll use a blue score between that. So it's a much more structured notion of air.  Okay, so that's the generalization there.  And  Importantly,  in machine learning.  This statistical loss is often called the risk  Because in statistics, a risk is basically the expectation of some loss with respect to some distribution and this is indeed the expectation of a prediction loss with respect to distribution. Okay.  And when we talk about empirical risk minimization is basically minimizing the empirical version of this expectation, because actually this expedition can be computed because we don't know what PFA it is  But if we have training set, we could approximate the true PA feta, with its empirical distribution from the data and then we just try to find the function f, which minimize the empirical version of the risk, which is basically the training. Okay, so that's pretty standard machine learning.  And it's called the risk okay but me.  Just to distinguish it to another risk that will sue very see that very that will see very soon, which is called the frequent this risk. I will. I call this  The vet Nick risk.  Because basically  Then you may have let that Nick, who wrote a very famous test text books on statistical learning theory and support vector machines and all this kind of stuff. And he was a big  Kind of like a proponent and it kind of like popularize the terminology like structural risk musician from the empirical risk musician principle and stuff like that. So, so the risk is talking about is this  This expectation here. So this last this ethical us. So I call it the epic risk to differentiate it or to distinguish it  From the frequent is risk that will see very soon.  The frequent this  Risk  Risk  Actually, that's right, the frequent this risk and then I'll answer some questions. So basically, the decision rule.  Is mapping  The observation to a function. So I'll use f hat as the submission of my function. Right. And so in terms of terminology for machine learning.  You can think of delta. It takes a training set and output. The output of a learning algorithm, which is basically a classifier right or regression function.  So, so that that here is the learning algorithm, right, because it takes a training set as input and it gives you an output some kind of classifier of some sort.  And have had will be the prediction function.  So this is basically the prediction function, you could call it the prediction function or, you know, the classifier or the aggressors, right. So that's what it's called.  And what is the frequent this risk the frequency is risk.  Is where we computed when he did the virus virus decomposition  It's the expectation of the statistical us  Over to a random observation.  So I will have  P i guess P of theta, just to make it  Like theta and then  The randomization. Oh no, sorry, I need to make my decisions so  I have the action that I would take if the observation was the and then I think the expectation of that respect to all possible ramifications.  Okay, now there's a lot of questions. Let's see what you see  In practice, how do you assign a distribution of Theta.  So let's skip this question for now we're doing theory. There's no practice here it's theory and putting a theoretical framework and then we'll get back to how does the theory relate to practice.  And if I have not answered it. He's asking it.  If P equals the product of the joint p or an approximation. Yeah, capital P in this case was the product of the the the joint on X i. Why I given by piece data.  Okay, so, so now there's a long discussion about the notation. So I said, like, you know, usually  These pairs of observation or ID from some distribution and now somebody confused. I probably from is GD because or something because they say well,  In practice, you go over the whole data sets. So it's more sampling without replacement. So I'm expanding here how to generate the data set.  Again, and, for example, like that. So it could be the result of a bunch of coin flips and you know I can repeat the same observation I could have that the coin flips were the same, right. So, you know, I could repeat the observation.  Hopefully that will clarify this.  How do you calculate the frequent as risk would be if data is not known.  Very good question. So that, again, that's the the notion of  Theory versus practice right so in theory when practice, you don't know POS data. So you can have to compute the frequent asterisk.  In theory you could consider a different setup and analyze what different countries risk would be in this setup.  And then you could say, oh, well actually an estimator is better than another one. And actually, that's what we start to talk about now already is to try to compare estimators in theory.  But in practice, you can estimate the frequencies risk of an estimator, actually, you can actually estimate it frequently. It's really hard and you will need to have a lot of observations of training sets.  And so you can estimate the statistical loss.  For a specific action by evaluating that you know the trainer or the validation error or you have a different training set, right. So that's an approximation of their risk.  Or the web link risk. But if you want to estimate the framing of this risk, then you need to generate a lot of data sets.  And then evaluate run your, your learning algorithm on each data set. Get decision rule evaluate its last but  You need to know the truth for that, so perhaps you could evaluate its tester and the hell out test set. So that would be one way. So you could have held our test set.  Where. Or how about validation set to be more safe and then you just train your, your learning algorithm on multiple training sets you evaluate undervaluation set and you take the average overall that that's just an estimation of your frequent this risk.  Okay, so now  Let's talk about comparing procedure because that's that's the whole point of this theory.  Okay, so  Yeah, so there's nothing practical yet, right. So this is theory comparing procedure.  But, but the whole point of statistical theory is to also encode what we mean by the performance of a statistical procedure. Okay.  So let's say I have procedure one and I have procedure to  Okay.  And so as I mentioned the frequent this risk.  Of a procedure.  I'll use annotation.  Are and the first argument would be either PR PR feta, depending on the fiber pat tries to me of my world.  And the second parameter will be the second argument will be the actual procedure that I'm evaluating the frequency and by definition, this will be the expectation over possible training set.  Of the last statistical loss of  Doing delta on the training set. So capital D.  When the true world was given to us guess  That's the frequency with  Which is not the only way to evaluate a procedure, I will see other ways are very so okay  So basically as a frequent is you can compute different properties of your procedure and you compare these properties for different procedure and that's how you you can try to understand better how the procedure be  So the issue here is you don't know capital P  You don't know what's the world. So what you will do normal easy. We'll get something called a risk profile.  Okay, so basically  If I looked at here at the possible set of world of distribution, for example, I could have p one here and two different distribution.  Then I compare how does my frequent this risk of procedure one varies with a distribution and that could be the kind of risk profile, you get. So that would be the risk the frequent is risk as a function of  Of  Of the distribution for procedure one. Okay. And here's procedure to that's its risk profile.  Okay.  So basically here.  I have two different estimator or learn like  Yeah, two different estimator, and I'm trying to compare their performance, but their performance our functions function of what function of the unknown world. And so if I asked you is delta one or delta two is better, which one you choose. So how many people would choose that the one  Say yes.  How many people would choose the other two.  And somebody says, it depends on where the world is. Yeah. That's a good point. That's a very good point yet. So if, if the world I care about or that I see in practice is in this region. Well, don't they want is clearly better because it's better  If the world can be anywhere. And I don't like to be to have really bad loss and perhaps that that too is better.  But, you know, and there's no absolute answer in this case, right, because when you have two functions. You cannot compare to function in general like they're incompatible.  Unless one is strictly below the other one, then, okay, you could say one is below the other one but usually the cross and whatever, then there's there's no real winner. Okay. And so then what happened is in statistics. People have transform these risk profile.  To a scanner.  And so there's this thing called media max analysis, she might have heard and statistics.  Were in this case, you look at the worst case performance over a set of distribution. So, if you look at the max overall P in a family of distribution of your frequent this risk. Okay, so this is really a worst case.  So when you say that  Worst case. So that's the max and what you want is you want to minimize that max of the risks. So you want to minimize the worst case of that. So that's why when you say that an estimator is meaning max. That means it's actually minimizing the worst case behavior over a class of distribution.  And so in this case delta to from a worst case behavior would be better than delta long because it has lower worst case so that enables you to compare procedure, but why are you caring about the worst case.  Right, perhaps the worst case never happens in practice. So who cares.  So another thing you can do instead is to do a weighted average.  Weighted average  As you take the integral over possible world. Let's say we prioritize that this the world with a  With a with parameters data. So then I would look at the risk when the true world is described by pure theta and for procedure and then I have some prior or waiting over my partner or my distributions. Right.  So this has kind of a, of a Bayesian feel right.  And very soon, we will see that, indeed, you can relate the Bayesian viewpoint and the frequent his viewpoint by using these weighted average.  Hey, but this is also the idea of like, well, there's some distribution and I care more about to for the performance of my procedure and so I would say procedure, one is better and procedure to when I take this weighted average of the frequencies risk overall possible  World with some weight. Okay.  That. Yeah. So Jacob rephrase exactly what I said so. So did the choice of this waiting could be seen as the choice of a prior user avatar   Huh. user avatar   Yeah, so, so perhaps  The better terminology would be p of theta here. So I would have a parametric family of possible worlds and contrast by theta.  Okay.  Somebody is still say Is this still assuming we want practically be able to get RP of the well. So, for example,  We could just say, oh alright I'm doing maximum likelihood estimation of a Gaussian parameter that could say, all right, suppose my data is gushing with this parameter, what would be the frequent this risk of the Emily.  And now for every possible. Gosh, and parameter you will have a different fitness risk for it.  And then you will summarize that by saying some distribution over to the possible Goshen, and then it will become one number. So you can actually compute this whole thing right user avatar   So, so user avatar   Because here. We're not saying, oh, I know, for I don't I know which distribution during my data here was saying considered these possible distribution during my data. How well in average will my procedure do  Alright, so last topic. Well not actually. But let's say the last topic before the break. Just to highlight  Different ways to analyze things and different conventions, also in different fields.  pie in the weighted average is the waiting in the weighted average. So that's, that's the, that's the week.  But it could be seen as a prior sometimes okay so  Let's talk about the pack.  Theory.  Versus the frequent this risk.  So frequent. This was the very standard way to analyze a procedure in statistics. It's the expectation  Of over the the possible observations wasn't in computer science and machine learning because machine learning is coming, coming out a bit of computer science mainly depends how you talk how you do the history, but  Computer Science like to have these worst case.  Guarantees because expectation is not the same thing as the worst case right when you talk about the complexity of an algorithm.  You know, you want to know, how bad does it doing the worst case or how long is it taking the worst case, the same thing as doing an average case analysis original worst is not a thing.  But there's something in the log in terms of like performance of a statistical procedure where instead of looking at how well it does an average you look at how well it does somewhat in the world's worst case. So in machine learning.  You usually look  I mean, usually I think now is a bit less true but two years ago, which was more this kind of analysis.  But the thing is you look at our call tell bounds.  At tell bounds.  For the distribution  Of  The statistical last  Where the training set is random. user avatar   Okay. user avatar   So pack.  Theory.  Stands for  Probably  approximately correct probably approximately correct  And they're kind of statement, you would have is that the probability that you're a statistical decision loss.  In the context of the unknown world p  For whatever your procedure is. So for example, let's say we would do a pack analysis of support vector machine that's an example of an algorithm.  That would say, okay, delta would be the support vector machine classifier. And we say, well, the transition error of our classifier will actually be  Upper bonded by some quantity. So what we say is that the property that user avatar Blain-Montesano Yves  Depends. user avatar   Okay, so I lost my internet internet connection.  So right now I'm using my cell phone. So I suggest we go on a 10 minutes break and hopefully my internet will come back in the meantime. Sounds good. Alright, see you in 10 minutes at three to 3044  Okay, we're back.  I got interrupted in explaining the the pack theory to probably the approximately correct. So, what was I saying, so I was saying that  in machine learning. Often we talked about these generalization air about. So what do we mean is will say that the generalization era. So the statistical last four predictor will be upper bounded by some value.  With high probability. Okay, so that's why they're called probably approximately correct, right. So the idea is it's the bound is not always true, because there's some quality that they're not holding quality over what  Probably T overtraining said you could have a training set for which you are very unlucky. And then you're bound doesn't hold. Okay. But the idea is you will have the property of the bounce failing is small, which will be little delta. And usually this delta will appear in the back.  And so you would have something like the the the  The transition error of your classifier will be upper bounded by its training air plus some kind of complexity term error and something which depends on the the delta which is the property of failing of your guess what, that's the kind of  Analysis that people did in machine learning and let me now just do a little doodle have a drawing to differentiate this kind of analysis with the frequencies risk. Okay, so let's have the probably t  Then city.  Here.  And on the excesses. I have my generalization air. Okay.  So this is basically the tester.  The transition there.  And now, why am I talking about property density. Well, the thing is d the training set is random. Right.  So you could be lucky and get a good test error or you could be unlucky and get a bad tester for your algorithm.  So now we're just characterizing. What's the shape of the tester. And so you could have something which is like this. So that's what the, the distribution over possible test there for your algorithm. So this is the distribution  Over  Possible.  Test there for your learning algorithm. Okay.  So, you know, here the test error is small, because I'm. Oops. user avatar   And user avatar   So in this region here. I'm quite to the left so that the test. There is small and I have some property here to test her is high and here that this URL is in the middle.  And the frequent this risk. What it does it take the expectation of the test their overall possible training set. And so what you're looking is just had the mean of this distribution. So this would be  The frequencies risk, which is the expectation of the thing. So this is the frequencies risk.  So it's the meat of the distribution.  The pack analysis doesn't look at the mean it looks at a nice upper bound  Such that the mass above the upper bound is smaller. That's why it's called a tail bound. It's bounding the tail of the distribution. Okay. And so the mass here will be smaller than some small value delta, Tim. Right. So that's the. That was the whole point.  Of this statement here says the quality of the tester being to be bigger than some value is small. Okay, so, so this would be the this here would be the pack bound  So you find some value such that, oh no, I forgot to that this is the  Pack bound  So the idea of the pack analysis as they find some kind of upper bound on my test there which is true with high probability. Okay.  And so this is kind of a more worst case guarantee it's much stronger guaranteed mean because it means that with high priority, your performance will be always better than this bound  And so that's actually a very nice certificate in does the kind of thing that usually use in machine learning and more like traditional Colt style learning theory analysis.  OK, so now the few questions.  In the frequencies should be the mean over D, not the last, so the frequent this risk is the expectation of the test there.  Of the statistical loss. Right. So the expectation of L. So, it is expectation over de de right so the randomness come from the D. But what I'm averaging is not this is, I mean, what I'm averaging is is the last itself.  And is the distribution over training sets our test sets the distribution is over training set distribution is over.  But we are evaluating so this capital L a machine learning will be the tester right so it would be  This thing here. This expectation of over a distribution that I don't even know. So that's why, you know, I call it a test there and you know because you could use a test set to evaluate it to approximate it  And what we do is we look when the training set varies  How, how, what's the distribution over my possible test there. And so that's that's what I meant here. So the. This is a test air which depends on a training set.  Can we see that backbones, do some form of a scientific analysis. Nope. So pack bound do some kind of worst case analysis with high quality.  There's no notion of aesthetics here yet the ascent ethics is when you know look at how does the germination area bound or the frequent this risk varies when I change the size of my training set. When a very deep end. Sorry. When I very M and and indeed.  Standard traditional statistical analysis will do some tactics mean when n goes to infinity, you'll get something, but when it is finite, you would have not good idea  But when you look at generalization air and computer science. Usually the also do finite training set analysis that you will have a generalization, about which will also depend on n  And if n is too small. For example, if n is too small compared to the dimension of your problem, the bound will be useless. It will be bigger than one. And we always know for example, if I have  If my, my last was the 01 error, then it's always smaller equal to one. So if I give you about have to. What does mean anything.  But I can't read the word inside the property L bigger stuff. It says stuff.  This is stuff.  Which just mean some complicated expression of  Stuff, but like as an example of  Why is this disappearing. So stuff is  So an example of a generalization about  Would be that the  The test air.  Of  If hat would be bounded by the trainer.  Plus some kind of  One over a square with n, for example, that's the standard example of where the  Dependence on the number of training simple would appear of square root of  Complexity.  Of a hat.  And then there will be usually some kind of log of one over delta  Us.  And so I'm not actually being very specific here I'm just getting into shape. So, for example, like the complexity of a hat could be the VC dimension of a set of distribution. So that's the kind of  Generic balance that people but we won't go into these too much in in this class. This is just to give you a high level overview of the and also because you could have a you know a whole set of lectures on just this topic.  Learning Theory is actually very vast it's just to give you the the terminology and the high level understanding of the different ways you can analyze these procedures.  In my adventure trip prediction and optimization. I go in more details. Because I will do these kind of analysis for a stricter prediction methods.  Okay. Any more question on pack analysis or frequent this risk analysis. Now we will talk about quickly about vision, mission theory.  And why the frequent this perspective.  Can explain also why Beijing procedure work well.  Their last equation.  That's your question.  Or what's the question about the last equation.  The last equation was saying.  I can bound my test error which is just this  Statistical Russ, such as a set a different word f hat was the output of my learning procedure and I can say, here's the stuff that was the stuff that I was talking about. So that's an example of bounce right  And the problem here is that the trainer depends on the training set. So that's a random quantity but  You're allowed to put  You're allowed to put on this right hand side. So this d is random. You can also put some function of the of the D here also, because it's just a problem statement.  How is empirical risk different from the tremendous risk. Yes. So that's what I was explaining here with the VAT Nick risk. So the Vedic risk.  Is the test. They're actually from a machine learning perspective. Okay, and I cannot compute the true test there because I don't have access to the true distribution. user avatar   Okay. user avatar   And so  What you can do is you can approximate the true distribution with the training data. Right. You can just say, Well, suppose my training data was my distribution and then you can compute the empirical  Error on your training set and that's called the empirical risk.  So when you do and when we will come back to that when we do regression and least squares regression, because when you do the square regression, you're basically also minimizing the empirical L to error on your training set. That's how you estimate the parameters for your regressive  The frequent this risk is taking the expectation of this thing with respect to the  The well okay this thing doesn't have any randomness because  F here is fixed, but the frequent this risk now says okay, my, my function will be the output of a an estimation procedure and a random training set.  And I will now look at how well does this do on average over possible training sets. So that's this outside expectation. So this l was already an expectation, like this one. And I'm thinking, another expectation, but respect to the. So that's the frequency is risky.  Okay.  So let's do Beijing. This isn't theory now so I told you about.  Frequent disc risk. That's the expectation of the Sabbatical last it's the simplest analysis you can do tell bound does the pack analysis. And then you can also be a Bayesian  Which is very different. So when you're a frequent disk.  You were  Averaging or concentrating possible training set in your analysis. Right. So I was looking at the distribution of my performance when I change the training set.  When you're a Bayesian you don't care about other training set. When you're Bayesian you just try to act well with your current observation. Okay, so it's a very different  Philosophy instead of like averaging over the data, your conditioning on the data. So you're always conditioning data.  So as a Bayesian, what you do is you condition and data. If you remember as a Bayesian you would compute the posterior over the parameter mysterious conditioning on the data.  you condition on data. And so what if you are a Bayesian and you would try to do analysis, you would look at the quantity which is called a Bayesian posterior risk.  I will use invitation are subscript be for their Beijing risk.  And this is not a function of a procedure, it's it's it's for a specific action and its condition and an observation. Okay. And by definition, what it says.  This is the integral over the possible parameter of the last the statistical last suppose the world with data and I took action eight  And then what I think the expectation with. It's like a weighted average, but I actually use the posterior as my Wait, what's the posterior given my observation, the theta. Okay, so this is the  Hysteria  Over possible words.  And this is proportional  To the prior over to possible world times the likelihood of your observation given user avatar   Okay. user avatar   And so that makes a lot of sense from an agent procedure because  Vision perspective, if you remember the vision philosophy is I have uncertainty I encode all my information of according to the the thing which aren't something with protein distribution.  And then I just use laws of properties to act. Okay. And so here, the thing is, I want to take action, a  And if I take action. A and the true world with data, I would have this loss.  Well as a Bayesian what is encoding the uncertainty about the the true world is all in the posterior because I've already saw the observation need. So now I look at the process. This is my belief about the pastor world.  And so my goal as a Bayesian will be to minimize the risk. Okay, so if you're at the bit and and the beauty of the Bayesian approach here is like the optimal thing to do is clear.  This is how I should act. But the nice thing is I should act is only a scanner, right. This is like the problem before was that I don't know Seta  I don't know, say that, but now I've integrated our data disappeared.  The thing I don't know is not there anymore. All I have is my action and the data. The data. I know, and the action. I know that I can take. So the optimal action is clear. So as a Bayesian you will actually minimize the vision for serious. So the Bayesian  Oops.  The Bayesian optimal action.  And will use in a frequent disk notation as a decision procedure if you're a Bayesian given the training set, what you would do is you will just minimize the perceived risk you'll do the admin over  The action, not the parameter  The action.  Of the vision perceived risk.  So that's why it's so cool to be a Bayesian from a philosophical points perspective, because the optimal thing to do is clear just computer posterior  Then take the expectation of the procedure respect to the take the expectation of the statistical loss for the different actions respective was there and minimize that. That's the optimal thing to do.  Because when you were a frequent disc, there was these risk profile are not comfortable so you don't know which one is better was here. There's no risk profile because theta has disappeared. I just integrated up  Okay, so let's do a, an example of a Bayesian approach here with more concrete aspect. So if I'm doing estimation. So in this case, the action space is a set of parameters. So we're doing estimation  Okay. The other question, I'll get back to the questions because there's too many questions.  Alright, so if I do estimation. This standard statistical last for that was the error.  Then exercise to the reader.  You can show that minimizing the Bayesian busty or risk.  When you use the L to norm.  You would need to take the bus the expectation of of the posterior so it's the person you will use as your estimate the the expectation over theta given the training set. So that's the posterior me  Which we already saw also by the way we looked at the binomial example in conflicts was also the disturb  But this is because I'm use the Altoona. If you use a different if you use the  For example, the, the absolute value that same were in one day to simplify  Let's say use the absolute value. So that's like the norm. But let's say we're in one day.  Then  The Bayesian estimator is not the place to your mean but it's the posterior media.  So the optimal action depends on the kind of last few years, right.  So when you do look at the when you look at the LT LT norm if you're very, very far you penalize much more than if you use the absolute value. Right. And so depending on whether you want to penalize more or not, you will use the mean or the median.  OK, so now let me relate the frequencies approach with the vision approach and then you can ask all these questions. user avatar   So, user avatar   To summarize, so we had this statistical less  Which depends on the unknown world data and the random data set d of our procedure.  And then what we can do is we can be a frequent this and we can take the expectation of that.  Was spec to the random training set.  And then when we get as a frequent this risk. user avatar   Groups. user avatar   Their frequencies risk.  Which I noted by r theta and delta. Okay.  And then the problem is that it depends on data, which we don't know which is super annoying. So while we talked about, oh, I could also do a weighted summary I could summarize the performance of my procedure over different data by taking the average  Theta according to some waiting function, right. So this is the weighted summary.  That I talked about. So I think a dig deep think Romain frequent this risk respect to some weight function by  Okay, so that's  One way to get the actual  Skylar out of my procedure.  Now, if I'm invasion. I don't want to average over data set. So what I do is I will actually first take the expectation over the unknown world according to make posterior  Okay, and what I get them is the Beijing mysterious  Of my action given the  This is the vision posture risk.  That I talked about  And then the procedure is clear, as you just minimize this vision perceived risk over all actions. And so now. A as a function of the  You will use the Bayesian procedure.  Which is minimizing this  And so now  Now, now, now, now this depends on the training set.  And so if I want to be frequent this again, I want to know how well does the agent procedure does when I change my training set.  I didn't let it sit. Because so so the Bayesian they don't really do.  Any analysis because the optimal thing to do is clear for in their mind. It's like, well, you have the right prior  If I mean you figure out the prior. How'd you figure out the prior you don't. That's the hard part. But if you know the prior and the everything then  What you need to do is clear, as you compute the steer and you you compute the perceived risk and you minimize that. And that's the optimal action and that's there's there's no notion of analyzing because you know that's the right thing to  Frequent this approach. I mentioned the consider different estimators the compute properties of these estimators and the compare them.  So, let's say, Now I put my frequent this hat. And I say, well, how well does the Bayesian procedure. Right. How does for this specific prior how well will this Beijing procedure do  So what I can do now is, what's the average of the vision for steer risk with respect to the training set, so I could take the expectation of that respect to  The training set and now there's the question of will. I can actually obtain the same weighted summary from the left.  If I use as my distribution. The marginal distribution. So, if this is the marginal distribution here.  To what I mean by the marginal so I will define the marginal over possible training set.  So here I'm using a bit of terminology. Usually I use a not a capital letter, when I talk about the density right but I already have capital D and I want to use little d because it all these like dimension so  So well perhaps that's put little d user avatar   Because user avatar   They use P over weird stuff in the best  No, I never did. Okay, let's do that.  I'll say  Let's do this.  Man.  All right, let's call it the marginal and so P marginal  P Marge.  Of d  Will be the integral over theta.  Of peace Ada user avatar   P user avatar   I guess I did use capital D. All right, I'll keep my big sorry about this is a bit of annoying Titian because I don't likely to be but  I'm not being super readers.  Anyway, so it's the marginal of the from the joint on both data and the random training set.  And the whole point. Now why am I putting those two this diamond is the idea is that if  The waiting function that I use my as my prior then the vision procedure will actually minimize the weighted summary. So the Bayesian procedure.  Database  Actually is optimal. So it will minimises mean it will minimize been preserved minimises the weighted summary.  When you use as your weight the prior function.  So you'll use by a theta will be just P of theta, the prior I used to define my procedure, procedure.  And so from a frequent this perspective.  If you care about.  The weighted summary of a frequent this risk.  Where do you have some put some weight function over the possible worlds. Well, then the best algorithm for from this perspective is actually the Beijing procedure.  Where you use the prior the correct prior over this the same that you use as the waiting function to the prior to Beijing prayer.  But if you care about the different weight functions was important. You're a frequent this and you care about.  To put more mass on say  Big variance in your Gaussian, but you were a Beijing and you believe that the the variance was small, so you put a high prior on the small well then this patient procedure won't be optimal because it  It won't do, as well as the because it has a mismatch between the weights function us TV things and the prior he's  Okay.  He  So there's a question.  The marginal of P capital T equals little d. Yeah. That's correct. That's what I meant by this notation here.  Yeah, so P marginal  You could think of.  I mean, to be clear, I could have said the p of capital T equals little d and blah, blah, blah, and then use capital T equals little the everywhere.  And we have been a bit more rigorous for more complex algorithm is the Bayesian approach tractable know  The main approaches almost never tractable, which is one of the why, you know, even though it's so beautiful in theory and practice. It's much more complicated. Okay, so, so in particular.  The optimal thing might be clear to do but it's tractable to do it to you do something else. Well then wins. You want to analyze how well does something else is doing.  Which is why the frequent is lens is very important to a piece, you know, analyze the properties of this approximate procedure that you do.  And then somebody asked if I was a frequent desert Asian right  So I'm actually  I think I'm a bit more frequent isn't the vision, but I'm a practical vision. So that means that I think the Bayesian approach is very powerful to to get good procedure.  But you still need to analyze this procedure with the frequency sense. Okay, so, so I would never just believe blindly my my prior  Which is what a subjective vision or religious subjective patient does  Yeah, I think it, what you're saying is correct.  Yeah. So I think, I think.  I think it's good to have both side like you you because the Beijing procedure actually have very good properties. It was with see that later when we talk about the legalization or the James Stein estimator.  But on the other hand, they're not practical. And you need to. Well, there are, I mean, you need to also have analysis tools to make sense of what you're doing, which is why the frequencies perspective kind of is useful.  But different kind of perspective doesn't tell you what to do it, just tell you  How to analyze things, which is why you can get ideas of what to do by being Asian, and then use the frequent this analysis to see how well they're doing. So let's give me the. I'll give you an example of estimators because  That's also the idea of a frequent this as well there's multiple ways to estimate things that's now analyze their properties.  Examples  Of estimators  Alright, so let's say I'm trying to estimate some parameters.  From data.  So I have a function, I want to fight. So the estimator will be a function from my observation to my parameter space.  So already talked about the maximum likelihood estimator. Another one is the map estimator. The message maximum plus series. So you have some prior and then you will instead of maximizing the likelihood you will maximize their likelihood times the price.  Then a third one which is different. It's called the method of moments.  I'd say em small capital.  And so the idea of the middle of moments.  Is you find and objective. So one to one.  From the parameter space.  To the moments of the random variable.  Okay, what are the moments. Well, the expectation expectation square. It's address. So these are unscented moment. You can also look at centered moment like the variance  Etc.  And so, and then once you have this mapping, which says, oh, if I have this parameter, then my moments are these you can invert them from the empirical moment.  you invert.  It from  The empirical moments.  To get data.  Right. So, so what are the empirical modern so the empirical moment of X is basically sort of a true expectation. It's just the empirical expectation. So it will be the empirical average of my exercise. And then if I have say d squared empirical moment was the same thing. Now, but  With the squared.  The average of the squares.  Etc. Okay.  So that's the idea. Okay.  So let's do an example.  So, for example,  Suppose I have a gash in  Let's say my observation is a gash in with mean new and very in sigma square  Right. So now I can look how am I, what are my moment. So the mean is just Mew. And then the squared is actually the variance, plus the means square  So that's the function from it's a function from my parameters to the moment and  It's actually 2D and it is objective.  Yeah.  And so I could say, Okay, if I know mew n sigma square  Then  I will get a  New ID sigma squared plus b squared, right. So that's my that's telling me what are, what should be the moment as a function of the parameter  And so now what I want, as my estimates is I will use well okay the estimator of new and the estimator of the variance will just be the inverse of my function.  And then, but I don't I don't have the true moment because if I had a true moment, then I would get the correct parameters. So what I have is I replaced my true moment with an approximation from the empirical moment. And so I will just use my empirical average and the empirical squared.  And I will apply the rest of my function to get my estimate of the pepper.  That's it.  Okay. And it turns out here by the way that here.  This estimator.  Is actually the same.  As  The maximum likelihood estimate, right, because you can see here that  inverting this piece is pretty simple, right.  Because now I'm just saying, well, I will set my estimate of the mean to the empirical beam and the empirical mean is the maximum likelihood estimate of a gotcha.  I guess I haven't derive that yet, but we will see how to derive that later. Alright. Press the assignment you might have to, I think you do that to the assignment.  Perhaps next assignment.  So, um, but this is not true in general. And this is actually a property. This is a general property in the exponential family.  So it turns out that the method of moments.  For the correctly defined moments is equivalent to maximum likelihood in when you suppose that the distribution is in in a specific exponential family.  And this relationship cannot actually be seen through the lens of leveraging duality. So that will be one of the most beautiful result in this fast where I will show you why.  maximum entropy with moment constraints which is a bit doing like them with a woman and maximum Ecuador equivalent  But when you're not an excellent show me.  The Muslim woman that is different and maximum next year. And actually it has in the last few years being quite useful for latent variable model.  For latent  Variable models.  And so  These are model where I have a latent variable z that I don't observe and I have a simple observation model like x, given z is a Gaussian  So when I marginalize out to z i get a mixture. Okay. So for example, this could be a mixture of gases. And if I want us to meet the parameters for a mixture of gal shins. Unfortunately, the log likelihood is not  convex or concave in this case. And so the finding the maximum negative parameter is actually very hard.  But instead of finding the maximum naked parameter. Instead, what you could do is just relate the the from the parameter of your mixture of Goshen, find a function to a bunch of moments which is objective and then just inverted and these are called spectral methods.  Because basically  You will create these  So, for example, that covariance is basically a matrix. So, you will recover these these these these inverse function, you will need to do  As video analysis of some kind of like matrix observations.  so starstruck asked, what do you do some moments not exists, well then you're in trouble. You can enter it into the, the, the method of moments, if, if you are using a distribution for which the mean doesn't even exist, the cushion distribution doesn't have a mean. I mean, it's not  Defined and so you can do the meta moment for that, I guess.  There's no perfect is the reflection function really  The universe function here is just  I didn't say what was the inverse of f. I just said, I will use as my estimate for my parameters the application of the inverse of f to this vector, but I'm too lazy to compute the inverse explicitly, I will need to think about it.  Actually, it's pretty probably pretty easy. It's, it's like  X squared minus the other one square right so basically this is equal to  I will have the empirical moments here.  And here I will take the empirical moment.  Of the square and I will just subtract the empirical moment square because there was  That's the  For the mess. At the moment, you have to guess the distribution. I have to dis, guess not. The distribution, but you have to have a parametric  Family characterizing your phenomenon and then so. So in this case, because we're doing estimation here. So we, we need to have a Patrick family and we want to figure out what are the parameters of my distribution.  Ah,  OK, so I remember already a bit out of time.  So I guess I'll have to continue next class in Access. I'll talk about a fourth example of estimator, which is the empirical risking position that we talked about and the James Stein estimator, which then means the ability  So somebody is asking me why the method of moments have been used. Recently, I didn't get why they are preferable to me. Okay, so when you have a latent variable model. First of all, the method of moment is different than the maximum flexibility.  When you have a latent variable model that can make sure if Gosh. And it turns out that the optimization problem, you need to solve to do Emily is intractable. Sometimes it can be NP hard to solve it. So even though the Emily might have nice property. You can even computed  The method of moments and in different there's a different  Procedure which in prints out for the latent variable that you can actually compute, sometimes you only need to do some as videos of matrix. And we have an efficient algorithm to compute as VCS. So then, the nice thing is you can actually analyze a different procedure, which is  computationally tractable and it's still consistent I eat when you have infinite data, it will convert to the right thing. So that's kind of the nice thing. The Emily is consistent, but you cannot compute it. In this case, so it's kind of useless.  Any other question.  Right so Abdullah I'm ask you a quick question.  Or well he's asking is, quick question.  The don't forget that there's an assignment God Tuesday.  And it will be you before the beginning of class so that you don't get all book done during the cast with the assignment and  Jose as office hour on Monday. From one to 2PM so do take advantage of that.  Ah, OK. The assignment. There's a question why why or no n is yes or no like call. And then he said, Cool. Alright, so I hope you enjoyed this abstract class. If you have trouble understanding some concept. Don't worry, it's non trivial I either go back over your notes or also i mean  This this class was men just to give you a general idea of these things that you don't have to master them yet, because these actually are tricky, tricky, tricky concept.  And in particular, there are mixing ideas from machine learning and statistics which are fun or a bit seem differently from different communities. Alright. Have a nice weekend.
  Yes, so last class was fairly abstract with the so called decision theory and then I finished by just mentioning different estimators  Which we would might want allies, the properties. And so I mentioned it. The Emily, the map method of moments. So now, today, I will give you a last fourth type of estimator, which relates to what we do machine learning.  And talk about a few properties of Emily and then we'll go. We'll start to go much more concrete on  With classification approaches and regression. So we'll talk about linear regression, etc. So that's the plan. user avatar   Let's see. user avatar   So today will do  Finish.  Example of estimators  And we'll do  Linear regression  In a bit more gory details that you might have seen in other classes.  So,  So,  Another example of estimator.  So the example for that didn't have time to do last time.  So this basically is doing empirical risk musician. So if we are in the context  Of prediction.  In the mission running sense of just learning a mapping from input output. So the action space is, you know, set of functions from x to y.  This is just quick notation to mean that. So x is the input space.  And why is the space.  Then an example of estimator for that.  So this is, I've been different. And like the other example I gave, which was this meeting a parameter here, you know, the parameter represent the whole function.  But so an example of estimator which from observation would tell us what's the prediction function we want is using  empirical risk position.  Empirical  Risk  Minimization user avatar   Maybe user avatar   My position.  But the risk between quote because this is the Vatican risk. You remember  Like the  The Vedic risk, which is not the same thing as the frequent this risk.  IE basically the generalization air.  And so this is often called er em.  And  And the idea is, I will have my laptop risk. Oops.  I Jewish an air.  Wrong color.  So,  We have our generalization error which dependent on the true distribution p that we don't know, and our action which is F in this case. So by definition is the expectation over or random possible test example.  From P of the prediction loss. And then we have possible ground truth why. And then our prediction effort capital X right so that's just a transition error.  And the idea in our M is you replace this  This intractable. Well, this unknown expectation  With the empirical version. Right. So the empirical expectation  Of the loss.  Which means that we're using the empirical distribution of the data. So it's basically summation over today said  Average of the observed last on the training examples you guess  And yeah, we could use  It's us because now we have instantiated the protocol. So it will be. That's a why I user avatar   am so excited. user avatar   There's somebody didn't mute themselves.  So now the effort in the ER an estimator f hat er M is just minimizing  This training air.  Over some hypothesis guess  Would be the hypothesis.  Okay, so very standard approach in the shirt.  And you could even have a rigorous empirical risk and innovation by adding a regularization term which, instead of just minimizing the training error you also add  Some notion of measure on you know penalty on on different functions which could be the norm of the parameter square, for example, help to know. We'll see. We'll see this when we talk about rich regression  Okay.  Any question about this estimator.  Okay. So Jacob is asking whether we would do the same kind of approximation for the frequent this risk.  So the thing is to do an approximation to the frequent this risk. You will need to have multiple training sense  Right, so this is if you want to estimate the frequent this risk. It means you want to estimate the performance of your learning algorithm over multiple training set.  Which is fine, but it's different because here, what we want to do is fine learning algorithm actually which means it has some training set and I want to use the training set to  You know, get an estimate of what should be my prediction function. It's only had one training set. So now, the idea is I actually use this training set to approximate My, my, the supposed test set performance of my argument.  But so that's, I think, kind of a philosophical difference between those two approaches. The other thing is I can say that, indeed, in, in, in probably 10 statistics you  Will in a lot of different places do empirical average to estimate some quantity. So this happens all the time. And there's also this notion of like bootstrap procedure, which is to estimate uncertainty about your, your method. So, for example, indeed, like if I want to estimate the  Variation of my prediction rule when I change my training set and I want to know what's the variants of my prediction when I change my training set. So that's also a notion which I need to  Take an expectation of our training set. So what you can do in kind of the bootstrap type of approaches us resemble with replacement. The training sets to get multiples subset of the training set.  And then you train your classifier on each of them. And then you look at what's the variation, then you can average these every variation and that gives you an estimate of how it varies. When I changed my trainings.  So this is called like a bootstrap estimate user avatar   Okay. user avatar   And why does that Nick risk equal jurisdiction error. This is just from the setup so so so the journey mission. So the  Yeah, the case of prediction is just terminology. So I'm just saying, like, when we talk about empirical risk musician. So the, the risk in the empirical risk was this kind of quantity which is also where we call it missionary modernization.  So,  Okay, so somebody made a very good question. What's the difference between this hypothesis class and my set of functions.  Good question.  Usually you might decide that they're the same. So the idea is that this thing here.  Was a formalization of the problem we're trying to solve. Like, it's like this is like from a statistical theory perspective and saying,  Look, I'm trying to do prediction means I need to learn some kind of mapping from input output. I might decide that you know this. Include. Include all possible functions. Right.  And this is a high evaluate my prediction function, even though I don't know p. So those all of the ingredients for the problem and trying to solve.  Now, the way you try to solve it doesn't mean that you your, your estimator could decide to to output arbitrary function, you can decide that I'm only putting linear classifier.  And so I would restrict my class here and of course you know if if you're  The best if the distribution you have will have a prediction function which is very far from linear well this estimator will be very bad.  But that's fine. I mean, you're free to define estimator however you want, doesn't mean that they're good estimator. And actually, the assignment I gave you an example of an estimator, which was not very good. So user avatar   Yeah. user avatar   So, so that's this  The cake close case called fold cross validation would be closer to the frequent this risk. user avatar   Hmm. user avatar   That's a good question. So normally  The cross validation is to get a good estimate of the test there.  Because of training air. Let's say, for example, I do see I do classification  And I use the nearest neighbor classifier. Well, this always have zero training here.  So that doesn't give you a very good estimate of what's the test error because of course you cannot just say it would be zero tester. So doing k cold cross validation gives you an estimate of what is the the the  Oh no you're right, because you're changing the hyper parameters.  Your  Changing the decision rule when you do your basic training on a subset you estimate the value and use normally that's for model selection.  So yeah, I guess you could say that the cold cross audition would be closer to their frequencies.  It which which that are you talking about training error was a generalization there. Can you rephrase your question.  Yeah, so that make there's literally no training area and the Vatican risk because it's the true expectation. So the ethnic risk is is this expectation here which is not the training set. It's actually the, the true distribution.  Is just that we will replace it with an empirical version. When we have access to some like training examples.  That's the the empirical version will be the training air empirical risk is indeed training air.  True risk is the transition user avatar   Okay. user avatar   Alright, so  Let's talk about the James Stein estimator because  I told you that Emily has issues.  The James Stein estimator.  It's a very mysterious estimator.  So this is basically an estimator.  For the  To estimate the mean of  Random of Gaussian random variable.  So you want to estimate domain.  Of some Gaussian with me new I'll put a vector, just to say it's a vector.  In multiple dimension and  Independent covariance matrix. So basically I have D independent Gaussian  Gaussian variables.  They are basically x i, or  Independent  normal with mean new I and the same sigma square. They're not ID, because they have different means. But they are independent.  Right and so  There's this estimator. So if you do maximum likelihood estimate of domain of gushing random variable that's something I think you did, didn't the knowing the 72 the estimate of the violence but  The estimate for the main is just the empirical mean except for cash is very variable that just take the empirical mean and that gives me the maximum likelihood estimate  And it turns out that deed and the empirical mean if you think the expectation, you get the normal mean so it's unbiased estimate. OK, so the maximum IQ this estimator is unbiased.  The gemstone estimator instead of being the bias is actually biased.  Okay, you basically shrink your estimate towards zero. You do.  But by with the cost of a bit of bias, you actually decrease the variance significantly  And so let's say, but much lower variance. Then, Emily. user avatar   Right. user avatar   And if you recall  The bias variance, the composition  That we use in the assignment.  For the square less  You had that the frequencies risk for the squared loss when the true parameter is theta and my estimate is data hat was the expectation  Of theta minus beta hat in to norm.  That's just the squared error expectation respect to the training set right and it composed into pieces. There was the biased.  Minus data square and the variance user avatar   Okay. user avatar   And so for Emily, the bias is zero, but it has a high variance  For James Stein I increase a bit the bias, but the decrease significantly the violence and the some of the two is actually smaller than the Emily. Okay. So it turns out that the James Stein estimator.  strictly dominates the maximum likelihood estimate  The maximum likelihood estimator for  The bigger equal to three. So it turns out there's this weird phenomenon that in a low dimension, you can beat it. But if you have at least three dimension.  So this is the dimension of the meat right  Then using this this shrinkage, you can actually do better than maximum likely estimate  And what I mean by strictly dominates. It means that the frequent this risk.  Of the genocide estimator.  is smaller than the one for the Emily estimator, but this depends on data actually this is true for all theta.  And there exists some data.  Such that the risk is tricky smaller  And so from. So in this case, you remember like I talked about this risk profile and usually the cross. Well, what happens here is that the risk profile for the Jameson estimator is  Is always below the one from the Emily. So it's just a better estimator. So you should not use the value from this perspective.  Which means that when when that an estimator, as is the median by another estimator. It's called inadmissible in statistics. So Emily is  inadmissible. user avatar   In this case, user avatar   So by definition in the midst of all just means that gets limited by something else and to delete means well it's a bad estimator, because there's something which is tricky better than that. So why would you use it.  And so if you're curious about this. I recommend you look at the Wikipedia article on James time you'll see the estimator. It turns out that the the James Stein estimator, you can interpret the gemstone estimator.  As a empirical Bayesian approach.  So we'll see that will see what I'm pretty cool Beijing means in our  Much near in the class when we talk about the Beijing methods and more in more general it but a beige and mess up the idea was, I would put some prior to my parameter and then do these kind of like posterior kind of  Updates, and then pretty cool Beijing means that there's some parameter of my prior, which I didn't know how to pick them and events.  So if I have uncertainty about an element about some parameter, I should put a prayer over it. So you can actually. So let's say  I want to decide what are the parameter of my prior will. These are called hyper parameter. Well then, if you don't know how to choose your hyper parameter, because as a Bayesian you don't have a good  belief about it. There's uncertainty, then you need to put the prior over the hyper parameter. So this is called a hyper prior and then you can have  Also the time you might have uncertainty about the parameters of the hyper prior and then you put another prior so you get this your key of prayers  And being empirical Beijing means that I will fit some of these hyper parameters from data instead of just like coming up out of my thin air, which is what the true subjective patients would do. Okay, so  It's really cheating because of Beijing would never do that. But  Basically a practical frequent. This will use Beijing methods will do that.  What's the stepping crypto. Yeah. So this is kind of like  Yeah, so in this case means for the bigger equal to three. That can, or more specifically for the ocean.  To estimate the gun, the mean of a Gaussian, because in general like there's other places where the Emily's is admissible, but for  The setup of estimating the, the, the mean of a Gaussian and dimension three and more where, you know, the variance ball. There's a few kind of assumptions here, then it's an indivisible and  The user avatar   Only. There's a lot of punishing user avatar   Question. user avatar   Is it true that the L two frequencies. This is the same as the means squared error.  Well,  Yes and no. So yes, here, and no because means squared error, there's a question of what they mean respect to what right  And so the frequent distress would be the mean respect to the random training set, that's where the mean is coming. But for example, like in signal processing, you could just look at the main square error of your prediction.  And there's no notion of frequent this risk there. This is just an evaluation. This is just like something the square of your method.  Over some observation. So there's no frequent this risk notion there. So that's why I'm saying I mean squared error when the mean is respect to the  That possible training set that will be different. Once this risk for this squared loss because I forgot this risk would also be valid with other lost in the square last could be the binary that's or something.  Okay. Another question is, is the risk dependent, the loss function. Yes, it is. Does the gemstone dominate only for square loss.  Good question. I don't know if for the say the Edwin norm loss, it still would be the same. So this actually don't know.  Somebody knows that does know or you can also look it up, but I know it's true for the Altoona.  And then somebody asked is that it will the chain ever stop. Yes. So the funny thing here is like, Remember there was like  At the end of the earth or something. There's turtle. And then there's a on top of turtles and type of turtles effort. There's like there's like this mythology of like turtles on top of each other all the way up and  And indeed, for the prior over prior prior prior there's a notion of, well, when should stop and and the point is, like,  At some point there is not really uncertainty about the dependence, because they don't really matter too much. So it turns out that when you go higher in their yard key and then you tweak these parameters.  What's happened down, it becomes much less important like it. It's very insensitive. And so at this at this stage, people are fine say this properly encode my belief. I don't need to. I don't have more uncertainty.  I guess that's their answer. But indeed, in general, like you should only stop when you know that this is the correct and coding of your belief.  Okay.  So I could actually spend a whole lecture on James Stein, but I think we have other things to cover. So I'll move on.  But it's kind of a fascinating example but the main idea of James time is it's a bit similar to what we'll see in like  These regression, stuff like that. But we talked about trigger ization so when you read your eyes, your method you will usually bias it but  It will increase the bias, but it will decrease the, the variance and sometimes it will decrease the variance, much more which will make the method and more stable and nicer and Emily, usually called over fits and that way it has high variance  Yeah, I think the so Omar asked if the notion of an estimator being in message will depend on the wrist function or we always suppose Lt. So that's actually  A good question. I don't know enough of the details of the statistics terminology, because usually a lot of these terms when you take a statistics class during the basic  Classical setup and then the classical setup. It's all too warm and elsewhere loss and stuff like that. So, and, and I think  I think it's, I don't know. I don't think there's a theorem which says that if you're in admissible for one last year in Espanol for the other losses. So I'm pretty sure inadmissible is will be in the context of some kind of us.  All right. Alright, so let's talk about some properties of Emily to wrap up these properties of estimators  Properties of Italy.  And I guess these are synthetic properties.  So under suitable  Regulated conditions.  On the parameter space and you're Patrick family.  And I won't go into this conditions because actually forgot them and they're fairly technical and I think it's outside the scope of this test. But if you can look in any graduate statistics textbook, they will tell you.  Or I can dig them back up if you want  So basically if we define our estimator as the arc max.  Over the power of space.  Of the empirical  log likelihood  Expected like you would say to just  log of p of x i data.  So here I am.  Assuming that the data is ID.  And so when I evaluate the likelihood of the data, then it's the product of the likelihood. And so then when I think the login becomes  So whether the parties have this estimator. Well, the first thing is with very weak regulating conditions. It is a constant is consistent estimator.  So,  It is consistent  I eat converse the right data. And so the idea here. This is supposing that  We suppose that the end the training set is coming from.  P of feta race to the end. So basically, of course, you want to be consistent in the sense if the true parameter data doesn't lie in your parameter space, right. So if you don't have. If you're not modeling the correct set of distributions, then you won't be consistent.  Though you'll minimize your conversion to the minimum kill parameter  So you have consistency. You even have central limit theorem.  And so that means that when I looked at the deviations between my estimate and the true parameter this converge in distribution.  Through a normal with zero mean and some covariance, which is called the  Information matrix.  Which has to do with like  The derivative of the log likes you would  an expectation  Here I'm basically give you some keywords that if you want to go in more depth. You can look at the statistic textbook.  And then it is also called SM tactically optimal  And this is called the Kramer Rao lower bound.  Which basically means that  It has a minimal  Asset that exists variance  Among  All reasonable estimators  And reasonable as also some kind of regular condition. So what I mean by the aesthetic variance. Right. So the idea is  When I looked at the deviation of my estimate with the true value.  For for this means that  This random variable here will converge with it will have a distribution which is close to a normal as n goes to infinity, right. And so, in particular, what I can do is I can divide both by squared and and so it means that the variance here. It's basically divided by n.  And so, what you get is that  As an increases the Gaussian becomes more and more concentrated around zero, which means that your the difference between your estimate and the true thing is is is very, very small. Right, so it becomes very concentrated okay and  So, so this tells you how it various with the samples and and the value here. This is called the essence that the variance. So the bigger the essence of the variance  The, the more samples it take to get small deviation. Right. And so, so, so, so here it's a matrix because it's in multiple dimension, but if it wasn't dimension one, this would be just  Sigma square. The difference of your, of your the essence of the variants of you've got should have your estimator basically have your deviation of the estimate. So that's what it means by  When we say that Emily has minimal accepted the variance, it means that actually the other estimators will always have accepted the variance, which is actually  strictly bigger well this turkey bigger but big are equal, then the information matrix. The matrix is the best you could do, which is done by the Emily. Okay, so that's basically do what does Kramer rebel lower bound sets.  That was a bunch of questions.  Oh week i'm not saying week regularize I'm seeing regular routine. So this is regularity.  Here this is  So by week regularity conditions. I mean, there are some assumptions you need to make on the  The density and the parameter space to make sure that these result hold  Very good law. user avatar   Please curate user avatar   And indeed now important question from Suffolk, how does this a synthetic optimally to relate to the gemstone that's estimator.  So what happens is that for finite end the gym sign estimator dominates the Emily, but as NGOs infinity, the James Stein estimator becomes like the MLS there's no difference. And so they have the same except that the variance  And it comes a bit with the fact that the there's a there's kind of like you do a bit of like when you have a Bayesian estimator. When you say you do math estimate  There's a prior, and then there's a likelihood the effect of the prior becomes weaker and weaker, as you have more data point, because that's where all the information is going user avatar   OK. user avatar   OK. And then the fourth property.  Is invariance.  So basically the ML. He is preserved under reprivatisation  Okay, so what I mean by the. So, suppose  You have a by injection  From one set of parameter to another set of parameters will use prime  Then  If instead of estimating feta me stating  The premises ation FF data.  And that put a hat here. So that would be the Emily. This is the same thing as first doing Emily for theta and then mapping it with it. So if I do Emily in the transform space.  And I look at the powder which maximizes the likelihood in the transform space. It's the same thing as doing the Emily in the original space and then mapping it to the transform space.  And so this is actually very useful because then you don't have to worry too much about where you put the hat. So for example, let's say I want to estimate the, the variance, just something you didn't the assignment.  So I parameters, my gosh, and by sigma square  Well, this is the same thing as looking at the so this means you will take the derivative with respect to sigma square right because that's the parameter  And you can instead just take their respective sigma IE estimate the maximum that could pander in the sigma land where sigma is positive and then square it. So there's no difference between those two.  And similarly, you could have some crazy function like oh, let's say, now I want us to make  You know, I use sign in the proper because you have to have objection. So you need to restrict your, your possible parameter, but let's say I want us to make now that Emily, but I you sign of sigma square as am I my parameters, all you can just take sign of sigma hat and then squared. user avatar   That's the same thing. user avatar   And now, if it's not about rejection, like the sign example is not a bad direction.  You can actually generalized  The Emily.  With something called the profile like you  The profile.  Likelihood  And so  What do I mean by that. So let's suppose I have a mapping  From theta to a new set of parameters.  But there's multiple theta which are map to the same at that  So then it's a question of if I do Emily in space.  Which of the  Which of the parameter, should I use  Right. So by definition the profile likelihood would say  Likelihood  By definition,  It's like you will define on this new space. What we do is we actually look at the max over theta.  Of the points which are mapping to enter  Of its likelihood  So I will associate the likely I will define the likelihood of a specific ETA to be the likelihood for the parameter which actually maximize the likelihood of the data for all the parameters which are all mapped to the same at that  And then if we define  The maximum likelihood parameter in this transform space as just the arg max.  Of the profile.  Likelihood then we have  That the maximum likelihood in this space.  Is the same as same as just mapping the maximum next you in the originals, but  OK, so this profile accurate trick is one way to handle when there's no by injection user avatar   Oh, user avatar   Can I give an example of the profile acute situation.  Yeah, so let's say  Instead of prioritizing  Let's say g  Is  Let's say I have my mean I have a gash in with me view and vibrancy my square. And now I will say, gee, of mew is new square  This is different than the sigma square example of before because  It was only positive that which matters here now is that if I have a plus one or minus one.  So if I have a plus or minus mu their map to the same parameter. So I cannot distinguish a positive and a negative me  And so now the problem is, suppose that so I need to define what's the likelihood  Of my data given the value of new square, but now it's ill defined because there's multiple parameter, I could use in the original model and you have different likelihood  Okay. And, and you could have issues in the in the relationship between the maximum I could parameter. If, for example, sometimes I decided to choose the parameter  In the new space, which had lower likelihood of the data that I could like there's two parameters which are match to new square  And I picked them you which actually had smaller IQ. And so this means that, well, I won't pick this one because it has small IQ and then there's others which are bigger like you  Even though the other one which was mapped to the same one might have very high likelihood that that would actually be the maximum likelihood parameter if well defined. So that's basically what I'm saying.  This is just a way to make sure that the maximum likelihood in the transform space correspond to the maximum likelihood in the original space by making sure you always pick the likelihood in the original space which is maximized.  Does that answer your question, Dora.  Don't we end up with a worse estimator Indian  And in this case, know because  All you have so so  So basically, what it means is  Here when we estimate at Emily always says, well, we don't care about the design of the mean. Oh, we care is the square to me.  I'm trying to estimate. Describe to me.  I because it could be that indie like I have some observations and I wanted to estimate the square of the observation. I don't care what's the sign of the observation. Because the square is insensitive to the site.  And not just making sure that you know  This is well defined so that everything works.  And so if you didn't do this. Let's say you still need to define what should be the likelihood of a square and then there's a problem because there's multiple possibilities.  And then you will need to, for sure. Solve the maximum next to you in this transform space to get an estimator. And this, it's not guaranteed. In this case it will be the transformation of the original  Yes, I think for the for the Nick has answered, Jacob.  So I think  We're good. Oh, and by the way this terminology here, this, this is called a plugin estimator.  In the sense that, oh, well, we want to estimate a function of something  And. Well, one way to do that is just estimate this thing and then apply the function which is mean I plug my estimator inside the function  And for the Emily in this framework, it actually doesn't change anything. There's other places where this might change things. But here, this user avatar   Technology user avatar   Is there any other questions about Emily or properties of estimators. So the plan is. I think I'll take a 10 minute break and then I'll go over  Linear regression and logistic regression user avatar Remi Dion  I have a question. Sure.  What's the, what are the constraints on G here if we have any user avatar   Regarding condition.  That's my lame way to to escape so  I think you need some constraints like you, you need  Could you really just have an arbitrary function which is even is now in continuous user avatar Remi Dion  You said that even the  Non aggression. user avatar   As  If g is not about rejection then to apply this framework, you need to to properly define what would be the likelihood of the transform  Problem with this this profile like you, right. So we'll say will define the likelihood of a parameter eta as the max over all parameters which were mapped to this eta of the original model. So if you do that then doesn't matter. It's not by injection  Because it's a function, by the way. So this means that g is defined on all the feta. Alright, so, so that's one aspect.  Yeah, so I think I don't see any. So you might have some  Rigor the issues if these max.  Or not well defined right if there are infinite or if there's infinite. But if there's they're not achieved.  Their achieve at the boundary or something, I don't know. So there might be some weird stuff happening.  But, uh, yeah. I think it's very gentle.  Another question. user avatar Oumar Kaba  Yes, I did have a question that I asked in the chat.  For Amy's question school user avatar   Do you want to state it in words. user avatar Oumar Kaba  Yes. So I wondered if when doing a Bayesian parameter estimation with map, for example, or any other when  You can always find a prior that will make it equal to the maximum, like you mentioned, is it the case because in most example. I think we've seen in class, you can actually find a prior that will make it the same estimate the maximum likelihood  Formerly user avatar   So the answer is yes, but you need to generalize the priors. So there's this thing called em proper prior, which is a prior which is not a correct distribution, because it's infinite. It doesn't integrate to one.  So for example, let's say I do a merely of the Gaussian  Random variable. So I want to, I want to estimate the meat. The meat is could be anywhere on the real line. So it's unbounded set. So if I don't want to introduce a prior over to me and I need to put a prior overall real numbers.  And I want it to be uniform. If I don't want it to change the Emily right the map. Sorry, it's if the map to be equal to Emily. So then you need to put the uniform distribution to roll the real which is not possible. And so  What happens is, in many places people would call it an improper prior, which is just like you don't care that it's the prior was not normally visible because the posterior will be normalized once you add the an IQ than you realize. So so so that's kind of like a formal trick.  And so, but in general you can, as long as you define some kind of uniform thing.  Over your the  Thing you estimate then because when you do maximum map you multiply both the likelihood and the prior but you don't care about the normalization because it's just a  Constant. Well then, if the prior his uniform. There's no difference with the maximizing of their likelihood, which is what Emily's doing user avatar Oumar Kaba  I see. So it seems like even if you're a vision, you can still Backward. Backward. Backward. Backward engineer to find the prior that you want to satisfy any estimation want to do that. user avatar   Ah, I'm  Not necessarily, actually. So, so basically, there is this industry of approaches which is. That's what I'm, what I'm doing a support vector machine.  Which is not even Emily or whatever. It's just an estimator. Okay. Is there a probabilistic interpretation for  Estimating the classifier of support vector machines using properties and perhaps Bayesian approaches.  And then people came up with fancy distribution that when you do this approximation XYZ, then you get as VM or something, and that  For them, especially if you're Asian that's feel very satisfying because they say, oh, it's, it's like stuff. I know. And it gives some it's inside because oh  There's like the probably teams behind it. So it is kind of attractive as a as a method, but sometimes it's really, really hard to come up with distribution so that it work, and I'm not sure.  You could show that it's always possible. I don't think there's some stuff, which are weird. So already you cannot, you can already not even do it with proper distribution like I like okay I gave the example, if you're  Set is unbounded, then there's no way to define a uniform distribution on that. So this is kind of ALREADY CHEATING to do improper price. user avatar Oumar Kaba  Okay, thank you very much. user avatar   And Dora says if you repeat experience many times, well, if you have another training set. Sorry, a very large training set, a lot of observations, then  Map becomes like Emily. That's true. Yes, I'm saying that usually the defect of the prior becomes stability swamped by the data. Usually when so as NGOs and feed the give the same thing as small data question. user avatar   Yep. So it's kind of a very general thing. And I don't know if there's really a way to answer this kind of shortly.  But just generally kind of the process of choosing a prior and going through the whole Bayesian process kind of didn't completely stick to my head and intuitive way. I kind of get the math and I kind of get why works but but still like baking a prior. It just seems very odd to me.  And I've kind of had a hard time just going through the examples. So is there maybe like a resource or something online you'd recommend reading user avatar   Ah, that's a good question. So  Right. So the first. So the resource on nine nine, we need to think. So perhaps like something like the practical Beijing or something. There's good books, but their books they not sure if there's a short text them that  You could also look at keywords like prior elucidation which is basically how to  Figure out prayers on things and or what would happen is usually that see your statistics and you would talk to experts and the expert has some good intuition about how things should behave  And then you try to kind of talk to them a lot to figure out how to formalize their belief about the system so that user avatar   You know, you put the user avatar   You construct the correct prayer, according to their user avatar   expert knowledge. user avatar   And Dora has suggested Beijing methods for hacker. Okay, so I don't know this book breakfast. Good. So  Two things I would like to say one thing is, first of all, you know, doing this kind of statistics is is an art which takes a lot of time like like people are trained statistician do a lot of years of training and practical  Things to build up this intuition about  Which statistical procedure to use in which situation or if they're Bayesian, you know what, how to build my prayers and support.  So it's, you know, it's definitely non trivial and we won't be able to do it to just in this class, and the other aspect I would say is that the general rule of thumb for you to come up with a prayer is just think about if you had observed this phenomenon in the past.  Right. So, for example,  The coin flip. Example is  The idea is, when you put a prior, which is a beta, you have this parameter alpha. It's a, it's a, it's a, heads or tails. So you have the parameter alpha and beta for your beta distribution.  And these are called prior counts in some sense that say you put one in what it means that you've observed once in the past head and once in the past that tail.  And  And that's why, in this case, it's still  Yeah. And so if you instead put 1000 1000 as your alpha and beta  The beta destruction will be much more concentrated  Around one half.  It's mean because you've seen a lot of observation and the were very, very equal  And so you're more. This is like a stronger prior in the sense that you're you're  you're committing much more on the fact that, oh, I know that the pounders should be around one half because I observing the passive thousand head in 1000 user avatar   Tail. user avatar   And so then that and then when you add your likes you update you will use combine your observation with that and it's just adding the counts. You've seen right so the to start with alpha beta prior  Than the posterior becomes alpha plus the number number of time I've seen becomes a beta plus here with off up as number of times I've seen head and beta number of times details. For example, and so  And so  You can think on the sense of constructing the the prior from just prior observations or interpreting what are the parameters in the prior from like observing things user avatar   Okay. user avatar   Now there was Ezekiel. user avatar   Um, user avatar ezekiel williams  Yes, I asked a question earlier that I think was just missed in the chat. I was wondering, are for starters, if we're talking about a consistent estimator, then the  The variants of the estimator will converge to zero as the sample size goes to infinity, right, or user avatar   Well, so this is again going back to this technical point that I discuss in Slack about the assignment is that if you have  So so consistency here.  I've used the standard terminology of consistency, which is convergence and probably  When you have the bias and the variance goes to zero.  You have that the squared, the expected squared error.  By the bias vertical position will go to zero. So you'll have something like  Expectation  So if the bison the reins goes to zero, you will have that expectation of beta hat minus Theta.  Norm square, this goes to zero as and ghosts and fee. Okay. And this is called El to convergence for a random variable and an L to convergence implies convergence and protein.  And so it's a stronger thing.  And so if you have user avatar ezekiel williams  Or sorry, go ahead. user avatar   Yes, if you have the bison the variables to zero. It's so it's definitely consistent also in this week or sense, which is this one here. user avatar ezekiel williams  I guess. So it's possible to have a case where you'd have convergence and probability that the variance would not go to zero.  Yes. Okay. user avatar   Can you think I gave an example on Slack. user avatar ezekiel williams  Okay, then I will, I will look at check that out. And then, I guess, I guess, in that case, in that case, then this is when  Because I was wondering, in particular about some topic optionality because if I guess if you have, if you have out to convergence when esoteric optimal view somewhat a moot point right because various will converge to zero, regardless of your if you're just looking  Out to convert yes leaders, so user avatar   That's a good question. But so here it is a subtle points. So what happened is that  I have is, I have multiply by square deviations or multiply by squared circle event and so  That's why this doesn't go to zero because but like if I only care about this deviation. Then I go I put this in the denominator. And then the variance goes to zero. So what's important is not  So when we talk about the synthetic variants, of course, the difference of my estimator goes to zero, in some sense,  But we want to know how fast it goes to zero. So that's the dependence is it's screwed. And is it and there's actually sometimes which some  Estimators which are much faster going to to the normal. It could be an end instead of scrutiny, for example. And then once we figure out the, the rate, there's still the constant, which could differentiate different estimators and that would be the constant, we're talking about  And so in a synthetic comparison of estimators you will look at the variants of one versus the other one. Yeah, something variants of one resist something variants of the other one and by the aesthetic variance. Here we mean the scale variants, I guess. So I guess, perhaps, to be  To be like more clear scaled in a sense that there is this square with end here.  So if I don't intrude. If I don't blow up my deviation by screwed. And there is something parents will be zero for everything. So there's no difference.  Okay. user avatar ezekiel williams  Yeah. user avatar   Cool. Alright, so it's 334 let's take a 10 minute break 344 user avatar   You recording user avatar   Okay, so perhaps one comments important comments about statistics because  I feel, perhaps I think statistics is frustrating. Okay, in the sense that  It doesn't have a clear, you know, here's a method that you should use, and that's it. Don't worry about right so so  Let's say you know you do physics and am I muted.  No. Right. You hear me so because I saw Jacob like playing with this. user avatar Jacob Louis Hoover  Know my head, my headphones are not on. I'm sorry for confusing you user avatar   Okay, good.  All right. So yes, I was saying that, let's say you want to, you know, send a satellite to to to  Tu  On on on our, you know, you want to send something on the moon. You can use physics like Newton's laws of physics we can compute a bunch of stuff. It's actually a bit a bit unclear what to do, in some sense, firstly, you don't have simple things like laws of  Newton. Newton. Sorry to the like yes the mechanics laws for in statistics. So as I mentioned at the beginning, it's statistics is an ill.  Polls in risk problems. So I have observation I want to figure out with the model. The problem is that there's an infinite number of model.  Which could have explained the data when you only have a finite number of observation. So in general.  There's nothing you can do without some assumptions and then there's the question of, well, okay, are these good assumptions. Well, that's where the art comes into play.  So if you'd read us statistics textbook or if you take a lot of statistics class you'll get a lot of techniques.  You'll get a bit of insights on how these techniques behave and which will build a bit your, your intuition and your, your  Your knowledge of where to apply where but you will never have a foolproof guaranteed results. And so that's the bit unfortunate situation there.  And when I say often like it's an art with comes with a lot of experience that Cubana via data scientists also a lot of experience, like, you know, you'll start to build up some intuition and understanding which methods were where and why  Yeah, and myself. I don't actually have that much experience with doing a lot of applied data analysis. So I'm not  That helpful i a lot of other faculty, you have way more experience with that which can teach you more this kind of intuition.  So I have more background than the theoretical aspects of these that that analysis methods and  Once you start to juggle with these insights you also start to get a bit more understanding of what it means. Right. And so  And so now I'll talk about linear regression and prediction and you already start to get out, you'll always see some example of what I mean by getting some intuition and some some insights on what are the assumptions.  In the, in the problem. Okay.  So right now it's a bit vague, but you'll see it will come much more a bit more concrete. So let's talk about prediction first  And then we do talk about the linear regression space specific case so prediction.  I use the terminology prediction to mean that we want to learn or estimate a prediction function.  Or a classifier or aggressor from some  Input space to space.  And then if y is 01. This is binary classification  If y is a you know say k minus one classes than this is multi class.  Multi class classification and if, why is our  This is called regression  And x now will be, let's see, in our to the D right so doesn't have to be. But let's save. For concreteness, it's a it's a vector and Rd  So,  Now we start. So we have two random variables. So we can already talked about our simplest graphical model at two nodes graphical model I will have x i will have white  And now there's two ways to factor this joint on x and y, which gives two different perspective on modeling this thing. So if I looked at the joint on x and y. I can rewrite it as the conditional of why given x  And the marginal x. And so the conditional of why given x this in some sense, give you a prediction model, right, because  If I observe x. And I want to know what's the why associated with this x world natural one is just to maximize the property of why given x. So if I have pure why given X. Sometimes I know how to predict the best way  Is a prediction model. And this is model over x.  Basically  This is telling us  So let's say for example, why was a class like x is an image. And why is like is the zebra. Is it the cat or whatever. So perhaps these are naturally images. So p of x. That would generate that would be a  Model on what kind of images I would see in the world. Okay, which is 50 non trivial to build  Why given x as well, giving this image. What should it be unstable. So it's a bit easier. And actually, now you know CNN or really good OR RESONATE or these fancy neural network are very good at giving some class.  So I can also pantries, and the other direction, which will they now I want to know what's the distribution of X given why  And I have a discussion over why. So why the PR why this is just a prior over classes. And this is just telling me if I don't know the observation.  Over my distribution are there are some classes which are more likely than others, right, you could have that perhaps the example. If you look at video  Or yes, if you look on YouTube videos. Cats are overly represented. People love cats videos of cats. Right. So you could put a higher priority you for the cat class in this case and  P of X given why this would be called a class conditional. So now it's saying, oh, I know it's a cat. What does it look like in terms of images that would be what's called a class condition.  And there's this terminology that people have used, which is called  The derivative perspective for classification  So when we talk about the generative approach in the context of classification  What we mean is we will actually model p of x as well.  Whereas the another perspective is the conditional perspective.  Where we don't care about how images arise because we care is given an image, what should be the label. So we just want to know about what's the distribution of why given x. So we only model poi given us  Here and  At the beginning of the 2000s. This approach was called this competitive.  Was traditionally called  Descriptive  And the sense that it's trying to discriminate between different classes in a position towards the journey of approach, which tells you how to generate the whole data, though the whole x and y.  Was the. This was our approaches. I don't care how Gen X is generated and given x. Now I want to predict what's why. So I wanted to discriminate from which possible. Why should I assigned to this x  And I like to put this on a continuum.  These generative approach versus descriptive.  And that's what you work in the assignment. By the way, you will have some general approach and some conditional approach so logistic regression is an example of conditional approach to classification  And I do a continuum and I will have generative conditional. And then I was at something I would call fully this community.  And here you're getting something which is a bit  My own perspective on this because, by the way, I didn't make PG thesis on this primitive methods. So, so I've been working a long time on this medium approach and  In particular I relate the descriptive philosophy to statistical decision theory which I already explained to you, but it may explain  A bit more cuter what I mean. So first of all, I say it's a continuum. So on the left. When you are generative, you're not this competitive.  And the more to the right, you go, the more descriptive. You are so conditional is more competitive than generative, but it's still not fully distributed. So, and I'll explain what I mean by that.  Alright, so if you're generative from a class. If you know like a classification perspective, you would model the joint on x, y, so you'll have a parameter for your distribution on x, y. So that's why I put a little theta as a subscript  And how you estimate from data, you will do maximum likelihood estimate. For example, you know, you could Max, find the parameter which maximize both the property of x and y. So, so the extra seen is that will influence the parameters.  If you're carrying all the above the conditional you'll only have a parameter for the conditional of why given x. So the you won't  Say how x is generated the pampered won't talk about x and the way you will estimate your parameter will be by maximizing the conditional like you  Which is just what I wrote PFA like why gimmicks. So you only want to maximize the quality of the wise, given the X. You don't care out the X were generated because they're not useful for prediction.  And  The fully this community of approach.  Is  From a statistical perspective, it's focusing on the task, you're trying to solve. So if you're trying to solve prediction. See, my, my best at the beginning. I just said.  All I want is a prediction function from X to Y i didn't say I wanted to probably T on why given x here.  I'm giving a prediction function age and I will use this sentence transition era to evaluate it. So there's no notion of probably tease anywhere of  In terms of like what, why, given X. So if the task is to get a prediction function well the fully this kind of approach will focus on this task, which is to get to model directly the prediction function. So, it will model.  The prediction function. So, it will have a parameter for each prediction function.  And unlike the conditional approach this prediction function is not necessarily derived from a distribution rights not derived from  P of why given X, necessarily. I got already sent me mystery.  And how do you learn. Well, you could do. For example, like  regularize CRM, for example.  Etc. So so the so the way you will learn will be based on the thing you care about what's the carrot. The thing you care about the test error which has a loss in it and stuff and so  The idea is you will try to find a prediction function which does very well on that was when you do maximum conditional likelihood user avatar   Like user avatar   Here they will be user avatar   You know, user avatar   There will be a notion of  This prediction loss on the data set.  And so, in particular, if I change this loss, it will change my estimates.  Was I believe instead of squared air. I care about at Warner or if I do mission translation. I could put the blue scarf. Here are the rules score are different score, they will all give me different prediction function because it's used in the ways to make money method.  Was if I only do log likelihood  There's no relationship between this last here and the log likes you direct. It's just a distribution here. There's no notion of, oh, if I use Bruce core versus rouge.  Then what's happening is, like, No, I'm not talking about rescore rouge here I'm just talking about what was the distribution of why given x  Is. That's why I say this is not as descriptive as this one because it's less focused on the actual task, you're trying to solve, which was prediction, it's solving something more general which is getting a distribution why given  And so what happened when you go from left to right, is that the more you are to the left, the more assumptions, you're making implicitly  Because in the journey model. I'm also modeling X, even though I don't need to model X to predict why given x  So I need also to talk about this one over x which is not super necessarily directly related to the task of trying to solve. And so because of that. It's actually less robust for prediction.  So you're trying, you're actually solving a harder problem or a bigger problem you're trying to the submission of x and y.  You've had to dispute over x and y. You can condition to get why given x. And if you have condition why given X. You can even try to find the y which minimize the  The expected error, according to this last giving you the submission. You found so you  Once you have this insufficient. You could also derive a prediction function which which is good if the decision is good. So it's a more general thing.  But if you screwed up because your assumptions are wrong, then you'll have a bad distribution and then you'll get a really bad prediction function.  Okay. Whereas if you go into fully descriptive setup, you're in some sense making less assumptions.  Because in this case, you're not trying to model X. You're not even trying to model why given x. All you care about is predicting why from my ex.  And because the way you do it is by trying to that it does well on the the task, you're solving so  Even though perhaps you didn't have the right prediction functions. Perhaps the optimal prediction function is not in your family, you will still try to find once, which does  Pretty well, and your data set. And so that's why this actually using you get better prediction performance and it's more robust  Okay, but if you have good prior information if you add good journey models, I can part of your if your data is really  This is really generated from the journey model that you thought about simple, I think, oh, it's a gash in an x and y given x is  Is another guy ocean or something like that. And this is correct. Well then, then the jury of approach will actually do better than the discovery that approach because it has more prior information which is correct. Okay. So,  So that's a bit like this is different approach.  And you'll see this affecting the assignment. So in the assignment you will have different classification techniques. Some are generated  And the journey of approach will actually do very well when you, Jerry of assumptions are correct and they will do really bad when you're touring of assumptions or wrong. Where's the this way to approach to actually do fairly well in all situations because the more robust  Alright, some questions.  Okay. So Jacob asked, well, why isn't it the fully this primitive set up a  Just a special case of the conditional approach, where instead of having a distribution of why given x I put the deterministic.  Distribution right that's what you're saying, Jacob.  And the answer is, well, from a modeling of the prediction function. This is true, but from the way you learn, it's not the case because  You couldn't do maximum conditional likelihood. If you put  Property zero on the wrong thing. So, so that will give actually not very well defined criterion and  Moreover, here you have different type of learning algorithm with, for example, like, super vector machines which have nothing to do with  Conditional likes you. And this is what people use. Okay. And so, so, so the learning criterion. So you both have the fact that you model prediction function and the learning creature in the US is also more related to the actual statistical tests that you care about.  And actually this thing that I'm talking about here I go into much more detail in this advanced search or prediction class in  Which is that I'm teaching in the winter and the whole motivation there is that instruction prediction. These structured losses here are very important and it makes a big difference. Whereas if you just focus on these conditional like you would approach. You can miss that.  Well, that's a lot of questions, but I'll pick some of your questions. So the first thing is that  You're asking that rules is not a loss function. If I was an evaluation metric. Well, this is basically how you evaluate your errors. So if I have  Seven doing machine translation. So I would have a ground truth reference sentence which is the correct translation or one correct translation.  They will be your predicted translation and then you want to evaluate how well they're doing. So you want to evaluate that. So that's where you could use a blue score score for that, or this kind of stuff. There's different kind of  evaluation metric for that. Okay, so that's so that's for this customer. So it's not the same thing as a loss in the sense of training nuts right or or  What I call the surrogates nuts, which is what you use to train your own women.  She actually says that if your ad regularization in the fleet is going to have you're putting some assumption. Correct.  But and you know the the main difference here is that because you're focusing on the last year, in some sense, a bit more tied to the task, you're trying to solve. And so it's it's kind of a quote weaker or more robust assumptions, then if I just go to the probabilistic approach.  Can you rephrase your question it.  Or perhaps ask it. user avatar   It's your voice. user avatar Hattie Zhou  Can you hear me. Yeah. Okay. I guess I was just asking like when we model the function directly and we know exactly what the target is which is like modeling and distribution or how do we know we're getting closer to the right answer. What would be the user avatar   Yeah, yeah. So, um,  So basically when you're solving when you're here.  You're not doing prediction. What you're doing is is distribution modeling.  Like when when you know maximum likelihood is like the idea here is, I want to find a good distribution which model, my, my joint over x and y.  Here's the same thing, except that I like cares, the modeling to conditional. Well, okay. And so if you could have multiple observation why for the same x  You might have multiple wise, there's a bit of noise. For example, it might be that if you ask multiple humans the label for an image that could be a bit of  disagreement between them. And so you could. Now if you take a lot of humans, you take the limit of that would get a distribution over why for all the possible class. Right. And now what you would like is to really match that. That's it. Right.  So that's really a problem of distribution modeling. Now, if I say, well, now I want to tell you to make a decision. I don't care about the decision. I want to make a decision, which is what prediction is doing.  Well, what you could do is the basically the the motivated part would be that he  Hat of x.  You would do the ARG Minh over  Why tilde.  Of the  expected loss of predicting wide till the when you're learning model is correct, right. So, it will be summation over why of priority over that, you know, you have estimated your conditional priority. So we have plenty of white given x and then you look at the last between why and why  So that would be the well motivated way to predict if I'd give you a loss function like a blue score, whatever. And you learn to distribution of why given x, then the optimal thing to do. Kind of makes sense from just doing them right and then it turns out that if if  The last that you use.  Is the zero unless  Alright, so this is the zoo unless  Then  You get that the decision is just the Ark max over the property. user avatar   Alright, so user avatar   So you will just find a label which maximize the quality, you might sometimes give a different answer if some mistakes are more costly than others.  So if some mistakes are very costly, even though they might be a bit lower probable, but because you really don't want to screw up on them, you might decide to still predict them in, even if you're not complete. Sure. That's the, that's the one. user avatar Hattie Zhou  Thanks for clarifying.  You're welcome. user avatar   Thanks for asking. user avatar   A nice painting in the background.  Very beautiful. user avatar Hattie Zhou  Thanks. I didn't pick that user avatar   You what user avatar Hattie Zhou  I did not take that, but I like it. Okay. user avatar   Alright, so I think we're done with the questions. So perhaps uh let's move to linear regression  And so I will derive the linear regression or rhythm.  From a generative, sorry. Another during it from conditional approach. I will derive or motivate  With conditional approach to regression  Okay, so. So here we're doing regression. So why is a real number. And if I was just doing prediction. All I care is given x  I will predict the why. But now I will go in this middle category here where I was actually defined a distribution of why given x or perhaps I didn't put the thing. So I will define  A conditional of why given x  And that's how I would get the actual linear regression update  And so, and why do we do that well. Also, it gives you a bit of  Intuition on the assumptions we are making  Because, for example, if our general model of why given x is correct. That's how the data was generated. Well, then this will do very well if it's totally false  It might not do as well. Okay, so well. So let's see. So, what, what is the the model that we could think of. So we'll say that the distribution of why given x  Will use now w as a parameter is a normal on why  And the main parameter is the duck product between w x and then I will have some fix  Noise variable sigma squared. Okay, so this is just in our products. It's the same thing as saying W transpose x. So my W. Here is my parameter  So I will have as before. We said that x is an RD. So, W is also an RD. I have a dimensional parameter  And recall location. Right. So I have a normal with me. Mew and variant sigma square  If I want to represent the density of that. I will say the density on something. So for example, here I would say density on why  With mean you and sigma square. So that's a notation. Right. I put a condition here. It's just saying, I'm talking about. Now, the density on why for gushing with parameter Mew and very similar square  So that's our generative model of why given x  And equivalent Lee, you can rewrite it in a different way, you can say that why I  Is actually linear transformation of X with w plus some  random noise, where  Epsilon i is given x i, sorry.  This is always given them the observation or ID.  normal with mean zero and noise sigma square. Okay, so, so if if I say that, why is this linear transformation of x plus epsilon i  And if tonight is a guy ocean. You can just look at the decision and why directly and it becomes a gash and also with the same variants, but just to meet the mean is skilled right  And so in some sense, what we're saying is that, oh, if, if I look just at the conditional why given x I have. That's it. This is exciting. And this is why, then, it's a linear relationship because of this w transpose x and the mean is W transpose x  And I will have some kind of gal should noise around it.  Where the variance is sigma squared. So that's the distribution over why for a specific x  Okay, so that's kind of one model and  Now, importantly, a little notation aside.  We're going to use the offset the Titian for x.  We use a offset.  Notation.  For x  IE, you can think of x as being a vector where I have some original  instilled in Rd to the minus one. And then the last component. I'll just always say it's one user avatar   Okay. user avatar   And so x still belongs to our d minus one.  And one will be basically this is like a constant feature.  And so why do we do that. So this is to avoid having to always talk about the parameter which is the slope of the linear relationship. And then, then the offset, which is the bias, also called the bias. So you have like that.  If I take the inner product between w and x. This is the inner product with the first d minus one dimension, and this is still  And then I have the last I mentioned, but because the X is on the ones always on one in my feature thing. This is basically like kind of like the upset right so the last parameter basically presented bias or the  IE, you know, I want to have a relationship between x and y. I might. I don't want to have it to be go to zero. Always I could decide to move it up or down. And usually, you could use be for that. So, here the last parameter of w will have this role.  And this avoid to having to talk about W en de separately. I could just say the pattern is is just W.  And know that because  All my arguments here are conditional I'm conditioning on x, I don't care how x were generated I'm free to decide that, oh, I'm a pending one on all the exits.  So there's no problem because I'm not talking about a decision over X data decision over x. And there's this weird thing. How do you get this one appearing right so that's different thing.  This is basically a notation convention to go faster.  Alright, so now this being said, I have now training data set.  Of x y i.  And the model here from a you know modeling perspective will say that x is coming from whatever we're not modeling. How excited came in. We don't care how x game in the might not be even independent the X could be anything. We don't care.  What we care about this why given x  So we say that why I given x i. These are modeling assumption is, these are independent Gaussian not ID, by the way, are independent Goshen with mean WT transpose excited and then burn sigma squared.  So that's our model. And so now the conditional likelihood  Of our data sets.  The quality of my observed why, given my exes.  By independence assumption.  Is the product of my individual conditional  P of why I  Given excited  And so now, if I take the lug of this thing.  The likelihood  The product will become a son, so I get summation from one up to n, and then I take the log of the  Density of a Gaussian  And so basically, I'll get the you get the x minus things square in the thing. Now when with the log the X does appear to just get minus  Y minus its mean which is w x transpose x i square divided by two sigma square and then I have some normalization constant, which are just constant anyway. So this is lug. I know they're not constant respect to sigma square  Here we go. So that's just taking the lug of a gosh, gosh, and density  And so now if I want to do maximum conditional likelihood  I just need to maximize this expression respect both to the parameter W and the proper sigma squared. Yeah. It started with the sigma square. You did it in your assignment or if you're late.  You will do it now for your assignment. So I take the derivative of that respect to the variable. And, as you remember, because the Emily is invariant  I can decide to take the reading respect to sigma square, instead of just sigma which is what I'll do. And I want this to be equal to zero to find a century point. And somebody asked a question.  ID versus the band mean  So I, I did means identically distributed here, they're not identical distributed because they have different distribution.  Like here why given X are not identically distributed because the mean of my parameter depends on x, right. So we have different distribution, but they're all independent  You get  So again, the, the inner product notation. I said, it's just the same thing as a transpose  That product.  Alright, so I take the derivative of this thing respect to sigma square, so I get summation from Isaac was one up to n. So where's the see oh no no I'm doing things erased.  Summation i was one to n.  Alright, so  This thing here is a constant respect to sigma square. So let's just put it all. So I have minus y i. Oops.  Minus y i minus W transpose x i square divided by two.  And now I have the derivative of one over sigma square respect to sigma square. So it's like one over x, I get minus one x square, this is minus one over x squared, like x being sigma square. So, this is sigma square square  And then the derivative of  The second part where I get basically minus one half.  And then the log of two pi separates it's a constant. I get log of sigma square, so I get one over sigma square  That's the derivative of luck of sigma square respect to sigma squared.  Okay.  And so I want this to be equal to zero. So what I do is I multiply both sides by sigma square. So, this goes away. This goes away.  And if I solve for sigma square I basically get that sigma hat statuary point I'll put Emily for now. But right now we still don't know it's a family. We just know it's especially point, but we'll get that the special report actually gives you a unique solution which is  Looking at actually the current squared error, some of the, the average square, which kind of makes sense.  So this is the empirical variance  Of my  Deviation. Well, I mean, yeah, so it's  So,  W transpose x i. If W was the correct value would be the true means, so y minus W transpose x i represent the, the, the average deviation, which kind of makes sense.  Okay. And so now  This is just a starting point. How do we know it's the global max. Well, that's where unfortunately we have that  This is not a concave function of sigma square  Or sigma for that matter.  So, because it's not concave. It's not because we found a century points to the global max. So like I mentioned, and Lecture five, I think, or six, I forgot. Which one, when you have a differentiable function which is the case here. You want to look at the special reports and the value at infinity.  Okay. And it turns out here that  It's clear to see that if I let sigma goes to zero. That's the limit on one side or plus infinity that's limiting the other side. The likelihood is actually much smaller.  Right, because if  As long as  You know, as long as. So this is always positive. So as long as some Why is not equal to WT X I, if I let sigma goes to zero, this will blow up to minus infinity. user avatar   And user avatar   This will grow up to plus infinity, but much slower. And it turns out that the the the combination of the two will always this piece will win. So as sigma goes to zero. This goes to minus infinity. And same thing as sigma goes to infinity. This also goes to minus infinity. So,  Basically you have that  Objective goes to minus infinity as sigma goes to zero or sigma goes to plus infinity. So, conclude  That  This is correct.  Global max.  For W fixed  So I you what I mean is fixed W optimize respect to sigma square. This is their optimal you get  Okay.  So,  Oh, this is a bit annoying. I have a bit of the patient to do  Okay, well let's start doing it and then I'll conclude next class.  So that's how the maximum likelihood parameter is for the variance. The fixed noise term. What about w. So I will actually introduce a bit of notation to  Make things a bit more team. So there's this thing as statistics called the design matrix which appears everywhere, especially in linear regression method.  Design matrix.  What you do is you take your, your data in each row of the matrix. So, you will define and this is now capital X, not a random variable. But this is the matrix. So,  Careful, careful when invitation. Now it's capital X, capital X is now n by the matrix.  Because that's pretty standard to use and statistics and it's defined as I just take all my data point, and I put them as rows.  Of my matrix. That's what it means by x one transpose. So that's the first data point that was a vector I put it as a rule by transposing it  And then I have the last data point, which is x transpose and so you have the columns and rows. Why is it like this. Why not the other way around. Why not vector way. Well, that's because in statistics, people used to have.  text files, where each row represents one, say for example, like each sample is one person and then you want to have the observation about the person. So if you do it like  one row is all the measurement of the person when you do like MATLAB type of computation usually things are stored as vector. So we actually makes more sense to store the transpose of the design matrix because usually you work by victors.  But that's, you know, just a different convention.  Alright, so that's our observation of x. And then we also put I'll use why now this will be a victor.  Which will be  The observation of why all as one end by one vector  Yeah, there's somebody is asking about zoom links. So if there's anybody here who have not yet feel the class survey, let me write it down again.  So feel this survey ASAP. Send me an email that you saved it. And then I'll send you information like the book the link to the textbook, the slack.  The password for the recording all these kind of things, but you have to sign up to this survey that I just put in the chat.  The link is dead. No. user avatar   No know looks to me. user avatar   You sure internet is not dead. user avatar Mahdi ZAROUR  I tried  I tried. I tried to access the link, but user avatar   It did not work for me. user avatar   Okay, might be some kind of like internet filtering or something. Yeah.  You  Press drive with the VPN.  Alright so let me just finish the the notation so that we're done with this part. It's very elegant. So why do I care about this design matrix. Well, now it turns out that if I take the the application of the matrix that must be used on the matrix with the vector w  This is the same thing as  Taking the product between a row and the vector w. So I'll have X. Once this gives me a vector which is the duck Park between next one and w, blah blah blah, x n, n w  Okay and this belongs to our end by one  And the beautiful thing now is that the summation from it was one to n y minus W transpose excited, which are my squared error my  Coming from the light. Like you, I can just rewrite it very succinctly as the norm between y minus x w el to norm square, so I can use this vector notation to very quickly write it down. And so now, this means I can rewrite  The minus slug of p of why one up to end, given the observation which I'll use now just the capital X. This is the design matrix. I've just summarize their own evolution in the design matrix. This basically gives you  The norm of y minus x w square divided by two sigma square and then there's some function of sigma square, but we don't care when we maximize respect the W because sigma squared is a constant perspective, W.  And so doing maximum conditional likelihood  Is the same thing as maximizing  lug of poi maximizing lug of poi is the same thing as minimizing minor slug. Which is the same thing as minimizing the squared norm, which is minimizing to the square. So, minimizing  With respect to w y minus x w square. What is this can be interpreted dramatically as projecting  Why  On the column space of the design matrix.  On the column space.  Of the design matrix X.  Right. Because when I take x times w, what I'm doing is I'm doing a linear combination  Over my D columns of x.  So this is the Jade column of x.  The column space of a matrix is just all the linear combinations of its column, which is what you get by multiplying x by all the possible W's.  And I want to find that inner combination of column which is the closest to why right and so you can think of geometrically. I have say  In Rd one column here another column here. So let's say this is column one, this column two. And then I take all you know combination of that. So you can think I take the span of these vectors. So that's like this to the space itself only had two columns. And then I have my  My y which belongs in  All not nothing. And yeah, this word vector and dimension and sorry. So I have y which is also in our to the end.  Which might not be in the span that might be outside that there might be no linear combination of your columns which  exactly match it, but I want to minimize the LTV norm. And so it turns out with what you get is the projection. So what you'll do is you, you get this projection. Whoops. A. I said, Read user avatar   But user avatar   I don't understand. user avatar   Yeah, that's right. So I will have the prediction and this thing here will be x W star, so that the correct coefficients or main or combination, such that when I looked at it, it is actually the closest to y, which makes a 90 degree angle.  And so the last thing I will say is that, so the Emily.  Parameter for for for the maximum like the conditional maximum include here's the aardman over w in Rd of y minus x double you  And so this is also why it's called the square  Because I'm trying to minimize the sum of squared error between my prediction x w x transpose x transpose w and the correct label. Why, right. So, maximum conditional likelihood. When you assume Goshen air is the same thing as minimizing the squared error.  Okay.  So, which means that, let's say I don't think the error or Gaussian. I think the error or Laplace then instead of a squared error, you will actually have a an absolute  Errors, which is not the same thing. And in particular, this would be more robust to outlier, because if you have a point which is super far  That you expected because it was an outlier. Well, it will have a huge squared error and it will be screw up your, your, your estimation of the correct slope. Whereas if you would use absolute error. Instead, it wouldn't be as sensitive to the outside.  Alright any burning questions.  So next class, we will continue on basically solving now the maximal next you to submit how to solve the square by getting the normal equations and a bit of inversion. And then we talk about the rigorous version of the square and do a map perspective on it.  Okay.  Cool. Alright, so I'll see you on Friday. Have a nice week
  Okay.  So first announcement.  I will move back the due date for the homework to buy one week and I think I'll update other dates in the schedule. And that's because I realized that there's a lot of stuff. You will need for homework, do that. I will only be able to cover next week.  Like, I think the buttons are with them this kind of stuff. So, and actually last last year. Also, people had a bit more time to do the assignment to so I actually will update that also send an email to you, but be aware  So today, whenever you do today, we're going to finish linear regression  Linear regression, which was our  First example of  Supervisors learning approach inspired from  A Journey models, a conditional model of on why given x in the case of regression and then we will cover logistic regression  Which is for classification of into this regression. The name. It will see why it's called logistic regression  And so last class, we go back to last class.  Just so that we're all on the same page.  And kinetic is asking if all homework will be pushed back in perhaps not all of them, but they will be a bit of upset that I'll have to look back at the schedule.  Okay, so  So basically,  If we recall so  I'm not sure why this is not doing user avatar   Okay. user avatar   So do you see my screen. By the way, do you see  The like circle I just draw okay. That's why it doesn't want to  Let me stay in the  In the highlighting mode.  In any case, so we had this  As a motivation for the integration is we had this conditional model of why given x where we would suppose that basically why when I condition on X can be seen as a linear, linear transformation of x plus some Gaussian noise with a fixed  Variance  And and then if we do maximum conditional like you did this model, we got that the  Conditional likelihood had basically this user avatar   Why am I not getting this to see if this will work. user avatar   Is really annoying. Okay, so  So if I scroll I lose my pen.  Not sure why this is the case, then do that in the past.  So,  Yeah, so we had that the lug like you had basically this form where you had this quadratic  This some of squared error coming from the ocean. And then you also had a normalization. A coming from the Gaussian density which dependent on sigma and then  At the end of last test. I did that. Okay. If I maximize this function respect to sigma or sigma square, actually, in this case, because it's invariant to  We know that the maximum likelihood estimates and variant to reprint ization we just got that the estimate for or  Or variance term is just the square. The average square, which kind of makes sense, that gives you the notion of like how much noise you have in your observation. Right. So if you have a lot of noise.  You will have a higher variance and then we still don't know what's the maximum likelihood parameter for W. And so that's where I started to introduce some limitation. At the end of the last class I define the  The design matrix where each of our observation.  Vector for each input or or the rose. So it's an n by d matrix.  And and then I can rewrite the leg leg viewed as just the  The Ultra norm between the vector of labels so. So why is an n dimensional vector for where does interesting examples. So I put the the label or  Or the labels, the regressive value for each input and I just can compute the the norm difference between y and x the design matrix X times w, okay and so  That's a way to avoid having writing sums and also it gives us a bit of German chuckle intuition and, in particular, so  When sigma is fixed, all I have in this function is just this LT norm. So I said that  Maximizing the log like you. This was the negative log, like, dude, so it's like minute maximizing like he will is like minimizing the negative log likelihood. And so it's minimizing the square between y and x W. And so what does this mean. It means that I'm projecting  From a linear algebra perspective. I'm projecting the vector, why on the column space of x, because x w is basically when w various over all possible vectors. This is the span. This is the span of all the columns of x in the algebra terms. And so I've made a little 2D drawing here is that  You have that. Why is probably not in the span of their common space, especially if these not that big.  And so what you're when you're doing least square, you're trying to find W star which is the coefficient of combination between the columns of x such that I get the projection of why over a dyspraxia okay  So, somebody asked to sigma square that delta sigma square  This thing was, this was the if you're asking about this notation. This is partial derivative with respect to sigma squared. That's what this means.  This is not a two. It's a partial  There.  Okay, so that was the the recap.  So, so we can reinterpret the maximum next  Parameter for our slope.  I many issues with this thing. I'll have to reboot and find what's up with this.  And so we just need to solve this minimization problem, which is what we call the least square problem.  And somebody said, Why would be in the span if there was no gosh and noise. Right. So, indeed, if the why I are truly  Where is it, yeah. So if why I was indeed truly equal to just this then as you see  It's definitely a linear combination of the X right so I could I could if I rewrite this as a vector format would have that. Why is basically x W. That's what I would get so this is clearly in the listeners.  So the gosh and noise here in the general model is what make things perhaps not in the span.  But this is also, by the way.  You have to differentiate your model with the reality right so so so right now. This is your model and  We don't know this w right and so  Yeah, but we'll get there. When we do when we solve the normal equations or in by the way i in the notes. Last time I left a long detail information about how to do global maximization organization of function which are differentiable  But they're not as a convex or something. So I said, you find a century points and you look at the boundary. And as I gave a counter example in these notes.  Why you could even have some cases where there's only one statuary point, but this is not a global max our global men because what's happening is the global max or global men is that pending at the boundary  Even though it's a bit counterintuitive because you think you know you could have actually known it was even a weird example that it's not only a stationary point. It was a local men.  And there was a global men somewhere else at the at the boundaries, which seems weird because if it's a local men, you think, oh, there should be another story points so that it goes back, but that's because it's in multiple dimension and weird stuff can happen.  Regarding the boundaries. Is this a sufficient condition for estimate to be a global max and min  So if the function is continuously differentiable. If you checked the value of the all this point and the value add the boundary, then you know that's where the global maximum global men can appear. Yes.  The function is not completely differentiable. There's none did. There's, there could be like more weird stuff happening.  OK, so the shank. I will come back to your question later we'll see if it answers it because it's just solved the problem. So,  Back to  Oops, I don't want this. So back to the square  So we want to solve the maximum  likelihood estimate  Are you telling me that I'm not able to raise now. user avatar   Yeah yeah user avatar   Okay, I think I'll reboot the computer during the break.  So,  W ML. He is the art admin over Rd of the new norm between y and x w square. Okay, so basically let's do a bit of algebra. So actually this is nice because this is a complex function of why we just need to find the story points.  So, what we want is we want to compute the gradient of this and set it to zero.  Okay, so that's the century point. So how do we do that well let's just rewrite the  Equation in a bit more expanded form. So the norm is the product between the vector transpose and itself. So, this is this transpose times y minus x w. OK.  And now I want to take the dirt. I want to take the derivative of this  And set it to zero.  So this is what we want. Ah, so like, Okay, let me just try killing this thing and try again and see if it will work.  Is so annoying.  Well known as kidding up from the last time it was open. user avatar   Not surprising. user avatar   Answer a question in in the in the meantime.  What's happening now.  Looks like when notice having trouble.  Alright, so the question of The Shack was we don't know in the training that if the trading day that why lies in the span of x.  So yes, we never know. Because it's not because we make a modeling assumption that is correct. So indeed, it's. We don't know.  Whether this will be the case.  Okay, so what's the problem.  That me kill this thing and try again. user avatar   I user avatar   Think I need to reboot my computer.  That's the fun of windows.  OK, so now I'm able to get in. After killing the software use Linux.  Well, I agree. But the problem is that the tablets support in Linux is I'm not sure not and perhaps now it's good, but like, you know, I started to use these tools.  15 years ago at the time it was definitely not as good. And I'm now locked in. That's the problem. So I have like gigabytes. You know, like thousands of research notes all locked into one notes and I'm stuck there.  Alright, so  Apologies for this little  hiatus, but hopefully now I will work go back  Here we go.  You hundred 25  And now let me share it.  Okay, so we're back. And now let's see if this thing works.  Can I erase  Oh yeah, now I can erase it. Okay, good.  That's helpful. Okay, so, and  So we want to compute this. So let's compute this quadratic form. So I want to take the partial derivative of the quadratic form so  When you have these these  Two terms. Time to terms you can just do all the crust. Right. So why times y y transpose time why that's just the normal why  Then I have minus  Actually the cross term and you have two terms. This is a quick form. So this would be to Y transpose times x w  And then I have the last quadratic term which will be the boy you transpose x transpose x w user avatar   Okay. user avatar   And I want this to be equal to zero. So, I want this to be equal to zero.  So first of all a bit of gradient cockiness here. So if I think the grand and respect to w of the quadratic form with the matrix A here w transpose A. W.  This is equal to, in general, this is equal to A plus A transpose w. So we don't have to assume that A is a symmetric matrix this case, you have to be careful. It's not just to AWS, you could expect it's a plus A transpose  And by convention, when we use the gradient notation. This is a vector. So when I'll do a bit more think I'll do a bit later without without I think I'll talk about the maximum likelihood in a multivariate gash and I'll, I'll give a bit more  Tricks and vector calculus and notation. But for now, you know, let's just assume that we know these things. Otherwise, we'll get back in these in more detail you can just think of the gradient is a vector of partial derivative and that's it right  And so because of that, the first term, I get zero. Because why is a constant.  This gives you  The second term is just minus two.  X transpose y. So, not Y transpose x y transpose x is a row and I said the gradient is a vector. So, I will put it in vector form. That's why to the transpose x transpose y  That's the derivative of the second term. And then the third term, as I mentioned, it's a trance AS A transpose w. In this case, x transpose x is so, so this is a symmetric matrix. So this just give me two x transpose x w and I want this to be equal to zero.  And so if I solve for W. This implies that x transpose x  W star. So, any global min of this function needs to satisfy this linear equation which is called the normal equation. That's a pretty famous equation for  linear, linear regression, basically. So it doesn't mean. So this is called a normal equation.  So now I will consider two possibilities.  So if  X transpose X is incredible. So if x transpose x. That's a DVD matrix, by the way, is inevitable.  Then we have a unique Emily, right, we have a unique solution.  So we have that the W N le  Is X transpose X inverse times X transpose way.  And I just multiply on the left and the right, with the inverse. So I get the identity times w start on the left and transpose y  That well X transpose X inverse time x transpose y on direct  So,  What does it mean if X transpose X is incredible will remember that x  So x is n by d, which implies that it's rank the rank of the matrix X.  is smaller than the men have an end right so the rank of a matrix. It's the maximum number of linearly independent columns, for example, but men so it cannot be bigger than the number of columns.  Which is d in this case.  And it cannot be bigger than the dimension of the of the vectors of the columns. And in this case, the number of the dimension of a column is and that's why it's min of n d  And it turns out that if I take the trends. If I take x x transpose. It's the same thing. So, x, x equals x or x x transpose both both things cannot have  Actually, the same rank of X. It's a linear algebra fact. And so, thus  The rank of X transpose X can be bigger than men have an MD.  Which means that if x transpose x, which is a d by the matrix is convertible.  You need in this case that end is bigger than deep so you need more training example than the dimension.  So that's already some condition.  So if you're  In very high dimension when d is bigger than n, for example, then you definitely don't have any credibility and you don't have a unique solution.  And we'll get to this non unique case. But if I look at what is the then the prediction. So this is our maximum likelihood estimate  And and why in this case doesn't have to be in the in the in the span of the of the  Of the data right sorry does have to be in the span of the columns of x, which in some sense can be seen as  The linear combination of the of the  Data points.  And so  But we can still look at what is our prediction on each data point, given this parameter. Right. So if I look at our prediction.  On the training set.  For a model.  Okay, and by prediction I means here that  So if because when you do a regression, you can just take your, your estimated W and take the doc product with X to find what should be your prediction.  From a holistic perspective, you could think of it as well. What's your model is really saying is that you think that why given x is a Gaussian around it's mean  And so here, all I'm saying is you're predicting to me. Okay, which makes sense. And actually, that would be the optimal thing to predict. Also, if you use a squared loss on the on the output.  OK, so the prediction will just be your predicted mean which will be I take my my data points and I just multiply by my  Estimated vector w and by the equation above what this is, this is just x x transpose x minus one. And then I have a x transpose  Y. Okay. So this was the boy you, Emily. And I've just multiplied by x. And it turns out that x transpose x minus one x transpose. This is a projection of barrier.  To projection matrix.  In particular, and this is a projection matrix on the span of the column space of x.  On column.  SpaceX.  Again, so this is not surprising, because that's what I said here that the solution here, I would have that minimizing the squared in arm of y and x W. Basically, I get this vector here, which is the projection of why on the span of the columns of X Games.  And so here we just got  From a linear algebra fact just another perspective on that.  So this is, you know, recall  The geometric perspective.  OK, so now if n is smaller than D. If I don't have enough observation competent dimension.  So, ie either were in high dimension.  Or we have not much data.  Then we know for sure that st trends X transpose X is not incredible just by the rank argument. And I explained about  This and then there is no unique solution.  Good. So then, so then  If x transpose  Is not a convertible.  There is no unique solution.  And you could choose. And so any W hat, such that it satisfy the normal equation.  Is a maximum conditional likelihood estimate  So there's so as I mentioned in the past, maximum likelihood or maximum conditional IQ. It's the same spirit.  Sometimes it's not unique. This way, there's multiple possible estimator. And there's a here there's a bit of issue of which one do you choose.  And  In statistics, people have in this case chosen a specific solution out of the infinite number of solution. And usually the one that has suggested. So the one  Is to pick the one which has minimum Altoona. So one example to make things unique, you could say, Okay, let's find the one among all these infinite solution.  Which minimize the elbow elbow on  Yeah, so is the argument over all W such that w satisfy the normal equation.  That's a way to make it unique. So  Basically the L to norm can be seen as a strongly convex objective, they'll to norm square. And so when it's a certain context objective as a unique solution when it's optimized for convict set here the linear set of equations to conduct set. So, so this has a unique solution.  Supposing that. Okay, so this is supposing it's feasible. So supposing that this is the case.  But  Actually, that's a good point.  Because  You could also have that. Why is not consistent  Hmm, okay. Yes, it's the first time I think about that. So supposing that that there's a there's a  There are some parameters for which this is satisfied.  So the feasible set is not empty.  Then  Yeah, so that then there's an in the case that X transpose X is nine convertible, then it will be an infinite number of solutions. So that's the, the, in particular, suppose the rank of  Of x is, I don't know.  D minus 10  And  And so then there will be dimension. So it's not full rank.  And so then there will be 10 dimension, which are free to very to actually get arbitrary solution. Okay, so that's kind of a linear algebra fact  Alright, so the minimum norm solution is a way to make it unique. And it turns out that this minimum nom solution is this is a specific color inverse called pseudo, the more. So this notation here is this is called the more Penrose  pseudo inverse  And in this case, we're multiplying by way okay so  So the more Penrose  pseudo inverse. So let's say x. So the inverse. One way to define it.  When x is for rank.  Is X transpose X inverse times X principles. user avatar   Hmm. user avatar   And so in this case.  You would see we get back to the  So if x transpose x isn't vertical we just get back to the  The  The same equation as this one.  Here, right. So this is the same thing as x to the inverse of why in the case of x being convertible, though. Now we're generalizing to the situation where X doesn't necessarily have to be incredible.  I mean, so x is not enabled by the way. So x is never incredible because it's a square rectangular matrix. So, okay. So just to be clear, so  So x is nine Lord of all. So there's the pseudo inverse was to generalize and verse which can also be used rectangular matrix and but now we're talking about x transpose x. So if x transpose x is  Convertible then the this this pseudo inverse times y gives the same thing as the unique maximum conditional legs would solution, which was this using this expression.  So this, this is is only valid here.  When x is for rank.  If it's not full rank, then there's a another expression for it.  Which I recommend you look at the wiki big goal and the more more general suit inverse. It's very highlighting  box where you can computed by taking the SPD of your, of your matrix and all this singular values which are zero. You just forget about them in some sense, and then you inverse, the non than the, the one which are non zero because you're able to inversion.  Okay. Well, let me write this down. So let's say I do x i say this is you sigma V transpose. That's my SVG, then  You can say that the  pseudo inverse of x is just  In this case,  You take the trends can take the you will have sigma pseudo inverse and then you transpose  So you take the inverse of you envy you and we actually are a second all matrices. So the inverse is just their transpose and so when they take the kind of like the inverse of all these matrices, I go back. So I start with the inverse of v.  And so, sorry, the inverse of the transpose which gives me the then I cannot take the inverse of sigma is any because it might have some zeros on the diagonal  Because the rank is not, it's not rank and so that that's where I think the pseudo inverse and an episode of universities, I will have something like  I will have something like so this is this was an ID. So here, this would be n by d  And so I will have  Stuff on the diagonal like  Sigma one blah blah blah to sigma d  That supposing n is bigger than D. By the way, then I have to zeros everywhere else. That's my sigma. And so now when I think the sort of inverse of that all I have. It will be the transpose of this matrix and  The non zero element will be inverted and then when the zero I just forget about them.  Okay, there's a lot of questions now.  For our vertical experiences Judy method stands to get me know solution that is correct. And you actually read the green method also get the minimum nom solution that's something else will come back to that.  A  Yes. So x is he made a point that  Forgiveness to linear algebra can point that this is always in the column space of X and thus there will always be some solution here which is true, yes.  OK, so now an important question, should these calculations in crystal clear intuitive to us right now in the center of our goal of the procedure, but I'm probably less than a specific matrix, man.  Good point.  I think this will need a bit of reviewing to kind of let go over the steps so that it becomes clear.  Indeed, there's a bit of  Additional information I'm giving you from the linear algebra, in terms of rank and all these kinds of things, the important aspect is  We were trying to solve this, this maximum likelihood problem, which was just finding  The W which which minimize this L to norm distance. Right. And so now I'm doing a bit of manipulation to find out what are the properties of these solutions and, in particular, I've highlighted that  The quality of interest is x transpose x, which is a DVD matrix. And if x transpose x is in vertical, then there's a unique solution which has a nice shape.  Which is. And the nice thing about this also is that when you predict when you look at what would be your prediction on on the training set, you're basically just projecting  Your label, why on the quantum space of x, which was something already mentioned in Jimmy to contrition. And then if X transpose X is not in vertical  Then there are multiple solutions and and now I'm going to talk about the I'm talk quickly about the more the the the pseudo inverse formulation which is one choice of solution which is intuitive, which is the minimum L to norm solution among all those which satisfied in a whole equation.  And  And so, and it turns out that indeed the when we talk about optimization numerical condition technique like grading method or as GD to throw these will also find you. This minimum norm solution that's kind of interesting to know  And this minimum know solution can be expressed as just the  pseudo inverse of x times y. OK. And now I told you a bit of properties about the sort of inverse  You need to, you know, if this the first time you see that I don't expect that you fully understand all these details will also need a bit of reading on your own Wikipedia and the pseudo inverse  But basically, now what I want to tell you is that  So it's also getting more intuition about overfitting. Right. So what's happening is  When you're in high dimension or when you don't when the number of training simple is not that high maximal makes you is not a well behaved.  Estimator in particular because it will overfit and here it is highlighted in these equations in the fact that there's an infinite number of solution to the maximum likelihood estimates. And so, which one should I pick  And a lot of those actually will have very poor generalization performance. OK, so the minimum age to norm in some sense is kind of regularize and so you could think it's a bit more stable. And here I want to highlight something important is that the pseudo inverse operation.  Is the pseudo inverse of a matrix is not numerically stable. user avatar   Okay. user avatar   So what does it mean for something to not be numerically stable. It means that if I make a bit of numerical change on the entries of my matrix. If I make epsilon change on the entries on my matrix.  I would like that the solution. So, like the pseudo inverse matrix should also have small change not huge change because if it has huge change.  Well, that means that I need to really be careful about the numerical precision of my entries. And so, for example, usually in computers we use  Say a double accuracy or float or something like that. So we already have a few digits of accuracy.  And so if a small change in the last digit of accuracy makes a huge change in the solution, then there's no way you can have a good accuracy and the solution. Okay, that's what you want me to write the stable method to because you never have these entries with perfect precision anyway.  And it turns out that the pseudo inverse indeed if like I could have some similar value, which were 10 to the minus 16  So if there are zero, then I don't convert them if they're 10 to 16 I invited them and become tend to the 16th, which is a huge number, by the way. And so  And so it turns out that this is not a nice numerically stable operation and instead  It is better.  to regularize  To get similar effect.  to regularize the problem.  To get  Similar user avatar   Effect. user avatar   Okay.  Because it turns out that the pseudo inverse could also be seen as taking  Adding an L to penalty in the least square problem and taking the coefficient in front of this penalty to zero. Okay, taking the limit of this to be zero, but  Making it exactly zero, which is what you get, or the limit when it goes to 01, which is what you get when you do the  Inverse is not stable. So it's much better to use a very tiny value for this say alumni equals 10 to the minus 10 or something. And that's much more stable, even though it has basically the same.  And  And the reason I mentioned that there's two two reasons. So, one is that I will not talk about regularize the square. So it's another way to motivate rigorously square to is that it turns out that's an important fact of kind of like pretty deep fax machine learning is that  When you have  numerical instability in your algorithm. This is really tied to statistical instability, okay, because you can think of. I have numerical in accuracy is just because of numerical precision. Well, when you make measurement when you have data. You also have you don't have like the perfect  Value of things because, for example, like you don't have infinite data. So you don't know the exact distribution. So when you do statistics. You also don't have perfect accuracy on the measurements.  And so if an algorithm or an estimator behave badly numerically. Usually the will also behave badly statistic. And it turns out the direct the maximum likelihood estimator is not that great. When you have high dimension compared to the data point.  Okay, so let's go to the rigorous mission.  World.  regularization  Are you gonna rise.  And  Rather than saying, okay, we will stabilize the numerical techniques which is one way to look at it. I will motivate it from now a map perspective can be motivated  From a Bayesian point of view and using a map estimate  Okay.  So,  Was asking a very good question, which is a bit annoying but still very good question.  If it's not a good idea to use maximum likelihood estimate. Why is it so popular. Okay, so  What why why this question is lying is because we're talking about  Fundamental theory here, which is pretty old and  There has been a lot of progress since then, which is not as a theoretical, but more empirical okay so so there's there's a few things. So first of all, in  Statistics.  The maximum likelihood estimator was fairly common because it has nice properties when the dimension is not big when N is Big and the small  For example, I'm doing analysis of people I have 1000 people and I measuring their height their age and I don't know their their their french fries consumption, that's three numbers. So these three and is 1000 maximum like you have no problem. Okay.  So that the problem of Mexico. Mexico comes when you do high dimensionally T stuff, which  Originally the another tip or statistics appear mainly mission learning so admission learning regularization was super important. Very quickly, early on and then there is this kind of neck new wave from deep learning, which kind of like shattered. This  This  Kind of like perspective.  From  So being mainly. Okay, what happens is if you just run as GD on a deep neural network aid works. Wha, even though you're doing maximal magnitude. And so then, oh mean, who cares about regular rising or doing other techniques, it's not needed. It works well. Okay. And here.  The thing is, there's a, you have to think of it more deeply about what it means. Okay. Because the problem is  When you do as God you're trying to approximate some kind of solution to an optimization problem.  And it turns out in this case that when you do maximum accurate. There's still an infinite number of solution when you have a super deep network and  As GD will pick one, which is actually in some sense regularize and so you are implicitly realizing by the opposition algorithm that you're using.  So even though you are. You think you're doing maximum Max, you're actually doing something a bit more clever than that is using a good position. A good  In the sense of  Implicitly a rigorous method on this problem and that's why it works. But this, this is kind of a  Bit of a longer story and it's not trivial to talk about these things. But so you can think of Emily being popular either among people who don't know too much about theory.  And this is more about ignorance and the fact that there there is a way to that there is, it turns out that there are some algorithms that people have used to  Try to do, Emily. They think they're trying to do NLP and it works really well in practice. And that's why they don't worry about too much anyway that the fact that it's not there well behaved estimate in general.  And  Sheriff the solution of system of linear equation doesn't change mystically when you print terms and put I guess this is an insert to to the numerical stability against right user avatar   Yes. user avatar   Okay, so  Map point of view. So suppose we put a prior  Suppose we put  A prayer over or parameter  So now I put a prayer over w i will say it will be a Gaussian  Again, my convenience, but it will give also something interesting. So I will say, okay, it's a Gaussian ON W with zero mean and I will use as my variance  Sigma square identity divided by London.  Okay. And so this is the  End so  First of all, I hear is the deep idea identity matrix is this is a multivariate Gaussian  And lambda is the  Precision  Parameter so you can permit tries the Gaussian with either the variance  Which is sigma square or the inverse variance, which is and usually you can use like either lambda for this kind of  Value.  And here what I've done is I've really scaled the whole thing. I use sigma squared divided by lambda and you'll see why very soon, why I do that is because then the lambda becomes or usual regurgitation panic.  Alright, so this is my prior. So then if I'm a if I want to do a map estimate instead of maximum conditional likelihood. I want to maximize the posterior. So let's look at the posterior log of the posterior  Okay, so  I will answer your numerically stable question. user avatar   I user avatar   Think it's a good answer now. So he's asking about the  The numerical stability, right. So, all I'm saying is so numerical stability. What I mean by numerical stability. user avatar Carl Perreault-Lafleur  Is like, Sorry, this, this is my question. I just want to know what you mean by getting similar effect.  Just under it's written that rigor arising within will get you user avatar   Oh, good point. Okay. Yes, yes, yes, yes, yes, yes.  Thank you. Thanks for clarifying the question.  Basically, all I'm saying here.  Is that you will get that the W map.  As a function of lambda  As lambda goes to zero will converge to the w user avatar   Let's say that's called like pseudo inverse user avatar   And so if I if I would take the limit as number close to zero of a mind map estimate, I would get the pseudo inverse estimator.  And  And so I could actually compute sort of inverse by doing that.  And but you don't want to think the limit goes to zero because anyway this is like an unstable process. And that's what happens often when you're taking limits, you can get weird stuff happening.  And so you will instead you could choose us very small lambda which one gives you the political reverse it, but it will be much more stable directly and will basically also do the same effect of getting a unique solution and this, this is what I mean by by that. user avatar   Okay. user avatar   So it's compute the map.  That posterior. So I have the log of the probability of W, given the data.  This is proportional to the likelihood times the prior and I take the log. So I just plus this is lug probably to have my observation.  given x and W plus the prior log of the prior and then plus some constant, which are coming from minimization constant. And then I already wrote before the log likes you. For  Or conditional models. So this was one of two sigma square L to norm between y and x w square I had some function of sigma square, which appeared in my leg leg. Good. And then I have my new term here which is basically  Minus lambda divide by two sigma square  Norm of w square. That's my prior  Have a constant.  And so now the map.  To this  So now the map estimator for w  W map.  Can be seen as the admin.  Over W.  Of the norm.  Plus the regularization  And I remove here, the sigma square because sigma square is a constant. So rescaling an objective by a constant doesn't change its maximizers right  So that's why I have used this sigma squared by lambda to make sure that I don't have to worry about sigma square and the trade off between the L two error and it's legalization.  And so this is called rich regression  Which is basically  The square regression plus LT penalty on the  On the perimeter.  In  So you can think of this  So somebody is asking, why are we keeping the one half term.  Yeah, it's because when you take the derivative, the two cancels out. So it kind of neat. But indeed, like I could remove the one half as well. Right, so it's not super important.  So,  So that's the map and you can think of this as regularize er M.  Empirical Research position. Right. So basically I have  My empirical error on the training set.  In this case it's using the  The square loss.  But in general, I could have a loss. And then I add also  Some kind of regularization  And here I divided by n to get the average error. So if I take this objective divided by n, then I get basically what I've just wrote  It. So here, this was the squared last that I use.  And so this whole thing here is the empirical error.  And this is our regularization  Which, if you remember from your basic mission learning class this is to avoid overfitting.  And we'll see here the effect over their realization, it will stabilize the maximum likelihood that the map estimator, it will stabilize our estimator, and indeed reduce its variance, which will also reduce the overfitting.  And so in particular.  This LT term here.  So to the objective  So this objective.  Is what do we say we call strongly convex  In W.  Which implies that it has a unique solution.  So when you have where you're convex if you have a zero gradient. It is a global solution, but you might have multiple if you're strictly or strongly convex then and either a gradient will give you the unique solution.  In and a function  Is called  Whoops. Let's say this is an F is called lambda, lambda strongly convex lambda is basically the the strongly convex  Strong convexity parameter if and only if I can subtract to my function L to norm times landa divide by two and this is complex. This is still complex user avatar   Okay. user avatar   I mean there's multiple definition for a strong complexity, they're all equivalent. So that's one a simple one.  And yes so er M stands for empirical risk position.  It was a this type of estimator.  Or learning principle, which is basically you find the the parameter which minimize the empirical error. And then you can add a regurgitation to that which is basically some kind of complexity over your classifier. In this case, it's just the new norm up W.  So start tech is asking, okay, well why why L to norm. You know why, why not other random functions of w y el to alarm. So that's, that's a very good question. Um,  And so here I'm giving a bit of motivation that. Well, you can think of Altoona norm as what you would get if you are a Bayesian  And you put a quick. Gosh, and prior over w. So it gives you all to know if instead of putting a gash in prior over w. You put a Laplace prior you will get the LM so that's a different triggers a rigger riser. So while two verses one  And then there's a whole industry of analyzing the properties of different organization technique.  Particular if you do l one organization. One effect, you'll get is that the parameter will be sparse, it will have a lot of zeros. This is what less always doing  So if you think that having sparse solution makes sense for your problem, then the other one regularization might be a better regularization  But you can think of.  The type of rigor of ascension. You do kind of like favor certain functions versus others. And so this is a kind of a way to kind of get  Some bias into the learning process, which if your bias is correct. I he if the kind of functions will do well on this distribution.  Or the one that you kind of put about app or prior on like you put high mass, then you will do well and if your biases wrong. Well you on this. A dwell in. So that's kind of the, the, the general spirit here.  And it's a bit you can think of it also like a German model assumption gives you a method. And if your general model assumptions are are good. Usually the method would be good seminar also for the regularization so  And that's what you'll explain it in the in assignment.  So, so, side effects says so. The regurgitation is a prior to restrict them from the data function, sort of, yes. Okay, so one keyword, for example, is called structural risk musician. So this has come from that Nick. So the idea is  If you just minimize the. Why am I not able to do this.  Okay, so if you just minimize the empirical error, you will overfit okay over all complicated functions you overfit. And so the idea is you restrict your class of function.  And putting it out to regularization can be seen as I will consider a Yorkie of  Nested class of functions where you, you start with function with very small alarm function with a bit bigger L to norm, etc, etc, etc.  And what you want is to find the one the class of function which minimize the combination of its complexity penalty, which is the norm, with the training air. Okay, so that's kind of the  Kind of like a Occam's Razor type of idea you want simple quote simple functions which and simple is quantified by your organization which does well on the trainings.  Okay. And there's a, there's a, there's a lot of learning theory behind that and we won't really have time to go into this and discuss because there's too many things to do.  But if you're curious, you can have a look at  There's a learning theory.  Class that I give in my, in my lecture in advance for sure prediction of all the notes online, do you look for the lecture, which has learning theory and in one class I over I go over  Standard transition era bounds and basically learning theory for castigation and so that will also highlight a bit of a this kind of information.  Okay, so when our data as many outliers, which is realization is prefer rich or less.  So it's actually the last which matters in this case. So if you're, if you have outlier. The problem is, your, your, your, your error. So let's say if I go back to the square there. Do I have it somewhere.  No, I don't have it. But the problem is that if some why  Has nothing to do with the current process. So it's super far from extended W. Well, because I'm squaring it's error, it will have a huge impact on trying to be fit and it will really screw your, your estimate to this case.  And so instead of using the L to r squared error, you should use the absolute error in the states it's it's it's not changing the organization. It's changing the actual loss that you want to use for  So instead of thinking. Now you have Gaussian noise. If you had left last noise, you would have absolute value here instead of the square and this is more robust to to outliers.  And so there's this, by the way, there's this whole field called of robust losses and it's basically trying to define. Indeed, these losses which when you you you learn with them, you actually are more robust to outliers.  Okay, well, so let me finish the map before going to the break, and then I'll take a break. Sorry, it's good along, but I think it would be nice to kind of finish the map and then you can also ask question so  Blah, blah, blah. So, so this is our map. So now let's try to  compute it. So I want to take the gradient of this instead of two equal to zero and you can actually redo the all the derivative  That I did last time, you will still have the X transpose X appear, but then the L to norm, the grain of the norm just gives you lambda times w  And so, what you get is basically lambda times the identity.  And I want this. The times w is equal to x transpose y. Right. So before I only had x transpose x times w and I have x transpose x plus lambda times the identity and then the beauty of this thing is that this matrix here is DB the matrix is always in vertical  If lambda is strictly bigger than zero. Okay. Basically I it's like I add a little  Quantity on my diagonal. And that makes it in vertical and it makes it pull rank and Parker and so that means there is a unique solution which, as I said, because it strongly convex, it should be the case. So I have that mind map.  Or in this case, I could call it the rich  aggressor, the solution of the rig regression  Is x transpose x plus lambda identity inverse times x transpose y okay so  This works.  Always and there's no problem.  When even if d is bigger than it user avatar   Okay. user avatar   And in particular, if lambda is not too small. This is very interactive stable.  Alright, so is there any burning question before the break.  And the curiosity question of the shank is if you have, if you don't assume that the noise is constant, will it change something. Yes, it will change something exists is to the reader it say instead of having sigma square constant you had  It's a partition the data point in like three groups with each their signal. You'll see it, it will basically rewind the error in between.  Alright, so if there's no burning question. Let's take a 10 minute break. It's 239 so let's go until 250 that's have 11 minutes break because I took it too too long. user avatar   All right. user avatar   So is there any additional questions on linear regression or map.  Okay, so one last thing about  Linear regression. I'll mention, and then I'll go to logistic regression, but is that  There's a  Good practice.  So,  So when you have this this L to prior over the normal W. It basically means that each dimension of W are equally penalized. Note that in the prediction. You take W transpose x. So if you're really scale a feature.  Was apple. Let's say instead of measuring  Feature in meter you measure it in nanometers. So you just now, blow it up by 10 to the nine. OK.  So now, a small change in w will have a much bigger effect than it used to do before.  And so if the scales of your different feature are not the same. They're not kind of meaningfully equivalent this alto regularization is is not to see the best prior because it is assuming that each feature have about the same effect on the prediction. Okay, that's why it's good practice.  To  Either  Standardize or  Normalized each feature.  The features.  Or normalize them.  Before doing your, your prediction me and you're learning so standardize mean you make each feature.  zero mean and  Unit standard deviation  Are unique empirical variance empirical  The recall. Oops. No, not right.  Empirical variance  So it's like standard normal and normalize means either you could make x i unit norm.  Right, so you could reach scale things so that every feature as it is today norm, which is equal to one. Are you ready scale things  Scale features.  To 01 or minus one to one, for example, that's kind of like  You put them in this bump specific comparable reference  Then you ask, Okay, well, which one of those three  Actually don't know I don't know myself.  The main the main idea here. The main guiding principle is is  I don't think it would matter too much these different things, but it depends what you're measuring right so if you're measuring things which are positive, well then scaling them to 01 perhaps makes more sense than then normalizing them.  If you are measuring things which are all components number which could be arbitrary and they're all similar in spirit will perhaps standardizing them makes more sense.  If what you're measuring has a meaningful scale, which should be different, like, you know, like there's what could be good example.  That's a good question. What would be a good example of meaningful scale.  So you know that the first feature is  Is a distance.  And  The second feature.  Actually, I don't have a good example which comes to my mind.  Okay, I'll have to think about that.  But the main the main idea here is, oh  People are not seeing my notes. No, I think there's being shared. So the main idea is that if you're modeling something where you're  The what the quantity is your define the difference of scale between two different quantities is meaningful.  Well, then you can keep this. You don't want to normalize them because you think this difference should be  That the fact that to to variable has very different skill is important, then you don't want to. You don't want to scale them so that because the difference is meaningful. So that's what I meant, but I don't have a good example in my mind. I'll have to think about that.  So yeah, so is there any question about linear regression  Before I move on to logistic regression  No question.  Thumbs up, thumbs down.  Thumbs up. Okay, good.  I have good. Oh, that's nice. Okay. And I'm not talking to a computer. And so, so again, the big picture here is I gave you. Now one of the simple. The simplest statistical problem. I'm trying to predict  continuous values. I made a linear model, which could be seen as coming from a Gaussian noise generative model.  And already, we've seen standard phenomena coming from machine learning, like overfitting. Like if the dimension is too big. If I just did maximum execute or I just minimize a training error which is what happens when you do empirical risk and ization then  You you already get some issues that you don't have a unique solution and  I didn't give you the details, but you can trust me that the effect is that if you don't pick wisely. The correct maximum likelihood estimator, you get  high variance of your estimator, and you  You want to see how good your position or performance. OK.  So now we will  Go to one of the simplest model for classification. So I started with.  Regression now we go to classification with something called logistic regression  Okay. And somebody asked  If there's a rigorous motivation for pre normalizing the data. For example, could we show the diarization air will be better when we do so. I think so. In the context, again, of  If you would make some assumptions about the distribution during the data and then showing that when you just do l to regularization, it will be better behave if indeed your the scale of your inputs are comparable right so it's it's kind of get this uniform  Scale across the different features, I think, I think you could you could be able to formalize that  Okay, so let's just aggression. The setup is  binary classification  So now our output space is only 01. It's not the real anymore, but the input will still be some element in Rd  Odds told us capital X.  It's not a design matrix yet. It's just a random variable.  And then we'll again motivate the logistic regression from a journey of model.  So it actually it can be fairly generic and it will see that the assumption we make from those this aggression or not as strong as the other model, we will talk about with afterwards, which is the fisher discriminate analysis model.  So suppose that are on the assumption will make  Is  For now, there exists.  A PDF  A PDF, I either are densities.  In Rd  For each of our class conditional  So the property of X given that, why is one and property on x, given that y equals zero. So, these two conditional this distribution. Have a nice density, which would call it by the two p  And so now if we suppose it as, as the case. Now, if I want to compute the probability that Y is equal one, given that I observed capital X equals little x  That's the conditional that we want to use when we do conditional modeling. Right. Well, I can just use  The joint divided by the marginal. So this is p of y equals one, x equals little x divided by  Both p of y equals one, x equal x plus p of y equals zero, x equals little x, right. So this is the definition of the conditional. This was just the marginal because there's only two possibilities for for why  And so now  So here  That's fine for now. So I haven't done much  And now I will just do a bit of algebraic manipulation, I will divide by the this joint on the numerator and denominator.  So I get basically this is p of y equals zero, x equals little x divided by p of y equals one, x equals little x user avatar   Okay. user avatar   And  This is basically one divided by one plus and now I will just do x  Of minus f  So I wear f of x i just redefine things I'll define f of x as groups.  Fra x by definition will be the log of the conditional user avatar   Oops. user avatar   To these are conditional density P of x equals x given y equals one divided by p x equals x, y equals zero.  Plus the lug of the prior p of y equals one piece of y equals zero.  Okay.  So in general, I was talking just about general distribution and  How do I erases it  And so now  What happened is, in order to talk about  Okay, so what am I saying here. So because already this thing.  Already I was using the annotation of a density. So perhaps I use little pea sort of capital P  So this is a mixed. This is now we're getting into technical world, but this was a mix distribution where it is a discrete piece and a density piece I depart the part of respect to x is a density. Same thing here that will be  lit up. user avatar   Okay. user avatar   And ended up  So there's a few things I've done. So first of all, I put a minus here and you'll see why very soon as just to get the sigmoid function. So that's why I flipped the Y one and the y is zero. So here I had zero on the top and one on the bottom. Now I flip them around.  Here because of this minus i put an X. And that's why I have a login. And then I had a joint here the joint is the conditional times the prior. And so now be taking the log. I just get plus right so that's two pieces. And so now what are these pieces.  And so this is called the class conditional ratio.  This is called the prior odds ratio.  And now when I take the lug I get what are called the lug odds  Now, the whole point is if I want to know, you know how, what's the posterior of y equals one given X.  Given this model. Well, it will have to do about how more likely X is according to the class one versus cloud zero as well as are more likely a class one is respect to zero. So that's kind of like that, but I've included here. user avatar   And so user avatar   And why did I write it in such a general form well because it means that in general.  Just supposing there is a class conditional density  We can say that the probability of y equals one given X equal to x can be rated as the sigmoid function of with f of x as input. Yeah, where  sigmoid  Of z by definition is one divided by one plus x of minus z.  Right, so this called the sigmoid function.  Fewer neural network experts. You've seen it a lot in as an activation function in the old papers I guess now it's being replaced by reduce but that's pretty popular activation function and  What it looks like. Well, it has value one half at zero.  And then it goes to zero when Wednesday is minus infinity and it sent out at one when he goes to infinity. So this is kind of like this, this, this, this is called the sigmoid shape. Actually, the sigmoid function as a sigmoid chick. That's how it looks like. So this is sigma  And some properties of this function because will use them a lot.  Properties of the sigmoid function.  First of all, it is  It has a simple  Symmetric not symmetric property, but like, So, sigma lead of minus z is the same as one minus sigmoid of z.  Okay, which means that if I take the sigmoid of ze N minus c plus minus z i get  One. Okay, so that so I  And so that's pretty neat. And actually, it's pretty clear here because if I say that  The quality of y equals one is sigmoid  I know that the property y equals zero should be one minus the quality of y equals one. And so that's exactly what we're saying here is that the quality of y equals zero will just be celebrated minus, it turns out, and you can easily verify it.  In these equation that it's indeed segue to myself.  Another property of the sigmoid  Is that it's derivative  As a simple form, which is why it was using in neural network.  So the derivative of the sigmoid respect to z.  Is just sigmoid of z.  Times one minus z.  And this is just segue to Z.  Time to sing with of minus  So that's simple form for the derivative  OK.  So now.  So if I just have a class conditional model.  Hey, I assume there's some density on x.  Then I get that my protein of y equals one can be written as a depends on x would be the sigmoid function of f of x.  Now logistic regression happens when, instead of just having f of x, you actually have millinery parameters version. So, so, if f of x is not arbitrary function. It's actually a linear function of x.  Okay. And so to motivate that  So to motivate linear logistic regression  Linear logistic regression  And  We will show that there's a lot of there's a very wide class of distributions, which will give you a linear  Characterization of the function as a function of x.  Okay, so this is called  So, we will consider  The class conditional  To be in the exponential family right I already mentioned that in the past. That's a very important set of parametric distribution.  And will basically use them here to kind of get the same properties for all these members. So what is the next financial family.  Will have in future lectures, a bit more in depth coverage of exponential family. But here, I'll just give you the quick definition. So an exponential family. It's a parametric family where  The density of your distribution in your Patrick family has this shape. So you have some common  Scale or factor as a function of x and then you have this exp, you have the parameter transpose to have x minus eight of them.  Okay, and  That's it. Right. So, so basically the parameter that you use. There's different ways to prioritize them. You could use the mean pasteurization but here it's called the chemical composition. So, eta is the chemical parameter  And he of X and to fix.  These are the pieces which determine which exponential family, we're talking about. So these specify  The exponential family. And in this case, it's called a flat exponential family because we're using the chemical parameter  That we are considering  This to have x as also a name called, it's called the sufficient statistics because it is a sufficient statistics for the expansion of family in the statistical sense of US official statistics which would come back later. What are sufficient statistics.  And finally, this a of data. This is not used to determine the the can. The much a family, it is, once I did I determine h of x and to vex I can derive a of it is basically just a  normalization factor to make sure it's some it's integrate to one. This is called. This is a scanner function and it's called the log partition function.  The log partition function people in statistical physics have seen that this is basically just a normal user, right. So this is there a normal user to make sure that it's integrate to one.  Somebody asked about Cisco physics. Does the word chemical have anything to do with the clinical and symbol in physical physics. I don't think so. So clinical usually it's something  It's like this standard or it's kind of like something very natural. That's kind of what clinical mean  And so the clinical and symbols physical physics. That's a very natural basic and symbol for physical physics here, the chemical just means this is like the standard pasteurization that you want to use.  The other position that people can use our main path transition or even other transformation, but the Kennedy call one is the is the standard  Preposition you want pretty much with me.  And let me give you an example. So the beta distribution, the direction distribution. The plus song gamma. All these are next month enough me. They are all inspirational families.  So most almost all the standard if the decision, you know, or in the exponential family one exception is like the uniform deception is not an extension of me and we'll come back to these when we go into the  More details on the exposure family. But let's look at the gash in like, let's put the gash in and see how it looks in the goat in the exponential family.  So let's say I look at the Goshen. So I want, I look at my density as of the Goshen as a parameter you and sigma square. This is a minus one half.  lug of to phi sigma square and then I have minus x minus mu squared and i by two sigma square. So I think the log of my so you know I had this this one over to sigma square, square root I to belong. That's why I get minus one half. user avatar   It was a pain. Where's my bike. user avatar   And so now I want to express that in the shape as I had above so you can expand the quadratic term.  This so that it actually x square  Divided by  two sigma square  Minus  X times new divide by sigma square. The two cancels out the name of a plus new square divided by two sigma square. That's the last term. And then I have a minus in front of it, right.  And so  This could be seen as. So if I define as my sufficient statistics.  Oh, let me rewrite this word here. This is flat.  So,  Let to have X be the vector minus x squared away by two and x  Then you can see that the clinical parameter as a function of the  Mean parameter. Sorry. Yeah. So at of mu sigma square  Is  One over sigma square and then new divided by sigma square  And then so and so then if I take the product between to vex an ETA.  I basically get  These two terms here.  And this doesn't have x and this doesn't have X right these are just the normalization constant coming into a of so I can rewrite also a of it.  As being one of one half log of two pi square I was  A it  Was one half lug two pi sigma square and plus new squared by by two sigma squared. So it's all their remaining terms or just  Basically normalization constant. And so I put the gotcha now in an extension of me form, I guess just to be on the same page. This was the density of aggression. I guess I forgot to write this just assume  That's the density of the case. They got should in one  Okay, so I guess people have trouble with my writing.  What is the symbol between below to vex at the age or n and  Oh this yeah this is  So this thing year this is an ETA because I wanted to have the chemical parameter as a function of the meantime interview and sigma square  Okay, so that's just to give you an example. We'll go see more examples of the exponential family in further lectures. But now suppose that my class conditionally or in the exposure family. So what do I mean by that.  I mean that  The property. My density on x, given that y equals one, this will be p of x given at one, so I will have some parameter which described my distribution.  On X for class one and a different parameter for class two. Okay. And, and as a concrete example you can think as they're both different multivariate Gaussian  And the class membership tells me, what's the meaning the covariance matrix for my multiplayer Gaussian, but I don't want to assume that the red gas and I'm saying, I can do this derivation for any  exponential family. Okay, so then if I compute the laggards  To get my my logistic model.  Basically I by definition of my luggage if you remember when I just made this duration. It was  This thing here. So this is my  My, my goods. user avatar   Oops. user avatar   I think the ratio of my class conditional. So I had this was lug of probability of x, y equals one divided by property of x, y equals zero. And they had plus lug a quality of y equals one priority of y equals zero.  Okay. And so now if I just look at my model. I'm saying this thing here. This is a class conditional  Oops.  This thing here. This is the probability of X given at that one. This is probably T of x given it to zero.  This I'll call that just pie. There's a parameter, which I will use to quantify my prior over class one and so then if the other one was by this is just one minus by right  And so the nice thing now is because I have an exponential family.  And I take the log. So you can see here that  When I compare the class conditional for  One class versus the other, and think the ratio. I will just get the same I will get the same to have X, but with different parameters in front of them. And when I think the log. This just disappear. Right. And so if you just plug it in. What do you get as this is equal  To one minus eight zero transpose. Time to x.  Plus eight I have my normalization factor.  Versus, it's a it's a one.  And then the last piece is log of pi divided by one minus by so that's  How my logo ads various as a function of x given these parameters.  And so now I can  Re penetrates this saying, Okay, let's call this w transpose five x  Where did. Where did I get this W. Well, W. The parameter which will describe my, my, God is basically just a simple function of my  Classical additional parameters. So it's at that one minus beta zero and then the bias. There's also a bias term which is  Coming from I know musician constant. So this is a of it zero minus a beta one plus lug of pi one minute. Okay, so that's my parameter which kind of describe  My login. And then my feature function is just to x, which depends likes much of me and then I will have this constant feature one to get my bias.  So I just rewrote basically my my log function as a linear in paradise.  With using the power of w and five x. This is my my future map.  And so that means that what we're getting is basically a logistic regression model right so we get the logistic  regress regression model.  That the probability that Y equals one given X equals w zero x ray capital X equals little x  And so, Dora asked if he x cancels out. Yes, he cancels out because they don't depend on me. So when I take the ratio for to class conditional. The cancels.  So, this depends on w. So this will be the sigmoid of W transpose five x  So,  And that's what we call the logistic regression model.  Using five x as a feature map. So this is called five x could be seen as the feature.  So when you do the logistic regression model you portrays this conditional with the pattern W and you will just do maximum likelihood to learn. W.  And why did I talk about exponential family and all this kind of stuff. Well, I'm saying that if you make very generic assumption, Jerry of assumption about how the data was generated  By saying that, oh, if the data comes from class conditional and exponential family. It turns out that you would conclude that the the p of why given x will have this sigmoid  Function.  Variation with almost linear in the parameter. So, it is basically so so this piece is definitely linear in the in the parameter. So this is not linear, but this is just for the bias. So if so the only bias is kind of weird, but this part here is definitely linear and so you get that  In some sense, assuming this shape for your conditional is actually  Not making that strong assumptions on how that the data that came in. Okay. And in particular, there are multiple  Different class conditional which will give the exact same style of poi given x. And so you don't need to assume which one it is any of them will work.  Just why when I say to the the conditional approach makes less assumption is more robust than the general approach because this case there's actually multiple jury of assumptions which gives rise to the same conditional model.  And so, in particular, as I think you might do in your assignment that for guts. But there's an exercise to the reader.  So you basically try the argument above  Using the Gaussian as the class conditional. So you'll say p of x given why  Is actually a normal on X with mean UI and covariance UI.  And if you suppose that sigma zero is equal to sigma one, so they have the same covariance, then you will get that the feature map you get is just linear in X, actually. So it's x and then one  Okay. Otherwise, if you have different class conditions.  Otherwise,  If you suppose that the Goshen have different tests conditional you'll have that feature map could be  X X transpose X and then one. So you basically get a also the quadratic dependence on X appearing in your future map and then the sign doesn't really matter. I don't know why put a minus here, but this is basically the minus here is is optional, because  It's just a matter of how you define your quantities.  And so if you make the assumption that your class conditional are gushing with the same covariance matrix.  You get that the dependence on X is actually only linear, right. So, so you get that  W transpose x here, we'll just start there. Billy transpose five x will just be W transpose x like standard linear logistic regression was if you have a Gaussian with different task task over covariance. Then you also have a quadratic dependence on x.  And. Okay, so I guess I have two minutes I want start doing the maximum likelihood now because I don't have the time.  The short story is when you do maximum conditional likelihood using the logistic regression model.  You cannot solve analytically, the maximum, maximum back in the linear regression model, you could find the solution analytically. You just need to invert some matrix and stuff.  For there's a progression, you need to solve is something which is called transcendental equations which don't have analytic solution does why we'll have to talk about numerical optimization and particular gradient descent as GD util method editor.  And that's what actually you would be implementing one of these in the assignment.  So is there any question on logistic regression  Yeah, so let's just say question about the assignment.  You will receive the solution of the first assignment.  By email when the deadline to submit the assignment with no penalty. Well, to get credit as fast, which is next Tuesday. So people can are still able to submit a seminar on Tuesday.  With to get some marks after that zero. So then we can give you the solution.  Okay, so far as asking interesting questions. So if depending on which explains your family you use. There might be some constraint on eta, which we can lose if we do maximum black student W.  That is correct.  That is correct. In theory, I'm not sure if in practice is the case. So basically, it turns out that  When you have a clinical experience your family.  Let's put it this way.  So, so any eta is possible, as long as I can normalize my function. My distribution.  Otherwise Ayurveda is infinite and then it doesn't work. And so let's look at our data here. It was the first component here at that was one over sigma square  So he turns out that it. That is all. The first component of ethics here is always positive, because if I put a negative value here.  Which doesn't make sense from a mean privatization perspective. Well, it turns out that, then I can normalize my integral becomes infinite. Okay, so only positive at those are valid here.  And so then if you look from from from this perspective. Well, if I allow all the W.  Well here it still works because even though one component is positive, the difference between two component could be any size. So in this case, there's no constraints on on w. So  But let's say  If we, there are other types of constraints on eka in theory you might be not able to have all the possible W's here. And so that would have to be taking consideration.  But I think what happened is that  Your logistic model.  A I think normally what would happen is you would  You would get that your you would also have that your W will be restricted to make sure that this sigmoid model here whereas  A this one. Yeah. So, so this has to be a valid.  Oh, yeah. So here, five x, we don't need to integrate over five x when we compute that. Yeah. So you're so I think to answer your questions are fact  I think yes, indeed, if you would use a exponential family during model, you might add some constraints which are not the case in the logistic regression keys.  Which is why anyway I if what you care about is to do prediction of why given x i think the logistic model is better than these.  Classical additional models because it's it's doesn't need all these extra assumptions and in the assignment you will explore these different versions. Right, so you'll you'll explore the journey of approach versus the conditional approach of logistic regression  Okay.  Any other questions.  Cool. Well, if there's no other question I wish you a very nice weekend and I'll see you next Tuesday.
  So today.  We are going to  Finish the presentation of logistic regression  And because there's just aggression, give us transcendental equation to solve. We will now give a quick crash course on numerical optimization, because we cannot solve the optimal it conditionality.  So the numerical optimization  We'll talk about green method Newton and hopefully if we have time. Alright. Our LM s  So first of all,  Before I continue on logistic regression. I wanted to answer. Very interesting question that was asked on Slack by  Dawn.  So let me go back to  The last lecture.  And so f Cohen says in lecture eight linear regression. I have three questions.  Question number one, number one, I don't understand why having an infinite number of solutions in the normal equation of the maximum conditional likelihood is kind of an overfitting. Okay, so it's a very good question. So basically what happened is when I go back to  If I don't have any regularization  The maximum likelihood maximum conditional IQ solution or minimize or have this function and any W which satisfy these equation.  Will have will have zero gradient. And so it will be actually especially condition and it will be a solution. And so the problem is that  If this is not for rank, so I cannot invert it, then there's an infinite number of solution which satisfy this equation. And I said, this actually comes to overfitting. So the  The link is was not super explicit, as I mentioned last time. So let me talk a bit more about it. So the first thing is  So basically what is overfitting when when overfitting in an intuitive sense means you're  Both doing very well on the training set, but not well injure ization performance.  And  More operationally usually that happens because you start to fit signals or which are not signal if you start to fit the noise. So for example, if I have  A regression problem. Let's say ever linear regression problem. And so normally this would be my true model. And let's see, my observations are a bit noisy and so they're not exactly standing on the line.  Right and so overfitting would be that oh well you tried to pass exactly through all the data points and so  Your model is trying to really fit well the data. And by doing this, you actually fitting stuff, which has nothing to do with the feminine you're fitting the noise. Guess that's what we mean by basically the phenomenon overfitting. OK.  Now,  We're finding is an non trivial concept. And there's a lot of research going. What about what it really means and what are the sources and extraterrestrials.  But basically here at for regression, in particular, like the simplest way to think about it is, you're fitting the noise and us.  You're not feeding the correct signal and that's why you do bad into this area in particular if I, you know, try to predict  The value at this point my model says this is the prediction and that does nothing to do with the real right so that's why you're having a really bad test there in this case.  Okay, so  And  Yeah, and so  The reason now for linear regression is that this is a D.  This is a DVD matrix, right. And so if you have in order for this little bit for rank. You can think of having, you know, more dimension than the signal you have from the data point. And so that's why you can overfit  Now there's there's this thing I talked about last time, which was the the bias.  Variance trade off. Right.  Which is the composition of the frequent is risk for the squared error. So here we're doing regression. So our frequent is or we can use a squared error as our as our test loss as our loss.  And so if I want to know, with our method as well. I can look at an expectation of a possible training set. What's its is expected squared error in particular as  I forgot that it was great. So, and then  We saw that the the frequent this risk would be both the the bias square and square. What is the variance, the variance means oh if I change a bit the data set my predictor change a lot  And so that's why there's a there's a lot of variance. Okay, so this is just look at if I look at the expansion of our data set, what would be my prediction versus when it changed my data set how my prediction berries. OK.  So now when you have an infinite number of solutions, there's a question of will already there will be variation of your prediction.  Without even changing training set just picking one of these different solution right so that's already  Explaining why having an infinite number of solution is also related to high variance and high variance is related to having bad frequent this risk, which basically means bad diarization performance and not the great performance for your work with them. Okay.  And that helps to answer question to have F1, which is if we add regularization, we then pick a solution from an infinite number of them does this overcome overfitting. And the answer is yes, it's reduce a lot overfitting because  The first thing is  It will reduce the wall so there's there's two ways. So first is you can think of when you add regurgitation. Now you you actually add a bias in your method.  And so you actually increase the bias in this case. But you decrease the variance significantly. And so by decreasing the variants, you will actually usually  Improve the test there. If you have the correct value. If you are rigorous too much if you put the alumni equals 10 to the  28th, you will basically put the zero solution that will have very bad performance. It won't fit well the data. Anyway, so you won't be overfitting. But you will be underfitting  Yeah, so that's, answer, question number two. And then, question number through three was why is GD X, kind of like a regurgitation. Is it because it chooses from an infinite number of solutions which is also the outcome of regurgitation.  So yes, basically. So the, the opposition algorithm you use will bias from depending on what's your initials and you use a combination of initialization to your algorithm and the numerical division algorithm you use will  Basically choose some kind of solution which are not in the city arbitrary among all the infinite number of solutions. So you could think of it as also  Biasing which solution, you would choose. And it turns out that the one from STD is a good bias. And so that's so it's it's a bit related to regurgitation, indeed.  Okay, so is there any question about these three points.  On the narrow regression or does it help clarify a bit when I said in the last lecture.  No reaction DS doesn't help clarify a bit  Yes. Okay, good. Thank you. Okay, so if there's no further questions on the square. Let's now go over logistic regression. So as a reminder, at the last class I basically started by saying, if I just make  very generic assumption about the fact that  Each class conditional or some kind of distribution in some family.  And I looked at, then the conditional of why given x, then this actually look like the sigmoid function.  Where was this  A Yeah. And I said, actually the particular. I said, okay, to make it more specific.  Let's have these class conditional to be in in these things called exponential family which are wide range of parametric distribution.  And  We got that at the end we got basically the linear logistic regression model. Well, you got a linearly penetrates logistic regression model. So you got that the conditional why given x  Had some parameter W and some feature my fi, which will depend. The feature map depended basically on the sufficient statistics you use to define the expression of family.  And the parameter W was just a re scaling like a difference of chemical parameter in the expression of me.  And the whole point now is what we forget about this journey model these games from generative assumption.  We just now look at the conditional model and we say, okay, suppose that the conditional is basically this where we have chosen some feature function. That's the logistic regression model. Okay.  And the whole point of talking about six months to feminine. The last class is to say that there's a lot of jury of assumptions which give back to the same conditional model, which is why they're just aggression is fairly robust in its performance.  There's a lot of assumptions which gives the same logistic regression model. Okay.  Alright, so now  That I've reviewed logistic regression. Let's do a bit of computation. So  Let's see. So the logistic regression model. user avatar   Logistic urges user avatar   Tick regression user avatar   Model. user avatar   Alright, so again this is not for aggression. This is for classification. So the model we have is we say, well, the conditional of y equals one given X.  Is basically the sigmoid of W transpose and now we do it the linear version. Let's do it first with the linear version where we just put x five x will be in here.  And are possible outputs are 01  And I will. And so then p of y equals zero given x is just one minus the quality of one. So one sigma W transpose x and by the property of the sigmoid function. This is the same thing as sigmoid minus W transpose x right  And so that's our model. So there's two possibilities. So you can think of it, by the way, that why given x  Is a bird New Year in a variable. So when a condition on x, I have a burner year random variable with parameter sigmoid of the booty transpose x right because property of head or wife was one is sigma W transpose x and the other one is one minus  Okay. And by the way, as a side note, there's different conventions for the labels. So if, instead of having 01  You would use.  Plus or minus one which is not a burden in this case.  Guess plus or minus one. They're called rather Makar variable.  You can encode then the conditional of why given x as sigmoid and then you put why W transpose x right  Because you can see it here for this is plus one and here this is minus one right so I can just put the, the way that I want. And I get the correct quality. So that's a way to have both facilities easily  With y equals plus minus one. If you use instead zero and one, what you use is the burner ye, and I guess I'll put it in parentheses. So this is actually a. So, when, when we say that  Why is plus or minus one. This is actually called a random Makar  Random variables. So the rabbit the macro random variable is the same thing as a Bernie, but instead of being 01 percent minus one. So it's very simple.  Alright, so, but the Bernie. If you remember the burner ye PMS. We had a simple way to encode it is we just put the quality of one  And then raise it to why right and then the property of minus one.  We will raise it to one minutes why it's a one way is one you'll pick the correct one. And when y is zero, you'll pick the other one. So that's a way to encode in a succinct way both possibilities.  OK, so now I have encoded my my distribution. So, given some observation.  I will have XI Why I  From one up to end.  We can do maximum conditional like like you had to estimate or parameters, right. So we'll do  Maximum conditional  And we take the log of the likelihood, same thing. So look like to you.  To estimate  About yet.  So that's computer log line. So we have little l of W.  So I just have summation over my entertaining examples because they're independent  And then I have the log of the probability of why I given x i NW  And so this becomes summation i was one up to n. And so then I will have. So this is my, my, this is here. My leg like this is my likelihoods I think the log of that. So the way will becomes a different, so I get  Why I log of sigmoid W transpose x i.  Plus one minus y i lug of sigmoid W transpose that there's a minus the next I  So those is my leg leg. Good. And so now we want to maximize this function. So we can look at the gradient. First, let's do a bit of notation so that I, it goes a bit faster.  Actually not yet that's first computer gradient. So the gradient respect to w  Recall that  So if I looked at just a grid respective w w transpose x  Right.  This is  The gradient of the sigmoid function, the derivative of the sigmoid function. And we started the sigmoid function, the derivative was sigmoid of the same thing.  Times sigmoid of minus the thing. So that's my derivative of the sigmoid. And then I take by the chain rule. The derivative of the argument of the sigmoid. So the gradient respect to WW transpose x. This gives me x right  Outside. So that's a vector. So signatures a scanner and an outside to get my fix  So let's set. And so now let new I to be the duck product between W transpose and xi. So just, just some notation, because I don't want to write the blue transpose x i everywhere. And so then I have the gradient of the log likelihood  Is summation over i.  I get my my x term from the gradient of this thing outside. So I get x i.  Then I get all this killer stuff. So I had my why I which was just a constant.  I get the derivative of luck of the sigmoid. So I get one of our segments. So that gives me sigmoid of new I  And then I have  The gradient inside the log of this thing. So the gradient of this thing. And so this gives me like I wrote above of the sigma pieces right so this is  sigmoid of new i and then so of minus new I  So that's for the as it used  You raised think segue minus new AI.  And so now that was for this term. Now I do the same thing on this term.  And so I get plus one minus y i, derivative of the log i get the sigmoid on the bottom, but now it was minus new I and the rid of of  sigmoid of minus new I they will be a minus getting out outside  It's minus one and then they will have sigmoid of new I and sigmoid of minus new life.  Okay.  So that's the derivative and now I can just like cancel stuff. So in particular, I have that this cancels that. And this cancels that.  And so if I regroup.  I get summation over i have X I, I'm left with a why I  I can factor eyes. My why I so no note that there's a there's a minus here and a minus would because plus I have plus y times sigmoid of new I so I can just factor is this why I  And so I guess why i and then I have sigmoid of new I from the right, plus sigmoid of minus new I from the left.  And then I'm left with  The one which didn't have why i. So, this one here, there's a minus one and this segment. So it's minus sigmoid of  Then I have minus sigmoid of new I user avatar   OK. user avatar   So the other nice thing is we know by the property of the same way that this is just one so that simplifies. Okay.  So now let's rewrite a gradient of the loss of the log like viewed as this very simple form. So it's summation over i have my feature vector. And then I get why i minus sigmoid of W transpose excited. Right. That was new. I  That's it. That's a nice form.  Which kind of makes sense right when y equals one. We want the probability of  Y equals one to be high. So, and this is given by the sigmoid. So if so if the priority of y equals one is close to one. And we're happy if y is equal to zero. I want the segue to be close to zero instead. So it's so you know that's why I basically want the  The value of this thing to match my label.  My discreetly.  OK.  So now.  In linear regression  And by the way, linear regression when they look at the gradient. What I have instead is instead of a sigmoid I just get here.  The linear. I just want W transpose x i to be close to why that's the linear regression. Right.  Now is logistic regression, you know, there's this long year of transformation and I didn't use temporary Inc. user avatar   For that user avatar   Oops. Repeat.  Yes.  Yeah, so that's that's what I meant. Here, I said that you can contrast  This gradient  To linear regression  Where I have the gradient for linear regression is summation of i, x  And then I have y  Minus W transpose x i.  So I don't have this nonlinear transformation.  And so  Now the problem is if I want to solve this, this thing. So if I want to solve for  The zero gradient solve for gradient of l w equals zero.  I need to solve what is called a transcendental equation.  transcendental equation. What's a transcendental equation, it's an equation with like exponential term in them, for example. So, because what we have  Is that the sigmoid and I will have one plus one plus x minus W transpose x i.  And then I will have a bunch of other terms. So that's my sigmoid and I want this to be equal to zero. So I want to solve for this w, but it's  It's inside the, the argument of an exponential function. So this when I have like x of w plus w equals zero. For example, this is what we call an intrinsically equation because the  You completed a male equation is you just have polynomials of your thing that you're trying to solve it transcend all the equation. You're also having exponential and and signs or these kind of things. I guess these are triggered me quick question. So perhaps it's but exponential  And so that's why  So these don't have closed form formula which are simple, in general, usually. And so that's why we will use to we need to use numerical methods.  To want to solve for this W. We won't be able to do it and I'll take it. So instead, we will use  Numerical position.  And this is in contrast to linear regression where  Because the. This was linear in W.  Right, so. So the equation had W appear in the linear way, then that's why solving for w in this set of equations is just solving a set of equations. So, when we do the norm. When we sold the normal equations, which basically just solving a set of, you know, equations.  So you would consider that to also be having an analytical solution which is the if the thing is convertible. You just inverse matrix, though, you can still use numerical techniques to compute the inverse. But yeah, the that's a much more explicit solution in this case.  Okay, so. Any question about this.  So basically the plan now is to give a crash course in numerical optimization and then I will apply that on the logistic regression and it will actually. And that's something. Sure you will do in your assignment to implement Newton's method. Basically, it's for logistic regression and  The reason we do that even though that's not what you would do in practice now is because it's it's very interesting in terms of  Interpretation like you basically get that when you run Newton's method for logistic regression, you get a bunch of  These squares problem that you're solving where the weights become depend on your parameter. Okay. And so we'll see that right now. You're not supposed to understand what I just explained, but that's kind of a good be the path where we're going.  Okay, so let's talk a bit about numerical position.  So that's another example of, kind of like neat applied math numerical optimization that you will learn in this class.  You won't learn them in great details because the classes. It's not a special class. It's not a class specialized optimization. You could have a whole set of lectures on that.  But, you know, you'll get the important aspects here.  So,  Let's say we want to minimize  Some function of w LW  Where w belongs to our right.  So we don't have a constraint we just say o w is an RD. So this is called unconstrained optimization, because basically my possible though my parameter is  All of the Euclidean space.  Alright, so there's multiple ways to do that.  And so one of the standard in the most natural one is called gradient descent and you most likely know it.  Right. And this is called a first order method.  First order method.  And it's called first order because it's used green information rather than second derivative information which is the second order.  And I mentioned first order a second order I think earlier in in in this class. You can think of it. That's when you do with terror expansion of a function  You have the zeros order term which is a constant first order is the derivative times a linear thing. Then you have the second order term which is the quadratic  The second derivative times a quadratic function. It's your treasure. Then the third order will be a third derivative times a cubic, etc.  And so what's the this algorithm. Well, you start at some initialization. Let's call it W zero and then you iterate.  E T rate not at rare iterate or repeat, you will set the next value of the parameter as the previous value minus some step size which might depends on the iteration, times the gradient of the function at the current iteration.  And so the idea is you know that the gradient points in the direction of steepest increase of a function. So if you're trying to minimize you go opposite to the gradient, that's the direction of steep as decrease  And then you took a little step in this direction and then the gradient change direction. So then you know  Greek company the gradient and then you keep going. So it's basically it's like going down hill in a in a greedy fashion in a month and you always look at  With the steepest descent direction and they take a little step. And that's how I go down and hopefully at some point you'll reach a bottom  Which will be the local minimum.  And so that's it you just do that now there's already a question of one, when do you stop. What's the stuffing criterion.  Well, so a meaningful stuff being criterion is when you're close to especially point. Well, how can you  How can you  Quantify whether so sharing points, especially point by definition is when the gradient is zero. So the norm of the gradient this kind of a meaningful value of an approximate statuary points. So you look at  You will stop when the gradient at WT enormous smaller to some delta delta will be your stopping criteria. Right. So for example, you could choose delta which is dead to minus six.  That's not too stringent if you really want to go to numerical accuracy, you would use tend to minus 16, for example, but that's perhaps not needed for your application.  And  So in general, this one. Like if if delta if graph of f of WT is small. That doesn't mean you're close to a local minima because you could also be close to a subtle point or  Or even a local maxima because it's non convicts, you know, you might be unlucky and just be very close to somewhere where the gradient is is is zero. But then the curve is in their own direction.  And so just because the normal agreement is small, doesn't mean that you're close to local minimum. Right. In general, but there are a situation where it will mean something. So for example, if the function is strongly convex which already mentioned in the past. So if it's new strongly convex  And I said before that, that means that f minus  f of w minus mute by by to norm of w square is convex. So if you're able to take your function and subtract a concave piece because negative norm is concave  But for a small enough concave piece you still maintain convexity. That means it was what we call strongly convex  And the nice thing is this will imply when you're a musician convex that if the gradient is small, then you also know that you're close to a local men. Okay.  Actually a global men in this case because it's comics so you'll have that if the norm is smaller than delta. This implies that the value at a W two WT minus the optimum value or call it FW star is actually also small user avatar   And I think it's user avatar   I need to look back, but it's something like  New delta divided by mew perhaps some constant. I'll have to  Double check think I'll double check tech during the break. What's the constant here. But basically, so if the norm if small, you also have a control on the suboptimal at of your function. Okay, so you so you can know that  The difference between your function of value at this point minus the optimum value is not too big. That's, that's a much meaning more meaningful condition than just, I don't know. I'm migrated is not big.  But in practice, you know, you can just look at the number of the gradient and that's usually what people use because if you're a non convex. Anyway, you cannot really get guarantees and that's about it. So then all of the gradient will usually be what you use to stop.  But be aware that, you know, for example, here's a function  Right. So, this function is not convicts and here you know the norm of the gradient will be very tiny, but I am definitely not that the local men. I'm not even that. And I'm very far from the global admin, which was this point here, right.  Okay, so, so you never know that if the if the function is not convinced. It could be that you're in a flat region, but it will start again to go down later.  Okay, so that's the algorithm.  And then there's a question of all. What about the step size.  So there's multiple rules that exists in the literature, I'll give you just a quick  Overview. So there is a constant step size.  Which works. Totally fine for green method.  In the deterministic setting so constant step size. So a typical step size would be one over L, where L is the Lifshitz  Constant.  For the limits continue to constant for the gradient  For grad have if  Ah,  I guess I'm using F. Now, so perhaps it say I want to minimize not little l, but a F of the  That we're using the same notation here.  And what does it mean to be the chips continuous well the gradient function which is a vector function is that just continuous means that when I look at the gradient evaluated at two different point.  I think the difference this is smaller than a constant times how far these points are  So that's  That's basically what's live just continue to and that's a fairly common assumptions we make among the, the objective, we're optimizing that means that you know the gradient doesn't vary too far. So it's, it varies continuously  That's why continue to mean the function is continuous and it's even more than  Uniform. It's actually uniformly continuous and it's even stronger. It's, it's, it has a specific aspect which is called the it's Lifshitz come to this.  And that gives you a handle on how the gradient various too much and, in particular, you can get convergence guarantees for the grid methods on objective, which would depends on Capitol Hill.  So that's a first possibility. Another one that you will see a lot in machine learning is a decreasing step size.  And this is  A this is more common.  For stochastic optimization  And so what I mean by stochastic optimization well  You have that your function, am I using W here. Yes, my function of w is actually defined as the expectation over some random variable of a function of w and this random variable.  Okay, so when the objective, we're optimizing is the expectation of something random  We call this stochastic optimization and it, we might not have access to the expectation, because all we might have access or samples from this random variables. And then we would try to still up to my is using your samples so that that's what stochastic gradient descent does  Okay. So somebody's asking if newest positive definitely new is a bigger equal to zero. When we knew is strictly bigger than zero to talk about strong convexity if miracles zero would just say it's complex. It's not strongly convex  Side. So the expectation of xi.  Xi is a random variable.  So this squiggly thing is called side.  Okay, so what Hattie is asking an advanced question. What are some motivation for fancy learning rate schedules admission learning like cyclical ones.  So this is because of non convex optimization in unconvinced optimization as an issue because here. I'm talking about  Like the decreasing step size rule that I'm talking about. This is for convicts optimization where you do stochastic optimization and actually let me  Describe it so that everybody on the same page, and then I'll talk about defense here are cyclical ones.  But so a standard decreasing step size would be, for example, gamma t is some constant divided by t, so as to increases you decrease the step size you decrease at a rate of one of the t see would be a constant, which might to do, perhaps with the Lifshitz concept of your function and  So usually, so that's an example of decreasing step size and usually to get conversions guarantee you will have something that you want that the sum of the step size is infinite.  So turns out that the sum of one of routine is actually log of capital T. And so as capital T goes to infinity, this goes to infinity. So, so when use one of Richie step size.  You by taking even though they're, they're the same size goes to zero, you can actually get arbitrary far from the show ization because  If you just think the same steps. I was always in the same direction will get some of our one of routines direction in that conversation, very slowly, no longer capital T is not that fast increasing function, but you can eventually get to feed.  So you want that the step size. Some of the supplies and infinity. So you can explore anywhere because you don't know where the solution is  But because you want things to converge. You don't want to make too big step size you wanted the some of the step size square is actually finite and so one over t square actually is a finite. Some think it's pi over something  It's pretty standard infinite series. So yeah, so that's the standard decreasing step size. And this is actually used for stochastic optimization  Because in this case what you're stepping into  Oops. user avatar   To undo. user avatar   There we go. So this is for a split testing optimization. And so in this case it's the classic optimization. What you could do.  Is you could do Wt plus one is the booty minus step size and then instead of having the true gradient, you would use the gradient of the G function at WT an X it right so you would sample this random variable, and you would complete a gradient at your current a trip.  For the this simple read variable, okay. So you can think of, for example,  We'll go back to this in more detail later, but you could think of this as being the last  At the current parameter for a specific data point so excited, could be a specific index of my training set.  And so then what I want is to minimize the training air over the whole expectation of where the whole training set. So it's an empirical some  But that's too expensive. So way we can do is just like randomly sample at that point computer agreement for this loss and that's what we follow. So that's what stochastic gradient descent done instead of gradient descent.  The problem though is that if you use a console step size. This one converged was because of the noise of the gradient, you will you will never stop. Actually, you'll just keep bouncing around. So by using a decreasing step size, you can make sure that eventually converge to the minimum  And so that's if you had a convex musician. If you're non convex. In addition to trying to get to a local minimum. You also want to get to a good global minimum. So, there's also an idea of exploration. Right. So I think the cyclical  Learning rate help you to kind of like get to a local minimum. And then perhaps use a big step size again to to explore. See if there's a better local minimum somewhere else thing that's going to be idea.  So somebody is giving me the sum of whatever t square the states by square divided by six. Thank you.  All your star sac is asking me if the some over gamma t equals infinity and some of it, get a t square is finite related to burrow continue lemme somehow  Oh, you  Perhaps I mean there's different things, right. So the standard convergence result.  For as God or an expectation. So you take the expectation of things. And in this case, you don't have to worry about the priority of the event because you took expectations you just make sure the expectation decrease. And then these these conditions will come up very naturally  And I don't think I'm going to prove anything in this class. But if you're curious, you can actually look at the  At the lectures in my advanced search for prediction and optimization class I give. It's just a few lines of proof to show the convergence result of say  Even a more generic version of stick is the greatest sandwiches this upgrade method. And then you see that the, you will see that the, the, the sum of step size will appear in  In the denominator and this will appear in the numerator. Okay, so you'll have some of step size at the bottom and some of the safe side square at the top and you will have that the the cemetery smaller than that.  And so for this opportunity there to go to zero. You want the thing here to go to go to infinity. And you want the thing here to be finite, though. You could also have that this goes to infinity, but just much smaller than this one goes in 50 but  Anyway, so  That's what you would get for a standard commercials result now bro can delete this has less to do with the quality of a, of an infinite number of events. I think if you want to get conversions almost surely. Perhaps this will come up. But that's outside the scope  And what's G again. So, G.  So this G here would just, I'm just not talking about what is the classical musicians to classical musician is a position problem where your function that you're minimizing is actually written as an expectation. Okay.  Now, and so G is just the thing to define my my objective function. Now, in terms of practical examples. An example of G would be, you know, so for example.  You could have that g of W Cy is actually the loss and  Loss on why I and  hw of x i.  And I guess I don't like the last because usually this is discreet. So let's put some script L and and site here would be basically x, y.  And so you would sample an observation from your training set and then you would evaluate what is the prediction loss of your current classifier which is called HF W on this example.  Yeah, and that's the objective g. Now, I think the expectation of that with respect to the random training set, I do get just the batch training error of my  classifier which would be what I'm trying to optimize when I do  Empirical Research position. user avatar   Okay, so that would be user avatar   For empirical research diction. Hey, that's the example of G.  Okay, so that's decreasing step size rule. Finally, so Constance website is decreasing step size. Another one is  Doing some what is called line search  So in this case, what you do is you will pick over the step size.  The one which minimize the function in the one the direction that you use in your argument. So I will look at the current address WT I will move in direction. I'll say direction t  So this is the direction for the update  Update.  So, for example, it could be the negative gradient  But if we use this method for other methods, it won't be just negate gradient for example could be the Newton direction.  And the idea is you just fine in this one, the direction you you have a one, the optimization problem and you're just trying to find the step size which minimize the function in this direction.  And then the function is quadratic actually  You could solve this analytically at one deal and search is easy for a quadratic  For more general function like do that, I'll just stick. There's no simple and I'll take full Merlot to do that successfully costly. So this is actually  Costly in general because it's another optimization problem. So you still need to solve it and optimization problem. So this is costly in general.  But in network optimization what people do is they do.  An approximate search  Where you will try a few values. Values and then there will be some stopping criteria, and that's it.  And a standard example of line search technique is call me. Whoa.  Line search  Okay. And if you want to learn more about these things. I recommend that you look at Boyd's book I've put the link on the website. So this is a convex optimization book which is quite clear quite neat. It's free available online and the explain also those different lines rich technique.  Particularly amiable a matrix.  And the assignment. I think I'll just ask you to use a constant step size. user avatar   Yeah. user avatar   Okay.  So why why so and so basically what happens is the problem with the line search is that it's more expensive than just doing a constant step size or a fixed schedule in the step size.  On the other hand, it will make it much better update right so then there's a trade off. Like, okay, so I will have to make Ness iterations of the method.  Because each update is better, but each update is more custom so which one should I use it depends on the problem. That's where becomes also like an art of exposition. They're also conversions guarantees for different methods.  And so, yeah.  So, any question about grid method. Now, I think I will move to the new tense method.  Press. I'll take a 10 minute break. Then, and then we'll talk about Putin's but any question.  Yeah, jack Lew. user avatar Jacob Louis Hoover  Is there anything that guarantees, this, this, I hadn't heard of this line search  Arm your mind search for, is there anything that guarantees with this is actually optimize it, but like, could it not be continuous. user avatar   So you mean if f is not a continuous function, then I have problems. So usually you will assume that f is continuous, because if f is not continuous, you start to get like really weird. Yeah, from a numerical perspective.  Optimization. That's really make that much sense. Right. So if the function. So if I'm trying to minimize. For example, this function.  Right, so its global minimum is is is is here.  But I have zero information about it because everywhere the function looks flat, even when I'm infinity close. So a computer won't be able to optimize this thing. It doesn't even make sense to to talk about it. So what happened is  When you do numerical analysis. You want to try that, you know,  You're using methods which use floating point arithmetic. So, so you don't even have infinite precision on the numbers.  Okay, so, so you need to have things which behave nicely such that when I'm very, very close to something, then, you know, things don't go crazy.  If I get even closer right so and so, in particular here for this non compliance function unless I'm able to exactly have this number, which might not even be presentable in your computer, because if I do pie.  Then you will never be able to reach it. So, so usually from a numerical position you was supposed to function is continuous doesn't mean that the fortune is differentiable. So it could be the derivative is not continuous, but that's a different thing.  And then Dr guarantees for line searches.  There is the fact that if you do not differentiable optimization  If you do line search exact line search in particular, you could make your method non convergent. So that's something you have to be careful.  So it can turn out that the lines which will bring you in in in points which basically looked like you get stuck, or you get very, very close to a kink. But this kink is not the actual minimum  So that's a but that's also a bit outside the scope of this class. This is when we talked about non differentiable optimization will make many new things defensible position in this case.  If you're curious about non defensible position I mentioned it. My other advance which a prediction and optimization class.  Okay, so he means he always asking a good question about  I haven't posted more scribe notes. So either I should get a scribe today.  So perhaps I should do that. So let's get the. Can I get two volunteers for ascribing today.  So what does it mean, it means that you will make a cleaner version and latex of the notes after this class.  And then I will review your notes and I will post them.  Alright, so I have two already so I have Ishmael and the new law. Can you send me an email. Both of you with your email address. And then, I will send you the instructions by email. Okay.  Cool. Great. Was there any other question about optimization and method.  Nope. So let's take a 10 minutes break it is right now 326 so let's go until 336  Okay, so the first thing is  Oops.  I want to scroll. Okay.  Okay. So I mentioned here, I would fix up the constant. So it turns out that what we have here is one over to mute norm of the gradient  Square. Right. So actually, I'll put the  Square here.  Because that's kind of the better scaling, in some sense, the, the suboptimal UT in function is ready to the norm of the grading square. So, I think it makes more sense to look at the normal green square  As a stopping criterion. And so in this case. That would be so if I have that the normal degraded small, then this will be smaller than delta divide by two.  Okay, that's the guarantee I was talking about earlier.  Okay, so that was great method. Let's talk about  New method.  So this Newton's method is a second order method because it will use also the information not only of the gradient of the function, but also the second derivative or the  Second order method.  So,  There's multiple ways to derive Newton's method. But one way is to  Think as  A method, which will minimize  A quadratic  Approximation of our function.  So there's something in numerical optimization as this thing what you call a model in the medical condition. It's not like a model in machine learning where it's basically just  An approximation of your function.  Through your function could be this very complicated object, which sometimes you don't even. It could be, say, the function could be the  The objective could be there this the the solution of a simulation right like like you're modeling a power plant.  And you have a simulator and then you want to minimize I don't know the temperature of your tank.  And then there's all these parameters of your power plant that you could play with and then when you change these parameters.  This complicated simulation give you a different temperature and then you just want to minimize this temperature  But you don't really see you have a simple form for what this function is objective is  And so what you haven't said as a model. So, for example, a linear model or a quadratic model of the objective that you're trying to optimize  And so, and and and so a lot of optimization techniques are motivated by making a simple model of my function and doing exact update on the simple model. And that gives you an algorithm and. Okay. And so if I make a quadratic approximation of my function. So by Taylor expansion.  Around my current iterate.  I have that f of w is f at WT that's zero order plus the gradient at WT  Inner product with w minus wt. That's the first order term of your expansion and then I'll have my second order term. So it's one half.  W minus WT  transpose H of WT which is the essence or the matrix of second derivative then W minus WT  And then there's the remainder term. If I do a credit expansion. The remainder term is is cubic. So it's actually order norm of w minus WT cube. Okay, so that's basically the Taylor's remainder  So if your function is well behaved.  You can have this stellar theorem.  Example you want. In this case, I think the second derivative to be continuous. In a neighborhood that's what you would have. So as long as W is not too far from wt. This is a good approximation.  And so HF W.  This is the Hession  So,  Each of WT is a matrix. And if I looked at his ID JS entry. This will be the cross derivative of f at WT respect to w i and then WG  Okay, so that's a  So let's call this first quadratic term. We call this  The Qt  Called this function cutie depends on where I did my extension. So, depends on WT it's a function of w and then I have a remainder  So this all thing here. That's just call this cutie of W. That's our quantic model. So this is our quadratic model approximation.  So Newton's method. The idea is, I'll say that Wt plus one.  Is obtained  By minimizing or quadratic approximation.  Respect to the body. Right, so I want the gradient expect to W of Qt of w equals zero.  That's my starting point. And so if I looked above  This is a constant, it doesn't matter. And I take the gradient respect with W. I'll just get this piece.  And here I'll get the linear return right so compute the grant of this whole of QT. I get that grad of f of wt. That's the first piece plus SDN WT times w minus wt. That's the gradient of my quadratic function and I want this to be equal to zero.  So now if I solve for w in this linear set of equations. Suppose the headset is in vertical. It has a unique solution. So, this will be W minus WT is minus as sin inverse at WT  Times degraded. user avatar   Okay. user avatar   And so the the the Newton update  Is simply that my new iterate is my previous a threat minus  A linear transform version of the gradient  So that's the new terms of date.  Okay, so somebody asked me about my weird notation so  The question.  Let me rewrite this.  This was the cross derivative, right. So I have  Partial whoops.  This is partial square f of WT and then partial derivative w i partial derivative, wha, so this order criss cross derivatives.  Okay, so the first thing is  In general,  Implementation.  You need to compute. You need to invert a matrix.  Or you need to solve this system of equations right so too. So basically,  Saying that the direction t is H inverse grad of f is the same thing as  H dt is equal to grow out of it. Right. So solving for the direction I could either invert my matrix or I could solve a system of equation in general, solving a system of equations is order dimension cube. So this is actually  Order d cube time  To compute  In general,  Like if I have  So if you have structured matrix. Sometimes you can do it faster, but for a general matrix A DVD matrix. If I wanted to solve a system of equations.  The number of operations is the cube. Okay, so if the is a million tend to the six acute that that's tend to the 18 that's, you know, on tractable on your computer right now.  Then let's say it's in a few years, it becomes tractable now instead of a million just computed billion and now you're, you're back again to problems. So this is super expensive.  It takes DQ time and order D Squarespace and so for machine learning in high dimension. This is not really feasible in, in general, unless we have a structured and there's a lot of modern techniques which does that, which is why Newton method was not super popular  in machine learning in the last few years, you know, in the Big Data era and first order methods which don't have this DQ berdych the the square dependence is only order D were more popular. Okay.  But  The new method is actually converts much, much faster than the grid method and let's talk about this now.  So, so  So there's this thing called  By the way, so what you will run in the assignment, I think, was it dem Newton or its standard unit. Actually, I forgot. But there's this thing called dem Newton method.  Which is much more stable.  Than the Newton method. So in the new method for some function, it will just diverge.  For some initialization. It will just diverged. So, you know, you have to be careful. So to stabilize things, what you'll do is you'll add the step size.  You add a step size. user avatar   To user avatar   Stabilize Newton's method.  And so the update for dem Newton is the same as you're done, but you add a step size here in front of the headset.  Yes, you could put it could be also depends on TV, you  Gotta have it for the week.  And so this is the step size.  And so what happened is, if you would take normally the the Newton step you could overshoot too much. And actually, that's where you can start to diverge.  But by by using a step size smaller than one you would actually dent the update you won't go too far. And this actually could make it now convergent. So the, the kind of update user avatar   You know, user avatar   I'm not sure if this is the kind of function, you could have it.  But you could have something like like Newton's method could have these kind of a date where, you know, it just blows up to infinity.  Whereas if you start here instead of going to this point if you dump it and go here, perhaps, now you start to convergence that  That's the dampening effect.  I think normally this step size is fixed, but you could also do online search, you could do other things. So let's just put it more generally, with some general step size which could vary.  OK, so now let me give you a bit of the intuition behind Newton's methods. So the first thing is if your objective is quadratic  Right then your model cutie is exact, this is zero if their function is quadratic because it's a quadratic function. So,  And so one step of Newton will optimize your function and one iteration. So that's super fast. Now if your function is not quadratic  For example, like logistic function well then you you you want this the conversion monster. Right. But in some sense if it's not the idea of Newton is like, okay, what if I not too far from a quadratic, it should go very fast.  So the question now is,  Can we use Newton up to a certain point and then then as we get closer. It's actually the other way around. What happens is when you're close to the optimum usually you have a nice quadratic ball.  If you think about making the tiller expansion of the function around the optimum. So, and you didn't actually convert superfast it's it's fiction has something called the quadratic phase convergence, which is faster than green method which is linear.  And so the place where you want to use the damping is when you're far from the optimum where the function could be really weird as nothing to do with the quadratic  And then if you don't, then you can actually divert. And so, so normally there's Newton's will have two phase. There's the, the, the, the convergence to a neighborhood phase, which doesn't convert that fast. And that's where you use a dumping and then  There's the quadratic conversion face where even if you would try to optimize over the step size, you would just put a step size of one because that's the best update you can make because it's like a quick break.  Okay, so now  Let's give a bit more intuition about  Newton's method and why we would like to use it.  So it has much faster.  Come convergence.  In the number of iterations.  Versus the gradient method.  So every update of Newton is much more expensive, but the number of updates you need is smaller, so perhaps  This can actually be beneficial, especially if you want a high accuracy.  In some sense,  For example, like if you really want to get to tend to my 16 year Chrissy. Usually you want a second order method to get there.  So there's this thing called inferior point method. By the way, which is a very generic method for convicts minimization, where you have constraints.  And basically, these methods so that a lot of commercial software use that like mosaic or see plaques, etc. And the main idea there is you actually basically use Newton's method in a clever way on some luck barrier function.  So there is asking if people use a mix of first order and second order methods.  Well, it depends what you mean by a mix. So first of all, there's this thing called queasy Newton's methods which instead of computing the Heston exactly approximated the hashing.  And in particular, there are methods which don't even compute second derivative at all. They will approximate the SDN by looking at how the gradient vary across my uterus. So be FGS  Are limited memory be FGS, for example, is an example of causing you to method, which never need to compute the secondary they've only use graded methods. So in some sense, you could think of it as  A first order method because it only use first order information, even though it's trying to approximate the second order approach. So that could be, I guess, interpreted as a mix of first order in second order.  Another okay so so basically you're done method is faster convergence in number of iterations versus being descent and a nice property has, by the way, is it is called a fine covariance  Okay, which means that the method.  Is invariant  To rescaling of the variables. user avatar   Okay. user avatar   So the problem with the first order method like gradient method is that when the  level sets or not. Well, conditioned when the husband is not a world condition matrix, the level set are actually quite ellipse. And then the gradient just kind of a seat. Okay, so basically if I look at the level set of my function.  f of w  And if I have some directions which has small eigenvalues and one which has big eigenvalues in terms of the history and matrix, you get this kind of like elongated level set. And so then  The green and method will just keep us eating like this and it would take forever to reach the global man, which is here. Okay, so this is basically the gradient  Descent Method.  So what happened here is because of this weird elongated shape the gradient is not pointing towards the global men.  In So following the gradient is not super informative. Like if you keep like changing direction and USC it. Okay.  Kiss small is asking what a level set again.  The level set of a function is the set of points which have the same values. So this thing here is a set of point W such that f of w is equal to some constancy.  So that's a level set and you know that the level set of a function, the gradient will be perpendicular to the level set  So that's why you know the direction I'm moving into here.  Is perpendicular  To guess here. That's the negative gradient  And that's the user avatar   Right angle. user avatar   And the beauty of Newton method is to kind of like take this very badly condition quadratic and transforming back to a nice circle. Okay, so, Newton's method.  In some sense, can be seen as  Is using the hustle.  To make f well condition.  So in particular, if I define  Let's say this is  It. This is w, the space W and I will define my new variable z which is Hession one half times w  And this would be my new space z.  You can actually look if it's a quadratic function that the level set now will be perfectly circular  OK, so the nice nice thing. Now if that the negative gradient is actually pointing directly to the global mint.  Like on the trick to see that is the level set here.  Basically I have w transpose Heston W. If it was aquatic function equal some constant. By the way, this is called a quadratic form.  And so now what I can do is I can  Dive analyze my Hession. So this is the same thing as one half W transpose some orthogonal matrix transpose sigma p  W equals c and this is diagonal  And so when we talk about the, the square root of, yeah. So somebody asked what was this he exponent. So this was the square root. So it's one half.  And so I can have. I can see. Let's if z is just H one half W and this would be basically  The diagonal of sigma one half times  W.  And so that means that  The equivalent level set in disease space is this is basically Z transpose Z is equal to constant user avatar   Right. user avatar   And here I have used the expansion, where he was P transpose sigma p which is possible when he is symmetric.  And positive semi definite which will be the case if the function was convex if it's not complex, you can you do the exact same argument. I just did. But for a complex function, then this works.  And so basically the idea of the running the headset method and oh yeah so. And the last thing, which I didn't mention so you can actually try now with these transformation.  To convince yourself exists reader that if I make a gradient updates in the z space.  step size gradient of f respect to ZTE when I transform I obtained z by making your initial session of w with the house in this is the same thing as making an update in the original space. Oops. This was W.  WT minus step size and then I have my Sn inverse appearing  Right so doing green method in the transform space is the same thing as doing Hester Newton's method and the original space.  But the transform space, the space as the beautiful property that the level set or perfectly circular if it was a quadratic and does it converge in one step. So it's very well condition.  Now, if the original function was not quadratic, you don't actually have these correct ellipses middle level set  And so then you won't get that it conversion one step grid method one conversion one step in the transform spits, which is why Newton's make multiple iterations. That's kind of the idea  And what happened is, if I scale one variable that saying, multiply, let's say I started with a nice  perfectly circular function. So, like the LT norm. That's a nice circular function that it will have a meeting, it will have circular level set. If I multiply one of the variable by a huge number.  Then if I look at the level set, it will again make these super hoops. We didn't want to do that.  It will again have these very  Long gated level set right so that's why I'm saying the grid and method.  is sensitive to the scaling of my variable because if they re scale one variable it change the direction of my gradient. And so it will make now the method, which could converge very fast to convert very slowly, whereas  You can scale variable when you run Newton's method you can sell the rescaling with the Hession  And so then you don't have to worry about the risk getting so that's what I mean by the method is I find covariance, it means that  If I make an affront transformation of my variables running new does method in the transform space is equivalent to the running Newton's method in the original space and then making an effort and preservation of the threats. So it's just the same series of interests. So there's no difference.  Looks better be.  Okay. Any question about Newton's method.  No question. So let's apply now Newton's method for figuration. Okay, so that was  Kind of an interesting  Sideways.  So Newton's method.  For logistic  Regression  And this has actually a specific name. It's called iterated be weighted least square when we'll get to the actual algorithm. I will redefine IR s  Right now, just think of it as it's some acronym.  So let's go back to our objective. So recall for the log likelihood  We had that the gradient of the log like you had was summation over i, x  Y minus sigmoid of W transpose x i. That's my gradient  And so now if I compute the second derivative because I will use into the algorithm and you take the derivative of that. So, the historian of the log likelihood  Is actually  A bus. If I take a different of this I will get  So the first piece here, they would have this doesn't do anything that's a constant. So, this is this one, there's a minus. So, I'll get a minus in front, put a minus  Minus I'll get an exile from my gradient of the W transpose x. So I'll get x i x transpose  And then I take the derivative of cigarettes. I get sigmoid that we transpose x i and sigma minus W transpose excited. That's my husband.  And now you can already verify that  The question is positive.  negative definite, sir. So this is  Negative 70 different  So if I take a vector times H v.  So we transpose HIV.  This is equal to minus summation and then I will have V transpose x i and then x i transpose V. Then I have segmented of stuff and segment of minus stuff. But this is just positive number. So these are positive number.  This is basically X transpose V square. So, this is also a positive number. So that means that we transpose hv is negative for all the in RD. So that means that or Hession is negative semi different right so that means our log likelihood in this case is concave  So that's neat aspect means that if we have a gradient. We know it's a global Max said the granite equal to zero.  Okay, so there's a question here about the assumption that your function is well model approximated by the aquatic implies  So basically, if we think about the gradient method. The green method.  Has guarantees when, in particular, your gradient function is continuous and Lifshitz right  And that means that the first derivative, the second derivative is not too big. If I have a bounded secondary if it means its limits.  And it means basically the gradient is giving good information about where to go, and the stitches constants will tell you, you know, what's the worst case variation you can get and it would tell your conversions result.  And so you can also do something else with Newton was saying like, Okay, well, how fast is my, my second derivative changing. So if you have that the second derivative  Is Lifshitz continuous. You can also get a convergence result guarantee for Newton's so it's kind of similar in spirit, then the great method. So that's one way to kind of control the well behaved this of your function. Okay.  But this answer your question, Jacob.  Where was I  Oh yes okay so  I got my leg.  Okay and so  Yeah, so there was something subtle is that  What happened is when you do the grading method and you want to maximize the function you go in the positive direction. If you want to minimize you're going to negative gradient. OK.  Now the magic is Newton, you don't have to worry about whether you're maximizing or minimizing because they're healthy. And in some sense, will tell you which sign, things are so if I run Newton's on a concave function.  The hesitant is negative different which means it will flip the sign of the negative gradient. So instead of going in the negative direction. I will go to a positive green direction.  I mean it transformed version of the great and but it's since it's like a positive now great and direction. Whereas if the Heston is a positive significant a convex function, then  The he will be positive, it will always it won't change the sign. So if I move into negative great interaction. I stay in the Navy either getting direction right so that's why here. I'm saying in this case Newton will be maximizing  Instead of minimizing just because of the  Definitive  I mean, also, another way to think about. It's actually Newton doesn't care about minimizing are maximizing like you can  Think of Newton as a other way to derive Newton is to think I'm just trying to solve the linear nonlinear set of equations graph of f of w equals zero. And I make your first order approximation of that. And that's also gives you the Newton's update  Because you have greater than or equal to zero, you will take a derivative of that you get the SDN appearing and then you solve  And then you get the Newton's updates. And so in some sense on you tend to try to get as a statuary point  And so, especially point could be max could be men could be saddled could be anything.  If the function is next in the story points are global men, the function is concave especially points or double max and then you really have maximum in but if the function is non convex, then you could get anything  Alright, so now to try to interpret this new terms method. Let's introduce a bit of notation as before that, as we did is for linear regression. So recall the design matrix X, which was an end by the matrix. This is  X one transpose  Blah, blah, blah. To x N transpose. That's my design matrix.  And so now let  Me you I  To be sigmoid of W transpose excited  So you will depends on w and it's here. It's just a sigmoid of W transpose excited. So this belongs to 01 excluded and there's W's infinity.  And so now I can rewrite the gradient of my log like you would in a very simple form, it's simply  Recall it's summation of er ay ay y minus sigmoid of W transpose x i, o, which is just knew I just knew I  And so I can rewrite that as x transpose  Y minus new when where why and moon are organized as a vector. user avatar   Okay. user avatar   And then the Hession  Which was a negative summation of her i x x transpose new i one minus view I because remember the hesitation had this SIG sigmoid of the thing and sigma minus the thing, right. So that's UI and one minus new I user avatar   Oops. user avatar   And so this is actually minus x transpose  That diagonal matrix of w that I will define and then times x.  Where  The AI.  Is defined as new i one minus p. So I just put these new i one minus VI on a diagonal. These are basically constantly scanning of my features.  Alright, so now let's read the Newton's update  So I have that Wt plus one is WT minus the Hasson and verse. So here the SDN is minus x transpose the TI x minus one.  Times the gradient. The grand above was x transpose and then I have y minus beauty. So I put here at the index t from you because user avatar   You know, user avatar   Basically I guess muti by definition is sigmoid is a vector where each entry is sigmoid of WT and then x i.  OK, so that's my gradient  And now I do a bit of manipulation.  First I will factor eyes.  This thing.  On  So that I will just put it in front. So I'll have X transpose DT X and verse  And so to get back the identity. I NEED TO PUT IT AGAIN. SO WE'LL HAVE X transpose DT x times wt. That was the first term, and then the minus minus became a plus, plus. And then I have x transpose y minus beauty.  So why did I do that well because I want to write this  This new equation here. So this is the same thing as Wt plus one is x transpose DT x minus one.  Times x transpose dt. And then a new variable called ZTE  Okay, so that's my update  For WT and now what is the ZTE so ZTE  It's just the thing I left. So it was x times WT  And then the other piece.  There was no d there and I multiply a D on the left, so I have d t minus one and then y minus UT  This is where  I define the ZTE  Okay, so what's the point of all this. Well, the point of all this is kind of like to identify  At least square solution. Okay, so this here.  This is a solution.  To generalize the square problem.  To a  Weighted  Least square problem.  So you remember like the solution to the square was X transpose X inverse  Problem.  Alright, so for at least square, I would have X transpose X inverse an extra expose and then I would have some targets here. I called my target ZTE  Now, the difference is I did a diagonal piece this diagonal is coming from the fact that I could have different wait for my air. That's why it's Carla wasted the square problem.  So let's move this  Did I user avatar   Not get my user avatar   Was supposed to be erasable ink.  Alright, sorry about that me just erase the  Temporarily ink. user avatar   Okay. user avatar   And so this wait at least PR problem. What is it, it's, I would like to minimize respect to w, the norm of D in  Heaven of temporary thing right so minimize respect to w  The norm of the one half of z t minus x w  Norm square. Okay. And so, compared to the square is I have this weight diagonal wait matrix.  And instead of having why I have not been you target, which changes as you run Newton, because basically what happens is  You will compute the target using the proxy their current quadratic approximation of your objective and then when you move the threat somewhere else, then there's a new quadratic approximation. So the target moves.  And so  What this  LT norm thingy expand to it will be summation of where I, I will have said i minus W transpose x i square. And instead of having sigma square as we had before in the log like you would for a linear regression model I will have di minus one.  Okay. And so basically this is to be compare  With a Gaussian model.  A Gaussian noise model.  For the square  And so if you remember in last class or two classes ago somebody asked me. Oh, what happened if you don't assume that you have the same noise for all variable. I said, what you get is a reawakening and here it becomes pretty obvious. So I would have  Basically normally to sigma. If I change the  The, the noise storm for every data point, then I could have a different sigma square. And here, this is the di right user avatar   So you can physically interpret them. user avatar   And so basically, how do you get a target. When you do  U turns. Well you take your current prediction and then you basically compute the Halcyon which gives you the this diagonal term.  And then your risk scale, the current era that you have right so muti is is the transformed. It's a sigmoid of your current a prediction and then why is the target. And so that becomes a new target and subsets. So you're trying to fit the like the missing piece.  And so  Now, this explains why we have IRS, so you have that Newton's method.  For logistic regression  Is equivalent to something called iterated  Related  Least squares.  So that's what are our LM S stands for because  You are  Solving. It's called iterated because you iterate multiple the square problems and at every iteration you relate  The possible square there according to your current expansion and that's why it's called really these words. Okay, so it's just kind of an interesting interpretation of Newton's method for logistic regression  Yeah, so the denominator here is inverse, right. So, this is  This thing here is inverse, the i minus one inverse  And as a side note,  You know, if you're trying to solve the inverse problem.  A very narrow set of equations when we meet. So if you're trying to solve a set of the equation. So I want to find an x such that x equals be equivalent to you also can minimize over x. The norm of x minus p. So, could also be seen as a  Optimization problem.  All right, I see is asking what's the integration for the weights like is it waiting, the more difficult. Examples more  Not sure if it's the more difficult example right the wave has to do with the the inverse Hasson and so if if the SDN is is is is big.  You will say that you have  A small noise. If the SMS is is large, then you will have a big noise. So I guess, indeed, you will way more the directions which have  Big question, meaning that there's a lot of variation in directions you really want to be careful to approximate things well. Because otherwise, because things vary so much you'll perhaps like overshoot. So I guess I guess that's a good interpretation.  Okay, I have four minutes to do big data optimization. That's a bit unfortunate.  Well, let me still tried it.  I want to, I want to wrap this up so  It's do. So this is new terms. So that's what you will implement the assignment as a nice the tactic introduction. But what about if I want to do. Big Data logistic regression. Right. You cannot run Newton in dimension, a billion.  And so let's talk about as Judy and all these things.  And so  Basically, when you have a lot of dimension, you cannot do.  Order the square or even the cube.  Operations.  And so then, instead you restrict yourself to first order methods.  Which is why first other methods are so  Popular in big data optimization, because you know we're in large the  Now in machine learning. Not only does large. We also have enlarge  In this case, you can do batch method.  To batch methods. So what we mean by batch method.  So,  The gradient of f of w  Is actually the empirical average over all the training set of the gradient of the losses. So let's call this graph of f i have w  And so  This basically would be the  Gradient.  Of one function.  Like the last on a specific training example.  And when I look at the green on the whole training set by need to some all these gradients over each training example and the Batch gradient is competing this awesome. This is called the Batch gradient  And to compute it now is order n times d. Right. So if n is a billion and these a billion, you're back to the square, you have a problem.  And so instead what you do in this case is you do  You use incremental green methods. So as GDS especially example of that. So incremental  gradient methods.  And so, for example, you would use stochastic gradient descent. user avatar   Oops. user avatar   I think I would go a five minute over time. Sorry about this. Let me give you the gist here.  As Judy.  And so what you're doing as Judy is you'll say, okay, my current it's or it I take  I take my premise interrupt, I think, a little step size and then instead of using the Batch gradient, I will use graph of f at some of these randomly sample function I it  Iteration t. Right.  So now this would be order D instead of order India to do an update. And so where it  Is picked  uniformly at random.  So you is uniformly. So I guess uniformly at random. Okay.  So as Judy you pick a random training example, you look at the last on this training example, you take the gradient of this last takes order the diamond. That's what the update you do in expectation  You would have the correct direction. But this is kind of a noisy. You can think of it as a noisy version of the Batch gradient  And this actually converges use decreasing step size, for example, etc. And so, as God, what happened is you get cheap updates. So the updates.  Are much cheaper than Newton's  But you get slower overall convergence.  For iteration.  And batch gradient  You get expensive data updates.  But faster convergence.  And so for example let's say this is my my objective. So this would be the Batch gradient update was as Judy, it could like  Go around and you know converge at some point. So it would take many more iterations to get close to the optimum. But each of these iteration or n times cheaper. So it could perhaps  Compensate and, in particular, if we do a little plot.  There's this if we look at here, we put the time here. We put the lug of the optimization error.  Right, so this is log of f of WT minus f star.  So it's a semi log plot.  The, the green method is as a linear convergence, it means it has a leaner. It has a nice line on similar blood. So it looks like this. So, right. So first, it takes forever to complete the gradient make then it makes an update. So we improve the objective  Then it takes forever to complete the gradient, then it takes an update. And so you get this nice staircase shape.  Which has a linear convergence. The as God as she has a severe convergence in this case.  But it start, because every update is cheap. It make progress right at the beginning. So it actually started very fast and it goes slowly.  So at some point the batch method will  Overs will supersede the SCD method, it depends on which level of activity you want. So if you want to go to tend to know my six or eight then you definitely need linearly conversions are women.  So then there was a question a few years ago, which has, is there a method which starts fast. Oops.  It's console.  So is there a method user avatar   Repeat. user avatar   Repeat. user avatar   Their method, which starts fast and stays fast.  Okay. And the answer is yes.  And these are the variance reduce method.  Which was a one big breakthrough. About eight years ago in  Context optimization. And one of the first one which was proposed was a sag.  For stochastic  Average  Gradient.  And so the idea is actually fairly simple. So I have the grand descent update  I would take my current it or it sorry my previous sitter it take a step size and then I would have the Batch gradient  grad of if have I at WT  Now sag. It's too expensive to use the best gradient. So what you do is you use storage.  To actually  Approximate the Batch gradient. So you would have one over n summation over i have the I  NZ is basically stored in memory and  You have that  Vi is basically the gradient of f of i at some previous w. So it's an old W parameter  Okay. And at each iteration t you will only update one memory location.  You update  Only  The it and you will set it to the gradient of it at the current it to it.  And so you keep every Thracians you update one of these member location. So after enough updates.  If you're having moves too much, you basically have recruited all the gradient. And so what you have, as the sum of the Vi is an approximation of the batch screen, but you never have to  Go over all the training set at in one iteration. And it turns out that this method actually converge, like the red line, I told you so. With a big constant step size of conversion in early so it's kind of magical  And I will mention just one less algorithm. And then I will shut up. So there was a problem with sag. Is it took to prove the convergence of this result.  So this was in 2012 that was proposed.  And by the way, this one the like the this paper one the Lagrange optimization price, which was for one of the biggest contribution in the last three years in comics exposition.  And this paper was by Nicola know who Mark Smith and horses back so he could Allah, who is actually a researcher at Google Brain in Montreal.  And then the problem though is it to prove the convergence of this algorithm. It took them like 20 pages of super complicated proof with numerically found constants that was really hard to  To  To  Prove the convergence, which was a quite technical feet. And the reason is because in expectation direction you move into is not the true gradient. It's called a bias method and so Sega was proposed by an intern, which was visiting when I was at at India, and this was in 2014  And you just make a very small change. Basically, you will have this the update will be  Grad.  Of fit at WT like as Judy and then you have the memory part so it will be one over n summation over a j of the J.  Minus the new computed  The it  And  This part is what we call the variants reduction correction. So that's what we do is the variance of the update  And so  And now if I think the expectation of that respect to it. I get that.  This would cancel that. And so then I will just get this expectation here, which is the true batch gradient. So then it's an unbiased method. So this was just a small tweak inside to make you can bias.  The result was that, then we could prove in a few lines the convergence of this method.  And then you can show a lot of extension. So you can this is for unconstrained optimization. You can then do constrained optimization. You can do many batch and on in from sampling. You can do a lot of different things so sad guy is actually  quite popular. It's actually my my most cited paper, by the way. So I was a coarser with. So the first author with Aaron defense your horses back was again in this paper and I was the  Last after. And the reason I mentioned that, by the way, is that if you want to do large scale logistic regression. This is the way to do it. So this is the default  Method.  In psychic learn for logistic regression  In the machine learning package psychic learn which is very popular. If you might know it, by the way, one of the main author of psychic learn five young kid goes is also a researcher at Google Brain. So we have a lot of these  Somebody is asking, Is it possible to clarify the practices in the Sega expression.  So this is grad of fit at WT  plus summation over my memory minus V it right, which basically, this would just be a grad of fit at WT user avatar   That's what this means. user avatar   Okay, so, yeah. So, so just so be aware that these these variants reduction reduce method and then somebody asked a very good question. It's like, oh, well,  It's so Dora asked if sag is always better than his side always better than as Judy. If yes, why do people use as Judy still  So, depends what you mean by people. Right. So which people are we talking about. So if you do convicts immunization.  Then Saiga, for example, in the case of logistic regression will destroy your Sgt method. Okay, so, so it's just like there's there's no comparison in terms of getting high accuracy fast.  If you do non convicts unionization like you're doing neural network, then things are very different. Right. So that's where there's there's both the issue that  When you do non convicts going faster to a local men is nothing to see better  And also when you do deep, deep learning optimization, you actually don't care about the position air with you cares about germination area. So that's also a different thing. And it's GD might have better  diarization or emphasis realization performance and violence reduction.  And so, so yeah, so actually this is active area of research I my research team has a lot of papers on this topic and, in particular, I will highlight if you're curious.  So for it turns out that variance reduction method for deep neural network training is doesn't give any well. People have tried them. You don't really help.  But when you go to stuff like adversarial training like again Jerry of every single network where instead of doing a musician, you have a min max. It turns out that actually then you can helps a lot. So I have a paper on this.  Last year where we use violence reduction method to solve games and it makes a huge difference there.  And start that is making interesting literature point, which is that is the violence reduction corrections, similar to the use of control Barrett's for rents reduction. Yes, exactly. It's, it's all in the same kind of spirit idea.  Okay, so that was for the Crash Course of on large scale optimization. So I didn't go in that many details but I give you a few keywords.  So hopefully it will give you a nice introduction. So next class I will now talk about  Here we had conditional model for for classification logistic regression. Well, no, go to a generative model for justification, so call Fisher, they know this analysis. And that's something you would compare with the disintegration India.  Okay, so see you on Friday. Feel free to ask question on Slack. If something wasn't up here. Bye bye.
  Okay, recording started and let me get the  Where  I lost the cameras. Okay.  So Hi everyone, welcome back to  The prostate graphical model chest so today as a follow up to the question that was just asked, we're actually going to cover general approach to classification. So we'll look at the Fisher.  linear discriminate analysis model.  And that would be a good excuse to see some math tricks.  And how to do  Emily.  For the multiverse Gaussian  It's your work already did it for a scale or one the ocean. So now we'll do it for a multivariate. Gotcha.  And  Yeah, so we need  Two volunteers for describing the lecture of today.  If you can let me know in the group chat.  So,  We have Abdel Rahman  And each man again. Alright, thanks.  Again, each of you please send me an email.  So I have your email address, reminding me that you just volunteered, and I'll send you the instructions shortly. Now that the Ikea deadline has passed. How have the time to do that.  So yeah, so before starting with Fisher to narrow this analysis. I just want to make a note about the assignment. Okay, so there's a it's  For homework, too.  Okay, so you're asked to implement Newton's method or the ether did return related least square  Approach for logistic regression, which is just running Newton's. You don't have to do least the, the, all the transformation. I gave you in math class. This was just to  Give interesting interpretation of Newton's method, but all I care about is that you implement Newton's methods for logistic regression. It's a new 10 if you remember, you need to to compute this update right so WT so Newton's  WT minus step size.  Well, I guess if you use a step size of one will forget about it. And then you have the SDN, and verse WT times grad of f of wt. Okay, so that's the official update  Now we start to get into numerical computation len when you implement numerical algorithms on the computer, you have to be careful how you do updates. So this is the kind of thing you would see if you think a numerical analysis class.  Where you learn, for example, about cancellation errors. So for example, if I if I say if I do like a minus b where  They are both like or extremely or a and b are very, very close to each other.  Then I get very little. For example, like let's say this, this was a, you know, six digits of accuracy.  But if a is roughly equal to be when you do that, the answer, you'll get will have only a few digits of accuracy, because all the other digits are cancelled and you're left with perhaps one digit of accuracy.  Which means that now if I use this quantity somewhere else in my computation. I only will have at the end one digit of accuracy not 16 digits of accuracy, like a float with that.  Okay, so, so that's what we call like cancellation error which is really bad. And so for example if I know that A and B are almost exactly the same, or perhaps even equal I could do  I could change the computation to to avoid that. So a good example is I guess now. I'm also a bit more sidetracked. But, you know,  I was planning to talk about this, but I think it's still, you know, helpful background information for you to be aware of, let's say we talk about the quadratic  Solution formula, right. So I want to do x plus b x squared plus b x plus c equals zero. I want to solve that the roots are minus b plus or minus square root B squared minus for a seat divided by three. Okay, so these are the standard quadratic rules there. There's two of them.  And what happened here is if  Let me see if I remember this correctly. So if be is huge.  Let's say be is much bigger than for AC. Okay. Suppose suppose that be  Square is much bigger than for AC as a as just a number. Okay, so the square with a B squared minus four ac is basically almost equal to, to be because it's dominated by the bigger term right  And so then I will have minus b plus b. So I have b minus b. So, I get this cancellation. Right. And so the problem is that then  If I just compute in a naive way minus b plus square root b squared, blah, blah, blah. This yields cancellation error and it's not an numerically stable way to compute this route.  And so instead, what you can do is you can instead you can multiply both the numerator and denominator by the same thing.  This is kind of completing the square for the square root. So you have that this thing is the same thing as minus b plus square root, blah, blah. Divide by two eight. And what I do is I multiplied by the, the other thing which is minus b.  minus square root, blah, blah, divided by minus b minus square root, blah, blah. So I just want to play by one, right. So from an exact arithmetic perspective. I didn't do anything.  The beautiful thing now is because I have something minus something times something plus something I get these the square term, right. So this is the same thing as be square  A minus the square root square. OK, so now I'll just get the square root. So I get b squared minus four ac THAT'S THE SQUARE ROOT square divided by  Two eight times minus b minus square root  llama.  Okay, why did I do that well because now this term here which was problematic because it was a huge number. It was the meeting all the numerical computation cancels out. So the be square and the b squared cancels out. So disappear. So what I'm left with is for a see the minus cancels out.  Divided by two. A times there's a minus. And then there's B plus square root, blah, blah.  And so now the eight cancels out the two cancels out. So what I'm left is to minus to see divided by b plus square root B squared minus four ac ok  OK, so this is just another equivalent expression to so perhaps I'll put the, the original one. So the plus the Plus version.  Let's look at this one. So, this this thing, this is equivalent to this thing.  I it's the same quantity, but it's just it written in a different way. And what happens now is that if I want to compute  This thing in a computer and be as big. The nice thing now is I don't have this guest catastrophic cancellation. Right, so I will have  16 digits of accuracy for see this is a big number. There's no cancellation here, this, this, this denominator will be roughly basically almost to be  When he is huge, right. So basically I just get minus c divide by D. That's basically one of the route when he is huge.  And so that's so if you if you implement in your code, this expression. It's much more stable. Okay.  So when you do just mass on  On pen and paper. You don't care. You can write this or you can write this. There's no difference when you implement in the computer. There's a huge difference between those two. So you have to kind of  Start to think about these things when you do numerical computation on a on a on a computer. Okay.  The same thing like you never want to test number like is a equals to be when a and b or like floats and like real numbers because I could have  A identical a mathematical expression on both sides. So for example, I could have computed  This thing and I copied these things. They're supposed to be officially equal but if I do it on the computer. Do you want equal at all. So then what you want is you want to check if you really want to check in a mathematical numerical algorithm that A equals B, you will look at so instead  Of  Where they want to go. user avatar   This is different stuff now. user avatar   Instead of checking say x equals y  You will do something like x minus y smaller to some tolerance, right, which means that, let's say they are within  You basically say all these number are the same, right from numerical precision. So that's a much more stable way to check whether two numbers or quote the same. Okay.  All right, so, so this is just kind of like to keep in mind the importance of numerical analysis. And so now, what's their link with Newton's method. Well, the new cars method, is that okay like suppose he is in vertical  You could just compute the inverse of the matrix then multiply it with the vector, but that's not a numerically stable way to compute this direction.  This is really bad way. So I already mentioned in the past. The problem that if he is ill condition. So, if the its largest eigenvalue over its smallest I get a value in in absolute value is huge.  Computing the inverse will be not stable, meaning that small numerical accuracy in just the entries of my matrix will you will give huge difference of solution, which means that you know I could  Change by tend to my 16 and numbering my matrix and the direction that will follow will change by 10 to 15, for example, or something, right. So that's really bad. Okay, so that's why we might even say things are not stable.  And so, and some you would you should almost never normally computers, the full matrix and verse. Usually, so instead  What you can do well. There's also a question of how the  Numerical  Routine that you're using in your software. How do you compute an inverse. Right. So there's actually a lot of different algorithms to compete in verse, but the the the numerically stable way to deal with that and also the kind of the pro way is to instead. So let's call this  DT, DT is a direction you want to move into. So instead, what you want is sold for at h dt equals grab of if WT right  Right. So, for  So we want to  Solve for the vector DT in a system of linear equations. So that's already  A better way to think about computing the inverse time a vector.  In particular, you know, it could be that if he is structured and this vector is structured that solving this is actually much more efficient than competing the inverse of, ah,  OK, so that's already like terms of efficiency. It's usually much more computationally efficient. And then, moreover,  In terms of stability of solution, if suppose he is actually not in vertical or it's almost nine vertical. Well, then there's many direction, which are closely.  Solving this system of equations and and and then you want to find kind of a stable version of this direction. And so the actual thing you want to do is you want to find  Minimize over d the norm of h t d minus grad of WT right  So that's how you want to find the direction. So you find a direction.  Which either make this equal to zero when you can do it or you know you can have small, you might have some small miracle air. It's not the end of the world. Like, you don't have to have everything exact because it's fine. It's part of numerical computation.  But you want this to to solve that. So this is a lease square problem, which is why, like the function in Mumbai to actually do that. And that's what you should use in your assignment is from the linear algebra library. It's called the square. So, LSD square, so use  This function. Okay, which basically solve this problem. And this function. What happens is, suppose that he is full rank and very well condition and everything. And it's a square matrix.  It will actually do standard like new like perhaps like row each then form manipulation on your matrix to compute the direction right but if he has no condition, it will actually use a different technique which is more numerically stable to find that direction.  Is there any question about this.  So basically the, the high level summary here is, first of all, be aware that when you do  numerical computation, you have to think a bit about what you're doing, are you dividing by our number which is close to zero. Are you doing catastrophic cancellation. Are you trying to inverse, something which, like a matrix which is  Perhaps a condition or perhaps you don't even need to embrace the matrix because all you care is multiplying this inverse by a vector. So all you curious to find a vector not whole matrix.  Because our matrix is. Oh, there's the square was the vector is only older deeds. It's much more efficient, even from space perspective to not have to  Look at to store somewhere. The inverse of the matrix before taking the product of this inverse matrix with the direction the gradient  And for the assignment. Just use the square as the library. Okay, so that's an important note that I didn't have time to talk about early year. So let's do fisher linear discriminate analysis.  So now.  We will look at a generative model for classification. So logistic regression. It was a conditional model. I didn't care about how x was generated, we only looked at the conditional. Now we will say, Okay, let's make also modeling assumption about X and one of the simplest is called Fisher.  linear discriminate analysis.  It's often also just call LTA  But there is also one of the most cited paper admission learning, which is topic model paper by David Lyon, all in general art doesn't three. I think it doesn't three which is called Layton there a shell location.  And it's also LD. So that's why I like to call to put the Fisher, just to distinguish it from the latest location which is super popular and well known a mission or any statistics or no very well. The Nordstrom analysis, but in machine learning, we have. And so that's why I'll use FL D.  As the abbreviation instead of the W, even though  People in statistics would also usually  Okay, so for classification. It's get again it's. We call this the setup we will have y which is a binary variable.  01 and x will be in RD. Okay.  And so because we have a generative approach.  By the way, don't you ask a question about the same solution which one, which  Which problem are you asking about. Are you talking about the SDN one or the quadratic user avatar   Equation problem. user avatar   Alright, so we have a general approach.  Okay, so she's okay so guys asking if  Using the least square approach gives us the same solution as this one.  If he is in vertical  Then indeed, this has a unique solution which is this one, and it gives the same solution right so analytically. Indeed, the give executive same solution.  If he is not full rank well this is not well defined because it's an inverse of a matrix which is not a convertible, whereas this is still well defined.  So that's kind of nicer and actually the Newton direction is still fine, even if he is nine verbal still makes sense to look at these direction. user avatar   So, yeah. user avatar   OK, so back to the  Yes. So we have a general approach. What does it mean, it means that or  Joint on x, y will be penetrated by some Member theta.  And  I will model this both as the class conditional X given why  And I will also have the now the priority over the labels. Right. So instead of in the logistic model. So this is in contrast to the conditional approach.  Like we did in logistic regression where all we model was P of why given x  Right, so here were actually modeling also the quality of the observation. And let me just check my phone if there's an emergency.  No problem. So my actually had my Doug  Had a surgery. Two days ago because it's a puppy, he had to be neutered. And so I need to check whether he's not starting to beat or something. So that's why, that's why I'm, I'm looking at my phone. Sorry about that.  Okay, so this is basically the class conditional  That I mentioned before, and  And then I I gave the example in two lectures ago of just assuming class can this show and the explanation of family. And I said, oh, if you do that, you get that the conditional of why given x when you use a rule.  Is just a there's a sick regression kind of model. So now what we do is we actually look at one of the specific  Look at the specific, concrete example of that with them. It's a very good question. And then we got a we got a  Interesting question from the audience. If we can see my puppy. Unfortunately, the problem is that the puppy barks, and he's really annoying. So when I'm teaching. It's not super nice. So  I'll show you pictures of the puppy instead of the real puppy.  Rather, I'll put I'll put the background image of the puppy. He's very cute. He's a  Miniature poodle and his name is Teddy, like a teddy bear, because he looks like a teddy bear.  Alright, so  Back to the fisher model. So let's look at the class conditional model that we use for fishers, so the for the fisher model.  What we do is we will assume  That the class conditional p of x given why theta is actually  Murdered normal. So it's a gash in an ex with mean which depends on why so for different class we have different means. But we use a shared  A fixed governance which is shared across the two classes. This is shared.  Across classes. That's why I'm not indexing the parameter for the covariance. By why but the mean the varying for the class.  And so basically  If we have data in 2D.  You could think of. I have say my class one.  Somewhere, and I have my class zero somewhere else. And we're supposing that the shape the coverage shape of the garden is the same.  So, so like these ellipses, or basically oriented. The same, the difference though is that I have here the mean one. Whoops.  Mean one and here I have  I'm so puzzled by my flickering  Screen. Good thing I don't have it be lipsey so  That means zero  And so the parameters for this journey model will be both the mean for class one that mean for class.  Sorry to mean for class zero demean for class one, the shared covariance and the prior over class one. So, right. So this is mean of class zero. This is a shared covariance and this is the probability of y equals one, right. So, this is  This is for this piece here. Okay, so these are the parameters of our general model which give us to join on x and y.  And now as I did in two lectures ago you can then show  Guess I'll see as before.  Could show  That the conditional of poi given x  And data.  For this germ all is actually a sigmoid of W transpose and in this case, five x is just linear  where w is a function  Of us zero Yuan shared governance and pie.  And that's basically like, you know, just because the mitzvah Gaston on the excellent chef Emily and  This is basically five x four.  Yeah so. And it turns out that when you have so you have here the class conditional. We're in the word. Our next month. Chef me. And because we're sharing the covariance. The quadratic part of the of the of the  probabilistic model of a Gaussian cancels out and I'm only F with a linear part. That's why I only have X here. Okay. And in the assignment. I think you will also derive  Their conditional  For the FL the model or actually, it's not the FLT model, but for this model, if you use different class governance. So if you use sigma as zero and sigma one  Then you don't get cancellation for the quadratic term for the decision boundary. So you get something which is called quadratic  Discriminate analysis.  Basically Q da instead of LD LDS linear, quadratic will have an x square  And so in this case, what you get is sigma W transpose five x where five x is quadratic user avatar   Function of x. user avatar   Okay and so see homework. user avatar   That's what you'll do it can be user avatar   Okay, there's a few question do we set the covariance to be as our tropic. No, you don't have to  And then the shank same currents for both testers is good assumption for your problems.  And you will see. I mean, for real problems. Okay, so the first thing is  There's a few things right and that's what you'll explore in the assignment and the assignment.  You'll have data which are generated according to credit discriminate analysis model, you'll have data which is generated. According to a linear model and you see how these different models behave. The problem is when you have  A full covariance matrix instead of a diagonal conference matrix you have more parameters to estimate. Then if you have also different current matrices for both the, the two different class you have even more parameters to estimate  And so  If your model is  Not generated from a real from Gaussian  You might have the Q da, which is what you get from the approach we do will do worse than LTA just because also like you're trying to estimate more parameters and you have the wrong prognostic assumption. Right, so  So,  In any way like if you really want to do castigation I don't recommend to do these approach. I recommend to do logistic regression, which in this case doesn't care about shared or non shared covariance matrix.  But let's see what's the general approach right to kind of distinguish it from the conditional approach. So in the general approach.  We estimate or parameters by maximum joint like like you. So it's not conditional it's we do joint maximum likelihood estimate  To  estimate the parameters.  So feta hat.  Is the arg max over theta in a parameter set of basically the the livelihood. Right. So, summation of i love of p of x i, why I say that. And so, note that in  It versus  Summation over i love of p of why I given xi theta.  For which is what you use for logistic regression, for example.  So even though the decision boundary for Fisher and Nina. This analysis is of the same shape as they'll just take aggression. Right.  Like like this thing is just a linear function of x. So the decision boundary and even the probabilistic model P of why given x is the same kind of model for both logistic regression, which gives this and  fisher linear this Manassas, so they give the same kind of like why given x  But the way you estimate the parameters are different in logistic regression you estimate W directly. You don't care that double you could be computed from all these other variables.  Was efficient and Linda smile says you do just maximum likelihood, which means you estimate the mean estimated covariance. You say the price.  So since you also have more information at the end because if it's a if this was a good model for data at the end you also know how to model X. You can also answer queries about  Oh, if I have a class, what should be the property of x which is different classifying right so so so the direct model approach gives you insert two more questions. But it makes more assumptions.  Alright, so now let's compute  The Emily formal to read gossip.  And  So we need to now do a little foray in  Math tricks, because we'll have to compute derivative respect to gradient. So let's do Emily for a multivariate Gaussian  Which is needed in this model.  Alright, so now I will simplify it. I forget about classes. I'll just think about X. Suppose I have x i, which is generated ID with mean  With a gash in with me new and sick covariance Sigma Nu is in Rd and the covariance is a DVD. So because it's a covariance sigma is symmetric.  And to have a density for multi read. Gosh, and I need that sigma is strictly positive difference. Otherwise it's  General, it's always PSD because it's a covariance, but to get a density, I needed to be  strictly positive different  And and just why is it symmetric right to remain like sigma by definition for any variables, not just for the Gaussian is x minus Mew.  X minus new transpose. So, this is for the covariance matrix for a vector are random variable, right. And so now, if I take. So this is sigma. So now if I take sigma transpose. I just get  The transpose of this by linearity, which gives the same solution give the same thing.  Okay, so what's the density for a Gaussian  It'll reminder. So I have the normalization factor which is square root two pi race to the D. And then I have the determinant of the covariance  Then I have exp  N minus one half.  Then of x minus mew transpose sigma inverse x minus p  So if it was one d i would have x minus you square divided by the inverse of the currents which is sigma squared. So it's divided by sigma square. So it's two sigma square, but now in the matrix form, it's I just have the matrix reverse  And it will be convenient and you'll see why soon to just manipulate this  This term here in the exponent so I can rewrite this as the trace of x minus you transpose sigma inverse x minus mute. Okay.  Why well because  This thing.  This thing here is a scanner, where the Scanner Scanner is a one by one matrix.  The trace is the sum of the bag and alternative a matrix. So the sum of A one by one matrix is just the one in one by one entry right so when you have a scanner, you can always add the trace and it doesn't change anything.  Okay, well, so why the point of doing that right so that's always like these math tricks is always like you multiply by one, you add zero or you take trace of something which doesn't change anything.  Okay, so somebody is asking me to go slower.  Sure, I'll try to not go too fast. user avatar   But please keep user avatar   Putting  You can, there's a there's a little blue thing. You can also put in the like go slower. Oh, it's, it's great, actually it's a blue in the buttons in the participant list.  Alright, so  So we have that the trace  So why did I do this trace. Okay, why did I do that well because there's the sacrament property of the trace, which is trace a b is equal to trace be a user avatar   Right. user avatar   EBS not equal to be in general because matrix don't have to commute. But in terms of when we look at the trace, it's actually, it's working. So as long as they  The dimension of these matrices are compatible to switch like that then you can do that. Okay, so that means that will do is I will move this this matrix here to the right. So I have that this is equal to trace of sigma inverse and then x minus New X minus new transpose  By the circuit and property of the truth.  So,  Now, why do we care well. Now the nice thing is because here actually. And by the way, now we've changed the dimension. Right. So this was a one by one matrix was this is now a d by d matrix.  Right. It's the product of two matrices. So it's kind of magic that the  This quantity is the same as just the trace of this quantity  But it turns. But the reason I write it this way is because it turns out that the trace of two matrices is a next is a way to express dot product between matrices. Right, so I can  Say this is by definition the inner product between the matrix sigma inverse and the matrix which says rank one x minus New X minus new transports user avatar   Yeah. user avatar   So more specifically, we have that the duck product between two matrices of the same size. You can, by definition, this is just the standard  Some over the product of pairwise entries like like the word vector, this would be pretty obvious. So this would be a big times big  But it turns out that this sum is the same thing as the trace of A transpose be  So that's why using trace of A transpose. Be is a way to express linear product between matrices. So it's kind of, it's, it's a way to extend the the inner product vector space structure from Victor's to also matrices.  In particular, because it's that that highlight that the this trace operation is linear and each of his argument, because in our product is linear in each of its arguments.  And no tear that ideally I need to have transpose, but because the covariance is symmetric. It doesn't matter. Right. So that's why I didn't put the transpose here.  Okay, so let's go back to the log likelihood for that. So our parameter in this case will be new and covariance  And notice that again terms of notation. I'm a bit doing event abuse of notation, because I'm calculating a vector with a matrix.  So you can think of this as just like using a bit of a computer science notation, where this is a couple and each element of the puzzle could be any type of object. Right. I could put a I could put a I could cut in a different one. In a couple  Function vector and a matrix like  What kind of like a mix of CS and mathematician.  But if you really want to, in fact rise the whole thing. You could just vector eyes, your matrix and just think of it as a very long vector  Okay, so now let's go back to the lab, like you're given this trick. So we can rewrite the log like you  I will have  Summation over i.  Have the log of the probability of X I, if I have any observations from aggression.  And it depends on the data, of course, because I want to estimate the parameters. And so this will be just a constant, which doesn't depend on data.  They will be  So now I just think the log  Log of this term. So, this will give me  There will be a log of the determinant for each data point. So I get n  And so I get minus n divided by two lug of the determinant  So the square root becomes a one half.  And then  The log will hit the x and so  The x is cancelled out and I'm just left with the argument of the exp. And that's where I use the linearity.  Of the trace operation. Right, so I will have this some and I can move the some inside a trace because it's linear. And so I'm left with minus n divided by two.  Summation inner product. Whoops, I'm not  Doing so minus and divided by two inner product of the inverse of the matrix, the DAP that's the trace part right inverse of the covariance and then I have one over n summation of our I  Have x minus New X minus new principles.  So here I did a few many patients quickly as you had a hard time to fill that don't worry, you can just go back through the equation just work it through yourself to convince yourself shouldn't be too hard.  The main idea here is I use the  So you use  Linearity  Of that product to move the summation inside right so i would have, I would have summation over i. And then I have here and then I can just move this inside  And so that's why I have this inner product between this kind of two matrices and. Now the nice thing is I can give a name to this thing, because this is, this doesn't depend on my parameter sigma depends on new but not the parameters sigma. The yeah and so I'll call this  The kind of like empirical covariance, which depends on view. So it will be a function. So I'll just call it  And we'll see when we do the maximum likelihood respect to sigma why this is important.  Okay, so that's the function, we want to maximize and take derivative of okay so importantly we have this luck determinant  So we have basically this this linear. So, so if I if I want to think about  The how to deal with the covariance. I have two terms, I have a linear term in terms of the inverse covariance and I have the luck determinant part  OK, so now I need to know how to take derivative of the log determine function. So to get there, we need to review a bit of metrics calculus and then somebody has a question.  Is our prior still part of theta, nothing this case because now I am just reviewing the maximum likelihood for a multiverse Gaussian. There's no why so we don't care about why now, so we'll  When we go back to the full FL, the model will will reintroduce the parameter for the prior but right now we're just focusing have  If I have multiple gushing observation. What's the Emily parameter from you and sigma and new is basically the empirical mean and sigma is numerical covariance. But how do we prove that that's what we're going to do.  Alright, so just before the break so that you can think about it over the break. Let's do a vector derivative review.  Vector derivative  The relative review.  So,  Suppose I have a function  From our M to our end. Okay, so, so my vector. It's affect my function as vector inputs and also vector outputs, perhaps, of different dimension.  What does it mean to take the derivative of this thing, right. So in vector calculus, you'll see that f is differentiable  At  A point. It's a zero.  If and only if  There exists a  linear operator.  I will denoted by the differential of f at x zero  So this linear operator. What is it, it's a it's a linear function which takes as input the same dimension as my input space.  And the same dimension as my output space. So it has the same signature as my  Vector function. user avatar   Okay. user avatar   And what does this in your operator has as a property. Well, it's a good linear approximation of my function I Ron Rex is zero. So basically, for all perturbation delta  Which are victor in Rm because that's my input. I have that f of x zero plus delta. So, if I make a change in zero in the direction delta  And I looked at the difference between f at this value and f x zero, this is actually equal to the linear operator. I guess I'll put it in purple to just to make it more obvious. So the F F zero evaluated in the direction delta  Plus a little all of the norm of that. So basically we have that if the function variation is equal to its linear approximation with the differential plus something which grows  Which grow slower than linear around zero.  So just tiny basically  And so this  The F is what is called the differential of it right  And so if I'm able to find a linear operator which satisfy this approximation, actually this interpreter will be called a differential  Yeah, it is small or big or  Small all basically small oh  Or little or guess  Is it's called Little own that small  Little all  Basically means  That  That it's a function  It's quite H of norm of delta such that the limit as norm of delta goes to zero.  Of H normal delta divided by norm of data goes to zero.  So big all means that the ratio between the function you say is the ego and the argument of the bigger than this ratio is bonded and when you use little you say that this ratio goes to zero.  Okay, so that's the end. So here  We have this is the thing which tells me how how fast something grows  And this would be a remainder term which could be very, very complicated, but all we know is that if we call this remainder term ah we have that he goes to zero faster than linear in delta. So that goes to zero faster because it could be a quadratic in normal delta or it could be  Normal delta to the three half that's also goals faster than linear  Okay.  So that's what you would have as a formal definition of the linear prayer and vector calculus and somebody that's the always differential unique  Good question.  Well, under regular conditions. Yes.  Yeah, I think, I think if there exist in your partner, which has this property. I forgot from my property from my vector calculus class, whether you can prove that it is actually a unique  So I'm not sure if it exists, it means that it is unique, but for most functions that we care about.  When it exists in the neighborhood. For example, I think you can say that  But we won't go to this level of technical details of derivative. So for example, the differential of a  You know Lifshitz function is is is easy. It will exist. user avatar   No, that's honestly true user avatar   Yeah, so actually you need you need some nice properties for something to be differentiable  Which I think I give below.  Okay, but the first part is just let's let's unwrap a bit this notation. So the f of zero is linear. So what do we mean by early near a parameter. Well, it means that  If I do the f of x zero of delta one plus c scanner times that the two. Well, this is just the same thing as the F zero dot that one plus b.  The F zero delta. Okay, so that's what we mean by linear and when delta is a vector we can represent linear operator on a vector space using matrices. Right, so you can represent these linear operator.  As a n by n matrix.  Which is called the Jacobean  The Jacoby matrix.  And the standard representation in the standard basis of this matrix.  Will be  That  So this is a matrix and it's i j entry will be the partial derivative of f.  I so the ice component of my vector output function. This is the ice.  Component.  Of f  Where's the spec to the eighth  Entry of x. user avatar   Okay. user avatar   And so then in this case.  When I have this representation, then d, e, f of x zero evaluated indirection delta is the same thing as just taking the f of x zero as a matrix.  And then multiplying by delta as of the vector right so it has a right dimension so that that isn't dimension RM matrix is n by n. So the output of that will have dimension and so it matches the dimension of the output.  Okay, so let me just give you two more ideas and then we'll take a break and then you can ask the question, What's the mission of delta delta is in Rm right it does the same dimension as the input of the function f.  Alright, so the first thing is  This gives a way  To get the difference, the derivative or the Jacobean or the differential  For basically anything  Okay and so  Here we can also right now. I'm talking about vector in Rm but I could just think instead of thinking of vector and arm I could think of matrices as vector. And I did. I told you how to take inner product of matrices. So we can also do matrix.  But we could also have other type of input, like we could have a tensor. Or we could have even as a function as an input, an infinite dimensional function.  And so now I said, Oh, well, the input is in Rm so instead of thinking of the input as in Rm when the input could be a space of function. This is a infinite dimensional function. And as long as as a norm vector space on these objects. I can define this  linear approximation right so so so linear operator on on infinite dimensional spaces will be fine. And the this little barrier is also defined, even if my my direction here would be an infant dimensional object.  Okay. So Lisa is asking  Why is this equal to that. Okay. So, linear operator is an operator, which I said takes as input.  Vector in Rm and gives us output vector in our end right so this could be an arbitrarily function in general.  And so it so oh I didn't do the user avatar   Invisible think user avatar   Can so  Yeah okay so and so this is just saying, Okay, well my operator, I can give it as input delta and gives me now a vector in our end  And this would be a value notation, even if this was a nonlinear function. It could be like a sign or whatever.  But because it's a linear period or I can also represent it as a matrix and then evaluating any new operator is just the same thing as taking the matrix representation and do matrix multiplication with the input vector. So that's what this equality mean  Umar is asking that the definition depends on the norm. You use well in finite dimensional space all the norms are equivalent. So it won't change anything, meaning  What it means is in five dimensional space all the norms are within a consonant each other and little organization or Big O notation doesn't care about constant. So, this in five dimensional space.  Differential doesn't depend on the norm in in three dimensional space, like functions based yes you could be differentiable with to some norm and not with respect to the other know so it becomes a bit more complicated.  And in some sense, by the way, you can always think of a  Function like a very long infinite dimensional vector. So a lot of things which just work for finding vector space can be carried out in infinite dimensional vector space or with function, though, you have to be careful with the regular with the conditions.  And okay so that's why I talked about this differential is because now we can talk about differential of weird object like also matrices.  And the other thing and we'll go. We'll go through that after the break. But the other thing that you have to keep in mind is just be careful with the dimension.  Okay, so that's there's different convention and you need to remember what is what. Right. And so for example if f is a vector function function which take vector to Skylar  Then the differential or the Jacoby matrix is actually a row vector  Right, because we said it was em bi n and n is one here. So, this is  Actually no, it was n by m. Yes. So I said here that the dimension was n by n. So the number of rows is the same as the output of  F. So this is a  One by M matrix. So, it is a row  And that means was when we talk about the gradient, we, we talked about the gradient vector, actually. So, so that's why  The convention is that the differential or the Jacoby matrix is actually the gradient of f.  Zero.  In this case transports  And this is just a convention of the gradients, we decided was in a vector form the differential is actually as a nice dimension because why we care about definition of the differential is because of chain rule is very simple. In this case, so the chain rule. So if I have  A function.  From RM to our n and then I have another function from our end to our let's say q  Then the differential of the composition of G.  All F. I don't know how you say that in English that somebody knows how you say their little circle in English, French, is all  Let's say, gee, composed with f  X x zero is the same as taking the differential of G.  G at f of x zero and then composing that with the differential of f at x zero and  This is the same as just the  Matrix product, right. So this is the  Matrix.  Product.  Of chickens. So  If we use their matrix representation  Alright, so we have  G of f as a sentence or compose our composite. Okay.  God, oh my God, I went up.  Because that's basically what it means, right, this is, this means  That I'm looking at the function. Oops. Why is it not blue. It's a blue  G of f of excellent  Okay, so is there any burning question before we go for a break. You guys only or you all. Sorry, guys. You all need a break. I'm pretty sure  No burning question. Well, think about your questions and then come back to, to the break, we'll, we'll work through a simple example of this chain rule.  And then we'll do take the derivative of the log determine and function and that will be one of the coolest linear algebra results, you've ever seen in your life. I guarantee  The bones doing recording  Okay, so we're back to business. So there was a question by the newish  Easy. This person said, is it correct that in this multivariate gash an example the log likelihood calculation is still the same as well. We had earlier conditional. Probably the examples I mean the difference  Between Jerry of unconditional method is not still relevant in this example. Correct. Right now all I did for the military reaction is I, I just said.  I have observation here which are ID Gaussian. So when I take the log. Like you, I just get some of the log like you'd have the x ray when I did the conditional model. I also had independent  Why I given x is so across eyes. So when a condition on the excise things were also independent and instead of here, I just said p of why I given excited. So, you know, but you know it's similar idea.  Alright, so now a, let's do a concrete example of this chain rule.  And so you go back. So here's a concrete example. So for example, if I use as my function.  Of x i use x minus new  And I use as my function. Je  Je je je of x as x transpose x  Then if I take g compose with f  I get basically what we have in the Goshen case where it's x minus you transpose A, and then x minus B right now, if I look at the Jacobean of these individual function, I have the f of x 04 x minus new users, the constant. So I guess the I get the identity.  Node that here. This is a vector function. Right. It goes from Rd Rd  G of x takes Rd goes to a scheduler, right. So, now, d, e, f of x will actually have a role structure. And in this case we have DG at x zero would be  X transpose A plus A transpose. Remember I already did the gradient of this earlier, but now I because the differential is the transpose of the gradient  So it's now a role. That's why there's a transport and so now if I take the Chain Rule d of G compose with f at x zero, I said this is the same thing as taking the matrix product between the Jacobean at f of x zero and then  I guess I don't need to do the composer station, because these are matrices, then I'll just have the of f of x zero and so the F F F zero. This is just the identity. I said, and now this is evaluating  This thing but replacing x by f of x, right. So then I get basically  Oops, I get x minus view transpose times a plus a transports  So that's my derivative  So that's a simple way to to the chain rule.  And so now if we go back to the gash in likelihood  For the Gaussian  We had in the log life. He had we had minus one half summation over i.  X minus mew transpose sigma inverse x minus new  And so if I take the derivative of that respect them you I can use the linearity over the sun. So I will just get minus one half.  Summation over i have the gradient of these quadratic form. Now I'll take the transpose, if you think the greatest I think to transpose of that I think the transpose. And so what I'm left will be  True, because it's a symmetric matrix, the covariance to sigma inverse and then I just get x minus view.  Right. And I want this to be equal to zero. So I can solve for new that implies that new Emily is just the empirical me  There's like two or three steps to get there. You just push the thing on the right. And then there's n times new the divide by n, and you get that  Layer is there would ask, is there an example with the  The composition is not a multiplication, if you would go to infinite dimensional spaces, then  You know, it's better to think as composition, because  Thinking of infinite matrix is a bit tricky. So it's better to think of them as really just operator and then compose them.  Okay, so that's the so I already found my my maximum likelihood camera for the mean, which in this case as long as  As the inverse was a PSD so it because if there's zeros. Then it changed a bit something but we assume that this was  strictly positive definite. And so I can just multiply by sigma on both side, this cancels out and then I'm just left with this right so so new me lead doesn't depend on this case on the other grants and now we want to do the coherence part. And so that's where  You will be amazed by the wonders of linear algebra. So let's do now, the  The example to  We want to take the derivative  If it did,  Ah,  Yeah. So the trick is, like, why did we write a cushion companies in a product because here. For example, I didn't do it like I didn't keep it as a new product it will come when we go to the to the application.  Of the legitimate function unlimited. Gosh, but there's there's one step, I need to do first. First I need to compute the derivative of the legitimate function. And that's what I'll prove now and then apply it on the gushing  So a bit more patients again. So I want to compute the derivative of say f of a. Now, which is the lug determinant of A.  Okay, where I will assume  That A is symmetric.  So, for example, we'll use this with sigma and a is strictly positive different user avatar   Okay. user avatar   Alright, so now this is not a function from RM. This is function from a matrix, but you can just like reorganize your matrix as a vector, and then it will become a function of RM but we but it's nice to keep them as matrix. Okay. And so in this case I can represent  The derivative  Of a function  From matrix.  To a scanner.  Right here, this is the output is a scanner.  We can actually present that using a matrix.  Okay. So normally when you have a vector to a scholar, you would represent a derivative with a role vector right  But now, instead of like using the vector representation of matrices. I'll just keep them as a matrix. And I want to do is that this will be a linear function.  Over matrices. What's the linear function of matrices. It's linear function to scatter. It's the inner product  Right. So then what I really want to identify this derivative is that I want something of the form f of a plus delta x minus f of they were delta is a matrix direction is trace a transfer. And basically, that's where they would. It would be so we call it f prime  F prime of a  Transport transpose delta plus little all normal data. user avatar   Okay. user avatar   And here the trace, as we call is just a linear inner product, right. So this is the same thing as f prime have a and then dub, dub. Okay. So that's a way to represent my linear function with the signature of same thing as the input to scanners.  And so if I'm able to access to expand this difference and identify a little trace and all the rest or little of norm of data. I found my direct that's a way to find it there. So that's the kind of like the formal way to get these objects. Okay.  And so let's do the, the, one of the love that. So there's one way to do the love debt by expanding the debt with co factor expansion and blah, blah, blah. It's super complicated now, I'll give you a different pro way which I got from the lectures by hostess back  Which highlights a lot of neat properties of the algebra. So, at the same time you will just review a ton of cool properties of the algebra, which is why  I think, actually, this proof is so elegant. It's so elegant actually that ever bored at home in my kitchen.  And I was actually talking to, to my girlfriend at the time. Now she's my wife and I said, oh, there was this cool proof that I taught and she was like, oh, what does it  Actually wrote it on my board and it stayed there for a long time. The proof, if it's a few lines. And actually, I had a housewarming party at home.  And then there was a lot of scientists who came and then the question was like, oh, do you know this this thing and then he tried to go through the proof. And it was quite a lot of fun.  A lot of people from DeepMind. In particular, it was cute. Anyhow, so little anecdote. So let's do it. Let's start.  Oh, somebody said, Oh, why do they call that that metrics direction. So basically the meaning of the differential is I want to know how my function very when I moved in some direction, right. So, so delta is a direction, and in particular.  If we were just talking about simple vector scatter function. This thing will be the directional derivative in direction d  Is it would be the product between gradient of f and the Delta. Right. And so that's what I that's why I call this like a direction of movement because it's like the directional derivative. It's just now. I use a matrix as my perturbation of X zero, okay.  Alright, so let's start that will be fun. So I want to do lug debt of  A plus delta  Minus lug that of it, right. That's, I want to look at. I want to make it a linear extension of that around it.  So the first thing is because a is a strictly positive definite. So because A is strictly positive definite by the guest spectral serum. It has  It is in vertical and it has a unique square root  As a unique  Square root  Call it a one half. Okay, so now what I do is I just doing it all the factorization. Okay, so the first thing is the first term, I have the luxury of the determinant of now I would put a one half on one side.  Identity plus a minus one half delta  And then  A minus one half.  And then a one. Okay, so I haven't done anything I basically multiply by one. So I factor eyes. My a into a one half a one half and  I remove this a one half on both sides for the delta. So I need to  Basically re multiply by A minus one half.  Okay. So Jacob is asking delta is in Rm and all I'm saying is I  Could either stay in Rm by victimizing my matrix. So I take the their columns of my matrix. And I just put them one after the other. And that's a big long vector, and I could do everything in Rm M is the sum of the day, it's basically let's say men matrix is user avatar   A user avatar   Key by L then a number of entries is key times l, so I could just set N equals k times L. And that's, the dimension of my vector space for the matrix.  Okay, but it's annoying to have these victories version of the matrix because matrix are kind of you'd like to keep them as matrices. So I can all do everything by keeping things as a matrix.  All I need to is to define linear operator on my matrices. And that's what I've done here right to the trace with this trace here or this inner product I have any new operator and matrices. So then I can just keep things as matrix. user avatar   Okay. user avatar   Isn't that the Jacobean no delta is the direction in which you make your perturbation, that you will use to compute the Jacob and so f prime is a Jacobean f prime is a derivative and which is it's represented it's matrix representation is called a chicken.  Okay.  So,  So here I've done nothing apart just like factoring out a one half on both sides.  And and why do we do that is because now I have the product of three matrices. Right. And the determinant of product is the product of the determinants. Okay. And then the lug of a product is the sum of the lungs. Right. So, things were kind of cancel neatly.  And in particular, now I still need to subtract my look of determinant of A.  But now I said the this is the lug and then the determinant of a product is a product to the determinant. So this is the determinant of A. And actually, the determinant of a race to power is the same thing as the  The determinant of a race to this power. So this is a race to one half. Then I have determinant of i plus a minus one half delta A minus one half.  And then I also have the determinant of A. user avatar   One half user avatar   And the beautiful thing now.  That was the whole point of this is that now these are just killers, like the determinant or scanner. So, it computes so I can you know group these two together.  And I have a one and a half that just gave me the determinative eight so I gotta lug of determine have a which will cancel out this one so I can cancel these two together.  Okay.  And that was the whole point. So now I'm left with the lug debt of identity, plus a minus one half.  Delta a minus one F. Whoops.  And so now what do we use we news. So now we use that  The debt of a matrix is also the sum of its eigenvalues  Sorry, the product of. It's like the traces the some of the eigenvalues, the product, the determinant is the product of the eigenvalues of  These are  Values of the  And and so and then if I take the log of the product of eigenvalues, I get the sun, right. So this is the same thing as summation over i lug of I against our use of i plus  A minus one half.  Delta a minus one f  OK, now the beautiful thing is that the eigenvalue of the identity percent matrix is the same thing as one plus the eigenvalues of the second term, right. So I get. Okay, so here I use this  And then I have this become some log of one plus i get value of A minus one half.  Delta t minus one half.  Okay, so now we're in a much nicer term because I have a bunch of scale or stuff log of one. These are killer. The only matrix part is in the eigenvalue keys. Okay.  And so now what we do is we will do a terror expansion of the log function so log of one plus x is actually equal to x plus order of x square as long as x is smaller than one. So for small x small perturbation around one of love whimper sex. It's basically linear in X.  In first equality for a slug should be loved. That's right.  So, so the absolute value here is the determinant. So I have to determine in there.  You know, luck basically login, just to be clear lug that have a is the same thing as lug of determinant of A ready absolute value for matrices is that determined. Well, they have to add the bar notation is the determinant  Okay, so I do not return expansion of luck of one plus something and with again values. So, what I get is summation over i, the linear piece which is lambda i have a minus one half delta A minus one half.  And then I have plus order lambda i  A minus one half delta eight minus one f  Square. user avatar   Okay. user avatar   Now this was valid for X small enough here x is the eigenvalue of A minus one half delta A minus one half. There's also other property.  Is that the eigenvalue have a minus one half.  Well, I guess.  That's right, it again though you have a minus one half data. A minus one half.  This is basically big all of norm of delta. So, so a is a constant. So, basically this the scaling of the eigenvalue has to do with the how big delta. So, as when when delta  Goes to zero the eigenvalue will also go to zero. So it's a nice continuous function. And so that's why by choosing delta which is small enough, then I'm sure that the eigenvalues will be small in one and and so I can use it to the expansion.  And by what I've just explained. I know that this is order of data norm square. So it's little all of norm of data. user avatar   Okay. user avatar   So I have my little know a little oh of nominal delta. So I'm left with with this piece here.  How do I make this a linear function well then we use another property. So I said, the determinant is the product of the again values the trace is the sum of the eigenvalues. So we use that the fact that the trace of A is the same as summation over i have the eigenvalues  trace of A is the same as summation of i have the eigenvalue of D. Okay, so here I have  The  Summation of my eigenvalue second just replace it with a trace this becomes trace of A minus one half delta A minus one half.  Plus little or of norm of data.  So it's getting shape. So that's what I used to get this thing.  And now, finally, I use the circle and property of the trace to put the H together. So I'm back to trace of  A minus one half. Sorry. A minus one by bringing the A minus one after getters.  Fans delta  Plus little normal data.  And this is the same thing as the duck product between a minus one half and other right and then recall, by the way, that normally does. A transpose for the trace. But recall that a symmetric, so I don't care about a or a transpose  Okay, so that implies  That's what I wanted to get right. So to to identify the derivative. I need just to get this different to become a linear function of delta plus a little or of data and whatever I have in the linear part will be my derivative. And so we have that the derivative  Or the differential, I guess.  The differential, the derivative  Respect to the matrix of the lug determinant function.  As  It's been purple then. So the derivative  Of the lug that function.  Is just the inverse of the function of a isn't beautiful  Alright, so somebody asked a question here saying why I guess it's probably around here. They said, why isn't lambda I'm multiplying the  Identity as well. So first of all, lambda is not a multiplication. So this lambda AI is actually a function. It's the eigenvalue function. It's it tells you  It takes a matrix and it gives you the ice eigenvalue of this matrix. Okay. And so, and I'm saying is the the the eigenvalues of the matrix identity, plus some other matrix is actually the same thing as one plus the eigenvalues of the second part. Okay, so that's the user avatar   The what I've done here. user avatar   Any other question about this.  Crazy proof.  But there's so many different properties of knowledge abroad I debt trace log expansion circulate of trace  I, yeah, I think I saw somebody asking about a good reference for these ninja properties. This feels like a lot to process there.  Yeah, well, so okay there's, there's a few things. First of all, just be aware that that's a lot of algebra like going to be expecting to see to master all these topics in general. Like, it depends. Like, how much you work with these topics.  But  Good reference is basically like the cookbook calculus or something like The Matrix cookbook thing they have a lot of properties about matrices. And, you know, Japanese, and even derivatives. So I think I'll put this up on the website, if it's not already there.  What would be the derivative of lugged eight  A without the determinant. Well, first of all, now this is a, is this like like a, like that's a bit weird already like you need to talk about the matrix exponential. So this is your matrix, so it gets a bit tricky. Right, so  So I won't go there.  They're either the matrix exponential, though, is just a metric exponential. So that's the nice thing.  OK, so now let's conclude our  log likelihood of dementieva Yashin right so so now that you're you've got a bit more intuitions out to work with these quantities.  Back to the league likelihood of aggression.  So if I express it in important terms I get. So why do I have a plus.  It's go back. Where was my leg like huge blah, blah, blah, blah, blah. Oh.  Here we go.  Ah, OK. So the first thing is I will express everything in terms of inverse covariance, because it's convenient and you'll see why.  So I have here a linear term in the in the precision matrix which is the inverse of the conference. So instead of having a minus log determinant. I'll take a plus lug of the determinant of the inverse, right, because I I have that  I have that user avatar   This is true. user avatar   So, user avatar   Is the same thing as the determinant of the universe. And now when I think the login, get the minus one in front of it right and so  I can write the leg leg seared with a plus and over to lug of the determinant of the inverse matrix called the precision.  Minus and divided by two in our product between the precision matrix or the inverse of the currents and then there was this like  Sigma tilde of mew that they express last time, right, which doesn't depend on the covariance right and that's now why I care about this inner product formulation. Right, so it makes clear, and it is linear in the precision matrix. Okay.  So it turns out that this is actually this is actually a concave  Function.  Of the precision matrix.  So it's not concave in the covariance, but it is concave in the precision matrix and the decision matrix is actually the kind of Nicole parameter of the exponential family. So that's all the beautiful pieces which goes together.  Yeah so concave function of precision.  Which is sigma minus one.  Just as a side note. So if we think you find a zero gradient. We know it's a global max.  And you know that when I do maximum likelihood. I can choose whichever parameter ization I want to take the derivative. So instead of prioritizing by covariance, I can pass rates by coburn's inverse or the precision. And so I think the derivative with respect to the precision.  And Leo is asking is it supposed to be negative for the first term. No, because I normally I didn't have a minus one and I had a minus, right. So now I just took this minus one.  And basically take this minus, put it back here, then put it inside the determinant, because the determinant of an inverse matrix is the inverse of the determinant  Oh, and I didn't choose invisible Link Alright.  Abort. Okay, good.  Alright.  So,  So now if we take their respect to the precision matrix.  Take the derivative  With respect to  Sigma inverse which is just the precision.  Then this piece will give me an over to the demand of a love of a matrix is the inverse of this matrix. So, it will be sigma inverse. That's the matrix. And I think its inverse  Which is convenient because that's just sigma  And now.  This piece here is very trivial because it's linear in in sigma inverse. So when I think they're really if I just get the thing inside. So I get minus and over to sigma tilde new  Right. And I want this to be equal to zero.  And so, that implies that the maximum likelihood estimate for the covariance assembly this weird tilde function that I had defined before and where you use the Emily parameter from you, which by definition is one over n. Some issue I have  X minus view, Emily.  And then x minus new Emily transpose. So, this is basically the empirical covariance matrix.  Meaning that if you take the empirical expectation  Is that it's too to  Sorry, you make you take the expedition over the empirical distribution. That's what you get the empirical  Covariance matrix.  So somebody is asking me if there's a geometrical sin geometrical meaning to the derivative of the legitimate function I  You know, if you have a diagonal matrix, then you'll see pretty clearly like the determinant is a product of the diagonal just basically the volume.  And then you take the log  You know when you think that the the log of the when you take the derivative of a love you just get one over the thing. Normally, so here you get the inverse of the matrix which is a bit than one over but generalize to matrix that's strange geometrical but that's I think a bit the intuition.  Okay, so that's how you get the Emily of the  For one gulshan and now how would you do the fisher determined linear demand analysis. Well, what happens is you actually have to do it in your assignments, so I won't do it for you.  But it's fairly simple. So the idea is you will just write the joint look like you would. Let's go back to the journey of model.  Here we go. So that's our Fisher model. So we would have  In this case,  Both conditional class which will look like Russian and then the prior. So when I think the the log of this it will just become a son, right, I will have log of p of x for the  And then I will use the correct mean so it will basically group, the data set into pieces. So you'll have all that that that point, which as class one.  They will all have the will share all the same parameter for the me and you will have all the data points for class to which are all the parameter for the for for their own me  And so when you do the gradients respect to this thing to get what's the Emily, it will just separate right so you can estimate the mean of new zero and the mean you want separately.  West for sigma, then it wants separate because the share the same covariance, but it will be a very similar derivation that I've done for  A single. Gotcha. So instead of having to gotcha now. I had one and you can easily generalize this argument for when I have two different Goshen.  And you don't have to read derive the derivative of the law, get them and function, you can assume I already told you, so you can just reuse that to the in the assignment.  Okay.  Is there any last question.  There was, I have to run because I want to go to the Mila Agra Meetup.  Somebody is asking, what are the boundary parameters for the covariance matrix. So, basically, the current matrix as to be symmetric and  And so so theta, basically. So, so  Yeah, so actually I guess it's what I wrote earlier this year for the question. user avatar   Where is user avatar   There we go. So, so these are the condition on my covariance matrix to be a valid parameter for a gotcha. So it has to be a symmetric matrix and it has to be strictly positive different  So strictly positive infinity. So, this is the PSD code. So it's actually a comb and it's not a closed set because zero is excluded.  But what happens. The nice thing is when you do log likes you. Would you also have the effect that you have a barrier functions, the love that function actually act as a barrier function, which would push you away from  None in vertical matrices. And so you can still. Let's see, we do grant the sensitive. Here we have the analytical solution. But let's say you would run  Actually, no. You couldn't do your grandmother. But the point is, yes, here, you don't have to worry about the constraints. When you said degraded, the derivative zero because like by construction here or optimum point  Where is it the empirical occurrence matrix. So, this thing is symmetric by construction, and it is PSD and it is tricky. Positive defendant as long as there's two data points which are different, so it's fine. It works fine. Okay.  Cool. Alright, so on this. Now you have everything you need to do to do the assignment.  And next class will start to look at unsupervised learning. So now we did classification both conditional modeling and the full Jenner modeling and that's what you're exploring that assignment next class will start to look at unsupervised learning where we don't have this week so they  Have a nice weekend.
  Okay, so  So basically, so far what I've done in this class is  Give you  A bit of review or probability theory and all the lectures about like maximum likelihood Patrick models.  But bias variants decomposition statistical decision theory that was to give you the the theoretical tools to  Think about, okay, how do we evaluate a method and what are the basic principles to think about statistics. OK.  And now what we're starting is to go through concrete examples of prognostic approaches and statistic problems right  And so the first one we started with linear regression. So we had an error regression. So that's the very simplest setting where we have y which is a continuous variable and x is a country could be anything but it's a vector. And we had a prospect model to to model that both from  You with like this this discussion era assumption. Right now, if we do maximum likelihood. In this model, we got back to the standard a standard estimator for for estimating the linear relationship, which was the d squared estimator. Right.  And that was an excuse to talk a bit about also some numerical mathematics. Right, so I guess no. That was for logistic regression. So then we went for classification  Against the the big picture here is we have these simple to variable model where X is one variable, and why is another variable. So you can think of it as a graphical model with only two notes.  Later on in class will start to see more complicated graphical model and actually today when we start to talk about latent variable will already talk also a bit more about these these these these little, these multiple variable model.  But so step one was  Regression step two was classification and that's where we saw the logistic regression model where we have a simple why given X model.  And that was a good excuse to tell you, also a bit more about numerical techniques for optimization. Right. So we talked about the medicalization and Newton's method and interrupted really redid the square which is what you implemented in the assignment.  Then I talked about during this classification because I mentioned before, we had the conditional approach or we had the general approach, which also Model X, not only wide given x  And so that gives you an alternative way to estimate  Parameters in a in a model and decision boundary for classification and that's what you're exploring in the assignment. Right, so you can compare it to just the progression.  With sure this plan analysis with even like doing a linear regression for even though it's a classification problem. Okay, so, so basically  I think in terms of big picture you can see that in this fast. I'll give you a kind of a tool bag of different approaches on specific topics. So that's why  Like the unifying theme I mentioned in the past where how do your present probably distribution.  How do you learn them. How do you estimate them and then how do you compute quantities. How do you do in France they so there was a nice three teams.  And then I'm going to go over different applications or topic like classification regression or sequence like time series model and and I'll give example then of, okay, how do you  What kind of protein distribution, you will use for that, then how do you learn them and how do you do probably stick in France in them, right. So that's a bit the unifying theme, but  I think this class is more better seen as a tool bag a two. Yeah, a bag of tools, rather than a simple unifying  Story subsets. Okay, is does this help for the big picture, Amy. user avatar Remi Dion  Yes, exactly what I was hoping for. Thank you. user avatar   Great. Okay.  So, user avatar   So today. user avatar   We're going to do unsupervised learning. Right. So basically in the last few lectures. When we did  When we did  classification and regression. This was unsupervised. This was supervised learning. So we had labels why an unsupervised.  We had the input x. So now we're going to look at situation where we don't, we're not given labels we just have data. So this often call. Oh yes, sorry. Thanks for the reminder. Let me share the correct screen.  And share  Thank you for the reminder. Alright. So today we'll do unsupervised learning and, more specifically, we'll start with the came in the algorithms.  And we'll talk about the expectation music ization algorithm. So these are  Standard approaches to unsupervised learning. And this is a build up to get to the Gaussian Mixture Model that will see next picture so gotcha mixture model.  Will be basically one of the simplest unsupervised learning approach that exists which will already highlight a lot of the issues with these kind of approaches in particular, they're all non convex right so you have problems in this case that there's no  There's no like simple way to estimate your parameters like you will have that you get stuck to local in local minima and there is you need to deal with this non convexity and we'll, we'll see that in practice today.  Alright, so, so let's start with  A bit of background on supervised learning  So basically, as I said, is here, we have x without any labels.  And if you're familiar with like self supervised learning and  I don't know what other terminology exists nowadays.  But they're the same thing as unsupervised learning, but with a modern opposite. So a modern outlook has way more nuances into unsupervised learning.  Let's now just start at the most basic simplest situation where all I have is just  Data and there's not, I'm not trying to learn a specific relationship between one variable and the other. That's kind of the idea of unsupervised learning. And I just have data I'm trying to model it directly. And so, and in the case of a, let's say I had like  Two.  How could I say let's say I had to population. That's what I meant. So, and I could have observation which looks like that. So these are just this is pointing to these  But this could be to measurement. I made about individuals. And it turns out there's two groups of individuals, but I, I didn't. I don't know what the labels of the individual artist. I don't know. There's two groups. All I know is there's observations, but you can think of it. Oh well.  You could say, well, this is group one and this is group to right from just looking at it. But we're not getting that  And  The an example of of model for this kind of data is the Gaussian Mixture Model.  Which we already seen in the context of the fisher discriminate analysis model.  So the Gaussian user avatar   Mixture Model. user avatar   So we call this GMM  And this can be obtained.  From Fisher in our discussion analysis.  So what was it the Fisher, Linda. This man is this model, we said, Okay, well, why is coming from a mood to normal with parameter pie right and pie with some parameter in the policy simplex  And then we said that x given y which is equal to some class was a Gaussian with mean new Jay and in this case it was a shared covariance matrix, right.  So in the case where we had observed why that was the joint model. We could have a model and x and y.  So what happens in this model is we if we don't observe why we can marginalize the variable out right. So if I look at the marginal an x in this model. This is a summation over why of the joint on x and y.  which by definition of the conditional by the product rule is just p of x given y times poi so my class conditional times my prior classes.  And in this case, the quality of a specific class is given by pie. So this is summation over my classes I have classes, the quality of my class and then  The gushing probably t that I would see for x, given the mean of class Jay and share current metrics.  So the marginal distribution over x. What I don't observe why in this case would be what is called a Gaussian Mixture Model why it's a mixture is because I have multiple mix Goshen component with each have their own mean  And I'm taking a mixture of them, right, so, so, so these are basically the the mixture coefficient. So, this is called the gosh and external  And so, more generally,  We could also have  We could have  Different covariance for each classes.  If we wanted as a different model.  And so if I look for example at the data here.  I say, well, actually a Gaussian Mixture Model seems pretty good with two classes. Right. I could have a mean here, you know, new one. You too. And then some covariance around it. And that kind of miles data right so we could just do maximum likelihood in a gym and model to fit this data.  Okay. And the big picture here is, well, how do we do that maximum actually in a gym and model. And there's an algorithm called em.  Which is kind of a nice way to handle the non convex aspect of the maximum I could problem.  And K Means is a way to kind of get that without going to the plastic model, right. So, k means it's kind of like a hard version of the gym.  And so somebody is asking that what I just wrote is an extension of Fisher and leaner this Medicis with multiple classes. Correct.  So I told you about Fisher and linear analysis. In the case of binary classification, but there's as trivial extension within instead of having two classes you have classes. Each class has their own mean  And they all share the same current metrics and it gives you nothing. Just like aggression. It gives you like what is called basically it's also, it's mostly class logistic regression with. That's what you get in terms of a  decision boundary. So it's a simple extension.  Yeah, so I guess I'll mention here. So this is extension.  Of it fell D to multiple classes.  I mean, FL D. The standard FLT actually was also defined for multiple classes. So it's not really an extension, but the one I presented to us in the class lectures.  Okay, so before going into came k means and these  The details of gems. Let's talk a bit about graphical model.  So the graphical model.  For way we caught for for for this.  For the GM model, we can say it's a latent variable model.  A latent variable model.  Because why in this case is not observed which and so instead of calling it why because we use for why for labels.  When we have something which is not observed in or in the latent variable literature will use the so it's a break.  Not all papers will use z, but often sees us as the latent variable. So, what you have is the class membership of a point we were presented by the variable CI.  And it's not shaded meaning that we don't observe it, whereas the actual observation, we call it x i, we shade it because it is observed.  And we could say we observe and different individuals and they will all be independently identically distributed. And so what we use is the plate notation, right, this is  Some notation. So let's unwrap this notation. So the square here. This is called the plate notation.  And what it means is repetition.  OK, so the the graph structure that you have within the plate with an index is just copied multiple times, right. So, so this is equivalent  This notation is equivalent to saying, Oh, well I have Zed one x one.  And then I have Zed two  X two.  And then I repeat that up to  Zed n to x, right. And so the the  I'm  The thing I'm the end this is that I'm running over is indicated here at the bottom of the plate is from one up to it.  Okay. Is it possible to have a quick definition of a latent variable please a latent variable is a variable in a graphical model that you want. Observe. It's. That's why it's called latent because  It's like dormant like you don't see it, I guess. I don't know why it's called Eden.  And  How is the latent variable model and GMM related  So in the GMM  So in the GMM  Said, I would be distributed according to a multi normal pie.  Right, and then x i givens that I will be dissuaded according to  Annex I, with mean new which depends on, said I, and coherence fixed score. It's  Actually, you could also have  In a journal. GMM you would have  That the covariance can dependency. So I'll say  It's a gash in on X I given you on Saturday and sigma which depends on the day.  Okay, so, so I lead in variable model for now is just this generic relationship between variables.  And if I this I give specific distribution. I say, Oh, this isn't normal. And it's discreet and this is a Gaussian given z, then you get a GMM okay  But you can also you have other latent variable model where you could decide that the class conditional is not a Gaussian. It could be a solid. It could be anything.  And  Z equals i. In GMM  Well, so the thing is, uh,  Yeah, so I guess he our roads. Why is boots, no milk to make the link with the fisher leaner and leaner.  Sorry discriminate analysis model.  But in GMM we will use z instead of white. So usually, because we don't observe it, why you normally we observe or we would like to think about was z as the online version. user avatar   Is this clarify it. user avatar   Okay, so I was not done with the parsing of the notation. So, the point was that  When we don't shave a variable in  In the graphical model notation. That means that the variable is unobserved.  And observed is often  That's what we mean, often by latent variable. So it's a variable which is not observed  Whereas when we shade a variable.  That means basically  That the variable is observed.  So that's the semantic  So is there any question about the latent variable model.  Okay, so then  There's something very important. Now, to think about, which is, which can be quite confusing for people who first look at graphical models, when I  Remember the first time I take graphical model. Many years ago, I was confused by that and it actually it was not very explained very well at the time. So now I'm trying to do a better job of explaining to you, but there are two views.  On the marginal and x in this kind of model right  So there is  One which is unstructured  Where all I'm talking about is x i don't talk about z i don't care about it. I'm just saying, oh, I have data, I will have my my variable.  I guess I could use perhaps X. I'm a bit user avatar   Not super user avatar   Rigorous here about my random variable versus the instant station. So I guess all these by the way they were normally capital letters.  To be because nodes are associated with random variables.  I don't know why I'm I was lucky with it.  It's not super important. But yes, to be consistent. We tried to be  To have these capital letters. So yeah, so I could have  Okay, so there's a good question here by I go, Well, why are we marking them as observing observe. Is it for the sake of representing the model well with efficiency or did you behave different  The don't really behave differently. It's more to kind of like it's a it's a useful notation to tell you more information about the kind of model, you will be talking about right so so when you shared a variable, you mean okay now I will think about my model when I observe this variable.  And later on we'll talk about when we do in France in a graphical model when some variables are observed and some are not observed. So then it will be helpful to say which variables are observed. And then what what it means in terms of probabilistic consequences.  Then not here means multi new you are multi normal with n equals one, they're the same, right. So, but it's multi new EP  Okay, so  So,  The two views of the marginal. So the first view is x is a mixture distribution.  This tree.  Tree vision.  So basically if I looked at his density on x, it would look like this. Like, for example, a mixture of gushing would look like this. I would have to bumps.  One bump around each of the mixture component, but I'm not identifying them. I'm just telling you the distribution on X is has this density multiple bumping  The other view is to actually have a structured model, which will be a latent variable model.  Where, in this case, I'll say, oh, actually, I can think of the distribution over X where X is observe where I have a latent variable said, which is not observed  And then I tell you what are the class conditional given said as is value the conditional is a Gaussian with this means  Is that as a different value that conditional the class conditional is a different gushing with a different mean and then I put some prior distribution over my my latent variable. And that's my whole distribution. Right. And so in this case the marginal over x.  Is still that have these two mixture. But you're saying, Oh, well, this is the mixture.  Component for is n equals zero. This is the mixture components that equals one. And the way you get the whole marginal over x is take a mixture combination of those two elements with the quality of each mixture component. Okay.  So this is kind of like a  The structured representation user avatar   Okay. user avatar   And what's the difference. Well, in terms of a distribution over x. There's no difference between those two views.  The margin over x. In the case of the latent variable model. I need to marginalize out z, which means I get a mixture distribution and I get these two bucks.  In the case of the mixture distribution. I always only talk about the module of X and I have these two mixture. So the marginal on x is the same in those two views.  The difference with the bottom view is that instead of adding a complicated mixture distribution on the marginal I break it into simple pieces. I tell you, well,  Actually, the x is not just a complicated distribution. It can be explained from a simple  Conditional of x, given z, which has the simple component and then some distribution of receipt. So it gives you a bit more handle on the pieces. So by commenting simple piece, you can get complicated distribution.  And Simon is saying that we can define the structure one s p of x and the structured one as that individual pieces. Sure, yeah, p of x, given z times pod.  And so when we have p of x, given z and z we give a bit more we we have split the marginal in  And actually it's not just P of X given zip ties. PMC. It's also marginalization, right, so here. So here it's p of x. Whoops.  P of X is summation of z p of x, given z. Thanks. PMC  Oh, but I remove my temporary Inc.  P of X is summation of z p of x, given z times PMC  Okay, so, so there's that we defined the marginal in this case is isn't with this marginalization aspect.  So because this fast is about graphical model basically the idea is, is we will define often these very structured  Relationship because it's easier to handle complicated data and to build complex distribution from pieces, rather than just saying, oh, I have a complex distribution and that's it estimated  So Jacob asked if we need to explicitly model P AMP z in the search representation. Correct. We need to also say, what's the distribution over that.  And then Dora ask when would not use this and work with this directly  Well, when we don't have a good idea of how to break things and pieces, like for example here, in some sense, I'm already assuming that I have gallons as  Class conditional. Well, how do I really know I have gosh inaccessible national and so they're from a statistical perspective, like, like the, the problem here is a problem of what is called density estimation is I have observation of  Data and I'm trying to fit a distribution to it. And here I'm making a nice parametric  I could make a nice time a trick model where I say why get x, given z is a Gaussian. I just need to identify now it's, it's me, it's covariance and z's and B2B  But this are. But what if the data is not gash in like right if it's something else. And so to estimate these kind of mixture. Like, there are also method, which are called non parametric  Which are much more powerful, in some sense, and you could use for example, like a histogram type of approach which build a program of the data and then you really don't have a mixture type of model. So you would just trying to fit.  Fancy histograms on the data and there's no notion of a latent variable model. So, so it can also be approach.  It's a bit. Also, you can think of it also having a black box models versus like a more explicit model like you like having these pieces help you to have a bit better understanding about the thing, but you can try to just fit the whole the whole distribution directly  Perhaps you understand it a bit less in this case.  Okay, and then the. The other important thing is that right now. We're saying for the GMM model or these kind of latent variable model will say that does that is the latent variable or independent then later in class.  Will see now will introduce some dependencies between  Between the latent variable. So, we will add  The time structure.  So where we have  dependencies between the latent variable. So I would have X one, that one, and then I would put an arrow to the right. So they're not independent anymore.  Blah, blah, blah.  Then I have said ti  X t. Whoops.  And this is the hmm  Model for example the hidden Markov model, which I had mentioned at the beginning of class.  The shank. Can you rephrase your question. I'm not just what you're referring to.  Which any method would choose former. So what's former referring to in your question.  Is there any other question about  The latent variable approach. user avatar Oumar Kaba  Yes, so I did have a question that you missed in the chat. I was just wondering what the indices refer to this as one to treat, etc. user avatar   So,  X one X two X n user avatar Oumar Kaba  Yes, exactly. user avatar   Yeah, so  It could be, we have observed and  Individuals, which we assume are identically and independently distributed according to the mixture distribution.  And so we could say, for example, like let's say I have, for example, the individual could be people and the observation could be attributes of these people like height.  Or, you know, income or whatever. And I say I had a lot of and I'm trying to model these individuals. Right. But I could have independent observations of just and different individuals.  Right, though it could also be that I made multiple observation over time of the same quantity or the same measurement, right, that could measure to the temple, I could that  A good example for hmm would be I'm tracking a plane. So I'm making measurement in a on a radar and I say, oh, where I think the plan is. And so then the index could become time could be like multiple  Measurements. And in this case, you would not be independence, because there will be correlation across time because plane is  The same thing and that's where the hmm model.  would be more appropriate. In this case, the latent variable because it's a, it's a, say, the position of an object would make more sense to be continuous rather discreet. So this where we get to their common filter approach. Okay. user avatar   Thanks, all. user avatar   Carl is asking so Zed i is a latent variable. So, which is not observed, but we still need to make a we still need to model. It's distribution.  Yes. Because to get the distribution of our observed variable P which is x  It's defined from the distribution of received. So if we don't tell you what's the decision of Rosie.  Just by the conditional x, given z i don't know what the decision over X right so it's just part of the the modeling.  Assumptions build up. So that's why we we say okay well to get a decision over x one ways to define a latent variable model where I'd say, what's the decision of Z was the conditional of x, given z. And that gives me then by marginalization was the decision of x.  And then when we start to work with the GMM example and estimate variables, it will user avatar Remi Dion  Become user avatar   A bit clearer. Also, like how to make this complete Creek right  So there's again in the gym model will have to estimate what are the means and the covariance is of these conditional as well as what's the prior over classes, either. user avatar   Of user avatar   The prayer of related variables.  Okay, what's the difference between a latent variable model and a prior distribution.  So prior distribution for a vision prior is anything you're uncertain about right so and so, for example, you can think of z i don't observe. I don't know what it says. So I'm uncertain about it. So then I put a prior over Z right  Though I can be a frequent this here and still have this model. There's nothing Beijing about it and then I will not miss the call this a prior. I would just call this a distribution over unobserved variable so prior is a terminology which is normally introduced by vision.  But given that  P AMP Z could be seen also from a patient perspective as a prior. That's why often people intuitively could also say, well, the prior busy and I often say that instead of just saying a little variable as the distribution over latent variable.  And the Remi, you have a question, you raise your head. user avatar Remi Dion  Yeah, I wanted to try to feature.  I wonder, along with the question of you.  Since we can't see it. We're not observing it. How can we apply an approach that is different and they use you. How, how could we apply for conscious approach if we can see it.  I mean yes understood what you said. user avatar   Yeah, so, so, so, so the difference between frequent. This is Beijing is not whether you see things are not the difference is how you estimate unknown quantities. Okay, so as  Frequent test, you will come, will use different techniques like maximum likelihood or a method of moments or maximum entropy that will see later to estimate quantities. Okay, so for example as a frequent is I could  Decide that my my my  My problem. I will say, Well, I suppose that my distribution is made.  Of mixture of Goshen, so I will have then to escalate.  Through some techniques which normally will see very soon maximal magnitude, but we could also use method of moments.  What are the parameters for my class conditional and my parameters for the thing and then and then for the little variable and  That's just an estimation problem. So, and there's no quote uncertainty.  Left or talking about what my decision. We're parameters in this case that will be like I have observations. I'll use my estimator technique that gave me an estimator. And then I can analyze  Its properties in terms of like frequent this risk bias variance etc to kind of see if it's a good estimate or not, but the the as a frequent this  All you do is basically defined some estimators and compare them as a Bayesian. The difference is the way that you  Handle uncertainty is clear. It's canonical it's anything you don't know, put a distribution. And then how do you compute from data, you only you law of probabilities  And so, in particular as as a true Bayesian even though I could say p AMP z as a prior I still don't know what's the distribution of p of x, given z.  There's uncertainty about that. So then I could say, okay, well it's a Gaussian with parameter view and coherence, but I don't know what's new in governance. So then as a Bayesian you would put  A prior over these parameters you would put a prior over the mean the prior where the grants which okay how you put a distribution over governance matrices that starts to get a bit hairy. And this usually it's called the  Fig the either the Wishart prior or the inverse, which starts. If you go on the prison matrix, but we're getting a bit hairy.  And then once you put this prior you still don't have an answer you need from the observation, you will just update your belief about these unknown parameters using for stereo in France. So that's the vision way would be to use  Bus there in France to just get answers to all your, your, your, your answer your questions.  Okay, so  So that would be the big difference. So in the frequent this you would have parameters and then you could estimate them and you get point estimate as a Bayesian  You would actually have to do plastic in France to talk about what's the distribution of our partners.  And so P AMP z in this case would not just be a prior it would normally in a, in a, if you actually have Beijing and you do a latent variable model, you would put a prior over the distribution up over the parameter of P AMP z. So,  Does that clarify things for me. user avatar Remi Dion  I think it does. Let me try.  Let me try this.  Frequent test will have parameters and essentially all the estimator tools are there to validate the parameter  And the Asian would have the distributions and as far as you don't know the parameters, you'll just apply furthermore distributions to the those parameters until you  You reach a known value. I guess because it could be unless to just apply another distribution generator distribution to another distribution. user avatar   Yeah, so, so this is approximately correct so so to to refine the terminology here. So applying a distribution doesn't really mean too much so what what we really want to say is, you, you, you, for example, do define  So more specifically right p of x, given z would have a parameter theta which quantify that. And then I would also put the distribution over theta. And then once I observe my my data. I could compute the distribution over theta given my data.  And this is how you perhaps in your terminology say applying to the submission. What, what, this is just computing the posterior so there's just, just using laws of qualities and conditioning on what you know, which is actually the data, which is the observation.  Whereas for the frequent this the way you get access to the parameters is just by estimating procedure and there's different which has been which which are obtained from different  Principles are sometimes it's just somebody says, I think this is a good estimator, and then you analyze its properties, even though you just came up from like nowhere like you can say, oh,  I instead of using the empirical mean to estimate my meat. I'll say, I'll take  The empirical meet I'll race to the cube and I'll divide by 55 I'll add three or whatever you can do all these oppression and that gives you another estimate. Perhaps it's not a good estimate, but then you can try to analyze its properties.  Okay.  So before the break, I wanted to  Talk about k means  Okay, and then I'll take a break.  When this people say no, we need a break. But basically, k means  It's not to do pure on supervised learning in the sense I mentioned above, where they're the meeting was just I want to fit a distribution for my data key means you actually solve us a similar problem, which is to cluster. So it's called to do clustering.  IE. You want a group data together.  Which is not a very well defined problem, by the way. Right. So how do you evaluate a clustering when you don't have labels and so intuitively what clustering means is, well, I have data which looks like that.  Which, by the way, could have been generated from a mixture of three options with, you know, three different means.  And then say, well, this really looks if I'm a human like three groups, right, like so. I want to say, Oh, these are all the same group. These are all the same group. And these are all the same group. Okay. And so the problem more formally is we want  Given observations we want to assign  The observation two groups. We want to get what is called a cluster assignment.  For every data point x  Right. So I have an observations.  Which could be again I have income level of different individuals their height, you know, which year they were born. This kind of things. And then I say, well I identify those two groups, for example, all these people  Were born in the winter and these people were born in the summer, and I've identified there's differences between these two groups of individuals because of when they were morning  And how do we present the cluster assignment will represent with this variable, which I'll call Z like a latent variable that's the link with what we talked about before. And so I'll say Zed i j is equal to one.  What does it means, this means that the individual I  Belong to group Jake that x i belongs  To  Cluster.  G. Okay, so that's what the meaning is. That's how we will encode our cluster assignment and Jay will be from one up to k, we will suppose that we are looking to cluster the data in K clusters. So this is the number of clusters.  Which has to be specified in advance 4K mean and will later on in the class will talk about, okay, well, what if we don't know the number of clusters. But for now, suppose that we know in advance how many cluster, we want  And so the idea is, I have some observations. I want to find groups and in principle, say, three groups and came in over them is an algorithm to do that, it will tell you, for every data point which group it belongs to  And it will even give you a representation for discussing  And you can think of came in, which is why we talked about Kenyan as a limiting version of the maximum likelihood approach in Gaussian Mixture Model.  So when they're actually the variance goes to zero. So it's kind of like, because in normally in the latent variable model. I have a priority for the weather a variable is  For the latent variable. I will say, oh, well isn't belong to class one, class to class three with some probability  And if I observe x i could then compute P have said, given X to see what's the posterior Polizzi given my observation of belonging to a specific class, whereas in Kimmy. We make hard assignment. We don't say, oh,  This individual belongs to this category cluster with some problems. He would just say, is no. This is belongs with this cluster. That's it. We just make a hard decision.  So that's why I came in, is like a hard version of German but because it's such a kind of nickel tick that that analysis technique. I'll start with that. And just to see how things behave and then we'll talk about GMM in the relationship between them. Okay, so that's the plan.  So now, is there any question about the setup.  Perhaps I'll mentioned also yes.  Some applications have came in, by the way.  I mean, it's used all over the place in just data analysis or data exploration. Right. You want to find groups in data, but an example of where it can be used. It's called vector quantization. It's to identify  Basically what you can do is you can identify groups and instead of transmitting the individual data points you can identify you can transmit the group membership and the group information and that make you save a lot of information.  And so, in particular, you know, from a coaching perspective will have that each group will have it mean  And  And so instead of giving all the detail about what's the location like point that could just say from, from which group would belong. And that's what I sent  And if you want, then you can also look about the errors that you make. But yeah, so Victor quantization. That's when ways to quantify the data like to to compress it. This is like for compression  And another another application, among many others. Right. There's a lot of application.  In computer vision.  People use came in as a pre processing step you can use came in.  To get what is called bag of visual words.  Representation from batches.  So you know in in natural language processing. It's nice. You have words. And you can say, I have 50,000 words and  When you you write text you just say you can represent each word as an index in a dictionary of like 50,000, for example. So there's no real number that you need to worry about.  It's just like index in in vision because you have these RGB representation of images. Everything is real numbers. And that can be a bit messier for the when we talked to, when we tried to do discrete stuff.  So what you can do is you can use k means on the RGB values of patches which are high dimensional vectors to find a bunch of groups.  And you will just label this is group one, group. This group to this is group three  And then you just tell me in an image I have five patches for group one, I have six patches from group to sit there and so then it looks more like a list of words like in text analysis. So this is a one places which which has been used  So Beijing has asked, what did I mean by the zero tolerance limit of GMM versions of which distribution.  So I just talked about this variance here.  We'll see that in more detail later. Right. So you obviously have to get it right now, but instead of having with serving this sigma will will have sigma square identity and we let sigma goes to zero.  That's the variance. I'm talking about. And then if we do maximum likelihood in. Hmm. When sigma goes to zero, we get back to the came in over them.  Okay, what's the last  Question, which is a bit outside the scope is so is there analogous to word embedding for computer vision vocabulary. So this is funny because  Word embedding in NLP have been defined as continuous representation of words. So I have said, of saying my words, is this is a, you know, an index of one to 50,000 in a vocabulary.  Or a sequence of letters, which is all discrete, you say, oh, well, this word will have this 1000 vector representation and this other verb as 1000 vector representation and the whole  Advantage of using that is then you can use techniques that I've been using computer vision and other which works like neural networks, in particular, which works very well with non discrete data.  Because it works with computers actors with language because language in your hands. He is not continuous. So that's the advantage of words and  envisions envision, you don't need word embedding because you already have continuous representation. So what I described with k means was to get a discrete representation of your image.  Now someone's asking, well is there now and then all of us have worked in buildings for computer vision and I, that's an interesting question. I don't know. Perhaps  And but that's already weird because we already had continuous representation. So why do you want to have this word, but  I guess is probably it could even be an interesting project question.  Alright so on these wise words. I'll take 10 minutes break. So it's 336. Let's go back at 346  Okay, so  Before we talk about the Kenyan algorithm. I wanted to mention also another example of where we do clustering.  You could have observation. These observation could be, for example, measurements on cancerous tissues and this could be three different cancer types.  Right. It turns out there's like different cancer types.  You didn't know which cancer type these tissues. We're going to first and you, you just by looking at data you are. Oh, there's like three different groups.  And then perhaps this could highlight more investigation to say what are these different cancer types into the need different treatments.  And perhaps you can then learn also a classifier which could tell you which of the cancer types yet. And so there's kind of a data exploration technique. Oh, identifying groups right  Okay, so let's talk about the Kenyan algorithm.  You might already have seen it. I'm not sure, actually. How many people here. I've already seen to came in our with them, say yes or no.  No knows  That people don't press the button or you don't know  I have to knows. Okay, good. And then among the people who have seen it.  How many have seen it as a block current optimization approach you clear the thing  Ah, look at all these knows. Okay. So, see that's that's the idea of this class, you go deeper to what you've already seen. So instead of just an algorithm to make groups, we can actually derive it as a minimization technique. So it can be derived. I think it's very elegant, it can be derived  As a block coordinate  minimisation or rhythm.  Of the following objective function.  Which is represented, which is quantifying the distortion measure  Of your cluster.  So g of z and Mew.  So z will ever present your cluster assignment.  Right, so, because what came in is giving you a cluster assignment two data points.  And so for every data points.  Will have a variable which will use by zero XE one blah blah blah to ze n  Our corners.  Of the probably to simplex like we use the one hot encoding. Right. So basically,  That's also what this thing is saying.  Is that for  Every I I have said i j from Jake was one two key.  I have key dimension and only one of them is equal to one meeting. Oh, I'm assigning that opponent I to class.  The one which is equal to one. So this is the usual one hot encoding right that we use the best of the cluster assignment. So you can think of them as corner of the simplex. So basically this is the one hot  Encoding  And then the muse.  The Rd cluster means. So you have new one level of that to UK because came in not only gives you a cluster assignment also tells you what is the mean of a cluster. It's kind of a presentation of the cluster.  These are in Rd and they are the cluster meters.  And the whole point is  You want to minimize the distance between a point and it's cluster me right if the clusters are perfect, you have very, very tight ball around a mean and so this distance is very short. Right. So that's kind of a good clustering. If you assign things to  Very far cluster mean then this means it's not really a good cluster assignment. And so you just compute that. So, this will be the some from overall your training example of what you're really want. By the way, so I will read it here.  Is the norm between your data point and the mean for the cluster assignment.  Of this mean right so I use annotations that I to say, oh, which index is equal to one, right. So, so I'm abusing notation here is that I normally was a vector in RK and now I can use it as an index to mean oh well with because basically, this means  Said I equals  Yeah, so this is just the index of the cluster represented by that. I guess that's what it means. And this is the  Cluster.  Index.  Represented by, said I.  Said I j is equal to one and all the other ones are equal to zero, then it means it's cluster j. So, this will be new, Jay. That's all it means. Okay.  So yeah, so what I really want is the distance between the mean for a specific cluster which is given by its customer assignment and the observation and I would, I just want to find  The means and the cluster assignment which minimizes distance. Which makes sense, right, that means it's a good clustering, and I will write it a bit more explicit because having, said I, as an optimization variable as an index is really weird. And so that's why I use this other presentation.  Which is very similar to when we did the multi normal representation. So we can some over my indicator variable. So I have key of them I have Zed and i j  Right. And then I looked at the norm between x minus Muji square. And the whole point is because only one of these is non zero this is this some here is exactly equal to that two distinct. It's just now I have express it in a way such that  The optimization variable which will be the cluster assignment is now explicitly kind of like in the expression  Okay. And so this you call it a distortion measure because so this J. Here is a distortion measure it's telling you how bad your clustering is and your goal is to find the clustering which minimize the distortions.  And the came in our rhythm is a specific algorithm to minimize this objective and it's called Blood cornet because it's alternating it alternate optimizing respect to each variable analytic. OK. So the algorithm for gaming.  Is first you initialize the means randomly.  Initialize  New  One. So, I will use these super the subscript subscript super script.  With parenthesis to to tell you different interests of the algorithm. So, and mew now is a collection of vectors its new one to UK right  And then what you do is you eat to rate until convergence.  Until  Convergence  And already use the EM notation, because we'll see that very soon in the EMR rhythm. So there's the East step.  But the east of does it will optimize respect to z. So, said at iteration t plus one. What I do is I do the argument.  Over Zed, which is a valid assignment.  Of the objective  Respect to Z, and I fixed the value of music to its previous value. Right, so I will have u of t.  Then the M step.  The M step.  Is fixing z and optimizing respect them, you  So the new value of my means will be the argument.  Over mew which belongs to  Dubai k, right, because I have k means and each one in dimension D. That's why you can think of new as the bike a matrix.  Of my objective.  My cluster assignment is set to experience value and I'm just changing the means  Okay, so this is why I say it's a block coordinate optimization algorithm because  It alternate fixing one of the variables to add value and minimizing respect to the other variable. And actually, in this case, you do a global organization right you you  Don't even know you're doing a green and step is just like optimize exactly respect one variable and then you fix this variable and then you'll to optimize respect to this variable. This is why it's called  blocking it so quiet approach is alternate between different coordinates to minimize problem and the block or it is just that will you have a whole block of variables together, right, because he is actually a multi dimensional object.  And now the magic of this is each of these updates is actually as a global minimum, which is easy to express right so when I when I minimize the distortion when the mean are  Like if I'm trying to minimize  Trying to minimize this was where my new is fixed. All I'm trying to find is the correct index. Well, you just pick the mean, which is the closest to your data point. Right. And so the the  Did I not  Perhaps I should erase this  Alright.  So the this first step has a simple analytic solution. So you have that your new assignment at step t plus one.  And I'll use the notation star. So, the j star.  Which is the optimum one hot encoding is equal one for  Jay star which is the argument.  over j of the norm between x i minus new J. Right.  At user avatar   Christie's fix user avatar   So you basically assign a point to the cluster, where the means that closes.  And then when you update the mean  So now if if these  If now the assignment.  If the assignment is fixed, and now I'm trying to minimize with the mean. So you can group all the points together. So for. So you want to find the mean, which is  Closes in squared error to all the data point in a cluster. Well, it turns out that, then the optimal central it actually. So these are not mean they're cluster central it, I guess.  Mean  Is the mean of the cluster. That's why they're called cluster means. So the update to respect to Mew, it implies that it's pretty simple. You can derive it yourself that new J it plus one is just the  The mean over that the current cluster. So, you can write it as summation over i have which points are in the cluster j. So, I can write it by multiplying by said i j  And then I want to divide by the number of points in my cluster which I can also write as summation over i have said i j right  So this thing is just the empirical mean of a cluster.  And somebody is asking me why did I call it the end am step because I told you that first of all that  Came in can be seen as a limiting version of the GMM maximum LIKES YOU WOULD PROBABLY APPROACH algorithm.  And actually, more specifically as running the E m all rhythm in GMM and then each step of the of the GMM algorithm will correspond to the this step that I wrote as he here in the queue.  So right now, doesn't make any sense to talk about em em, but when we see the EM algorithm, you see the correspondence between whites call expectation maximisation so in a few minutes. Once I'm done with the cavemen men or women came in already  So perhaps let's illustrate how it looks. Let me try that.  New share  I will try here.  So I have a little  So here I have  A link that they put in my notes. It's a visualization of the key mean algorithm. So you have, let's say we say new points. So I have my my points. There's no notion of labels these points, but you can see, you know, in this case there's basically three clusters.  And so that's why there's three groups and  I'm initializing came into three. So, so these three triangle here are the random initialization of three means, and I would run the algorithm. Right. The first step would be  Assigned the points to the closest center IDs.  Which is now what I've done. So all these blues are assigned to the central because they're the closes all the orange are assigned to this orange century and all the green is assigned to this green century.  Now the next step will be to update the central location. So for each green cluster i will complete its empirical mean and that would become the new me  Now you that for everyone. So you can see now the mean will move to the empirical mean of the grouping. Now I will repeat fine again a reassignment given these new means. So the color change now that much update the center and repeat and then  So now things are still moving a bit but at some point things with stuff moving. That's when we reach a fixed point which is actually a local many nights, not  A global minimum of the objective and, in particular, you can see here that this not a good clustering, because these are two clusters.  And I would normally would like to have a century on each part, but because of the initialization and the fact that it's a non complex decision problem. I got stuck in a bad local video  Yeah. So what I could do here is keep the same data point, but just run with a new randomization. So let's try new century decarbonisation. Now let's run the algorithm.  And again, it got stuck into a local minimum. Let's try with new initialization.  It got stuck to a local minimum. Wow. It's not very good right now. Well, guess these are difficult to do. I think this one will work. Okay.  So,  Are you serious also expect  Okay, this one will convert. Good. So now you can see it start to know it split  It's moving it's moving. Come on, move  Alright, so it's moving super slowly and it's not that great of a cluster, but that's already better than the other one at this I have three clusters. Now instead of these two being merged.  OK, but so you can see the big first of all dependence of came in on the initialization. Right. And actually, here you have the, the objective so 7001 31 is what  The Century point has been reached for this one. And I guess if I do a different one. Let's see. Now to convergence that looks like a  6700 okay so that's even a better minimum, then the other one.  Interesting.  Strange new centered  That's funny how this let's get your points. I don't like these points.  All right, what about this one. That's pretty clean separation.  That's run gaming and this data.  Okay, well,  That's one objective.  So yeah, so that's a you can play with it on your own. Let me describe a few properties of Kimi  New share. Let's go back to the user avatar   Lecture. user avatar   So let me describe some properties of the Kenyan algorithm.  So the first thing is that things will converge, we can prove that they will converge in a finite number of iteration.  To especially points.  Actually to better than assertion point to a local min  Of the objective  In CMU, so there's no local change in either Z or new I could make which would  Y z is difficult because he is actually  A discrete random variable. So what does local but you can make a small population of new and improve the objective. Okay.  But because it's non convex, which is often what happens with latent variable. It's actually empty hard  In general, to compute the global minimum.  To compute  The  Global  Mini map.  In Z Alright, so find the best assignment which minimize  The, the distortion.  Minimizing over news as well is NPR.  Oh yeah, so the mathematician asked about the computer science concept. So NP hard  Is  In short, means that something is very hard to solve. Okay, so basically in computer science. There's complexity theory which first formalize how hard a problem is. And then there's this notion of a reduction, which means that  If you can solve a problem.  And then I could actually use this solution to solve a different problem. And so that means that I've reduced see one problem to another. Okay.  And there's this class of problems which are called it, and p, which are basically pulling over the polynomial. The verifiable and when something is NPR. It means that if I can solve  NPR problem I can solve all problems in the class NP and why, then, we mean that some when we say something is NP hard. It's hard. It's because well hard in the sense of very difficult problem is that there's a lot of problems in NP that exists where  We were never able to find any  efficient solution. And so people have looked for like many, many years to find an efficient solution I economy pulling them your complexity and they have not found it so  If you could solve NP hard problem, you could solve all these other problems that we never knew how to solve. So it's probably because it's not solvable.  So NP hard will not be solvable in pointing on time. So there's this open question that we still don't know whether p is equal to a deal because meanings that if you find a polynomial algorithm.  To NPR problem, it means that you solve all problems in MP, which means that they're all police available.  But most computer scientists think that they are not the same like that it's not possible to find a polynomial algorithm for all the algorithms in MP  So when you're identify something as NPR, it means there's no way normally unless you think P equals and P, that there is a polynomial time algorithm which  Which means that, then you need approximation or you need to make more specific assumptions about the problem.  Will there be any change in convergence. If we use absolute last instead of squared.  So if you use the L one loss instead of the LT last the M step will change from being computing the mean to compete competing the median because the median.  Minimize the L one norm versus demean which minimize the L to norm. So this is called then it becomes the key median algorithm which is different than the K mean algorithm.  And the comedian algorithm will be much more stable to outlier because the if you have points which are super far which perhaps were  Should not be there. The L to error is because it's quadratic will be quite influenced by the outliers and it will kind of screw up your, your center. It was the median care less, because it's linear, the mistake unique so he median is more stable two years.  Alright, so  Basically, the objective. Why is it also NPR to get the global minimize basically the objective is non-complex right  Which means that the initialization matters. And there's this algorithm called k means plus plus.  What it does is that it gives you a clever initialization to make sure you don't get stuck in bad local minima.  Clever initialization.  And the nice thing about coming, plus, plus, is that it gives you some  Gary guarantees in terms of  The algorithm. So it gives it guarantees.  That the algorithm.  The decision, you'll get is within  lug k where case number of clusters of the global optimum.  And this is with high probably  Because it's giving us plus was a nice paper where they just do a clever initialization. And when you initialize it like this, you know, with high priority because it's a randomized algorithm that you will be within log key of the global optimum.  IE. The difference between I think it's the ratio of your objective to the global optimum is not bigger than lucky.  So if the global optimum is that  I don't know, like 0.000001 the fact that you multiply by luck K is not a big deal because you'll also have some distortion, which is really close to zero.  If your global optimum is at 10,000 when you multiply by lucky. That's a big factor in terms of still distortion measuring this guarantee doesn't mean that much.  And what's the idea for keeping plus plus.  Is just to spread out the initialization.  It will spread out.  As much as possible. The initial means  And this is to avoid basically why do you want to spread out. Well, let's say,  I have, you know, these points here.  And  You could easily get this kind of clustering.  If you know the means we're close to each other at the beginning and then you get this kind of  Problematic.  Initialization was basically came in, plus, plus, what it would do is with pick one of these points at random. For example, let's say I take this point.  And then what it says it will try to find them on all the points, the one which are the hardest.  And then basically, it will pick the next point with property proportional to its distance to the previous centuries. And so, for example, let's say think this point I'll probably pics this point here because it's very far.  And then usually when you have that this will make a cluster here in the cluster there.  Is finding the global men have come non Catholics problem always NP hard  Usually depends on what the, the, the non non convex problem is how it is defined, there are non comics bomb, for which we can find a global minima exactly very efficiently. But in general, non-complex problem. Usually they're hard to solve.  A alright so that's a clever way to initialize the method.  And then what about the choice of key right how do we choose key.  So,  One year a stick.  Is to use rigor ization basically so you will you will use an objective where you will have  J new ze n k two will have your original  objective of the Kb mean algorithm. Sorry. The key mean algorithm.  Said I j norm of new of x.  Minus new J.  And you'll add a penalty for k  In your objective.  And this lambda here is the hyper parameters. So you're you're placed a number  Of cluster with a penalty for  When I increase the number of clusters.  And then what you could do is you could you could try and different K minimize for every k and find the one which minimize this, this global air and the reason you want to do that is because when you increase K  When you add more clusters.  If we forget about the non-complex aspect, like the global minimum of your distortion will always  Decrease. And so if you would choose the distortion measure to choose number of clusters. You will always use the maximum size of key.  Okay, so here what happens is when you increase the number of clusters, the distortion decrease, but the penalty increase. And so there's a trade off between those two. So that's very interesting that has been used by people, and in particular.  Will see later.  In class.  That there are non parametric models.  For clustering.  Where k is basically infinite  Infinite  And we get, then what's the most likely K  By the posterior  And some keywords that right now. You have no idea what it means. But we'll see them later. An example of non parametric model for clustering is called the dish layer.  Process mixture model. user avatar   Okay. user avatar   And so basically  Non parametric model doesn't mean there's no parameters. It means, usually you have an infinite number of parameters and which means that, then the effective number of partners you use from the data can grow with a number of data points you have  And and in the case of I could think of, instead of having a finite number of cluster. I could have an infinite number of clusters, but most of the cluster, I won't need them.  From my model. Okay. And so the there's a process mixture model. It's a Bayesian models. And now we were Bayesian so as evasion. We have uncertainty also about the number of clusters. And so we can then find  What's your posterior over the number of clusters to kind of like estimate the number of clusters.  And their ship process mixture model is basically a limiting process of the Gaussian Mixture Model, where kid goes to infinity.  And it turns out that this linear. There's a paper by Jordan and other I forgot where the closer's are I think probably forgot her name.  I she's a great faculty at MIT.  And I forgot her name, but she was a student of my Jordan and I think she had a paper where the tomorrow. Bo, Derek. Thank you.  Well known people so star fact mentioned. So I think it's done, or I Broderick, but to be verified where she had a paper where  She can derive this objective as an approximation in a dealership process mixture model. So you make approximation in this model to cut because it's complicated to do exact in France, and you can actually get back this objective from them.  So that's kind of a is some kind of justification for the linear dependence on k because somebody asked why Linear A penalty, why not use it. Okay, square or K Q  And that's a very good question. Right. So there's nothing to see here in Aaron reason why it should be dinner, instead of quadratic or something. But one week would be to derive it from the dealership process picture.  Dora ask, can you do Bayesian model for non project models.  So, so non parametric approaches. There are frequent this and or Beijing approaches so frequent this look like histogram approach or Colonel  Approaches it's not colonel in the sense of machine learning, Colonel. The colonel in the sense of like smoothing function.  Was Apple I estimate my density by by putting a go ocean bump. I want all my points and I some that right so this is and the more points than regression bumps. I have. So basically I can have an infinite number unbundle number of  Components as my number my data point goes and feeling. So this is a nonprofit approach, which is frequent this Bayesian  Put distribution over everything. And they also non project approach in the sense that, then you have parameters which are infinite dimensional and then you are Bayesian with parameters which are inflated dimensional  And so in the case of the mixture model, if I have an infinite number of means, then, I haven't been number of parameters. Right. Right. And so if I'm now Beijing about that, then I'm still Beijing and non parametric  So we'll see that again when we talk about both nonprofits Bayesian approach and patient approaches in general.  Okay, so last property.  Is a  Key mean is very sensitive  To the distance measure  Right. So if instead of using L to I use  One or I use a mat habilis distance you get very different solution. Okay. And so in particular that the standard came in algorithm. It assumes spherical clusters.  So if your clusters are not very cold. It won't do very well.  It's very cold clusters.  And so for example if I have very elongated cluster like this.  The cannon solution would say, oh, this is one group. There's another group.  No, sorry. This is another key mean that's there. That's what you would like  Yeah, actually. So these are the cluster, I would like to get  But often because came in, but these are not circles, Kimmy will say, oh, well, this is one cluster. And this is the other cluster usually so this would be the key mean solutions.  And so one way to fix that is instead of having an L to norm, which basically assume spherical cluster, you will use a weighted like more like with something called them had my holiday bonus distance  Which when you do a Good gosh and mixture model that's basically what you do. So the Gaussian Mixture Model will fix that. GMM  fixes that because in GMM basically when you will learn the gash in component and you will learn to covariance matrix, you can learn that say the ellipse of your gash in or basically ellipse like this. Right, so you can learn that the orientation of the data.  That's right. If there's still more questions. So if I have not answered your question, you still have a question, let me know. You can just ask your question again.  So, Mr. Ask if there's a reason to use k means is gushing mixture seems tricky better and more explicit  Oh, good question. So K mean is, is much faster, right, that it's, it's, you don't have to estimate the covariance matrix, so there's less parameters to work with.  So if the spherical assumption is good.  It could do the job already. So, which is why you know for  Office was uploading these application of vector consultation or our competition. Usually people just use came in, they don't go to a fancy GMM  So I guess there's speed breath simplicity  And from  And when you do a Gaussian Mixture Model.  You will have to estimate the covariance matrix. So that's all the parameters so that then is sensitive to the noise. If you don't have that much observation. So that could  Create problems. So if I don't have that many observations trying to fit this fancy conference mate measure might actually hurt more than it helps if you already had good information like good prior information that spherical cluster is what you're looking for.  Okay so CMO or Simon is asking if normalizing the mean of your data help you work at least somewhat around the spherical clustering assumptions, definitely. So if you standardize your data.  But the problem is, it's hard to make your data spherical right because the only way to make it spherical is to know where the century is. And so there's a kitchen is a chicken and egg problem.  So if you only had one cluster. Then when you standardize this this cluster would be spherical. But if you have a lot of data, different clusters like standardizing it one is sunny make it very cold for each clusters.  So I'm not sure this will fix it.  Okay. So the plan is next class. I'll tell you about the EM algorithm for it at the abstract level and then apply it to the  Gaussian Mixture Model and then we'll see the link the link with teammates and then you actually your next assignment will be implementing that. And so somebody asked what does it say I seen that GMM fixes the problem. So it fixes. user avatar   That problem. user avatar   Ah ok so the distance  Alright, so people have answered the question, Mahalla know this distance here we go. It's hard to pronounce. And I don't even know how to spell it often. So that's why I say it's fast.  Good.  Any other question.  Alright, so I'll see you on Thursday. Have a great rest of the week. See you.
  Right, this is recording, so today.  We're going to see the EMR rhythm. So basically last class I started to talk about unsupervised learning. We want to fit.  Distribution to data where it has nothing to the labels. So that's kind of like the the very hand-waving or high level definition of unsupervised learning.  And then I presented the key mean algorithm, which was a clustering algorithm. So it's a very simple way to do unsupervised learning. It's not learning a distribution is just learning groups.  And this was kind of in preparation of learning about both  The EM algorithm and gospel mixture model where we will apply the immigrant. So that's the plan of today so em.  All with them.  And its application to maximum likelihood in a Gaussian Mixture Model.  And the link with k means is I told you that if you take the variance in a Gaussian Mixture Model and you let it goes to zero and you run em, you basically get teens. Okay.  Alright, so  Let's talk about expectation maximisation so this is this algorithm is basically to do maximum likelihood  In latent variable model like hood  In little verbal  Okay, so  Basically to recall the setup.  Trump with my eraser. So setup. So in or a latent variable model. One of the simplest is will say, Okay, we have some observation xi.  And these observation can be explained from some latent variable is that i which is not observed. So the exterior observed, but not as i said i and we could say that we have an ID copies of this model.  And  So x will be the observable.  And in this case, x could be the concatenation of all the excite together right so it's and z will be the latent variable, something we have not observed  And let's say would like to do maximum likelihood in this model. Well, so the log likelihood  What does it look like it would be the log of p of x one up to n, which depends on data.  So,  That is not observed. So why don't we do some method, we look at the margin all an x, right, and in last, last I mentioned, you know, that's the nice thing, having the structured object as the structure distribution.  Is you can easily make like these complicated multi modal distribution with simple pieces.  And and so the log of the luckier here. Okay, so as before. This is the log and because the model is independent, it will be product over my observations of the marginal on X i.  And then there's data.  And so that's nice because then there's just become a some right so that's the old thing that's also why we like these independent model. And that's also why we use the log like you instead of just the likelihood  Because the log will hit this the product and it becomes a nice some and so then what you have devices and some of you know functions.  But now the problem is that the marginal is not a simple  For example, exponential family. The marginal is written as the as the marginalization, right. So let's write this down. So this becomes summation over i.  Log  And then I have summation over my latent variable that's how I define my marginals of p of x i said i user avatar   Data user avatar   Okay. user avatar   And and again they put in a notation. So when I summon I put a variable there. What it means is I some over all the possible values of this variable, right. So here, all the possible values that said, I can fix that. That's the semantic  And now the big problem, compared to before, is that  Because of the summation. I'm not able to push the  The log insight. Right. So, so basically log of product becomes some of lugs but  The log of Assam is not the some of the lungs or some other versions. So this creates a problem. Okay.  And  And indeed, what happens is this actually what this some SEER with it gives it gives a multimodal distribution.  I guess we'll say, this gives a multi modal optimization problem.  And usually it's non comics.  So you will have, like, you know, little bumps. I'm trying to maximize that. And it's not a nice like a concave shape as before. Okay. So, for example, in the case of  A Gaussian or something like that, you know, this would be x of something. And if there was no some then the lug hit the X the X cancels out and I'm just left with like quadratic, for example. And that was very nice and compete.  But now, because of this annoying. Some in the middle, which is coming from the mixture aspect which is coming from the latest expect I get a non concave or non convex problem. Okay.  And  An e m is one trick to handle this this this annoyance. So, but let's talk about the options.  If you want to do maximum likelihood in a latent variable model.  So one option would be to do gradient ascent.  On a non concave objective.  And by the way, this is totally fine. So as an interesting historical note, I think.  More than 10 years ago, people who are much less comfortable  Working with non convicts Organon concave objective and so the fact that it was you know multimodal and on concave was seen as a big problem, which is kind of money that he had because he would actually work with convex if I version of the problem. So that's kind of neat.  But I guess with the deep learning craze where everything is on convex and things still works. I think people now are much less  Much less of a problem to just work run methods which don't have guarantees, because you don't have guarantees, because the thing is on concave so you can know whether you'll get to a global max. user avatar   But user avatar   But yeah, so. And why am I still teaching yen is both because actually it does really well in practice, it could work better than the grand method on the non cave concave objective and also it has a very nice interpretation and it gives a bit of insights on how to solve the problem.  Okay, somebody is asking what's the difference between a non concave objective and a convex objective or do the same thing. So, so  f is convex if in on the F minus f is concave. Okay, so by just taking a negative sign of your function, you can flip convex  Concave right so a convex function basically look like a ball shape function and a concave. Well, it's a ball in the other direction. If you just flip the sign.  And when I say non concave that just mean it's it's not concave, which means if I take a negative. It's not convex so  And I use convex concave interchangeably. Because okay if you want to maximize the function, you want it to become cave going to minimize the function. You want it to be convex. When you talk about convex optimization as a field you also include  Concave maximization in it because any way you can just always flip a sign and then maximizes and minimize so it's not the difference between concave, convex is not a big deal from a medical perspective. user avatar   Okay. user avatar   Alright, so first approach is do grid accent. And I'll talk a bit more about it later to compare with em, but the, what's the this EM algorithm.  The EMR rhythm. It's another blood quit method. So, so, you know, k means was a block coordinate optimization approach and the M is another one. It's a block cornet ascent method.  On a auxiliary function.  Which  lower bounds.  The yield.  So we will describe a function which is a more convenient to work with lower bound of the log likelihood, and we will push this lower bound up by maximizing the  Over my parameters and because it's a lower bound that I'm pushing up. Well, it will hopefully also push up the lights. The lights. So that's kind of motivation.  And it has a nice interpretation.  Interpretation.  In terms of filling in the missing data.  Okay, so, because he said is not observed and you can think of em as missing in the unobserved variable and then things becomes kind of like nice and convex because everything is observed. So basically, the two steps at the intuitive level for now. So the East step.  Can be sin. Sin seen as filling the Zed variable with soft values.  Why soft because we haven't distribution of disease. So we can just set said to one or zero, we need to have some problems with that. And the easy step is to compute these and then the M step.  Is to Max, the  Due to solve the maximization problem with respect to data for the fully observed model.  And so when you have, for example, in your assignment you did Fisher Leonard, it's coming analysis where you observe both Y and X right so you observe  You say X is a Gaussian, which depends on the labels, but you also observe the label. And so when you do maximum next year there, everything is nice. It's convex or concave. In this case, and you get a closed form solution.  So when you when you have the information about the class membership, things are nice and so the EMR with them can be seen as fill in the missing value with the step then  Do a standard maximum likelihood, which is nice and then repeat fill in the new again missing values and then fill in solar maximum actual argument. So now if you compare to came in. Let's go back to came in right so  Remember I mentioned the East step and the M step and you're like, Well, why is it D and m. So, it was exactly the same thing. So in the East that  We actually chose the cluster membership. So we were getting the value of the latent variable. In this case, which is a cluster membership and then in in the M step we were basically  Almost solving a maximal magnitude problem because it's like, what's the solution when you have us through McGough should model and you're trying to estimate the mean was the empirical mean, right, which is what you basically do  Yeah, so somebody asked what is blackcurrant ascent again so block grant ascent is  maximization of a function where you will fix the value of one block and maximize respect to the other variables.  And then you will fix the value of the other variables and then maximize respect to the the unfixed variable. So you can alternate you don't know how to solve exactly the maximization of are all variables. So you fix them. And then you maximize respect to the other ones.  Just like a coordinate that approach. And when you have more than one coordinate together, as you call it the block point approach because instead of being scale or it could be like vectors.  Alright, so let's look at a trick for em.  It's actually a fairly common trick.  That appears in the lot of  You know,  Places in probability  Theory. It's to use the  The Jensen's inequality. Okay, so what's the Jensen's inequality. So I will phrase the Jensen's inequality.  For concave function.  It's actually normally for a convex function, but you just put a negative sign and then it becomes for concave function.  In to die. I think I'll answer your question at the end. Okay, though I can answer quickly would GANs be an example of a block right method.  I don't think so. So, again, is actually a min max problems. It's a different approach to  Block wet optimization approach will be that if you optimize respect to one variable and not the other variable. At the same time, which sometimes people can do for GANs, but again formulation itself. It's not this to the a block MIT.  And for the rest of the class. If you don't know what I'm talking about. It's totally fine, because I have not said anything about what again is  Alright so Jensen's inequality.  So the nice thing is the expectation, with respect to a distribution of a function f.  is upper bounded by the function of the expectation. So you can flip the expectation and the function when with an inequality sign when the  The when f is concave  Okay. So, Q here is just some fixed distribution that's true for any Q  And  And if instead you would have FS convex, you would flip the inequality, right, because you can multiply both sides by minus one and then inequality flips and then F becomes in this case concave or minus f is concave and then you can use the narrative on the expectations.  But so I never remember this inequality, by the way, how is never remember which direction in this. So my recommendation is to just do a plot. Okay, so this is a concave function concave ball like this convicts ball up. Okay.  And if you don't remember which size the bowl should be well convicts us from unionization so at least. Remember that  And then if you try to minimize a function like this, you will have the optimum at the boundary which is not very nice. And so it's like this you want to use for having trouble with my  With my  Framing. Yeah, so this is comics. So now this is a concave function and you can think of taking the expectation of the function as. So this is, you know, this is f of that one. And this would be f of the, oops, sorry.  This would be Zeke, this would be said. One is that, too. And here would have efforts that too. And this would be f of zero.  Okay, so now I am taking an expectation of the function and basically taking a convex combination of points on the on the curve. And so what happened is that the was employee. If I take a convex combination between f of that one and two, I will lie on this curve here. Okay.  And so  Basically the expectation if I only have two points we expect to Q of f of z will basically lie.  Of on this line, so it will live below the curve. Whereas if I take now the convex combination of my points. So, so this point here would be the context combination of my disease. And if I evaluate the function at this point, I get this point here. So that would be f of expectation of the  And here I took only two points. You can think of when you take a an expectation as taking a summation over positive numbers times you're the thing you're taking the summation of and these positive numbers. Some one. So that's why it's called a convex combination  I mean, by definition, a convex combination is a summation a weighted summation of terms, where the weights or positive and some to one. So the weights basically represent the distribution  And so here you see that the function evaluated at the expectation is above the expectation of the function day which is what this inequality saying  Okay, so, so the trick to remember which one goes where is to do this little drawing  All right, so why am I talking about the Jetsons and the quality was because  We had this annoying.  And it's because we had this annoying lug and some right so there's some can be seen as an expectation, the slug is a concave function. So I will  Switch those two things by using Jensen's inequality and then I will have the lug of my joint  Which will now can if the joint has like is a garden. For example, I will have an X log will have the expo and everything will become simple again.  That's the beauty of the trick. So let me write this trick in more details. So I want to compute the marginal the lug of the marginal distribution. So this is summation of z of p of x and  Okay. So right now I forget the dependence on data.  And now, what I do is first of all I will do what you always do in some trick is you multiply by one, right, so I will multiply by Q AMP z and divide by Q AMP z.  So Q will add as my weighted combination to use a Jensen's inequality and like I still keep my joint p of x, z.  And so now what I have is the log of an expectation respect to Q of the function which is p of x, z divided by q AMP z.  And so by Jensen's inequality, I can now flip the expectation and the lug. So this is bigger or equal by Jensen's inequality.  That's the big trick to expectation respect to Q of lug of p of x, z divided by QC  And so perhaps we can now expand this a bit more. What is this, this is summation over Z QC  lug of  P of x, z. And this depends on data.  minus summation over Z lug of QC sorry there was accused in front of it. So that was a numerator. So the log of Peter right by Q becomes log of people plus minus log of q. So that's what I've done here. So of lug of QC  And so  This thing is the auxiliary function that I mentioned, which lower bounds, because by this inequality here we have that this  Quantity here is a lower bound on the thing that we care about, which is the log of the property of x ray guess I could have written here. This is log of property of x is equal to that.  And so by definition.  This funny objective will call this L and it depends on two things. It depends on the waiting that I use, which are the know by q and it depends on the parameters data.  And so by definition, what is this  So, the first term is the expectation respect to queue of the complete log likelihood. What is we call the complete like lucky because we have observed both x and z.  And I guess the expectation here respect to z perhaps output Z big just to make it clear.  And it depends on data.  And then the second term. It's actually called the entropy of Q  Is the entropy of Q. When we go when we talk about information theory we're going much more details on these specific quantity. So entropy user avatar   Of cute. user avatar   Okay. user avatar   And so this is the  Expected  Complete  lug likelihood  Like you're able to great user avatar   Thank you. user avatar   Okay, so  So to recap what we have is we have that  The lug of p of x, which depends on data.  Is lower bounded by this auxiliary function which depends on both Q AMP data for all distribution. Q And theta. Okay, so there's valid for all the solution Q AMP later.  And so then  EM algorithm is basically to do quite a cent on this auxiliary function which depends on q AMP data. OK. So the algorithm.  In the East step.  You will actually maximize respect to cute, so you'll set the new distribution cutie plus one.  Which is the arg max.  over q  And let's say what our cues recuse our distribution over z. Right. So all possible distribution over z.  Of the objective, I will optimize respect to Q, but I will fix theta at its previous value, right. So that's the cornet coordinate approach. So I will fix at its previous value data t  And we will see very soon that when you maximize over q this objective, there's a closed form formula which is super simple. You just need to set the distribution over z to be the hysteria.  Of z, given X, when I had the parameter of sanity.  OK, so the, so basically  Maximizing this auxiliary function with respect to Q. You just need to compute the conditional of z given x or the current parameter value.  And then m step.  You set your new parameter  To be the arc max.  Over  The opposite very you where I have fixed cutie plus one.  Security plus one is now fixed  And it's fade out I'm optimizing  And so  As I mentioned, this is the art max.  over theta of expectation where cutie plus one is fixed.  I'll put the Z dependence here just to be clear that this is a dependence on z of the complete look like yet. So a log p of x, z, which depends on data.  And so  So this  So this is  Another maximum likelihood problem.  But we're now the information is complete for complete information because we have fill in the missing value of the z by taking the expectation respect to cute.  And so in particular will see when we do the GMM thing that  When we had like a mixture of Gaussian model, then this maximum likelihood as an A plus form formula, which is kind of very neat. That's one of the adventures of em is often em will have closed form formula updates.  And often, not always, though, but for example when z is a binary variable so often.  You replace  Z with the expectation respect to q of z.  In  This expression.  Okay, so, and we'll see that in the case of  Of the  Gaussian Mixture Model how this  This  Manifests itself.  Okay, so then now there's already a few questions. When I say all possible decision of Rosie, are we not making any assumptions on the form of Q1 em instead of em, you don't. So, Q is really all possible decision of receipt  Will see later when we talk about various Chanel urine, because it's perhaps too complicated to work with all this mission of her Q we oversee sorry we will make a simplified some we will only optimize the verse, simply for centrify distributions. For example, it could be only gushing  We need some assumptions on the support of Q AMP Z A  I mean,  This is a bit of an abstract derivation  So right now you need that. So you're not allowed to have Q AMP z.  I guess hear the  Question. So, so you're not normally you don't like. So when you do these kind of things.  Whoops.  So if q AMP z equals zero. You know, you will have zero time divide by zero. So multiply by zero divide by zero is not a very nice thing.  So the convention normally here would be that  you exclude disease for which Q is equal to zero in this duration. So I think it's fine.  Thank you. Good. Have any Q in this case.  I will soon repeat the point about the posterior because I'll explain how the East step.  Gives you basically that the updated Q is the posterior  Assigning cutie plus one means redefining the distribution cute. Yes. So, so to t plus one just means that, because when I talk about the blackcurrant Ascent algorithm. I'm saying I will update these variable.  Incrementally like interests. Right. So I start with Q zero and Qaeda zero  Then I will find what's the next value for Q and I would call it Q1, for example. So this is kind of like you can think of it like when you do gradient descent or these kind of tradition techniques you keep updating the variables as you optimize  Okay, so now let's go back to  This fact here that I mentioned, which is that the solution for when I maximize the auxiliary function with respect to q  You get  That it's the posterior and actually let me see if I have somewhere.  Yeah, okay. So basically,  There's a property of the Jetsons and equality.  Alright, so this is unclean at me correct this.  Is cutie plus one, indeed.  My notes. Let's go back to where with  Accurate. Yes.  So,  We had  The log of p of x theta is an upper bound on the auxiliary function Q theta and this is from to Jensen's inequality.  It turns out that in the Jensen's inequality.  It says that you, you can actually get a quality. So in Jensen's  In equality.  We get equality.  Only if the function that we're taking an expectation  Is  Waiting  Okay, so let's be clear.  So,  lug. So basically I use that the lug of expectation of Q. Let's call this G because there's too many F now.  Because I use the inequality with so f is actually loved the lung function right  Okay, so I guess.  By the way, you can generalize this by  I guess I should have put here. You can also put a GMC  And a GMC they're  Allowed to also just take a  Function of random variables, right. So,  So as long as they keep it inside instead of z.  So why am I saying this because the Jensen's the equality here tells me that this is bigger equal then expectation respect to queue of lug of g of z. Good.  All right.  Now, what's happening is that  Because lag is a strictly concave function.  The Jensen's inequality wouldn't normally always be strict equality inequality.  Unless the random variable is digital and it is degenerate. Okay, so, so basically, and just an equality, you get  Strict  Inequality.  Unless okay and when  F is strictly concave  Which is the case for the log function is what I'm talking about that. So you get a strict inequality unless  The  Distribution.  Is the generate  The J naret. What does it mean to have a generic distribution, it means that the random variable only take one possible value.  And so then the expectation. There's only one point. And so the the thing is trivially equal  Because there's no sun.  I he takes only one value.  Uniform is not the same thing. Nope. So somebody is asking, well,  By degenerate. Do we mean uniform know we mean takes only one value so and so, in this case, ie when g of z is equal to constant  So if the thing I'm thinking, an expectation respect to q. So either Q is only putting mass on one variable or I have us as Josie a function such that  The output is only always the same value. So now when I think an expectation is always give a constant. So it doesn't do anything.  So I'm saying you get stricken of inequality, unless the distribution is degenerate. So, I mean, the distribution of G AMP z.  So g AMP z z is a random variable. So GMC is another random variable.  By just transforming its distribution.  So either Q would put all the mass on one value or G AMP z is a constant and actually now become much more clear here because  It will make things a bit more concrete. What was the G THAT WE USE IT WAS p of x, z divided by QC that was a function, we were taking the expectation of respect to user avatar   Respect to cute here. user avatar   And so  So g of z equals constant  Above so above. I mean,  In my em derivation trick.  This implies that p of x, z divided by q AMP z is equal to constant. That's what we're saying.  For all the right. So, x, x is fixed. So the only thing which is very is z. So, now if we're saying that the function g is constant, we mean that the ratio of px device P AMP z divided by Qc is a constant for Ozzy which implies that in this case.  QC  Is proportional to the joint.  Right, because their direct ship is on their constant  IE.  QC  And he was the star is the the posterior of z given x  And I'll put the theta back  Okay, because what happened is, is  This here is a strict inequality. So, this quantity is always smaller and this is the executive function. It's always strictly smaller than what we care about. And it will be equal when we set Q to be  The poster of z given x. Okay, so that's why I'm saying when I maximize  Respect to cue the rosary function, I get that. This is the maximum and I forgot to use temporary user avatar   So let me can so user avatar   Okay. user avatar   IE, the art max.  Over que  El que data t is equal to p z, given X at data t and  We have, in this case that the auxiliary function is equal to the thing we care about we get that L.  Of Q t plus one, some sense data t is equal to the log of x activity.  Alright so that is a few question.  Is in a very, very special case that God is a constant. Well, in this case, in some sense, it is but  The, the whole reason I was talking about His argument was to find in an easy way. The, the, in a close form. What was the  Solution of this maximization problem, right. So when I'm trying to maximize the spectral Q this auxiliary function well I said cue to be the posterior at different parameter and then it makes the absurdity function equal actually to the upper bound. So that's the trick.  And somebody saying well couldn't just GMC be uniform. No, because if this is uniform the lug of this will give will actually be the expectation outside the lug will change something so  The whole trick is, if this is a constant, then this expectation is only one value. There's no some over a bunch of stuff. And so there's no expectation. There's no expectation, then of course they're equal by trivial relationship. So the only reason that  Yeah, but because the login access to take on K function as soon as you take convex combination of multiple points you get strictly below which was like the curve I showed you before.  Here at any point on this line, except the corners, which are one point.  Or a strictly below the the concave function. So that's what's happening.  I'm not sure I get your question be no  So if you still have it perhaps  rephrase or  Give me more information.  Yes. So basically, let me repeat this part. So some people are asking why, why is there. Q star to star right  So what I'm saying here is that if I use this value of two  Makes  Equality.  El que theta is equal to log of p of x data. Okay.  So, so, so if I choose this distribution queue here I get equality between my auxiliary function and my upper bound  For any other value of the auxiliary of Q It's strictly lower. So if I maximize overall queue, because you can never go above the upper bound, then if I make it equal. This is my maximize  This will be greater or equal than all other values of l because of. Yeah. So basically, what you have here is that  L.  Of Qt plus one where cutie plus one is defined as I've just done theta t is an upper is equal  By the property. I just gave to the lug of p of x at data t. And we know this is a low an upper bound for the auxiliary function for all cute.  At the same data right  And does  This implies that cutie plus one.  Maximizes  L of Q data t with respect to keep all right.  Started stack is asking is Q, the poster. The only possible for  Yes. Okay, so basically the question here is, I get that Q is proportional to the joint and then I conclude that Q is equal to the posterior  Because Q is a distribution. So it has to be normalized one. And so, indeed.  You know, the only distribution which is proportional to join as a conditional  Okay.  Let's talk about some properties of the EM algorithm.  To make it a bit more concrete. So let's look at the properties  So,  The first property is that the log likelihood is actually non decreasing. So I have the dialogue. Let's do the Vex at theta t plus one is actually bigger than the log likelihood  At data t  And so I am. I mean, it could be actually equal. So there's nothing see a streak increase, but  Since I'm since I'm trying to push up the likelihood, and indeed this method does  And so here's the proof.  So the leg likelihood  Of x at data t plus one.  Is an upper bound on the auxiliary function evaluated cutie plus one and data t plus one.  And because when I did maximum like to respect to theta i was maximizing the auxiliary function respect to theta. So this is bigger than the log like you cutie plus evaluate at Qt plus one and data t because data t plus one was the maximum Iser  So this space fix and data t plus one is defined as the maximize or of dysfunction respect to data right so that's just by definition of data t plus one.  And now the magic is, I know that when I evaluate the function at to t plus one and data to like here, I get back the log length you  Because the, the, the auxiliary function in this case is tight. So this is equal to lug of  P of X activity.  And so that shows that  When a volatility plus one versus data T I just get higher, right.  So he's asking why is the only distribution that is proportional to the joint. The conditional  So this is you should review just says standard definition of conditional probabilities. This also explains why, by the way, I  I never talk about conditional. Like I always just say proportional to i don't care about the normalization constant because the normalization constant is is to make it some to one. And so by definition here, I will have that was the nomination constant of p of x, z, right. So,  The normalization constant of PM exists summation of z p of x, z. And this is just the marginal of x, right. So if I divide. If I divide p of x, z divided by px, I just get the conditional  So the magic that everything works.  OK, so now let me do a little plot of what's happening when we do em.  Okay, so what I'm going to blood here is just as a function of theta.  And I have my though you could think of the auxiliary function as being both a function of theta and parameters for q. So you could have, like, another dimension, but I will try to do a 3D plot because it's a bit difficult.  And so  I have my leg leg field which is probably non concave like this.  And what happens is  What do I want he  Let's say I started data t  And then I look at the auxiliary function.  As a function of feta.  Evaluated cute cheapest one right. We know that that cutie plus one.  And data to the objective function is actually equal to the upper body.  And so now if I looked at the very function. So this would be, for example, the auxiliary function.  And evaluated to t plus one. And now as a function of theta. Okay, so this is below the  The upper bound, and it is tight at delta t. Okay, this is tight here.  And again with a plot in blue is the log of p of x as a function of sailor. So I put a little dot for saying that's the function data. Now, what happened is I will find the theta which maximize this function. So, I will, for example, get  The new theta, which is here. So, this will be 30 plus one. user avatar   Okay. user avatar   So this is my max of my exit function. And now I will update my exterior. So I will update queue.  And then they will get  That the Q value will make the observer function tight at the value added plus one. So now you could have say whoops console. Let's put it in green.  So now this would be my new executive function.  This will be L of cute t plus two as a function of data. And that's the one. Now that I will  Optimize so it's it's tight at the point in, you know, I'll get by maximizing this one I'll get  See a new theta t plus two. And you can see that while I do that, indeed, the value  Of the likelihood at each of these points is increasing. So I am indeed while I'm doing this process. I am increasing my my my log likelihood. I will probably converge to this local max.  Because unfortunately this is non concave. We won't get a free lunch. We won't be able to do global max. We just converted to a local max.  And the nice thing is that as a function of feta these auxiliary function or nice and concave. Usually, it doesn't have to, but they're often nice and come, Kate. So that's why there are much  Nicer to work with because this was not concave so so that's why instead of working with the original non count a function. We work with these concave lower bound. So that's the the process of em.  And let me mention another property and then I'll stop for questions and we'll take a break. So basically one property of em Is that data T in EM  Converges  To a stationary point  Of the leg leg.  Leg of p of x as a function of data.  And what we mean is that the gradient respect to theta of lug of p of x data is equal to zero.  Supposing that it was  It was different.  And so like k means  Initialization is crucial.  Right, so depending on where initialize you get two different local max.  And so usually what you do is you'll do multiple Random Restart  And pick the  The solution among all these random restarts which has the best objective, because you can evaluate the objective when the bond is tight.  And in the case of GMM  Which will cover very soon. You can actually use came in, plus, plus.  To initialize the parameter mean this is a good way to finish he share lies.  The means of your assets.  Is this example, it looks clear we're not going to get the global max. Right. Yes.  But if I initialized. For example, here this is data zero, then there's a big chance that I would converse, the global next. So that kind of shows the importance of the socialization.  We couldn't have just taking the grand directly. It is asking  Yes. So that's the method one to do maximum likelihood is instead of doing em, you could do just gradient descent on the log like human  And the advantage here is actually each of these step was very powerful because when you do a grand step you just make a little step in this small direction.  Was in this case, you're doing global maximization of the lower bound at each step. So these are very powerful step like you can make super big step using em.  And the reason why it was efficient is because often these  Competition or close form so you can make a global max over q, you just compute the posterior, then you do global max over theta. It's a maximum I could problem, but it could have also closed form solution. So that goes much faster than if you just make small great instead  What was what's the after the break that sometimes it will be better to do agree that transition method than him.  So Marin is asking whether there's a relationship between him and a second order optimization method.  So it's not really a second order because there's no history and information in this case. So, so the EM is called, it's called a major realization or  I mean, if we if we flip the sign.  And we would minimize maximize then it would be called a major realization minimization algorithm. So modular ization means you will take an upper bound of your function, then you will minimize the upper bound  Here, what we're doing is guess mineralization maximization algorithm, which means we make a lower bound of the thing that we care about. And we maximize that.  And this is actually a generic approach to do optimization. So they're all optimistic method, which does that. But because there's no husson or gradient information here that it has nothing to do with first or second order approximation.  I'll get back to the rate of convergence after the break.  And if I forget. Remind me Karthik  Gaming plus plus I defined in the last lecture. Please look back at the notes from the last lecture, and when should we use GD instead of em, I will answer that after the break.  Because I want to talk a bit more about these properties. Cool. So let's take a 10 minute break. It is three to 34. Let's go back at  244 user avatar   Let's see. user avatar   For Logo IFT6269 - Friday lectures- Shared screen with speaker view  /  Speed   Audio Transcript Chat Messages Search transcript user avatar  00:02 So let's finish the EM part 00:06 Oh, and the funny part of these steaks face of Spotify is my wife. So this is a lot of it is from my wife musical taste. I don't have that much credit 00:19 Anyhow, so 00:25 One last property of em and then I'll answer a bunch of questions. So 00:30 I want to now show you what's the difference between the the thing that we care about and the lower bound. Right. So recall that the lower bound the auxiliary function. 00:42 Is the by definition the expectation respect to queue of the joint like you had p of x, z. And then there's a failure divided by q. Right. That's the definition of the rooms every function. And so if I compare the lug of the marginal like to the thing that we care about. 01:03 And I subtract the lower bound. 01:12 I will now have minus expectation respect to q. So, so I so this here. I'll just put it to their 01:23 Oh, did I forget to use invisible ink. 01:30 And so yeah, so minus expectation Q like of p of x, z theta. So, haven't done much and then I divide by Q AMP Z. Oops. 01:43 Log of this divide the QC and what happened is the luggage. 01:48 Luggage px, oops. 01:52 The log of px 01:54 It's the same thing as 01:58 Now I can put it in the numerator here, right. So this would be now my p of x data. 02:10 Right, because if I take minus of lug of one over px 02:18 Then the minus cancels out. I'm just left with minus the other pics 02:28 And 02:31 That's rule my cell phone because I don't want to receive suspected spam. 02:36 I hate these spam saying 02:39 Like your 02:41 Social your social security number or whatever it's called in Canada has been 02:48 Figured out by the FBI, you need to call this number, or you'll get in trouble. I get these messages. I guess the 10s of these phone calls. This is so annoying. 03:01 And. All right. So why did I do that well. So the first thing is that then we noticed that the joint divided by the marginal is actually the conditional 03:14 So this thing here is P of z given x theta. user avatar  03:21 Okay. user avatar  03:23 And so I can 03:25 This is just expectation respect to Q and I flip the numerator and denominator, because I have the login so this is log of Q AMP z divided by p of z x data. 03:42 And by the way, this is by definition. 03:46 This is the 03:49 To 03:52 divergence between Q 03:55 And the poster. 03:58 Given texts of data. 04:01 This is called a kale divergence 04:05 And we'll revisit this again. So we talked about entropy. Today we talked about kill divergence. We'll revisit both of these quantities later on when we talk about information theory. 04:21 But it but you can see that the difference between your upper bound and the lower bound is the kale divergence between the queue and the posterior 04:31 Okay, and it killed everyone has a property that it's always tricky bigger than zero on as the two distribution or the same so you can see why when we try to maximize the 04:42 The lower bound respect to Q. We can we, we can make the difference between the lower bound and upper bound zero by sitting Q to be the posterior right so now it's clear from this perspective. 04:56 And so in some sense you could think of. I have log of p of x data and then I have my auxiliary function el que data here and this difference is the kill between q and the posterior 05:18 And we can make this different zero by just setting to to the puzzle. 05:30 Piece in purple is indeed p of x data per episode can rewrite it. 05:36 P of x theta. 05:51 I'm so I need you to. So it's saying empirically, don't we cannot really have access to the posterior to assign it to Q Right. Yes. Because we observe x 06:06 So who get. By the way, this is not a posterior on the parameter, right. This is a procedure in the sense of the conditional of z given x. So, 06:16 Beijing could think of it as a, it's a bit abuse of terminology we talked about the kind of a beige and so 06:22 Sort of thing of the sphere of z given X, just think of it as the conditional of z given x, then it's clear and we have access to x, we know which data we're considering right now in the EMR with them so we can compute the first the property of said given X for this parameter theta. 06:43 So the lug applies to everything to the right. So you can think of the slug here and the slug like this and 06:53 The P disappear by the p of like I had, I had this p and this be, then I replaced the ratio by the conditional. And then I I have the minus i just flip them right so that's why I have Q and then p AMP z here. 07:19 OK. So now why oh no I again didn't use the invisible link I 07:28 Let me rewind. 07:31 Control C Control V. Okay. And so we will revisit 07:36 This scale divergence. When we will talk about virtual conference. 07:45 So we will both revisit when we talk about information theory and also when we talk about 07:51 To do virtual inference. 07:59 And in particular, when we do virtual in France, we could use a set of distribution which are simpler than all distributions. 08:17 And so 08:21 So when we do the East step in standard em. 08:25 We actually set 08:27 cue to be the stereo. 08:31 This year, but if our model, sometimes in our model, we won't be able to compute this theory. Exactly. We will need to do approximation to complete this posterior 08:45 And. And so one way to approximate this quantity is to minimize the kale between some simple distribution and this quantity 08:55 And then the queue that we obtained by minimizing the kale will be kind of the approximation. So for example, we could have something here, which is a complicated multi 09:03 Mixture of gosh, and I will try to approximate with some gosh, and I will try to find the Goshen, which is the closest in Kiel distance 09:11 To this mixture and that's what we use instead of the mixture. 09:15 And and we see this as a special case of a rational approach and, in particular, you could 09:21 In the East step of em, you could replace the, the men respect to all this sufficient to only with a subset of distribution. And then what you get is called virginal ear, it will get back to that later. 09:35 And Dora is asking if we said Q AMP z to be the posterior will get Caleb zero. How does this relate to your earlier point of TZ being a constant. 09:46 Yeah. So what I said earlier, is that the in the Jensen's inequality, where we did 09:53 That go back. So here I flipped the log and the expectation by using Jensen inequality. And that's why we have an inequality norms and saying is that this will actually be equal if this thing here that I'm thinking the expectation. Oops. 10:13 If this thing I'm thinking, the expectation is a constant. 10:16 So it's not QC, which is a concept. It's the ratio of p the joint divided by QC and if this is a constant that means that Qc is actually proportional 10:24 To p of x givens, and z, which means that Qc is the posterior so that was the argument. The thing about the constant was to be able to prove that the to make the kill zero or to have actually equality, which we can see it also from making the halo equals zero. We said cutud posterior 10:50 So Hattie is asking that we are free ourselves to choose what's the distribution or form for 10:58 What we don't choose what's p AMP z, given X. Normally, because in our case, we have p of x, given z and P AMP z, which implicitly defined P AMP z, given X. And so we, she said, Well, can we just choose one is easy to compute 11:12 Yes, and parts good or we'll see when we talk about conjugate priors later are in a class that that's a way to make sure that it puts theory is easy to compute 11:22 But it doesn't mean that it's a good model of the actual data. So it's not because something is easy to compute that it's a good model of the data. So there are constraints in terms of like 11:32 Ease of in France, ease of computation, but perhaps it's not capturing the complexity of the data with a more complex model, but then you can compute. Exactly. And you need to be approximation. And we'll see a lot of examples of that. 11:50 Okay. So, last point about em. 11:54 Is that I'll let me see. There was some question in the pack global 12:01 rate of convergence. Yeah. Okay, so that's both the question about GD versus em and rate of convergence. So basically, 12:12 Em is a blood cornet method. Okay, so let me show you an example of level set of a function where the blood current method is not fast. So this is cute. And this is data. And what happened is 12:29 When you do exact date you only move parallel to the axis. So, this case you'll go there. 12:37 You go there, there, there, there, there, there, and you'll, you'll just make very small steps until you know you get close to the optimum. 12:45 So when you have these kind of elongated ellipse like this, which are kind of like 45 degrees with the access because you can only move pal to the access in the corner method. 12:55 It might be faster to fold the gradient or perhaps a second order method. In this case, like if you use you turns on this objective, it might, you know, follow the new bike move instead 13:08 You know, I guess. Here the gradient wouldn't be too bad. 13:13 Whereas my temporary. Yeah, so the gradient would probably do something like this. So it's still a bit slow but faster than the current methods. And then if you use like a Newton method, you might be able to move like 13:24 In this direction so much faster. 13:27 And so now there are convergence rates for em and for grading methods. It's actually a fairly novel feel because em kind of comes from statistics and statistics didn't really do that much convergence rate for monetization perspective think there's a nice paper by Mark Schmidt. 13:46 Analyzed em. 13:48 Recently on and it's convergence rate. 13:52 And so some so so here the difference will be how many steps you need for to get close to a century point and then there's a difference in terms of 14:02 How expensive is each iteration. Right. So the nice thing is the M is often the updates are closed form. So it's very cheap to compute, whereas 14:12 The the the grid method or hesitant needs to perhaps to compute their large matrix. 14:20 And so, so perhaps it would take less iterations of Newton's or question Newton method for example, but each iteration might be more expensive because the EM update or close form which are very fast. And so it's not super it's not simple to compare the two. 14:37 So it's more like an empirical question of like, well try both and see which one works best or see what other people have tried before. But for exponential family model. Often the M works nicely. 14:49 And the Gaussian Mixture Model is there is an example of that. 14:54 Is there any other question about em. So now the plan is to apply this on the Gaussian mixture models, it will make things much more concrete. 15:08 Dora asked. Oh, why do most people use GD then well 15:12 Defined most and people who are you talking about, right. 15:16 So if you're in statistics. Most people use em. If you're in deep learning. Most people don't know anything about statistics and they just use gradient descent because they don't know better. So 15:27 It depends on the community, you are, and also depends what you work with 15:31 Right. So if you're working with her no network. There's not even a journal isn't even a plastic model behind that really like the, the, the, the last layer of your network being a 15:41 cross entropy loss is just a way to get the kind of the nice function to minimize it's not really problematic model. 15:52 No, this was still a good question. Right. So I think it's, it's very valid question. There's a question, though, of where do you get your, your, your, your impressions. But it is true. So what I could say is that 10 or 15 years ago. 16:10 People would use a lot less green descent for these kind of methods because there was these nice, kind of like close form updates with em. And also, I would say that 16:22 One of the big 16:24 Change from deep learning was also just having a lot of libraries available. So, you know, competing the gradient through 16:33 You know, a competition graph you can automate those that very easily. So it's easy to just like oh I write down my model and then put 16:41 The grades competed exactly automatically for me, whereas computing the, the, the, er, the end step in the arbitrary model will you will need either to use a library, which does that which existed probably stick programming. 16:53 But they're not as easy as standard. And so that's why I think a lot of people now also just using GD because they have libraries to do that. 17:02 So I think that's the kind of a big advantage. user avatar  17:05 Of the computer science approach to Mission ready user avatar  17:11 Okay, so let's start the GMM model. 17:17 For 17:19 Let's apply em on the GMM 17:22 The guests are the Gaussian mixture. 17:26 Gaussian Mixture Model, I guess, Jimmy model is a bit of a 17:31 Logical that when you repeat things 17:35 To and as 17:37 I think 17:41 Alright, so 17:43 The first thing is 17:45 We have a latent variable model. So as I mentioned, we have 17:50 Our observation which will be the X i and then we have the data variables and I and we have and observations and so the model for that for the GMM was that, said I. 18:05 Was a move to New he with parameter pay 18:09 And x 18:15 X i givens that I 18:19 Equals Jake 18:21 Was a Gaussian with me new Jay and covariance sigma j 18:30 Dan, and when I say is that I equals, Jay. This is a shorthand notation. 18:39 To say that said i j equals one, right, because remember that I have encoded. The key choices of my latent variable zero dy. 18:52 With one hot encoding right so it's it's actually a vector and dimension k. Now, when that says that i equals jam just mean that the, the one hot is in dimension G. 19:02 Which means I chose the cake, the JS component which is why, then, my observation will use the mean and the covariance of my faith component 19:12 So the, the parameter in this model. There's a lot of parameters. Right. So data is actually pi. It has the means for all the component, and it has the conference's 19:27 For all the components. 19:31 And also I will use 19:36 For em. You remember there was this generic x and z. And so the notation here will be that x is actually the concatenation of all my observation and z the same thing as a concatenation of all my little variable. 20:05 Now I would like 20:07 To 20:10 Highlight the simple fact of when you have independent distribution. 20:16 So basically, 20:20 From this model, you can easily show that the whole posterior z given x 20:29 Is actually equal to the product over I have p have said i given x, which is equal to the product over I have the conditional have said, I give an excellent. So in some sense, 20:46 Because I had this independence between my my my my different eyes when I compute the the the conditional in the other direction, nothing changes. I also have independent conditional user avatar  20:59 Okay. user avatar  21:01 And and this and this part. And by the way, 21:08 So this thing here is equal actually to this thing. 21:13 Which means that x i is so, said I, is 21:19 Condition independent of all the X jays given xi, which makes sense because there's no link here between, said I, and the other exciting. So this is kind of a simple 21:35 Consequence of our graphical model or independence assumption. So it's, it's pretty easy to show that 21:44 So I'll just leave it as an exercise. 21:49 So exercise to the reader. 21:52 So how would you prove that. Well, you would just first so 21:57 You could compute how to prove that I suggest that you compute this marginals, so p of that I given X. What it means is I take my whole conditional and I sum of all, does that JS or Jane of equal to I 22:11 Can I get the marginal and then you'll see that the marginal will basically just be this thing here. 22:18 And so then 22:24 He know you will see that the whole joint separate as this product of these marginals 22:34 So yeah alright so now that's computer computer left field and I'll get back to this fact because it will come back very soon, which is why I'm mentioning it now, but that's compute the complete look like us, because that's what we'll need in EM 22:51 Like the 22:56 Alright, so the lug 22:58 Of p of x, z theta. So this is my whole joint over all the x's and all disease by the independence. This will just be some over i. 23:10 Wanted to end of log of p of x i givens that I say data. 23:21 And then plus 23:24 lug of P have said i given data. 23:31 Well with Bamber theater. 23:37 And so now in our model we have that these are options. 23:43 And these are multi new you. user avatar  23:48 Mean the new user avatar  23:53 And so let's write it down explicitly 23:56 I STILL GET MY some over all my training examples. 24:01 And then I can have. I could choose the correct component by summing over my little variables. So Jay one up to k have said i j 24:13 And then I have loved 24:15 Have p 24:18 Sorry, the logo of the Gaussian. It's right it explicitly. So this is a normal on the variable x i and then the parameter or new j and signature. 24:34 So only one of those that i j is equal to one. So that selects the correct component. According to said because we were conditioning mindset. In this case, right. So that's why when we know z we can pick the correct component. And same thing for the 24:52 The distribution over z i have summation over k here that i j and then lug of pie. G. So, by we're giving my distribution for the middle 25:07 OK, so now you see 25:11 The, the Zed appearance in the company like you. And so when we compute in the East step it's already in the M step we compute the expected complete like like you 25:24 Expectation respect to queue up the log like you 25:30 Then bye linearity over the expectation, what I get is just the same. Some want up to in some Jake was one of two key and then I get expectation respective Q of Zed i j 25:49 And then I get my lug of normal x i you Jay sigma j plus log of pages. 26:05 And so when I mentioned earlier that often during the M. A. What did I say that, blah, blah, blah. 26:16 Yeah, so in the M step. 26:21 I say you compute the expected completely likelihood. And then you maximize to find the next data. And I said, often when you do the expected computer act like you, you will replace z with the expectation of Q AMP z which is what we had here is it. Oh, did I yeah 26:40 Right so so disease that we didn't know for the marginal aspect we replaced them with the expectation expect to cure that i j that i j is a variable which is 01 so what I get here. 26:55 When you take an expectation of binary variable, all you get is the probability that the variable is equal to one, right. So expectation of respect to Q have said, Jay. This is just equal to the product team that Q gives that said i j is equal to one. 27:14 And Q isn't sufficient over all the Zed sites, not just one. So when we write Q of that I Jake was one. This is a marginal 27:22 So this is a marginal distribution. 27:32 But they buy the property that I mentioned here. 27:38 That the joint is just a product of these conditions. So it's clear to see that when I will some to compute the marginal I'll get at the marginal is just this probably t here. So the marginal 27:55 Oh, sorry. So this is not well first of all I need to use 28:00 invisible ink. 28:03 And 28:06 During em. 28:10 So during em. 28:14 Will have that Q 28:18 T plus one. 28:20 Of z is equal to the probability 28:26 over z given x 28:30 At theta t user avatar  28:32 Okay. user avatar  28:36 And and so that's the part where 28:40 I will if I want to compute the marginal 28:43 Of this queue. It was the marginal of the conditional PFC given x which is simply the 28:51 The conditional and said, I 28:54 And so 28:56 I will use the annotation here this Q 29:00 During the EMR rhythm. I'll call them weights. I use annotation Tao I G or the index for my variable and it depends on the iterations are called tie Tao Te and by definition this would be 29:17 The probability that said i j equals one given X i activity and this is equal to two t plus one said i j equals one. user avatar  29:33 Right, which is user avatar  29:36 This thing I mentioned here, this is the marginal and the marginal 29:42 In em. I said it to the not the joint acid into the conditional, but then when I marginalized this joint by the property. I mentioned earlier, I can just look at the conditional on on the individual variable experience and 29:55 That's because of the independence between my data point, if there's some time dependence or relationship between my data points. I couldn't just read this and you'll see that when we talk about em in the aid hidden Markov model later it will be a bit more complicated. These, these updates. 30:13 Okay. And so to compute the, the, the expected companies like, like you said, I need to compute these weights Tao I GT to compute these weights, I need to compute these conditions. And so the East step is basic computing this conditional. Right. And so the first step. 30:32 So the first step. 30:35 In em is computing these conditional 30:46 Qt cutie plus one. 30:55 Well, okay, so you could compute the whole joint. As I mentioned before, 31:00 Z given x at data t 31:06 But by the property. I mentioned before, this is just the product over I have p of that I given x i data. 31:19 And so the marginal 31:23 On, said I. 31:25 Is just the conditional that I given xi, which is proportional to the joint on excitement that I so this is a product of x i givens that I 31:38 stayed a tee times probably the opposite. I given data t 31:45 This tells you 31:49 This comes from the 31:53 The previous value of pie that you use. 31:56 This is a Gaussian 32:01 And so, what you get is that the the weight. 32:07 That you use in EM i j t 32:12 Is equal to cutie plus one is that i j equals one. And so this is just a normalization of these products of pie with a Gaussian. And so it's basically by Jay 32:29 T 32:31 Normal evaluated x i with parameter you Jay at iteration t and sigma j at iteration t 32:41 RI normalized right so you son. So let's call this summation over l. 32:47 over k p by El tee captions X I evaluated with mean you L of tea and sigma L. 33:02 And so this thing here is the joint of x i and then said i j being close to one. 33:11 Given my parameter set it. 33:14 And this is my marginals p of x i given parameter theta. 33:19 So when I divide by each other. I get the 33:26 The conditions. 33:30 Okay, so that's the step you just basically compute 33:35 For each possible values of Z's. 33:40 Was the conditional of this value of z given x, which is basically proportional to a product of the prior of this component times the Goshen evaluate an ex I 33:52 With the mean of this component 34:03 And so 34:06 To recap, so the East step. 34:11 For GMM 34:14 You compute 34:18 Tao i j. Oops, I'm having issues with my writing. 34:24 So i j 34:27 T 34:29 For all the training examples for I was one up to n. 34:34 Using the previous value of the parameter. Thanks. So using data to 34:40 And then in the M step. 34:45 You will maximize with respect to the parameters. So you J. 34:52 sigma j and j 34:55 The expected completely like you, which I wrote above it was submission over I summation of OJ of these soft wait staff counts, so i j  35:11 And then I had the log 35:14 Of p of x given 35:19 Parameter mute mute mute j and sigma j 35:25 plus log of Baidu 35:29 So, by the way, this is very similar to the luckier you get in Fisher in earnest come in and assets. When you do the maximum like you would approach and so 35:42 It's an exercise because you already did in some sense to you in your previous assignment, but it's an exercise that then the parameters are have very simple clothes for updates. So the maximum likelihood parameter for pie. 35:59 At iteration t plus one. 36:02 Is just the proportion of mass that you've assigned to cluster G. Right. So it's the summation over i have these weights, I, J. P. 36:15 Divided by n. Right. So in some sense, these Tao are like the soft count. So normally when you do the 36:23 The perimeter of date and Fisher and leaner and semen analysis, what you have is that the update pie with the proportion of time you've observed 36:31 A specific class. So it's zero and one. Right, but now instead of reviving counts. We have actually the, the probability of these the variable being equal to one. So it's like a soft version of the cast 36:43 Just why I'm saying when you do em in negotiation model. It's like filling in the missing value with soft count value. Where does the softness comes from the quality of z given x 36:55 Well, these are stuff counts. 37:00 And similarly, now the new means for a company components will be the empirical mean of the vector where you use a weight. So it's the sum of all the training example and then you use a weighted sum. 37:19 Of each data point where the way that you use is actually the quality of that I 37:24 And so some over excited and then you divide by the mass that is this clusters. It's summation over i want to end of tau i g 37:37 And finally we have the covariance sigma j 37:43 Hat t plus one. 37:46 And then I get the empirical covariance where I relate again. Each of the entry. So we'll have a towel i j t here, then I have my standard covariance x minus new j hat t plus one. 38:05 And then x minus view Jay had t plus one. user avatar  38:12 Principles. user avatar  38:13 And I read normalize by the mass in this thing, which is summation of her I 38:19 Have to i, j 38:23 So these are the M step updates of a Gaussian Mixture Model in the East step I completed the Tao, and the M step I just update my parameter, according to these Tao, and the data. 38:37 So this is the M step. 38:42 For em. 38:46 For a Gaussian Mixture Model. And in the third assignment you will be basically implementing this algorithm. 39:00 Alright, so first of all, 39:02 I'm almost done. So let me talk about how to initialize because you need to initialize these parameters to run em. 39:12 So a standard example to initiate to have good initialization is you will get them you j zero by running key mean on your data. 39:22 Where a key is the number of 39:25 Components using your mixture model or an incoming dust. Plus, so that will find a nice, kind of like mean to set to your data and then the sigma j zero, you will use a big spherical 39:43 Covariance 39:47 So that's helps you to not be stuck in very low and small local minimum. So, you will use sigma square identity, where a sigma square is big. 40:00 And then the pies. 40:03 You could initialize them with the proportions. 40:08 Of points which when you run came in class best which fail in one cluster. 40:22 And as I mentioned before, so if you do em step in a Gaussian Mixture Model. 40:32 With 40:34 A fixed covariance 40:39 Which is just sigma square identity. 40:42 So you don't actually model the currency, just use it for variances and you let the width, the variance goes to zero. 40:54 You get the Cayman algorithm. 41:01 That's, that's what I'm saying. Key mean can be seen as the limit of em in a Gaussian Mixture Model when the variance goes to zero. And so to see this. You can look back at the updates. 41:15 So, 41:18 If 41:22 If I use 41:24 Very 41:26 Very narrow variance, which is the same thing as having a very precise direction. So it means that 41:35 Unless I'm very, very close. I have almost zero quality and what will happen is that when you compare the when you will compute this counts, you will basically get zero for all components we unless the one which are the closes, I guess. 41:57 The way you could show that is you could factor eyes in this expression. 42:04 Because I guess. Okay, so we can actually write it down here. So this here, what you get if I divide up in the bottom with this I get one divided by one plus summation of l not equal to j. 42:20 Of pi l normal x i. 42:29 You L sigma L. 42:33 And the thing is, this thing is x 42:37 Minus norm of x i. 42:42 That's this one. 42:47 X minus new el transpose sigma x minus one x minus new now. 42:57 There's some constant 42:59 Divided by 43:01 Two right 43:04 Alright, so I'm not going into details, but I'm just trying to give you the intuition. Now the problem is. All right, so if if I use the identity covariance. All I get is the norm between xi and UL and then they will have a sigma minus two. 43:20 And so when sigma is very close to zero, this will really blow up to 43:30 To a high value. 43:34 Oh, I forgot. There was a ratio. Okay, so 43:40 I get here, I would have 43:44 divided by pi j and blah, blah, blah, with you, Jay. Sigma G. OK. So this is kind of a mess here. Now, but the idea is, I will have the ratio of the distance 44:02 Perhaps I should have not tried to do that here. But basically, I will have the distance to L versus the distance to Jay and the one which is the closest will dominate and so 44:15 The result of this is that this data is at when sigma goes to zero will be equal to one for the mean, which is the closest to your point, and zero for all the other means. Okay. 44:29 And so 44:32 When you do em. 44:35 For a Gaussian Mixture Model. 44:38 Where you use 44:40 A fixed covariance, which is sigma square I with a very tiny covariance, very tiny, tiny variance, you get that the Tao, or basically zero and one. So there are hard cluster assignment like you doing Kimmy. 44:55 And then because these will just become zero. And once they mean update will just become the standard gaming update user avatar  45:03 Okay. user avatar  45:07 And as I mentioned during Kimmy. 45:09 Kimmy is like, assuming that all the cluster or spherical. So that's why it's the identity covariance 45:15 The nice advantage of gushing mixture model versus K mean is you will learn also the the cover and structure of the data. And so you will allow also to get cluster which are not spherical because you're learning this this conference. 45:29 So is there any question. 45:47 Yeah, so sorry attack is pointing out that if you do great descent in a Gaussian Mixture Model. You cannot just do green descent. 45:53 Because you need to make sure that your parameters are valid parameters. So, in particular the covariance matrix should stay positive. 46:01 strictly positive difference. So it's actually a constrained optimization problems you have to be careful if you do gradient updates was em here it's closed form updates, which or actually this case satisfied, a constraint. So it's not a problem. 46:16 So yeah, so doing grand descent on a Gaussian Mixture Model is non trivial. In this case, 46:22 You would need to do something which is called projected graded. 46:28 Any other question. 46:43 Oh, I guess I stopped. They stopped sharing. I didn't want to do it. 46:51 Sorry about that. I wanted to stop recording 46:57 So he's asking if came in, plus, plus. 47:01 Guarantee ensure conversions to a global max where the algorithm know 47:07 Gaming plus plus is just a way to get a good good initialization. But there's no theoretical convergence guarantees that you will get the global max for Kimmy Kimmy plus plus give you give you with high quality and approximation quality of the 47:22 Of the local max that you find it will be within luck K of the global max. But this doesn't really translate directly to the GMM 47:31 Global max. So this is more like a Eucharistic 47:40 So sorry fact mentioned that 47:43 Is that a problem that permutation of the parameters lead to the same likelihood 47:46 Well, it's kind of a problem. And it's also Why indeed like you get a non concave optimization, because you can just like 47:52 flipped the the the identity of by one and part two, and then new one in YouTube. And it's the exact same model is just I flipped the 48:02 The name of which cluster. I'm talking about. And so there are actually k factorial local max for sure in this problem. And so that's the problem. 48:12 Of latent variable model and it gives you kind of like optimization, kind of like issues, but we don't care because we don't care what's the identity of their cluster in this case. 48:23 I call this cluster one and this duster to have is versa doesn't change anything. It's the same model. So as you know, whichever yet to converge to is fine, but it will give kind of indeed optimization issues. 48:38 Okay, well I let me stop the recording. user avatar  48:42 And user avatar  48:44 Stop recording so
  Ok recording as started  So today we'll finally talk about directed graphical model, which is part of the title of the class.  So basically we have a bit of class. So basically in the last few lectures I talked about simple two nodes graphical model.  Sorry about that, and particular we did regression. And then we also talked about latent variable model where we had this mixture of Gaussian. So now we will talk about more general graphical model structure, in particular in preparation for the hmm structure where we have a time dependence.  And so the menu will first to do a bit of  Graph theory, a bit of review on graph theory because this will be important for  The properties of graphical model and then I will finally define directed graphic model in and their properties. So today we'll start with a bit of graph theory review.  Or if it's the first time you see it, you will learn that  And then talk about Derek a graphical mall.  So,  So again,  Terms of big picture.  I mentioned that  The idea of a graphical model is to combine  So a graphical model.  Is a way, it's basically combining property theory.  You know, I have a problem with my writing. This was problem.  Here we go, primacy theory and computer science.  And so from property theory basically were talking about modeling the distribution of random variables of multi dimensional data. So we have a collection of random variables are just big random vector and we have each of these components is random variables.  And from computer science, who basic talk about will use  Notions from graph theory to be able to efficiently represent these distributions and menu played them using you know concepts from that that structures for computer science and graph theoretical notions such as  traversing a graph or passing messages on the graph.  Okay. And so the idea of the graph here is to provide an efficient that that structure.  To handle these huge  Complicated distributions.  And in particular with the graphical model, we can deal in a unified fashion.  A lot of different distributions. So, so from a coding perspective, it's very useful to have this uniform unified interface and way to deal with like pro stick influence  And to give you an example of what we mean by efficient that structure. Suppose, again, that I have n random variables.  Which are binary right let's say x i is 01  And let's say n is basically in the hundreds  And so  I don't have coven I just met with Legos.  And so, if n is a hundreds  Then the number of possible joint assignment to my end variables is to to the hundred  And so  To specify in general distribution over these these hundred random variables. I need 200 to two day race to the hundred numbers.  In a table. And so this is intractable.  And so a concrete example of that is in this queue, Mr model that I mentioned I think it'd be in their classes and motivation, where some of these random variable represent the presence or absence of some diseases.  And the you will then observed  The variable at the bottom, which are the symptoms.  And then I have this complicated graph structure with could say that. Oh, if you have. I don't know. Care act as your disease, then some stuff happens through your eyes and this is encoded in this  In this basically graph structure here.  And so we'll see when we define our diapered graphical model how this graph structure translates to assumptions about the distributions, which can be exploited for more efficient.  And friends as well as statistics. So learning  And what does camera stand forward. I think it's I  Think the M is for medical and the q&a are. I don't remember actually. So it's Google it.  It's for fun, because I forgot.  To Mr. user avatar   Symptoms. user avatar   Oh, quick medical reference Olga.  I guess that's what because there's a queue, Mr DT Beijing network with 4000  Symptoms and 500 disease.  Yeah, I guess it's quick medical reference. So the idea there is that this network will be encoding prior information which has been gathered from like  Experts and medical know legend. So you encoded in this network. And then you can use it to do a conference on. Oh, I have observed the symptoms, what are the most likely diseases which could have caused that  Alright, so let's start. So that's the motivation for  Having these data structures. So let's do a bit of a graph theory review.  Going by the way the queue, Mr thing is also something where, actually, it turns out that doing in France in these graph is intractable exactly in France. So you need to do approximate in France.  And popular divisional approach that I've briefly mentioned in the past was a very efficient way to do it. And that's something for which Mike Jordan was known for in the 90s.  Like, is he was a leading figure in developing virtual method methods and one very important application was to be using these like big medical networks.  And so when we talk about version on France. We'll get back to that.  Okay, so  Let's define a directed graph.  Because that will be playing with these lot  So are directed graph.  Is also known as a di graph.  It's basically a set to know that and directed edges might. So here's an example I could have  Say 1234 and five, then I have edges.  Between my nodes and these edges or directed and so graph will use G to talk about our graph in this class. Some of these will be under Ted, some will be directed will still use G. And so the context would tell us if we're talking about a directed graph or not.  And then  It's represented as a couple of  V where v is the set of nodes.  For example, it could be one to end or one to d and these are called nodes or vertices  That's why I called the Vertex. Vertex  And he is a set of edges and it's actually a subset of the couples on the nodes. And so these are called directed edges.  And so, for example, this edge here.  I call he went to was Apple like he went through here, which is the directed edge from one to two is represented as  The double one two  Okay, and directed path.  I mean issues with my colors.  directed path in a in a graph is basically a sequencer is sequence of edges, such that the, the head of the edge is the  Base of the next edge. Right. So for example, I could ever a path from one to four in this graph, and I'll use the squiggly line to say it's a path from one to four.  And so it's it's basically a sequence of edges which are compatible in the graph. So this is for example.  Either the edge one to two and then one, two for the two to four, for example. So that creates a path or I could also have a different path which goes to the same node. So I could have one, three and then three, for example.  Okay. And so basically a direct path is a compatible.  Sequence compared the bull.  Comfortable I  Compatible  Sequence of edges.  And there's a set of nodes that we will use a lot in this class, which are the parents of a node so  Pi i and that sanitation, who will use it as a social convention, so pi is basically the set of parents have a Nolan a graph. So it's the set by definition. It's the set of nodes in my vertex set such that there exists an edge from this node to I, in my graph right so in my edge.  And so this is the set of parents  Of I  And so pie because of the Greek letter P for parents.  And so in this graph, for example, the parents of four would be those two notes. So, these, this would be pi four, which are the parents have node four  And then the children.  So there's like a parent child relationship. So the children are the  The nodes for which so children.  Have I  We won't have a special letter for the set  It's the same thing. Now, but in the other direction. Right. So it would be j in the such that there exists a know an edge from I 2G  In my edge sit  Okay.  So just setting up the notation.  So that's a directed graph. So what about the undirected graph.  So the undirected graph.  And under graphing  J Ji.  Ji Ji.  Ji Ji Ji. Yes. So it's also a couple v. So we use the same limitation. This all the their friends with the diagram is that the set of edges are not ordered topple they're just  Set of sites to they're basically two sets. So the elements where the elements  Of he  Are two sets.  IE set of two elements.  Okay, so does  We actually represent them not with a couple because a couple is ordered  We have, we use the notations curly brace because it's a set like i j  Is actually the same set as Jay i. So the way I the way I order my element in a set, doesn't matter because the set is not ordered  Versus I have that i j is not equal to Jay I when you think about the tunnel. So the order matters. user avatar   In a couple user avatar   Okay, and as an aside,  In this class we will never have self loops in our graph. And so the size of the, of the, the an edge will always be true. So there is no so there's no self loop.  In this class.  So I III. The size of any edge is equal to do so if you want to sell flute. You could present with a singleton edge like I that for present and  Nudge to itself.  And  So this is allowed and he is asking if we yellow cycles totally in in the undirected graphs, we're fine with cycles in a directed graph. We can also have cycle in general, but we'll see that  To define a directed graph model, we will have to have a second graph, which is my next definition, but let me finish the undergrad graph.  So let's do a little drawing similar to the one we had before so I will have 1234, for example, and now the edges don't have arrows, because they're undirected.  Yeah. And so in this case, I have a undirected path.  From two to three in this  In this  This graph. So it could be either this way, for example, or it could be this way. These are two paths which are possible.  And  Now because there's no direction in the relationship. We're not talking about parents or children anymore we're only talking about neighbors. So neighbors is a kind of like a symmetric relationship and so  Basically this the notion of neighbors.  will replace  The  Parents  Children.  Relationship from a diet graph.  And so for example before we had in in in the directed graph, we had the parents of four.  And we could have thought also about see the children of one, which are basically these two nodes here, we're not we're not talking about the parents of four will talk about the neighbors of four. And it's the same set as above, so two and three here. This is the neighbors.  Of node for  Or one for that matter. Can we could use annotation and I for  Neighbors, have I  Alright, so by definition, and I will be the set J such that in the  Sense that  There is a i j  In the butt here. There's no direction. So it doesn't matter if I put it first or second  So if critic is asking if probably Stickley does a self loop represent something different in a regular node. And I said, we don't allow self loop so they don't mean anything and interactive graphical model world.  Oh, okay. So Brendan is asking  Interesting question. Can we think of an underrated graph as a directed graph where the parents are the same as the children, ie, where every edge as a mirror image.  Well, in some sense, yes.  Like if you replace because in the sense like  In under the graph, the path or have no direction. But if if I if every edge is splitting in two edges in both direction, then I can replace my under at a pass in an under the graph which is directed path and they're all valid. They're all the same some sense.  So I think that's fine. I'm not sure. So for from an implementation perspective, it could make sense. I'm not sure if all properties are the same feels that would be careful with it.  But in this class. When we talk about underwriting graph, we'll just talk about this, these, uh,  These kind of set edges.  To not have to worry about direction.  Alright, so now  Somebody asked about, oh, can we have cycles. So let's talk about now as you click the  Directed graph. So let's define a DAG.  This is a directed a cyclic graph.  Is it a cyclic or a cyclic  I guess that's one, perhaps, where you have different user avatar Jacob Louis Hoover  It's not user avatar   Doesn't depend  So we went to you say it was taken. user avatar Loren Lugosch  As cyclic user avatar   And there is a cyclic user avatar   Okay. user avatar   Is there a consensus among the English speakers are actually you can say both. It depends on your, what you want to say.  Okay, some people say both. LIKE A. That's what I remember. But anyway, I'll say i a cyclic a cyclic. I think I forgot which one I was saying for it, but I'll keep saying whatever comes to my mind directed a basically graph.  It's a bag. So this is actually a directed graph. So I die graph.  With no cycles.  And the cycle is  directed path which comes back to the same node. Okay.  And this will be essential for defining or directed the graphical mode.  And something which is important, which comes with a bag is the notion of  Logical ordering  So,  An ordering  So basically, I'll take my set of nodes and I'll order them in a specific  Way.  One up to V  Is said  To be logical.  Paula Jake call for G.  If and only if  The nodes.  In the parent of I  Appears  Appear before  I  In the ordering  And this is true for all high okay i III. Suppose  Jay,  was apparent of I  We need to have that in your order we have put J.  Before  I  Okay, so basically I is mapping all the nodes in a in an order from one to the size of v. So, it's called the end user avatar   It's indefensible to end by user avatar   We will always have, and sometimes will become D or sometimes there's other letters which will represent the number of variables but for now it's us and  And the idea is you order all your notes, such that  The children appear after the parents. Okay, which means that because the arrows goes from parents to children. It means that if you order the nodes.  You know, from left to right. That said, This is my top logical ordering all the arrows actually goes to the right.  And here I have it turns out that in this first graph that I put here 1234. This was already in topological order to the 1234 was in topology corridor.  Again, and these are the here. I've just drawn the the edges. And you can see that all the arrows are always going to the right.  Okay, so, so when you have a topological ordering. So one, one way to quickly see if you ever to political ordering is whoops.  Is that there is no back edge.  Or during  All edges.  Go from left to right.  So there is no  back edge.  Okay.  So why do we care. Well, so we'll see that will prove some properties for DR graphical model, which will use this property in the fact that when we  Go down through the list of nodes. For example, let's say we start to do stuff here. We start to do stuff there, then we're sure that  We won't have a little edge going back like this, which could create some weird dependencies, or make things simpler and screw up or or arguments. Okay, so that's why it's actually quite helpful to have this order, we get a bit of a handle on the, the, the parents child relationship.  And then an important proposition.  Is that it is for. If you have a graph, a directed graph G.  Then it is a bag.  A DAG if and only if there exists a topological ordering  Polo.  Polo Jay call ordering on Jay  And so intuitively it makes sense because  If I have a cycle, then there's no way I can have it to political ordering because the cycle will have an edge going like this and then they will also have an edge coming back.  So let me do the proof because it's very simple and gives you a bit of  Practice with proofs. So the implication from the fact that if there's a topic called ordering, then it's a direct the doctor graphical Margo, it's basically trivial. It's what I just explained so because there is no back edge in your political ordering  There's no way you can create a cycle. So there's no cycle.  Because to come back to a node. You'll have to, let's say you follow the path. So you'll go to the right. And then at some point you have to come back soon. They have to have an  Edge, which goes the other direction in that contradicts the property of a topological order.  Now let's do the other application, which is that  If I have a DAG, then I can construct the political ordering. So how do you do it well. So the way to do it is you use the depth first search algorithm.  On the graph to kind of like you start anywhere. And then you go as deep as you can.  And then what you do is you  To label.  Nodes.  In decreasing  Order.  When  Have no children.  So basically the idea is let's say there's a there's a graph. For example, let's let's do it like this.  Use a random graph.  Okay and I start somewhere. So I'd say start here.  And then I do that for search. What does it mean is, you, you, you, you go through all your, your, your, your children, but you  You, you, you rehearse first by going as deep as possible. So you would stay choose this one, then you would choose this one and then you would be stuck because there's no children. Okay.  And so because this node has no children. I can put it at the last of at the end of my ordering because it has no  Edge. So there's definitely no back edge. Right. And so I'll say, and I have five nodes us okay this is five.  Okay.  And then I go back in my depth for search algorithm. And each time. I'm only looking at nodes which has been visited. So it's like if I had no children. So for example, this node here.  In this case, because I already labeled it's child. It's also has no children. So I could now put the next one I could say this is for  And then I'm not allowed to say this is three because this one has another children. So then I need to keep rehearsing my debt for search first and then visit all its children. So let's do that.  And so now I would, for example, call  Visiting the children of the, the, the, the second child of this node.  Oops. This was for us it  And now I get a node.  So this node here.  Because this one has already been visited. I don't have to keep going. So it doesn't. It has no children. So now I'm allowed to give the next available number. So, this will be come through.  And then the last note which I have not visited  Well, okay. So first of all, now  This could be too because all its children has already been visited  And then I have my last node, which I could start again from because I didn't know. By the way, I said, Start somewhere because you don't know which one is the route necessarily  So, but, yeah. So here's by doing that first search on this thing. I basically constructed the logical order.  And by construction, because I always only put node, which they don't have any children, which I've already been visited then I won't have any edge going to the left. So I constructed at the top political sort  Okay, so, so you can do this to construct  A sort  And this is in  Order size of the number of edges blahs number of nodes. So that's the running time of depth for search for this.  And the reason you can continue doing this algorithm is because there's no cycle because no, sorry.  I had to political ordering. No, this is I had a dad. Yeah. And so you know that you will always be able to have no back edge by the property. Again, that there's no cycle. So it's all circular  Okay, so I mean that was a bit of a fast proof. It's normally would be a bit more detail, but this is just to give you the idea  So is there any question about  This these simple graph concepts. user avatar   OK. user avatar   So now the plan is to introduce a bit of notation.  And then we'll talk again about  And then finally we'll define what a diagram graphical model. So let's do a bit of meditation.  Meditation.  For graphical model.  All right.  So some people ask about why did they use that for so that that first search idea was to find leaf very quickly and then once you find a leaf. I need somebody with which don't have any children, then you can label them at the end of your order.  Could you do it in the other direction. I don't think so, though it could be the algorithm. I know is you do that first search to do that to find the top logical sorting.  But if you want to know more. I recommend that you can just look the political sort for in Wikipedia, for example, they will tell you more about this algorithm. It's pretty standard for  A graph algorithm.  Alright, so  Alright, so first of all, we will use  So suppose we have an discrete random variable.  X one lab law to excel.  And  For a lot of ours, the stuff will do for graphical model, we will assume  discrete random variable for simplicity  Even though. Yes. A lot of these concept applies on continuous distributions.  But  For getting the simplest result first will use with think of discrete random variable. And the reason is discuss conditioning for continuous random variable is super complicated. So the conditional distribution.  Concept is actually very tricky to formalize  So right now in this class. I kind of swept this on the direct by just saying, Oh, we will define some kind of conditional density, we will have distributions, where we have these conditional then cities.  But if you take a formal protein class, you will learn to actually the conditional distribution is it's not a  Simplistic quantity. It's actually a random variable. Everything is a random variable. So conditional expectation is a random variable. All these things are random variable.  And and they're only defined almost surely so it gets things a bit more tricky. And if you're curious about that. I will say, see the borrowed call marggraf paradox.  Isn't the combo GIRL, GIRL, GIRL como growth.  Paradox.  On Wikipedia, which will  Yeah, so the Broadcom or grab that paradox on Wikipedia, which tells you  A bit where this trickiness come from. So this is basically the paradox tell says like, oh, if I want to define say have a uniform distribution in a sphere.  Okay, that's pretty simple to think about. Now I want to condition. I want to look at what's the conditional distribution in the sphere given I'm on some great circle.  So, oh, I'm I know I'm on a great circle, which is an event of properties zero on first thing that's already a problem. So you want to condition on the vendor for the zero and then I wanted to find out what's my conditional distribution.  And then depending on how you split your space, you'll get different answers and the splitting the space is coming from these boreal sigma algebra, which are the sigma field. Sorry, which tells you how you will want to define  What, what kind of policy statement, you want to make and your and there's different ways you can do it in this case.  Some people are making jokes about the order of the names  Yeah, so if you're in Russia. It's probably the Komodo have borrowed paradox. But Wikipedia in English. It's broken marggraf  And so if you had no idea what I just talked about. Don't worry about it. And if you're curious. Just have a look at the article.  Though, but keep in mind that to really formalized these understand how to resolve these paradoxes is very tricky. Okay, so that's the unfortunate aspect of having these these continuous distribution and trying to condition on an event of  Property zero  But when you have a discrete random variable, you don't have to worry about that.  Okay, so basically v in our graphical model. This will be the set of vertices  And the thing is, in a graphical model.  We associate one random variable per node. Okay.  So there will be one.  Variable  Random variable.  per node.  Okay.  So the nodes in our graph will be associated with random variables. And then the edges. We were present potential dependencies between my random variable.  And then the joint.  Of my random variable. So like P of X one equals little x one, blah blah blah, p of x n equals his little accent.  So,  I could just write this as  P of little x one, blah, blah, blah. Too little accent.  Even though you know like the little x's. Normally, or just dummy variables right they do represent possible and sensation, but I could use  Z or V or a or I could use any variable I want, but because we want to use a lot of like  shorthand notation in this graph in this class because otherwise it takes forever. So the fact that we use little x one.  will mean that all this is the possible value of the capital X one random variable, not the capital X, for example, though sometimes if we talk about relationship between values we might have capital X two equals little x one, but usually  This will be the Convention, which is why that  The in this sense, when I write this quantity here, it's clear that little x one is the value for capital X one and it will extend is the value for capital X, then  And then what we do is we were allowed to also use  Sets as substrate. So we can rewrite this SP of  Little, little X, capital V as a subscript. Okay.  Because for as I think we are. I'm not sure if I already had defining the stats, but you've seen it in my book. So if I have a set for any subset of V  I can define the marginal  P of X A, so this is by definition the quality  That  Capital X is equal to little excited for i in a okay  And so, so this a year is a way to get a subset  Of sub scripts.  And so this is actually a marginal because if a is not all the V i have a subset of variables there. And so by this is the marginal. How do you get the marginal. Will you need to some overall X for the other possible values.  Of for my other variables which are not in it. See, so a compliment. So super script see  Of the joint. Right. So this is pls A x a compliment. Right. And this is something  Over all  Possible.  Values.  Of  X i.  For I in the compliment of it.  I guess I can rewrite it more explicitly as v minus a and this is what we mean by a compliment. Right, so V capital V is the universal universe set just from the context.  So notation wise. If I think of X subscript say the set one to four.  This is the same thing as thinking of X one, X two and X four I can expand my subscript like this.  And often will just use x for the old concatenation of my  Observation. So, x one up to n.  And us  Sometimes we'll just right p of x ray  Instead of X subscript capital these substrate capital V is kind of understood  Okay so Dora is asking, Isn't the marginal here a joint over the observations in eight though. Yes, correct.  Yes. So, so that's a interesting way to talk about it. So from the context of the joint over all the variables.  X A is a marginal right so I need to some. So this is a joint overall variables. I have the variables in a and the variables which are not in a that  That's all my variables and then something out all the values in x, a compliment. I'm just left with the property or next but then if we only care about the variables in a we couldn't think of p of x A as the full joint on all the variables in it.  So that's accurate.  And the other question.  I mean, I think I've kind of talked about this notation before  So far as there's an interesting subtlety, which I'll mention here. So is, for example, p of x one x two x four equals  Two p of x two x one, x four.  And so  Is the ordering in my tea important and normally in math. Yes, definitely. So they would not be equal, unless you have specific properties. But in this class. It's actually yes usually  In this class.  We will use basically  The typed convention.  What I mean by type convention. So you could think of these variable has having a type. Like, for example, like when you do computer science and you program a function, you could have  You could pass a variable which has been declared an RA, or you could be declare a double. So these are different types  And in particular, you can do polymorphous ism, which is depending on what you call the function with or even the type of the thing you call with  You will have different implementation. Okay. And so here you can think of the variable name we use x one x two x four as defining the type, meaning that  X one will mean. Oh, this is the values that the capital X one takes and I don't care where I put it in the order. So for example, I could put x to first. If I want to next one after that.  I still associate X little extra to the value that capital X two tix. Okay, so there's no order notation.  Usually though sometimes if I want to talk about flipping the values of something around and putting it in my joint, then you will have that these are two different  Distribution, so it will be clear from the context, but usually  Will have that those two things are equal in this case.  Yeah. So to clarify something here. So this is something over all possible values of x i. user avatar   Can take user avatar   Okay, so  Last thing to talk about before we take a break.  And then we'll finally define what a director graphical model is  So let's revisit conditional independence.  Because it's going to show the dependence before was defined with like individual variables. Now we'll define it with set of variables.  Oops, with our new notation powerful notation. So let's revisit  Conditional independence.  So now, let a  B and C, the subset of my ground said capital D.  So if I say that x A is condition, independent of x be given X see  There's two ways to talk about it. The first one is the factorization definition. So the factorization definition would say that, well, when I looked at the  Conditional of x A x be given X. See this is factor rising as a product. So, x A given exceed times probability of x be given x and this will be true for all x A x be tech see such that the probability of XE is positive.  Can only condition on positive property things  So that's the factorization definition of conditional independence. But there's another one which is equivalent, which is the conditional  Perspective is saying that, well, if I condition on both x be an XE  Then because XP is independent of exceed knowing experience and change anything else. Once I condition and exceed. So it's the same thing as a property of a given X see only  Okay and this would be  For all x A x be x see such that the  Property of x be an XE is bigger than zero.  So if so either you have to factorization perspective of the distribution which tells you that there are conditional dependent or there's they're just the conditional. So either one or ways to to prove conditional independence.  And these are all in this case.  Probably tomorrow mass function because we're talking about discrete remember  And then in this notation, we can still talk about marginal independence.  IE not conditioning on anything.  Which was a standard independence. We first defined in this class. So we'll use annotation, say x A conditionally independent of XP. And normally, we'll just do that. But we can think, oh well conditioning on nothing.  Right. Or you could think of nothing as x empty set.  Right, so this is marginal independence, but if you don't put any conditioning. It's just the standard independence.  And that would mean that the basically this is just like P of x A  XP is equal to p of x, a time spirit. So that would be the factorization, or I also have the conditional version.  So, so where to condition on the empty set, just to talk about marginal independent  And two facts.  About  Conditional independence.  So one is that it will be  Convenient sometimes to repeat the variable in the statement. So we can repeat  The variables. user avatar   Oops, sorry. user avatar   In the statement.  For convenience.  So for example, we could write x is condition dependent of y and z, given z and w. Okay, so that's kind of weird because, why do I put z in both places. And so  Once I condition on z. If I put z on the left. It doesn't mean anything. It doesn't change anything because he becomes deterministic so it doesn't say anything. So this extra z here does not  Do anything  Okay, so it's fine to say this.  It's a weird statement because why would I repeat it. But when we talk about theorem, saying, Oh, these are all the conditional independent statements. Sometimes it's too complicated to exclude these repetition.  For simplicity. So then we will allow the repetition, because what it means is, it is just that.  Like this is equivalent  This is equivalent  To just x conditional independent of why givens, and W. Right. So, so the adding Z's on the left doesn't change anything. Once I condition of them.  Okay, so this will be convenient in some of our theorems. And then there's something that  I think you proven Assignment one  So this is the decomposition property.  So the decomposition property says that if x is conditional dependent on y z given W. By the way, you don't need the parenthesis, like when I put y comma zero. That means the same as the joint of y AMP z, but just for more clarity here I can you put them as a joint  This implies actually to this is stronger conditional statement this in place two different statement, this implies both that  X is conditionally dependent why given w and x is conditional dependent of z given W.  It doesn't go the other way direction. Right, so it's not anything on the, if, if you could have that x is conditional of why given W and access conditional end of zig and W, but not conditional dependent on y AMP z.  The x. For example, if is one of that.  And I mentioned it in the past that recall that pairwise independence.  Is not equivalent to mutual independence.  So see, I think it's lecture three. I'm not sure. And I'm a bit late this year. So perhaps it was extra for but I'll write lecture three four now.  To be confirm. So, for example, by defining said is x  X or of why  Where x and y are independent. I had that all these pair of variables or pairwise independent, but they're not mutually independent  And finally,  One last property.  Is that by the chain rule.  We have that the joint on all my variables is equal to the product.  from i equals to n p of x i given x one up to i minus one for any ordering that I choose it is always true.  And so the last conditional  So the last conditional  Is p of x n, given X one up to n minus one. Right. And so for each of the value of my n minus one, parents, I need to give was a property of x n. So the number of possible assignment on the parents is actually to to the n minus one. So this is actually a table.  With to represent this conditional table with  Oops. There's too many with roughly  Two to the end entries  So this is why  Just reading the chain rule doesn't help.  reducing the size of our presentation for the joint.  And what will what will happen.  Is that the inner directed graphical model that will see after the break.  Is that will say that p of x d will be in this case the product over my nodes of p of x i, but instead of having all the previous variable will use only the parents in my graph. user avatar   OK. user avatar   And now I get tables in this case of size to to the maximum number of parents that I have plus one.  So if the number of parents is small. So, for example, it's a tree. So I have only one parent in a tree, then I get basically small table of size for  I will get n tables of sites for so I get like for an to represent my distribution which is much more efficient than to raise to the right. So that's the whole idea of a doctor.  That will see after the break.  AND THANKS SIMON for saying that the notation is elegant  Not sure which innovation, you're talking about. But indeed, this is a very convenient notation and  This is I'm not sure who came up with this efficient, but it's very useful for talking about these complicated object.  So sign so So Brendan is asking, why is it does a chain rule old for any ordering, not just the top logical ordering because when we talk about the chain rule, we don't care about the graph. We're just saying it's a joint  I just decide to start with one node. If the channel is proved. Just by the definition of conditional  Right, and so I don't need to. There's no property of the distinction to be used. So there's no notion of not adding a cycle. I just order my nodes in some order doesn't matter if it's, there's no graph associated with it anyway. We don't care about that.  Okay, so let's take a break. It's a 337 so we'll be back at 343 38 at 348 user avatar   So let's take a quick minute break. user avatar   Resume recording user avatar   Okay, so one thing before I define a demographic a model is to highlight something  Which is that if I write this statement here. It's a circular statement. Why, because the conditional when we talk about P is normally define from the joint.  So the order would be I define a joint and then from the joint. I can compute it's conditional. But now I say that the joint is equal to a product of conditional  Because it's a it's a circular statement I have p on both sides of the equation.  It could be that there is no distribution which satisfy this property right so it there's no guarantee that is a well defined statements.  Which is why when we define a definite directive graphic. I'm all in the way I will define it. I won't use these conditional and the right because it's a circular statement, we will prove  well defined. First, are there think Africa model and then we would prove that from this definition we do get this property that I just defined and it's about it. Okay.  So let's do that now.  So what is a dark graphical model.  So first of all its associated with the graph. So let g  With edge set V sorry nodes V and edge. He be a bag. So it has to be from directed graphic I sickly graph.  Then  Directed to graphical model.  Also abbreviated as the GM  And implicitly when we talk about the the gym. It is associated with a graph.  And  It's also known by the way.  Also known as a Bayesian network.  So what's the Darth Maul so I definitely got from the model. It's a family of distribution.  It is not the single distribution. It is a family of distribution.  And that's a very important point to keep in mind. And it's subtle and confuse a lot people at first, but it's a set of distribution. It's not just one  And the reason why we talked about that directly graphical model is that it enables us to treat a lot of different distributions, the same way with our code and stuffing methods. We talked about their properties.  So it's a family of distributions over  X XV. So the random variables associated with our nodes.  And we use a notation script L of G in this class. Okay so script L of G that will represent the family of distribution of a with which is a directed graph model. So what are the distributions in this family.  So what are this mission in my script LG. Oops.  I am having issues with my pen so  It will be a distinct set of distribution P such that p is a distribution.  Over XV.  Such that it has specific properties there exist legal  Factors.  F eyes.  Such that the joint p of x, v is equal to the product of these factors if i X i given x by  Four. All right, so  So basically,  The distribution which lies in my family or distribution which factor is in a specific way where you have these these factors here which only depends on the parents. So I guess I could perhaps put it more. I will highlight that. So the whole  Where the where the graph comes into play here is that  You will have that the factor is only are allowed to depend on the parent of I. Oh. Oops, wrong pen expire. I  And what does it mean to be a legal factor.  So illegal factor is basically a function from the possible values of x i cross the bus civil values of my parents. user avatar   So these are the user avatar   The simple the  How they call these omega sample space expire. I  201 user avatar   Okay. user avatar   So there are positive such that Dr normalized, so if I some over X I, my factor x i, and I could put a comma, instead of a given but you know  Perhaps, let's put it  Expire I just notation that could could come are given, you don't have to. And as you know, it isn't discuss often I put given sometimes I put come out, or even semi colon.  So this sums to one for all expired. user avatar   Okay. user avatar   And so basically these factors D look like a conditional distribution, because they're positive and is some to one. And so basically Fri. This implies that FBI  Is like the complete conditional policy table so CPT  Conditional  Probability. user avatar   Table. user avatar   CBD is just a fancy word to say, oh, how do you present the conditional distribution. Well, it's a table, such that you say on the Rosa possible values of x i on the columns, all the possible they use the parents and you have that when you somewhere on the column, it has to some to one.  So that's a directed graph of them all. Let's put a little square on it.  Okay so Umar asked, why is it also called a Bayesian network.  I think the idea is if you're a Bayesian, you have to define a distribution over everything that is uncertain.  And so  The fact is that these these these graphical model is a very powerful and flexible.  Formalism to define complicated distribution in particular for a Bayesian so they're very useful for Beijing to encode their beliefs.  So usually you can use a Bayesian network to encode the beliefs of a patient, even though you can also use a vision network to do something which has nothing to do with being Asian, just define distribution, but in particular Beijing's will will love Beijing networks are different model.  The difference between a Beijing in Beijing will be that the parameters and this kind of stuff would not be known.  In standard frequent this framework because they're not random variable was a Bayesian would also put these random variable. Sorry, these, these parameters like theta, this kind of stuff as random variable as well.  Okay, so that's a D GM  And the terminology is  So we'll say that, let's say if we can right  For some distribution, the joint as a product over factors where  You get this parents relationship. And so the pie I here. This is what determine G.  This determines the edge set. And so then we will say,  That p  Factor rises act or according to G. P. Factor rises.  According  To Jesus  That's the terminology. And so the GM is a set of distribution which factor is according to the graph that we use to define the DJ  Alright, so somebody is asking your question here.  They're confused by the rotation.  Excited much as if I index and expire. I am. So the point here is, so for each node. I have a factor. The factor will have to have X I, as the argument.  And x pie as the thing you can condition and on i. And so, you know, you have to look at what are the parents of it in the graph. Okay, so that's my factor. And now if I some over the passive value of x i. So, is fixed here. If I some just a word, the values of x, I get one.  And and this has to be valid for anything I conditioner and that's why it's for any expired. I and you you have that this is true for all eyes.  This is clarify the rotation, Simon.  Sure. So if, if you want to be very formal. You say, for all i.  So here basically, I was just defining quickly what we mean by legal factors. So for any if I it's a legal factor if it does this property and because you know I use a lot of fit in my definition, then of course the to satisfy these property for all eyes.  I'm sure Jacob, and are there any methods to learn. G. If it's unknown.  Yes. So this has to do with the problem of model selection.  So some sense model selection. Is it possible models to talk about my my data which model should I use for example I could also choose between different  Parametric family of distribution. Should it be a garden should be a song and you want to choose that. So this is called the model section problem one molestation problem would be well which graph. So they used to talk about my my data and there are methods for that.  And we'll talk about it when we talk about model selection.  Okay, so let's give you a simple example. So, for example,  I could have this graph here on three random variables. I have x y as parents to Z.  Right.  And so if my distribution belongs to the graphical model.  Represented by this  Graph.  Then it's joint  There exists FX f y AMP Z legal  Such that the joint over y x, y, and z is the product of these factors. And so, and the factors has to respect the parents relationship. So x didn't have any parents. So it's just f of x, x  The factor on why has also no parents that will be why and then they will be a factor on z, which has two parents that would be z given x and y.  OK, so the distribution p which factories, according to this product with any type of factors there. The one in this  That could graphical model. And this is actually called a V structure and we'll get back to this very soon. It's basically the most interesting piece of data graphical  OK, so now  The next step will be to show. So the next step is to show that basically  These conditions are the same as the factor.  They're the same.  Which is why I'm allowed them to write this because we have that p of x is a problem factor and these factors are the conditional. And now I didn't use it visible link. So that's raised that, but we need to prove that user avatar   That user avatar   Okay.  So,  Before proving that the basically the conditions are just the factor.  I will prove something which comes in all in a lot of proofs.  For the GM, which is the leaf lacking property.  Which is basically a fundamental property of their traffic model.  So what's this leaf plucking property.  It basically means that when you marginalize out the leaf in a directed graph model. It's just removing this factor. And then you keep everything else the same. Okay, so let p be a member of the doctor graphical model family. And now let n b a leaf.  dingy  IE and  Has no children.  It's not the parent of anything.  And if n is not a leaf in G just rename your nodes from you the name of the nodes, such that any day.  So that's would be without loss of generosity.  So what are we proving with the leaf plucking property is that if we look at the marginals  On all the variable except n  Well, first of all, this marginals is a joint on a smaller number of variables, which also belongs to a directed graph model. It belongs to the data graphical model where I have removed and from my graph. Okay, so this means  This notation here means take the graph G and then remove  node n  To know nude and from G.  So it's a smaller graph on the smaller number of nodes you keep all the same edge from the other graphic 71 which were connected to him.  And moreover, not only belongs to this, the GM, but we have a very simple  Expression for it. If P of X one up to n. So if my joint for this be was a product of these factor.  X i, x by I  Then the marginals when I removed the last  Variable  Is the same product with the exact same factor, except that now I have a product over n minus one. And I try basically just remove the last factor. user avatar   Okay. user avatar   So if and is not a leaf. You don't have that. So it's very specific to know being a leaf. But the nice are very nice. You can plug them you can remove them from the graph and things are super simple. You just remove the factor. And then everything else stays the same.  So, the proof is very simple trivial.  So, probability of x and x one up to n minus one. So that's my joint  By definition of a graphical model, it will be the product of fn  X and X by n. So in some sense, there exists some factors as it's true.  And then I will have product over all my other nodes.  So let's see. Gina equals two, n of f g h i j x by Jay  And now because n  So let's start talking about it is yet. So now I want to compute the marginal. So the marginal were only having one up to n minus one, I need to some over accent my joint  And so now I need to sum.  This thing over accent. This whole thing. And now the key property is that  Because and is a leaf. It doesn't have any children IE. It's not the parent of anything. And so there is no n anywhere in these expression in any pages. Okay, so, and  The x and variable all me appears here. So when I some over accent. These thing here is a constant, there was no accent anywhere. And so I can just write this as summation over accent I can factor is it out. If an ex an invisible ink.  Summation of Rick extend fn X and given x by n.  And then the rest is just a constant for  SN Fj xj x by Jay  And so that's why I was able to factor is it like this. And so now, this by definition of the factor is just equal to one.  By definition of the factor.  And so I'm done. I've proven both that  The marginal was just this simple product here. Like, I wrote here. Sorry, this one here and  And these parents relationship satisfy the one from the graph where I just removed the leaf.  Okay, so somebody is asking something about my notation. This is. Oh. Well, isn't the the set minus notation. This slash thing.  Well, the thing is that g is not a set g is a graph. Okay, so I use minus just to define a subtraction notation on my graph. And anyway, this is just location. I'm using right now I'm defining  Graph minus set of nodes as removing the nodes in my graph, but it's not the same thing as removing elements of a set because Jesus, not a set. It's a couple both of nodes and edge. So you need to kind of like, do a lot of  It's both removing the the the node from the node set, but also the corresponding edges which are connected. So it's a much more complicated operation that just removing some elements.  Okay. So, is it clear this leaf property but plucking rotation. So if I just remove if I want to marginalize out a variable which is a leaf in a dark, the governor model, you basically just remove its factor and that's it you're left with the correct Marshall.  So now let's prove its use this property to prove what I was talking about.  The proposition.  So if  P is a factor is according to graph is a member of a doctor graphical model.  Then let  F j  Be a factorization.  With respect,  To je je je je for p  Then it turns out that for all i.  The conditional of p on exile, given its parents is just equal to this factor.  For all x by I  IE. The factors.  Or the correct conditions.  And because the conditional is is determine from the joint that also means that the factors are unique, by the way.  Because I never said with the definition that the factors had to be unique. It didn't rule out in a definition that there could be two different factorization, which are both according to the graph it, which gives the exact same joint, but there are different factors.  But because the factors have to be equal to the conditional and the conditional or a specified by the joint, then they are unique.  Alright.  So I'll do a bit of a not super clean proof, but it's just to give you the idea user avatar   So, user avatar   In some sense, normally the way you really prove that is by induction. But I'm a bit lazy. I don't want to do the induction hypothesis and stuff. So the way I do it is, I'll say  Let one up to n.  Be a topological  Sorting  The political sorting is just a political ordering. It's the same thing.  Of G.  And this is possible.  Because  G is a bag and I said that all DAG had a topological ordering  And so I called one up to in the political sorting, meaning that  I assume that my nodes names were already in the correct of logical order. If it's not the case just renamed them right so you had, for example, the original one.  Up to end nodes. They've been mapped in an order which flip them around. Now just rename these nodes with the their order the political sort sorted order. And that work.  So now, let it be fixed.  So we want to show  That the conditional p of x i given x by i is just equal to f  X i given x by I  Want to show that  So the way we'll do that is just pluck all the leaves up to it.  So,  By the logical sort of property and cannot have any children because the children are only before so it would have it back edge. So we know that n is a leaf.  So then just pluck it  I E marginalize out accent.  To get  Probably T of x one up to n minus one.  Is the product J one up to n minus one of f g xj x by Jake  And so now, what I get is a factorization.  For a DAG.  Where x minus one.  Is now.  A leaf.  Right, because I remove and so all the edges from which are connected to, and is now removed. So now I'm n minus one and minus one is still in if it could have an edge to end but and is not there anymore, so n minus one is now leaf in this new graph. So I can also pluck it  So then, like it as well.  Luck and minus one as well.  And so you keep doing this.  Doing  This  in reverse order plucking each leave  Using the fact that you have a topological sword that these always leave  Us okay as leaf because  Of topological order.  So you go in reverse order n minus one and minus two, blah, blah, blah. Up to eat i plus one.  So now you've you've plucked all the nodes up to  i where i is now relief again.  So now, what you get is that the quality of X one up to I  Is just si, si x by I  And then you have product J smaller than I  F G xj exposure.  And now, because I is again a leaf in this reduce graph.  Know,  That there is no x i appearing in this piece right. So for example, I will call this product here. I'll just define this to be a function and this function only depends on x one up to i minus one.  Because there is no x i appearing in any of the parents set  I E. There is no x i by the top political sort property. user avatar   Right, so user avatar   That's again our leaf, kind of like argument.  Okay, so now what you do.  Is you will partition.  The set of nodes one up to i into three pieces there will be I they will be the parent, the parent of i and then there's everything else I'll call it A, so A by definition.  Is just one up to i minus  Sorry, one up to i minus one minus the parents. So I just remove the parents.  Because it's a partnership i minus one minus the parents of  Gay. So then I can write the joint on X one up to I, as the property x i expired. I an ex. So I have three blocks. user avatar   Okay. user avatar   And so now if I want to compute the conditional of x i given x by I  By definition, this is the margin all of exciting expire, which is summation over x A  Of quality x x by x eight  That gives me the marginal an excellent expire I and I need to normalize this by summing over excited. Right. So, this will be summation over x prime  Summation over x A  Be an exciting time.  x by x date.  Okay, so now we wrote the joint, we said, what was the, what was the shape of the joint on X one up to i. Well, we had the factor here and some function, which didn't excite right so let's just rewrite this so the. Oh, did they not use invisible ink again.  Alright, so this is  F of X given x by I  And then I have g of x one up to i minus one.  And somebody is asking how is a define so it's actually  The set  So it's one up to i minus one then they subtract  Pie.  So it's all the elements which are in one up to i without I and without by  Okay, so why did I put a space. Well, the thing is, I can push the some over XE inside because there is no  Because in a there is no by i know i right user avatar   And so user avatar   When I some over XE exciting expire I or not in the summer. So this is just a constant. That's why I can push the some inside  So I have this and then on the denominator I have now. Same thing, summation over x A g of x one up to i minus one.  And then I have my f i x prime given x by I  And then I'm something over x i prime, but because there is no x i.  In the storms. Here again I can factor is it up. So I have your in factories it out.  And so then  Here I just get one.  And here, these things just can't sell that  And so I'm left with just f  X given expire.  Which is what I wanted to  Okay, so. And what I've used it that  There is no  X.  Prime  There.  Okay, so that completes the proof.  Because I was a returnee and so the main property I used was the top logical sort. So that's why I need a DAG.  If it was not a DAG and there was no topological sorting. I could have cycles. Sometimes, and then I will not be able to to marginalize out things and you will not get this this nice property.  And so because we have this property that if I have a member of the family, then the factors which were used for the definition are actually just a conditional from the joint. Then, from now on, we can safely say we can safely right  That the D GM on G is just a set of distribution P such that the joint actually is the product is the product of its conditional  This is not an empty set, and user avatar   It works. user avatar   Okay.  Is there any question.  About this proof.  So let me give you two properties and then  next class I will make things a bit more concrete by looking at the basic three nodes graphs. I'll talk about Markov chain Layton cows and expanding away with, like, more concrete example. Because right now, it's a bit abstract, but let me just mention two properties.  So when you add edges in a graph.  You make the D GM bigger  So adding edges mean you have more distributions in your directive difficult  Because you have bigger factors. And so what I mean by that is, suppose I have a graph, which is Vi n e and then I add edges. So I have g prime which is the same D. But now with the new headset. He prime such that he prime is a superset of he  Then we know that the graphical model on G is a subset of the graphical model on GPU, so they're more distribution. So the more edges you have, the more distributions. Okay.  And so then there's two extremes that will mention, so the smallest graphical model.  Is the one is the empty set of edges like when you add edge you make bigger things right. So if the edge set  Is empty.  Which is basically, in some sense, the trivial graph.  Then you have that the graphical model is actually the smallest and what it contains are fully independent distributions.  Only fully independent user avatar   Distributions. user avatar   Right. HE P of joint is the product over the node of its marginal  And so recall that PM xi could be think thought of x i, given the empty set. Right. So there's no parent  And at the other end of the biggest graphical model is actually containing all distributions. So if you have the complete  Directed graph.  IE for all  I NJ.  Either you have i j, which isn't the edge set or j is or you cannot have both because it has to be a cyclic  But basically you have an edge between every pair of notes.  Okay, so an example on four nodes. Let's say 1234 is I connect one to everything else, then I connect to, to the things I have not connected so far. And then I three there's only one left. Okay, so this is a kind of a complete directed graph.  Then by the chain rule you have that p of x, v is the product.  Of x i given x one up to i minus one.  And so this is like the chain rule.  From the graph I just defined. So one way to have the complete diagram is you connect  Basically one to everything.  up to n to to everything else. It's your dress. So I think this is what you get here.  Yeah, that works.  And so, that implies that all distributions.  Are on XV.  Are in the direction graphical model on the complete graph.  Okay.  So basically you start from the empty graph and you already have all the independent distributions.  Then you add edges, then you get more and more distributions until you have added all the edges possible you get a complete graph, then you actually have all distributions in this in this photograph. Come on. Yeah.  And in particular, the independent distribution is always a subset. So the independent distributions are in all directed graphical model. So there are part of all of them.  So that's important to keep in mind. And so what happens is when you remove edges.  you impose  Basically  More  Factorization constraints. And so you reduce the set of distributions, which which satisfy these constraints.  And we saw by the inefficient of conditional independence that factorization also yield conditional independence assumption and so  We also make more conditional independence assumption, as we remove edges.  And so in the next class will see the basic three nodes structure, which also give interesting condition independence assumptions.  And then we'll talk more about  Their relationship between conditional dependence or stunts assumptions and the graph.  Is there any question remaining question.  Hmm. So Brendan is asking if we can rewrite marginalization using matrix notation, maybe could help to simplify avoid some of these indices  Well, you can certainly rewrite them as matrix product. But the problem is you get very complicated matrices. So you replace the you know Index. Index kind of keeping track of indices with like very complicated matrices. I'm not sure it will simplify that much.  So Jacob is asking  A question, which is, is there a notion of distribution being one which requires all the edges in that graph like ruling out distribution that could be described also by simpler graph, those with fewer edges. So, for example,  The Independent distribution is both in the empty graph there the graphical model and the fully connected graph directed graph model, so it's it's a member of multiple Daejeon  Was there a notion that over, I would like to, to say it's a member of the smallest one.  Yes, and I think I mean, so this is this is something called faithfulness. You will say that the graph is faithful with to the distribution if there is some of kind of these that required since smallest thing.  And when you try to learn a graph from data, you will usually also ask that there's some faithfulness.  assumption about the graph in the sense that, because if there's multiple graph for which distribution could be a doctorate different model.  Then when you see which graph, it is well there's no notion of uniqueness. So by saying it's the faithful graph, then you reduce the set of possible graph. Usually they're actually still multiple graph which could  Be the same size.  But at least it's smaller.  When we talk about model selection for graph, we will see that  So Lauren is asking, oh, what happens if I reverse some edges. So we'll talk about that later, but  The complete diagram is not unique. There's a bunch of different  You know way to make a complete diagram out of end nodes, but they're all all these are equivalent, as the GM, they're all have all distributions in them, so they're all the same set of distributions.  And you have to be careful when you reverse edges because you don't want to create cycle. But if you're just did just use any possible way to connect all the nodes together and it's still a DAG, then it's also a different company graph which is equivalent in terms of set of distributions.  Alright. So last question is there analog to age weights in general graph theory for graphical models.  Right now the age, weight is one in these in these graph properties.  Later, there are some notion, at some point, when you talk about  You can also have weights on on edge.  But they're not directly used for a defining demographic, which is a different. It's a different application area.  Okay. Well, anyway, time is up. So  Let's I'm going to see you on Friday.  Have a nice week
  Okay. So, and, and do not forget that next week. There's no lectures, right, so next week is, it's time for you to spend on your, your assignment on your project and other  things you have to do so.  Today we're going to continue talking about properties of directed graphical model and we will define the underrated graphical model and after that you already have all the material to do assignment three  So,  Let's go back. So at the end of the last lecture to do this lecture.  We saw that adaptive Africa model is a family of distributions, which  satisfy some factorization properties. Right.  And so now I will give you the the clinical three nodes version graphical model. And as I had hinted last time the factorization properties.  Relate to conditional independent statement.  Right, so I think let's go through last class. Let's see.  I think I  Review some  Conditional dependence. Yeah. So remember there was these, these two equivalent definition for conditional dependence variable x a conditional covenant of XP given X, see if the conditional factors as a product or if I condition on both x and x. See, it's the same thing as conditional access  So so knowing exceed gives you all the information you need to talk about X A, you don't need to also know it's that's kind of the  The semantic. And so, so these like is factorization. And because the the joint have a distribution, which is in the doctor graphical model factories and the specific way. There will be also associated conditional independent statements for all the members in defending  And as you remember at the end of last class, or perhaps you don't remember, but I'll remind you, I told you about two extremes. So there was when you have no edge in a graph, then the associated Dr graphic model is only containing the fully independent distributions.  So that means  And I said, when you add edges you add distribution. So that means that the independent distributions are in order to overcome all basically  So even if you have a bigger graph. So,  So that means that, so why am I talking about this. Well, it means that  We will talk about  For a specific graph. What are the conditional independence properties which are satisfied by all members of the family.  But it doesn't mean that there's specific members of the family, which could have more independence and particular you will always have the fully independent distribution.  It's an all directed graphical model. So, so this is always in a directed graph model. So whereas, usually it's not the case that all members of the family.  Are fully independent except if it's the empty edge but it's very specific. Did you okay so it's important to keep in mind that  There's a difference between properties of specific distributions and properties that all distributions and the family share GUESS YOU WANT TO HAVE THIS WILL I will come back to that.  Okay, so that's a good question from Jacob. So not all distributions are represented will with a graphical model directed undirected right  Like summarization of larger models, for instance, we will be discussing a way of specifying for a given distribution model where it can be assigned a graphical representation. So  I think I mentioned the the  The life. The plucking property right. So there was this thing here why I said up if I have  A directive graphical model. And if I marginalize the one node which is a leaf, then the distribution, I get the joint I get is a member of a different director graphical model where I just remove the leaf from the graph.  And and this, this implies that not only the marginal is have this memory in place that if I look at the collection of distribution which are obtained by marginalizing the specific node.  I obtained that collection of distribution, which is exactly the same family as the one for this graph here. So there's a there's a  This smaller. The gym is exactly characterizing all the marginals I obtained from the bigger graph.  And we'll see that, indeed, not all operations yield family which are exactly characterized by a graph. So there's some  Families, which are not obtained from there to graphical model. On the other hand, any distribution can all be can be seen as a member of a family, in particular, I said at the end of the class, the DEF CON I considered the complete graph.  If I use their company graph, then by the chain rule. It's actually satisfied the correct factorization on the graph, which means that any distribution is part of this family.  And so any distribution can always be seen as into Dr graphical model where everything is connected that's different, saying that I get a family, which is characterized by the graph, and we'll get back to them.  As an answer your question, Jacob.  Are you you can voice your user avatar jacob louis hoover  Application.  Sounds like yes yes it does. I wanted to just like ask  This team is very similar to something I asked last lecture, which was and you answered it by saying that there is a turret this concept of faithfulness.  If it could, if this was like the best model for the best graphical model. So is this exactly represented concept, similar to the concept with the graph being faithful because, of course, you could always represent something with the super user avatar   So you need to do this. Okay, so, so, so, just so that we're on the same page or demographic a model is a family of distributions.  And so we need to distinguish when we're talking about a specific distribution or a family of distributions. And so now the concept of faithfulness will be for specific distribution. And I think the idea there is, it will be the smallest graph.  Which contain this distribution. So the complete graph always contain any distribution, but there are smaller graphs and so for example, if I consider the independent distribution any independent is fully independent solution, then the faithful grafted that is the empty graph.  Okay so faithfulness will be a concept defined for a specific distribution. Whereas, when we talk about families, for example, like, Can this set of distributions be obtained by a specific graph. And that's a question about a set of distributions.  And  You could not have that. And it's not the case that all the members of this family.  So if I consider let's say I have a graph G and I looked at the set of distribution LG  Then this graph won't be faithful to all the members of the family because I told you that the fully independent distribution is always a member of the family.  And then the only graph which is faithful to that is the empty graph. So, so there's some distribution in your family for which the graph is not faithful because there are smaller family which contains it  And you will have a good this sense that without loss of generic if you if you randomly pick the parameters of your distribution.  Then the graph would be faithful in some sense, did the family that gets in order to to get  These fully independent submission or something you need to specify the conditional in very specific ways and there will be a notion that the set of parameters for which does arise as measure zero in the set of all parameters. So there's a very specific families.  We get back to that when we talk about pattern ization and conditions. Okay.  Alright, so  Let's talk about these three nodes graph.  Conditional  So yeah, so basically, now we'll talk about what are the conditional independence property in a directed graphical model.  And before going in general, we will start with the basic three node graph.  Which will also give you a bit more intuitions about the properties of these TGF  Alright.  So the first basic graph will be called a mark of Shane.  And basically encodes Markov chains. And so in this case you have arrows in there in all the same direction. So, I will have the z. And then I have why and use it usually here I will use z is will be observed, just to get a sense of what's happening.  And you could think of.  X, for example, like this could be a good model where x is  The past.  Z is the present.  And why is the future.  We get what I mean here is you could think of all, there was a time step for that was an observation at each time step and X was the first Z's. The next one. And why was the third one. Okay, so, so that was also kind of market changes and nice model.  And it turns out that for all the members of the demographic a model, you will have the following conditional independence, you will have that x is conditionally dependent of  Sorry. Why I mean the same this symmetry, but I prefer to talk about the future as condition, they didn't have the past, given the present.  Future.  Is conditioned dependent of the past, given the present.  Okay and this is often called  Markov chain assumption. So it's a Marco via an app such assumption. So that's a bit also like where the these names come from.  And so you can think that often like physics has this property like if I know all the state of the system right now and there's no memory aspect, then I can already  Predict or I can already make my model of what should be the next steps. So what will happen next should not depend on stuff, which was  Happening in the past. If there's no memory property. So, so usually you have these models like if I give you like you follow like Newtonian mechanics, I give you the initial condition, I tell you.  What are the, the force of motions or whatever, then you know how the particle will evolve by just using the law.  Newton's laws, basically, but all I need is to this initial speed and initial position and and I'm fine. I don't need anything else, whereas I don't need to know where where this position. And where was the particle like 1010 years ago, doesn't matter. It's  fairly natural assumption. It doesn't cover all speed. I mean, I, of course, especially if there's a phenomena of like his services or memory where the behavior to system in the future depend on the what happened in the past, not just what's this current state.  So that's what you have for all members of the family, but  You don't have that x is marginally independent of why  For some member of the family.  Okay, so  So the fully independent distribution is is an all graphical model. So it turns out that if I look at the fully independent submission on x, z and why it will be the case that x is independent of by  So there are some distribution in is that that directed graph model which have that x is margin independent right  But it's not the case for all Members, which is why we say it doesn't hold for all Members.  Okay. And the reason in some senses that there's this this because of these these arrows here in some sense there's there's interaction between these variables intuitively. And so when I don't observe said I was still get a connection between x and y.  They're not separated when things are separated  You basically get the dependence, we'll, we'll see that towards in the middle of this class when we talk about the notion of graph the separation, which translate into conditional independence.  Okay, so that's one simple graph and that the thing is exercise to the reader, you could prove from the factorization. So here, what you have is that the joint p of x, y, z.  By the definition of a graphical model has to be able to form p of why given z p of z given x and p of x. So there's this factorization property.  And so here user avatar Lizaire Maude  Thank you. user avatar   So hear user avatar   You clearly see  So you don't care what you see. But from this factorization. You could prove that P of x, y, z is equal to  You know exercise.  That p of x, y, z is equal to p of x, given z times P of why givens. It's like two steps in some sense.  So that's why I'm saying that  This condition independence assumption holds for all members of the family because of this factorization, which implies this one, which is what the condition independence statement is saying.  And basically how you do that. You basically take this joint and and you flip it around and then you see that it works.  Okay.  So be new Laila is asking if the order mother.  So if I put  The arrows this way.  I get the exact same set of distributions. So indeed, the desert graphical model with these two arrows is the same, that when the arrows on the other way.  So the order does not matter from a demographic model perspective and I'll come back later on, on this property when I talk about reversing edges. But if I if I flip the arrows like this, then it's a different one. This is called a V structure. And that's the third graph.  But before getting there. Let's talk about the second graph. user avatar   Which user avatar   Actually has the exact same statements above. So it's the same set of distribution.  But it has a different name. So this is called the latent cause. Oops.  Or the hidden variable model.  And basically, what you have is you have, say, one variable which influence two variables. So that's the x and y and this would be Z.  And this will be the shoe size. And so, okay.  So that's the, that's the, you know, that's a darker governance model. And so basically the joint was separate as, oh, what's the probability of A, Z, and then given Z i looked at what's x, given z and then why given separately.  And and this thing says the same thing as above. You still have the same statements as above, ie X is condition dependent of z given sorry X is conditionally been in a way, given the  Same statements.  As above  So even though there's a different name because there's a different structure, it has the same set of distributions.  And let me give you a concrete example of a situation where this could be an appropriate model. Let's say z is the age of a person and X would be the shoe size.  And why is whether they have great air.  Okay, so what happened is  If you know the age of a person, there's a specific distribution of shoe size because if there are kids, they're still growing.  And there's also a specific disposition of gray hair, like, you know, the older, they are more likely to have to have gray hair.  Now if I don't observe age.  There's a relationship between gray hair and shoe size because indeed if I know that the the have gray hair. The have normally a bigger issue because they have finished growing. They're not kids.  Right, and so there's some relationship. But when I know the age I there's, it's basically dependent the shoe size of a person and whether they have gray hair are totally independent  Once I know the age so age is kind of the common factor which influence the correlation between gray haired shoe size. So once observed  Age than the are independent. When I don't observe age, there is some dependence, even though there's no you know direct relationship to them it's mediated through the age. Okay.  Alright, so that's little cars and and by the way this is also like Correlation does not imply causation. So Dr. Africa model. There's no, there's nothing calls all about them.  Inherently and a good example also was by by by the fact that you can reverse edges here, right. So I told you you could have the direction the edge the arrows in the other direction. It's the same family.  There's nothing which change in terms of the family. So you can have that these arrows, should the causal. In this case, what do we mean by causal will a good model of causality. So that's also getting philosophical but their cause all would be that there's a kind of like  If you intervene on one variable, and they're the cause of something else, then it will transmit the interaction was if you intervene in an effect, which was caused by something else. The thing that caused them won't be influenced by  Intervening on effect right and so  Under intervention, because it kind of appear. Whereas when you don't have intervention when you just observe. Well, you see this correlation between the two variables. So, in both directions. Okay.  So, for example, the temperature of a city and it's altitude. Right. So the altitude is influencing the  The temperature  But if I change the temperature, it doesn't change the altitude of the city. So I could move the city up and then the temperature would change. But if I just say heat the city. It's one change the altitude. And so there's a causal direction from altitude to  Temperature, which means that when a intervened. Isn't that symmetry. So this is not capture by graphical model is captured by what is called causal graphical model. And we'll come back later in the class on this topic.  So,  Alright so that was  Okay, so first, there's a question about why do they give a different name for the different structure is just because even though the characterize the same set of distributions.  The can be  More naturally described in different ways. So for example. So because the way you you specify the model here would be to give the conditional of why given Z and then the conditional of z given x right and  Using the quality of age, given gray hair.  Or age given shoe size is kind of like the wrong direction from a modeling perspective. user avatar   And so user avatar jacob louis hoover  So it's just that. user avatar   There's some conditions on user avatar   Its user avatar   So the thing is, indeed, like when you have a causal property in the world, it also kind of like makes more sense to to express something in the distribution using the same direction, it would be more natural. Yes.  But it doesn't have to user avatar ezekiel williams  The series may have just done.  Yeah, speak up for a second. So then, basically the reason that we're doing. This is just for effectively because it seems more natural. Even though, mathematically, there isn't really any difference. user avatar   Correct. user avatar   Yes, it's to give a bit of intuition.  Okay, so  I kind of forgot that I had to go upstairs because we are crazy, Doug.  So we'll take a five minutes break now we'll move up and I'll be back in five minutes. Let's do that. user avatar   I'll do it. user avatar   Zoom recording user avatar   Alright, so let's see why do we drafted graph different 50 represented the same mathematical model.  And so already mentioned is that some of these have more natural conditional distribution, which makes more sense. So the way you define the joint is different.  For the, the way you patronize a joint is different depending on this graph, even though it's the same set of distribution. That's the difference.  Yeah, so another place where you will see later on when we talk about characterization of the conditional  So instead of having opposite or conditional. You could start to have say it's a Gaussian distribution with a specific noise model.  Then the direction will matter a lot because if I say X is gushing giving why with a specific independent noise model. It's not the city of symmetric  Model, I can just flip things around. Okay, so there's also when we start to put Patrick distribution assumption on the conditional. It will change something  But we'll get it right now. We're just saying, Okay, let's talk about a family of distribution as a whole.  And I'm talking. I'm telling you what are the conditional. And I guess here. I went a bit beyond by also giving you a bit of intuition for the semantic of model. What kind of natural conditional distribution which these graphical model could be a model for  And somebody asked if factorization one one implies factorization to  Yes.  Because as I said, if the all the all have because they're all part. It's the same set of distributions, then indeed you can be factored in both ways. The same way and you know you can easily prove it as well.  Okay, so hopefully clarify things a lot. And now let's get to the third graph where everything is different. And so this is called the explaining and we  Or competing effect.  Phenomena  And this as a name called a V structure because when you put when you draw the graphical model with the arrows going down. It looks like a V. OK. So the model here is I will have x i will have y. And now the arrows are going down like this. So,  The arrows are both incoming the, the, the middle node and I will still use z here.  And here the conditional depend statements are opposite of the one above. So here, this graphical model will have that x is marginally independent of why. So when I don't observe z, then x and y are independent.  But if I observe z, you have that x is no more independent of why given Z again for some distribution, because the joint. The fully independent is also in this model.  For something because what's happening is that. So, in the previous model like in the late and coast. There's one cause which has to phenomenon which are independent. Here's different there's two things which are influencing something  So those two things might be independent, but because they they're influencing something once I observe something happening, then there's some kind of like relationship with them. And let me give you another concrete example. Suppose that X is whether I have been abducted by an alien.  Why is whether my watch is broken.  And z is I'm late to my class.  Okay. And so if you're observed that I'm late. Well, perhaps I'm late. Because my watch was broken. I didn't really see the time was passed.  But it could also be because I was abducted by aliens. Okay.  And the fact that I've looked at, and by the end or not with my broken. Watch. Let's say they're independent that fit alien. There's nothing against Western I'm trying to smash them. And so these are independent, but whether I'm late, or not depends on these two things. Okay, and  If I'm and basically now I can exit give you an example why there's dependence. Okay. And I'll tell you, and at the same time this will highlight something which is called the non monotonic property of conditioning mode know tonic.  Property  Of conditioning.  And I have trouble because there's a big  Set of buttons, where I'm trying to write from zoom and I'll try to move it. OK, so the nothing property of conditioning just says that when you condition and more and more stuff the protein can go up or down. It's not just in one direction. And so in this case, for example, like  A nice model for this phenomenon. So what I mean by a nice model. I'm just saying you could define meaningful conditional distributions to characterize this phenomena.  And what would be a nice model is normally that the probability that I'm abducted by an alien would be tiny right this is kind of a very unlikely phenomenon.  We don't even know if alien exists, but yeah.  Yeah, so this is a property that I'm abducted by aliens is very small. Okay.  Now, if I consider what's the, what's the property that I've been abducted by an alien if all I know is that I'm late.  Well, this will actually be bigger than a protein that I've noted by me, because now I have some evidence, I'm late. So perhaps I'm late, because I was abducted by you. So we increase a bit of property that I was abducted balian because I saw a consequence of it right  And so the protein increase. But now if I add the information that oh, well, I actually also have my watch. I know that my watch is broken.  Now the party will go down again because the  Fact that I was late is most likely do that my watch was broken, not from the alien. Okay, so when you don't observe whether the branches broken. You don't know. Well, perhaps you know you observe something so it increases property that I was alien  Alien but if now I looked at was a policy that abducted by aliens given an am late and that the watch is broken.  Now this normally in the a good model for this phenomenon will be this will be actually smaller than the property that I was a bit about it. And when I just know that I'm late. Okay.  And so here, what's happening is you started the property when up and then when down. That's why I'm saying I condition and more and more information and then they're probably just went up and down. So there's no more than two. There's no  Mana not only city in when I condition because there's something I mean on like the entropy. The entropy. When a condition, more and more can only increase. And we'll see that later.  So there I think exit is a hidden variable we want observe it. No. Yes. By definition of a non observer variable. So, but in this case.  So when we don't observe X and basically also remember the leaf plucking property. If I remove this node.  All I do is just, it's a leaf. So I just remove all these edges. So I'm left with a fully disconnected graph. That's why x and y.  Are independent marginally and then when we talk about marginal independence, what we mean, like, okay, we don't know. Z. It's not there. Now what's happening.  Was in the previous model when you had the latest Cause when I marginalize out Z. It's not the least, so actually this there's still no there's still a connection which should have been excellent.  So now the property again.  So somebody. So Martin is asking whether this property.  Should be smaller than the marginal quality of alien. I don't know.  I don't, I wouldn't need to put it in the marginal quality I think different models could have either that it's smaller or not. I think there are these still make sense.  Okay.  Well, okay. So, tomorrow is saying is going to do the calculation and see the problem though is I didn't define any of these numbers. And so depending on how I define these numbers I think different things can happen. And all I'm saying is that it does make sense that  Okay, somebody says it's Michael Jordan. I don't know. So I think it helps to clarify that.  They like I'm saying that from a defining these conditional, in particular the model here would be defined by saying what's the priority of x, which is  The marginal quality of being abducted was a party of why the marginal quality.  Of being that My watch is broken and then the next part of the model will be okay. Knowing whether or not. My watch is broken, or whether I'm abducted by aliens was uploaded that I'm late.  Okay, so that's the PMC given x, y. So you could define the stables and then you could compute everything. And I'm saying is, given this phenomenon. It would make sense to have this property, but  Okay. So, tomorrow is clarifying. It's not joking. And this is joking again saying is not joking. We're getting into meta level.  So all right, so let me, I would like to cover something else before the break.  This there's a burning question on this topic.  Will have time to go a lot on more properties of these objects. So to get a more familiar aspect, but let's talk about what are the conditional independent statements that we could derive from a graphical model.  So more conditional independence.  Statements.  In a directed graph.  Alright, so the first thing  Is  Is will define another set of of nodes, we call them the non descendant of I  This is the set of nodes which are not basically children or grandchildren of. So basically these are the know, Jay, which are different and I such that there is no path.  From Jay to I  And they're called the non descendants  Of I  Just have a concrete example. So let's say  These are parents of a node.  And this would be i. And so the set here, this would be the parents of is, we call them pie, right, then perhaps I has a child.  Which also has a child. So in some sense, you could think that this node. If you keep the analogy this node could be the grandchild of AI and is definitely a descendant of AI in the sense there's a path from I to this node like this that's the path.  And then perhaps the child of I has another parent  And by the way, here you can have more than two parents in a three parents. The parents, it's not like a biological thing.  And then perhaps this parent also has a pet.  And now the non descendant of i, or all the nodes where there's no path directed path from these notes to it so it's it's the parents of I and the parents of the children of i. So these would be the done this in in July.  Yes, sir. I have made a mistake in my definition. Thanks for noticing. So it's definitely there's no path from i to Jay. Thank you.  Okay, so what am I talking about that. Well, because a set of conditions independent statement which are derived from the graph factorization. I see it as a proposition. So if I have a member of a doctor graphical model.  Then, actually it's a if and only if  So it will imply that x is condition dependence of X on the non descendant of I given the parent of I  This is for all  So if I have a distribution for which all these conditional depends statements are true that x is dependent on the non decision I given the parent  So basically what, that's what I'm saying here is that if I condition on these nodes then  What's happening here doesn't matter for excite user avatar   Okay. user avatar   Now what's happening here definitely matter with excited because there's there's a direct link within right so that's also why there's, it's the non dissonant that you look at  And and if all these conditions event statements are true. It also implies that piece in the isn't the graphical model. So it goes in both directions. So if  That's prove that  And that's where we give you an example of  Of how we use the properties of the factorization or something. And by the way, so already here you notice something. So the parents are also non descendant.  Alright, so I am repeating variable here. So that's what I meant in the notation notes that I can repeat variable for convenience.  It doesn't do anything. So because I could have been said, Done here non descending by minus parent of I. But then it's just annoying because anytime is right now.  They might respond to that. So it's just number two, right. So, but because I'm allowed to repeat in this is in the conditioning part in the non conditioning part is true. And then the other property that I mentioned also before was that by decomposition  We have that if x is conditional dependent of X non descended I given the parent. This also implies that x i is commercial benefit of xj given x by I  for all j in another set of eyes or any subset. Actually, you could have also the subset, but in particular for the singleton for any j in and understand the way  Okay, so by the composition. So we have a stronger independence statement. It also implies all the subset of these user avatar   Non this event. user avatar   Okay, especially the proof. user avatar   Proof. user avatar   So the first will I'll show the implication  To the right. So I want to, I have that P factor is the right way. According to the GM and I want to show that the conditional dependence works. So the key property that we use is that  If I fix. I  Then, there exists a topological ordering  Such that the non descendant of I  Appear or exactly before I  Right. So what I mean by that is I have then in my order, I will have all the non descendant of I, then I would have I, and then I would have the descendant of I user avatar   OK. user avatar   So you could also  Add. Let's see.  So you need the arrows to go like this.  So you could not have a dissonant of I before I  Because then there would be a past like this and then poof, like this. Okay, so, so you have been the descendant of I have before.  At all. After I  Okay, so there's a bit of fun happening in the chat. So somebody asked whether you had the marginal independence of excited given an understanding of I  So if I don't condition on the parents.  Is it the case.  That  Alright, so it is true that this know the here.  Would be so perhaps then I need to add some more nodes to make it clear. So let's go back here.  I will add some parents to the not to the parents.  Because there's two types of non descendant. So let's say I have other nodes here, right, blah, blah, blah.  So all these are also non descendant.  And so it is true. Actually, that this non descendant is marginally independent of exile, because I can pluck all these leaves.  And then I'm just left with a disconnected graph. So this is independent of that. So that's true. So there are some definitive by which are Molly marginally independent of excited, but these are also on the on the Senate of I and these are definitely  Connected to I, if I don't observe pie. So that's the difference.  It does who asked the question. All right. Yeah. So that answers your question. Dora. Great.  Alright, so we have. So this is by a topological sort ordering. So that's, I told you that we always have. We often use this from the deck property.  And then, the point is, remember, we can always pluck the children the leafs and remove them from the graph and nothing happens. So that's what we do. So we will pluck these using the  Leaf plucking property of their Africa. Africa, all that I proved last class. So we pluck all these  And so after we do that what we're left is we'll have that the joint of X I end the non dissonant ally.  Is just the product of all the other terms. So I will have x i, given its parents  And then I have all the G in the non descendant of I  Have p of x j given expired, Jay.  But now all the non this event. I've been  All the descendants. Sorry, has been have been marginalized out right so that's by the leaf clicking property.  Okay, so now if I compute the probability of x i given  The non personal device.  Which included parents, by the way.  Like the parents also know this. And then, so this is the joint divided by the condition the marginal. So this is the joint.  Problem with my numbers didn't  Understand by, divided by the marginal onyx I  Know the wrong marginal and need to condition on the non the sentences. The marginal of the non descendant.  Here we go. And so let's just write it down. So by the factorization property of the graph. It's the conditional x i live in the parent, then I have product over the non descendant of I  Exchange given explain J.  And then I some over x i prime. Same thing, p of x i given x by I  Product over j  Of p of x j  J.  Now, the whole point is  I don't have  I appearing anywhere in the non descendant.  So there is no I  Also, it's exciting time. Right, so there is no x i appearing here.  No x prime here.  And so I can just put parentheses in the sun, because there's no xi on the riots, I could just factor it. And so now, what I get is that this sums to one.  And so now, these things cancels out.  And I'm just left with the quality of x i given X parent  And that I'm done. So I showed that the conditional x i given x by an ex non dissonant lie minus by, let's say, let's just remove that is the same as just p of x i given expired. So that's the conditional DEPENDS WHERE WE'RE TALKING SO WHEN I condition on the parent  Anything happens in here is not important. It's removing the conditional right  So that's this direction. So that shows the the conditional depends statement and I'll prove the other direction, that if I have all the conditional statement.  I also have that P is the correct directory difficult is a part of this has a factorization, a cryptographic yes that's what I want to say. So now we suppose that P satisfies user avatar   Satisfies user avatar   All conditional independent statement.  And then we want to show it as override factorization. And so now we let one up to n be without loss of generality, a topological sort.  Of je  Je je je je je je  And so if one up to end is not the correct order just rename the node. So that, that's one up to him. That's what we mean by without doesn't generally t  Then  We have to have that the node before I are included in the non descendant of I  For all I  By  Political sort property.  Rights of because if there is and the lemon before i which is not a non this and and I, that means it's a descendant ally. Well, it can appear  It see  So we're saying, Where was my order.  So I'm saying they're kind of be a descendant of I before  I  Yeah. So if I put that. So that's exactly the same argument I mentioned here, right, so I cannot have a descendant here because it means there would be a path.  Back to this notes there will be a edge to the left, which is a the wrong direction. So you would have a package.  Okay, so that's all right. So I have that  The nodes. Before I in my topological sort or non descendants. So because they're non descendant that will imply that x i given  X expired i is independent of one of x i want up to a minus one. So, one up to i minus one.  So this is by the composition  Right, so  The set one up to a minus one is a subset of the non descendant. We already have that it's conditional dependent given in the parent or with all the non descendant. And so that's implied  And so now what we do is we use the chain rule. So I have that p of x v always is equal to product over I have p of x i given x one up to i minus one.  This is by chain rule.  Always true  And  We have that  By the way, also the parents have, I have to be to the left. So this differently includes the parents of it. So this includes the parents ally.  And now by conditional independence.  We will have the property that  This is just product from one up to n of p of x i given x just apparent by conditional independence. Right.  And so we have that the factors, according to the graph right this is pi.  And so, that implies that P belongs to the directory Africa model as we want to show  Jacob by decomposition. That's a property that I define last class in the middle. So there was conditional independence properties. Where was this that that  Yeah, so that's this one here.  So we call the the composition property is that if I have independence on the on several variables. I also have independence on the subset of these variables.  So, this implies that  Okay.  X.  Y z. So, where there's, I don't see any bar. I don't know which, in addition, you're talking about  Because I didn't write it properly. Oh, haha. And this was a typo. Thank you. Yes, this is very complicated. I mean, that doesn't make any sense. That's what I meant it was a typo.  No. Yeah, that makes sense was too fast. Okay, think it's time for a break.  There any question about this.  So better after after the break, we'll talk about the separation, which will tell us all the other condition independence properties of integral  Alright, so let's take a 10 minute break. It's 240  So let's go until  The separation. user avatar   My recording. It's user avatar   Okay, so then  So I already gave you a bunch of conditional statements. So what other conditional independent statements. There are  And that's from the separation, which you can basically get all the conditional depends statements which holds for all members of the family. There may be other which holds for specific but  We're only talking about the conditional in the band statements which hold for all distribution in there too difficult.  So let's define the chain.  Between two node which is basically the undirected version of other directed path. So a chain from A to B is just a undirected path.  Between a B.  And so that means we're allowed to go in any direction of the arrows.  Graph.  From a to b.  So now we'll define a notion of separation in the graph, which we call the separation for directed separation.  And so we will say that the definition is we will say that two sets of nodes. So set a  And B are said  To be dispirited  See here will take the role later of the observe variable in the conditional statements but my. Now this is just a graph separation notion we don't have talked about the observed variable.  If and only if  All the chains.  From a to b.  From a  An element of capital ache to it will be an element of capital B or blocked.  By a given  Seat. So in some sense, the observation said, See will will basically create some kind of blockage rule in if there's no path between A and B. They're basically decent narrative.  And  will define now the blockage rule and and the intuition for the blockage rule.  Is to avoid the V structure which introduce some user avatar   Connection. user avatar   So we do not want  This situation.  Where you will have, let's say, the structure and then at the bottom of the structure that would be some kind of like observable.  Okay, so what's happening is if when you have this kind of structure.  In a directed graph. Our Kamala, if I observed here, then it said this is a this is be then there is actually an interaction between me  And so this basically is not blocking the. That means that I would have a path going like this.  Alright, so what do we mean by the blockage and and basically the disapprobation kind of encode these three nodes structure that I talked about.  Alright, so basically we will say that a chain.  From a to b.  is blocked.  At node. And I guess I'll say block, given see because there's always in the context of  The sea set  At a node. I'll call D.  If there's two possibilities. Either  D is an element of see so d is an observed  Random variable and you have the mark of style structure right you have Vi minus one de VI plus one.  And that's the kind of like the sub path of the chain.  So you have that Vi minus one D and VI plus one is not a very structured  Yeah. So for example, you either have the latest costing so you even have the Markov chain property where you would have for example like this.  D and then they would be VI in the chain.  Or  You could have an actually, it could be also the other direction as well. You could have either one of these two direction or you can also have the  The the the latent cause structure, right, because this, this introduce independence. That's why it blocks the chain.  Or the other possibility, which is the weird part is when you have the structure, then you say d does not belong to see  And  Vi minus one de VI plus one is a the structure  So now if I have a V structure indeed does not belong to see  Then it will not, it will block it. If all the descendants are also not NC  Know descendant.  Of D is in seek  So basically here we have the situation where we could have Vi minus one. Then there's d which is not shaded. Then there's VI. It's not in see then I also have a bunch of descendant, but none of them are observed. So there's no distance descendant.  Hoops of the sea. So I there's no condition. user avatar   Okay. user avatar   That's basically the rules.  And as I said, we saw before, like the see you could think of, see as  For the interpretation that we want to see this is the set  Of observed  Random variable.  Conditioning  So for example, if I have a V structure.  And so, in particular, here I have a V structure D is not observed, but a descendant is absurd. So that means that it's not blocked, that means my chain will be able to go through and a will be dependent of  Be this case. So it's, there's no independence. This is a if there's a chain, which is going from A to B, which is not blocked. Then you have dependents.  And same thing. If I would have observed  The first one here. It also created depends  Okay. And so, then why are we talking about the separation. So now there's a proposition.  Which is that  Which I won't prove, but it's basically not too hard to derive so you have that, if P. Welcome. So you have the P belongs to l j if and only if x A given is conditionally Bennett of x be given exceed for all ABC.  subset of V such that A and B or  D separated  Given see. So this notion of separation is characterizing exactly all the conditional dependence properties that we have in a directive graphical  By the way, we will soon various will see very soon undirected graphic Amal  And the underserved Africa model the graph separation is sufficient is the one that characterizes independence. It's much simpler.  For the data graphical model because of the weird V structure, then you need this more complicated the separation. So, and then there's graphs separation property, which is a bit weirdly define is the one would characterize the conditional defense statement.  And then in order to figure out whether to variable or a disparity. There's an algorithm which is called the baseball algorithm.  That baseball. But these ball.  Algorithm.  And it is basically an intuitive or rhythm.  To check disappears.  And the idea is you will have balls bouncing around which represent the possible chain and they will be blocked by the rules of when chains are black and if there are some ball which is able to reach another know that means they're not independent.  So it's basically a rules for balls, which basically represent chains to be black.  And so  What are the rules.  So basically, there's so there's the situation. A in the separation.  That was the non the structure. So you could either have like something like this, where the node.  So when you have a Markov chain. Instructor So you either have something is observed in the middle or something is not observed  And so if it's not observed the chain goes through. It's not blocked. But if the thing in the middle is observed, then  The ball is blocked. Right. So if the ball is coming from this it will be blocked so then it couldn't go through the chain will be blocked.  And so that's what I mean by the arrows arrows his direction of the ball and if if there's a if there's a blockage. That means it's blocked.  Now in the Layton cause model. It's the same kind of idea if it's not observe it goes through. So the ball here would be able to go through like this and if it's observe. It's blocked so like this. user avatar   And then it would basically hoops and user avatar   So then I would have the ball here is blocked and the bug when there is blocked doesn't go  Then there's the weird restructure  So that's the situation be in this variation  Perhaps I'll keep it like this C. B. And so basically, now it's the V structure.  And then the situation is opposite in the Wii structure, right. So when I don't observe it, the balls are blocked. So in this case,  The balls, if the ball is coming like this, it would be blocked and if the ball is coming like this, it would be blocked.  Whereas when it's observed it's going through. So let's say I have this  Then it will be able to go through  And actually, we're allowed to  So we can think of the balls can bounce. By the way, so you can also think of just this structure.  And you could think of a structure where the, the two parents are collapse.  And so it turns out here that you're allowed to kind of like switch direction. So we'd be able to do this. So it's like, think of a ball state was going down and then it's bouncing back up. So you're allowed to do that.  And so why do we do this kind of weird one know thing is is for these change situation, right. So let's say I had this situation.  Well, the ball will be able to go through, because here it's allowed to go like this set out to go like this, not allowed to dance.  Then it's allowed to go like this and then it's an article, like this, like this. Right. And so that's why there's a dependence here Bollywood be able to go through  And clearly, then if it's not observe it doesn't bounce rate. So for example, if I had this thing here, then I would have that ball here. Get up, go back  Okay. And so for this you can see if you want more detail. I mean, I think here there's already enough detail to. It's kind of like  Have the intuition and to use it. For example, the assignment. You can use it if you want to figure out the conditional depend statement that I will ask that you can also see more detail in the  Definite Color. Color and Freeman book.  Alright, so let's see.  Let me review some properties of doctrine graphical model.  And then let's define the under to graphical model because this would be useful for the assignment, and then we'll talk about more properties of undeterred graphical model.  After the break,  See how much time doing for that. user avatar   Okay. user avatar   So properties of 32 co directed graph model.  So the first one is, I already mentioned in the past its inclusion, you have that if the edge sets are included in each other. So if I add edges then I just increase the number of distribution. So there's an inclusion property of my dark graphical mode.  And then there's the reversal. That's something we asked was asked before, if what happens if I reverse the edges.  And so the thing is if G is a director tree.  So if I have a directive tree.  Or a forest. So what a forest is just a collection of trees, but they're not really connected  So when I have a direct to treat means that a node as at most one parent  So that, so that means that pie is a smaller equal to one for all i. And that means that there is no the structure  There is no the structure. The structure need at least two parents  Right. So when you have no fee structure actually there is not no difference between directed under the graphical model.  And  Basically  You can have any other direction of the tree on this tree and it will give the same set of distribution. So basically, I will describe it like this so  Let's see. Oops, I'm having issues with my pen. Alright, so let's say this is my director tree.  And in blue are my headset. Okay, now I can define a new directive tree by just choosing a route that say this is my new route and then  Moving away from the root for the direction of the edges. So this is the same edge, this will be flipped this will be flipped and this is the same edge. Okay, so let's call this, this is a new route and now these new edges that Scott he prime  And so and so basically generally let he prime be another  Director tree.  With  Same undirected edges.  So I know how to introduce a new edge.  But I can flip some arrows.  By basically choosing a different route right choosing a different  Day. So basically the idea is I will reverse some edges for that.  But I still keep that it's a directed tree.  So you have to be careful when you flip things around because you could create a V structure. So for example, if I instead I had put. Oops.  So if instead  I had flipped this edge here, then I have the structure. So I'm not allowed to do that.  So then if I just get a different director tree with the same set of edges in the same with without looking at direction, you will have that it's the same there to graphical model so LG will be the same as LG. Okay.  And so rephrasing hoops temporary ink.  So we have that L of G is equal to Hell of G.  So rephrasing  What we're saying is that all the directed trees.  For from obtained from another at a dream.  Give the same directed graphical  So that's also why the direction of edges.  So, so, you know, this is why  The direction of edges are not causal, right, because I can sit them around.  And so the the two nodes version is the simplest to look at it. Right, so I could have y and x like this or I could have  Y and X like this and you have that the joint here p of x, y is equal to p of x given why thanks Pierre why which is equal to p of why given next time p of x, right. So both directions, give the same thing.  Okay.  So when you have trees.  The things are pretty simple. So basically things only thing which makes things interesting industry graphical in interesting. I mean, also different than under the graphical model, which will see in a few minutes is the fee structure.  And now, something that I mentioned earlier was, when we marginalize  So if you marginalize a leaf.  So marginalizing a leaf just gives a smaller graphical model.  A leaf node and it gives us  A smaller  The GM  And so what I mean by that, I mean, let S be the set of distribution. Q.  On the node on the variable one up to n minus one. user avatar   On user avatar   X two minus one such that q is the marginal  one up to n minus one for some p in my original bigger graphical model. So I get a new family of distribution by marginalize out any P  Then you have  That s is just a graphical model on G prime where g prime is G with the leaf and  Plucked or removed.  So that was the leaf plucking property I mentioned in the past.  But if you marginalize things which are not leaves you can get something different. So this is not true for all marginalization.  For all marginalization and particularly when you have the structure  And so an example of something which gives a set of distribution, which is not exactly characterized by any graphical model. So let's say I have this structure here of those two v structure. And now I decide to marginalize this node.  You basically get  From this you will get a set of distribution.  Which is not equal to any  graphical model, the set of distribution for some  Cheaper  And so you can think mathematically, you can say that marginalization.  Is not a closed operation.  On dark  Because you can sometimes marginalized and get something which is not a dark day before tomorrow.  And actually there's a bit there. There are, there's an extension to graphical model, which will capture this  The sort of distribution and basically it's mentioned in I think in the color and Freeman book. I'm actually not super familiar with it, but I think is the put these double agents, so it's it's still directed, but then you have these double digits.  Those marginalizing this node kind of creative direction in both directions of this point.  Okay.  So this only applies to monetizing leaf nodes. The question was the closure of democratization.  Yeah, so, so your clothes, only four leaf. I think  It's not only leaves. So for example, let's say I have this graphical model, let's say, a marginalized out this one, then you get a one graphical model which is still fine. So it's so it's not only true for leaves, but  You know, you have to be careful with which part you're marginalize  Okay, let me define the other from Africa models, too, because then you have everything you need for the assignment.  So let's talk about directed  graphical model.  And I'll use you GM instead of do D GM  And it's also known as  Marco random field.  Like MRF or also just mark off network.  So all these are the same thing, they're all talking about under a good graphical model.  So in this case, now we're considering g to be a Ji Ji. Yes, to be a underrated graph. So,  The GB and on directed graph.  So now we don't talk about DAG anymore.  And we'll also have another thing that will define which is the set of clicks and we use a notation script. See, so we'll say that script. See, to be the set  Of clicks.  Of G.  So what's a click, click, is just a set of nodes which are fully connected in the graph.  It's a fully connected  Set of note.  Alright, so he if  If the set see  Is a click.  This implies that for all my element of see such that is not equal to Jay. Then I need the edge I to Jay in my headset.  And you know the clicks basically  On one know there's just one node a click on to know there's only one edge on three node. It's basically the cycle.  On for know that looks a bit like a tetrahedron. Right. So there's an edge between everything, so I need these kind of like to triangle thing.  And then when you start to go with more nodes. Well, there's just a lot of agents. Right, so I connect everything here, then this one is connected to everything else.  And then this one will be this and this, then it will have this. That's it.  OK.  So now when we talk about a under a graphical model associated with another graph.  It's the same thing as in the gym that will talk about distribution which factor is according to the graph. So, and now we'll use the same notation.  Script L of G, and then to know am I talking about adaptive graphical under a phenomenal well is G D GM or Eugene. So that's kind of the type quotation here.  And so by definition this is p  Is  A distribution.  Over x v.  Such that the joint.  Is equal to the normalized product.  Of potentials, which are defined over the clicks.  For  Some we call them potentials function.  So remember, in the case of the GM we had these factors here. We'll talk about potential. You can also call them factor if you want. So I see such that the only property, they don't need to be normalized. All we need is that these are positive for all that you have exceeded.  And we also define z z is the normalization function constant, by definition, it's basically summation of all my nodes of the product for my potentials. Right. So it's just to make sure that this is a true distribution. I need to normalize it.  And this has a name. It's called the partition function.  partition function is coming from statistical mechanics, the terminology  And this is basically the normalization constructed  Alright, so if P can be written  In this product of potential where, where is the graph. Well, the graph comes into fact that I'm only allowed to have the potential which depends on the variables on the specific click in my graph.  And so that's where so see is basically telling me what where the graph is coming from.  Okay. And this was a temporary ink. Let's go back. I said, This is my definition.  And remember that.  When we talk about some over x v x visa sets of this means some over x one, x two, x three, blah, blah, blah. Right, there's this huge expansion.  So we'll have to see using properties of  The graphical model. How can we compute this part this partition function efficiently.  In general, it's actually NPR, but we'll get back to that.  Okay.  So,  I'll mention something quickly because I don't have time to go over. So we'll see that  Basically  Will see you next class.  That  When  The submission factors like rank of g will have that x is conditionally been to x be given x s when s separates  A from be in the graph. Okay, so this so we replaced the separation that we use in in the gym with just standard separation. Right.  And separation just mean you know there's no path under the path between end in the graph. And so, for example, let's say I have a bunch of  Edges here. And I have a here and then I have s there might be edges like this and then there is  Be there might be edges. But there's no edges between be an A or neighbors have a the all all the past from A to B have to go through so so as a separation sex. So if I condition. And so then I'm cutting all these path. And so now I get comes from dependence. Okay.  And you can derive this property from the fact from this factorization. And actually, that's what we'll do in the next plus  Okay, so is there.  What does potential  Mean here.  What's the potential. So say, See, it's just a function. So I see  Is a function from the sample space on  xe to basically the positive real number that is just a function on the positive numbers which give positive numbers and the semantic  Yeah, so it's a different semantics. So in dark graphical model, you will specify are the conditional know given parents. And so from a modeling perspective. You say, Oh, okay. If I know my age, what should be the distribution of my shoe size.  In a other half of them are all you don't talk about conditional at all. You talk about potential. And the idea is you would give high probability  To configuration for which the potential value is our high end because we take product of positive numbers.  So basically, you could say you could give say low value or perhaps zero to configuration that you don't like that are not likely.  And you could give high value to the one that you like. So a good example of that coming from  Physics would be you have spins up and down, there's actually something called the icing model, which is a special case of that and then you will say I will encourage the spins to be either the same or in opposite directions. So you could have see an attractive or oppose it.  Either a repulsive or attractive potential then depending on whether it's attractive or repulsive, you will  Put higher value and the potential for configuration. If you want to encourage. So for example, if it's attractive you want things in the same direction.  You will say when this is one and when this is one and the key is actually just the edge of the two notes here, you'll say, I don't know, potentially stand when they're opposite the potential will be one, for example.  And and that's basically kind of the model. And we'll see the will power tries these potential perhaps with like X have some W times features like this is like these exponential family thing and then we'll learn this parameters. But basically, that's the idea.  I don't know what S H I T is what she  But his script. See all maximal clicks are just all the clicks. So script see is without loss of generality, you only need all the maximal clicks.  So that's something I will revisit next class because if you haven't clicked, which is not a maximum click, you could always just absorb its own potential in a bigger click potential. You can just redefine  Potential as a product of the bigger click potential and the smaller keep potential because it's a subset  Oh you Brendan, your question is a hard to  It's a bit difficult to answer the baseball or within seems like a kind of state machine. Is there a connection to political dynamic with the meta  I. Not that I know of, but one would have to think very deeply about this question because the baseball has nothing paralysis. In this case, it's just reach ability or rather  Just not defining a polity, but the fact is you use to define separation in a graph and us you know that's the link with but it's not the same thing as a thing of plastic.  But to Matt Dan would be something more where you define the transition with probabilities and stuff like the baseball right now it says 01 aspect or either your reach or you don't.  I said, what, what is a good way to think of a random walks and they're coming from. user avatar Breandan Considine  Model and how this is different from under the graphical user avatar   Let's revisit that once we see the under to go from a lot more in detail. user avatar Breandan Considine  Okay, thank you. user avatar   Okay so SH I perhaps somebody was talking about.  Side, which is psi, which is the Greek letter. I'm using site.  And  It was fools. Ah, OK. So does sigh for those property rule like put priorities know. So for example, you don't need you know  You don't need that the summation overall XE upside see of x, z is equal to one, right. So this could be anything.  So the summation of sight seek XE could be any number doesn't matter. They're not normalized probabilities. Unlike the factors in a directed graph model, which actually are conditional  And size see takes instead of pointing belongs to click as its input.  Alright, so I see take the value assignment to the nodes to it's some it's clear right because, indeed, like the argument upside see is exceed X is basically x  For I NC right so so it will be a joint assignment to multiple random variable. All the one associated with a specific. So for example, you know, for example, I could have just those two know this would be one in two. Then I would have  A potential for this click one, two, which will take the variable assignment on X one, two, right. user avatar   So that's the nature of things. user avatar   And you could use index instead of like a set to  Index. The, the potentials. It's just pure it's for notation purpose.  Right. Is there any other question about the uncertain graphical model I mean we've covered them in much more detail, give you a bit more intuition.  Next class. One of the main aspects of under the graphical model is that because the normalization is global. It's not local.  And it's much more complicated to think about it intuitively, like, like, in particular, it's not the case that, you know, if I remove a node then then nothing changes. Unlike in a directed graph model, right. So, for example,  In in a D GM  If I have, you know, this V structure here I am marginalize out this node and I'm just left with this independent set right  When you marginalize out  This note was in under Africa model when I will marginalize out this note and I will keep an edge between they will be interactions between my notes.  Coming from basically this  This global factor of normalization.  Can I scroll a little bit before deficient have a huge em associated with user avatar   G. user avatar   Okay, here we go. Any other question. I mean,  I think it's time to user avatar Breandan Considine  Get user avatar   I didn't get my coffee today.  Thank to go for the things got it's Friday social user avatar Breandan Considine  I just have a quick question about  How would you  Encode some other operators on on this graph, right, it seems like each node has a single up operator kind of just mixes the two distributions from it's  It's an ancestors, so  I was wondering, will maybe, is there a way to choose like or assign  Either or rather than a mixture or do some kind of computation on  On, on, say, to to distributions, you want to, I don't know, add or divide or doing some other operator. user avatar   You're talking about the DGA right now. That'd be huge. user avatar Breandan Considine  Yeah, for the DG. I'm like, how would you yeah user avatar   Yeah, so for the new GM. Basically, it's always like the conditional of a node, given its parents  So the way you combine things is very specific. So I think the way to answer that is there was actually when I was at Berkeley. There was a guy from electrical engineering  Define other kinds of structures on distribution that could be exploited. So right now, here, there's a factorization structure that we use.  And it's a very specific way related with conditional dependence and there's other structure that are not captured by these the GM that you want to want to explore from a commercial perspective.  Other symmetries or combination and I forgot. What's the name of the thesis. If you send me an email. I can try to dig it up. user avatar Breandan Considine  Thank you. user avatar ezekiel williams  Cool. May I ask a quick question. Yeah, I'm, I'm still  Confused with the idea of these kind of like to see kind of  Graphs.  And just how how you can end up having this dependence. When you condition on the on the graph of it on the note at the bottom of the V. I was wondering if you could do. Um,  Yeah, I was wondering if you have any specific kind of like reading recommendations to understand these a little bit better. user avatar   So, so the the V structure is P AMP z given x y times p of x and feel right. Right. So this would be z x and y. So the joint whenever we structure is from this right so now if I want to compute  The marginal of p of x.  Yes. So when you don't observe. So, so now it's clear that if I don't observe z i just some over z.  Then what happened is I will some of resist. So this will become one. And I'm just left with a product and marginal, which is why it's  marginally independent. So it's clear from this factorization. That is margin independent. Now, why is it not conditionally independent is because if I want to compute P of x, y given Z. Now,  What I'll do is I'll basically compute  What I want to do. So I will have  I will have to basically define I will have the joint p of x, y, z divided by the marginal results.  And now what I will get is basically P AMP z given x y p of x poi and I will have summation over X summation of why of P AMP z given x y p of x. These are primary px Brian pure white pants. Now the problem is is  Because of this dependence here.  There's no easy cancellation. So I cannot just say, oh, I will cancel out my p of x in my  Because what I would like here. user avatar   Is user avatar   Yeah, because I want to have a factorization right at the end of the day, what I would like is something like P of x, given z times P of why given and what's happening here is because of this interaction here in the sun. There's no easy factorization that you can use to get back this  So this is a bit too fast, and I'll perhaps not the best way to explain it.  So I think the the the reading from  Colon Fremont will hopefully explain more diffuse structure aspect, but if you just work it out just tried to prove if you just try to prove the condition pants. You see why it's not working. It's because of the just the way it's  Okay. user avatar ezekiel williams  Okay, awesome. Thanks, and I'm calling Freeman taxes do you give the full name of this text on the user avatar   On the website. It's one of the main textbook. user avatar ezekiel williams  Oh, wonderful. Excellent. user avatar   And perhaps I'll stop the recording. user avatar   Now, user avatar   The loops. user avatar Sarthak Mittal  So I wanted to stop the recording. user avatar   Doing to that step because
  Okay and. And so another note is  I am going to  Post the scribe notes from  Lecture.  Which which one we're missing.  Yeah so lecture 12 on GMM and em.  The volunteers who sent the sent me describe notes. So I will quickly scan through them and then we'll post them by tomorrow.  So that will be helpful for for your assignment.  And I will also  Post  The there's a proof of something that used to be the scribe notes. Sorry, in the NBA.  There was a statement in the M via scribe notes that they will also include in these in the scribe notes which are already online. I think the one for  Either lecture 12 our limits. So I'll post that on Slack and so that could also help to you about seeing other type of proofs for graphical model, which could help you also for the assignment which is due next Tuesday.  And the last logistical point is  So almost one and two has been graded grades are great. You've got the feedback normally from a wholesale if you have  If you have questions about your grading, you don't understand something competitive solution you're welcome to come during office hours, of course, a to to ask him about it.  If you think that you should have points for something that you didn't get  The idea here is, don't go too crazy about points. This is like a grad level class.  So, normally the grades is not what matters is basically your learning experience and and note that the the grading process in general is just a noisy process. It will never be  Always fair perfect everywhere in particular because also, it takes a really long time to grade these things. And there's 80 of you.  I think wholesale is usually quite fair, actually, and he does, he's pretty thorough about about how he does it. So the idea is there might be like minute  Discrepancies which normally deal with average out over the class because it's not like you will be specifically always making the same problem for for your assignment.  But if you if if it's really an obvious problem, like he gave you  Re grade the form to feel so that it, it helps them manage all these requests.  And  And yeah and and but be aware that if you try if you're like starting to becoming argumentative about like a rig rating aspect, which is very subjective like  You can decide to look at your assignment and be much more careful about the grading and perhaps you will have less point of view so  That's kind of the incentive, just to say, you know, chill out about the grades were trying your best here.  Perhaps if there are obvious mistakes. That's correct. That but but for the other thing which is like well I think perhaps I should get more points, this is more for undergrads like this is a this is a ground level class, we expect that user avatar   You can move beyond this thinking user avatar   And then somebody asked where did he get the feedback on the homework. I can see. So it's not in studio, I think, I think he sent directly the solution.  By email to the people submitting an assignment and the grade should be, I think, in studio  Correct, yeah.  Okay.  And if you have further question about the grading and stuff, please contact Jose.  Alright, so let's continue the under to graphical models so  Today what we'll do is we will finish.  The presentation of the underdog graphical model. And if we have time, we'll start influence  How to compute marginal or it should try in a graphical model. And so just to remind you, because it's been a while. So at the end of last class I mentioned what was under the graphical model.  So basically we mentioned that  Again, a huge under the graphical model, it's a it's a class of distributions right like Derek a graphical model.  It's a class of distributions, instead of being associated with a directed graph a DAG. In the case of directed graph model, a huge GM is associated with the undirected graphical undirected graphs. All right.  And what are the distribution in my family Dr distribution over all my variables, such that the joint factor is as a product of potential over  Subset of notes. Oops, I guess I'm user avatar   Not using temporary ink. user avatar   Hey, user avatar   Let's go back. Yeah. Now I want my temporary ink. Good. And so these potentials  They're just positive their functions of the possible assignment on a subset of variable on the which are associated with a clique.  And the click. As you remember, are basically set of nodes in the graph, which are all fully connected. So for every pair of nodes in in the clique, there should be an edge in my edge set  Okay and and basically we're allowing when we have a cleat were allowing to use in my ordering distribution definition, a function of all the assignment in the in the of all the nodes in the key. Okay.  And yeah, and so I and so  A joint distribution will be in my family if it can be written as a product over these function over assignment to variable over tix and  The only constraint on these is that there are positive. They don't have to some to one, because unlike in the gym where you everything is local. You know, realize because there are conditions in a huge em.  I only use a global normalization constant. Okay. And so z here is the opposition constant is a summation of all my possible a variable assignment of the product of my potentials. So to make sure that this distribution is really something to one. Okay.  And so an example of a clique is just having  One edge between two nodes, right, because two nodes or if they have one, which together as a set, they're fully connected. And so that means that if I looked at  I can have potential which varies on the  On the depends on the values of the variable on on the edge x one and extreme case. user avatar   Okay. user avatar   Alright, so now  Let's make some notes about this distribution.  About the GM  So some observation.  So the first observation is that unlike  A directive graphical model.  The potential  On the clique is not related.  In a direct fashion.  To the distribution and the kick  And  And so was in a dark graphical model.  You could think  Of our click  To be  A know that it's parents  I mean, it's not really a kick in the gym because there's no connection. This is lead between the parents, but let's just think of, oh, let's say this would be a click.  Then you could use as your potential  The probability of x i, given the parent  Right, this is a valid potential in the huge em terminology, because it's positive, and it only depends on the nodes in this creek.  And in this case, the potential as to do not with the marginal on the variable and the key. But then the conditional of excited, given the parents. So at least it is some kind of like distribution. Right.  Whereas we will see later when we do a inference in your GM that it's not the case that  That the potential is directly related to  The marginal case basically the marginal and exceed will depends also on what's happening on everywhere else in the graph, which is kind of annoying from an insurance perspective. So in France in uncertain graphical model is much harder than a directed graph model in some directions.  So for example, if I want to compute  The, the conditional of excited, given the FBI in a directed graph Gamal. You don't need to do anything. It's already given to you in the in the formulation of your, of your, of your distribution if you just read if it's the it will be the factor on site.  That gives you the conditional so you don't even need to do was, if I want to compute the marginal and then a node or even a conditional, I will need to actually propagate information from the whole graph in the region. So we see that later when we do influence  So Jacob is asking, Can I repeat what I mean by  The normalization font, the normalization being local versus global. So basically in a other graphical model.  The only normalization aspect is z, right, which is globally. My joint. My, my, that I will normalize my product of potentials, so that it's some to one globally. Whereas in a in a D GM. Let's go back to the D GM user avatar   Back. user avatar   Here we go.  So in in a D GM I have that it's a product of potential or factors, but these factors are already normalized, so there are constraints on these factors. So they're locally normalized, so that's what I mean by that.  Okay, so another observation to make about you GM is that you can scale.  The you can multiply your potential by a constant.  Without changing the joint.  Right. So for example, let's say I have my potential. I will call it new.  I have a new potential on exceed and I'll just define it as the old potential  Time, a constant.  Let's call the constant  For some  A bigger than zero, right. So if you set it to zero it will, of course, change something, and if it's negative, it won't be a  Legal potential, but if you just met the play by some constant bigger than zero.  It don't change anything, because this is a constant for all possible assignments, so it will be absorbed by Z later on because you will define like Z was the sun.  Over the product of the potential. Here we go.  So this will be multiplied by a if I've changed one of the punishable by eight. And so z will just be multiplied by eight because this is a constant, so it will just get out of it. And so then I will have a here and I will have a there, they will cancel out. Nothing changed.  So the potential or not well defined are only defined up to constant like you can change them arbitrarily, and it doesn't change the distribution. So they're not identifiable in some sense, given a distribution. You can say, oh, here's the unique potential to write it.  Okay, whereas for the case of directed graph model if I fix the graph, then the factors were uniquely define because they were just a conditional which are defined from the joint. So that's the difference.  So in some sense, unlike  The GM, where f is the factor where unique  Defined from P  So Jacob is asking if I know the potential on a clique, then I know the marginal on see up to a constant.  No, actually. So that's the problem. You don't know the marginal  Because the marginal on XC  So the marginal and X si p of x, see is a summation overall x  In the minus C of, you know, this big product overall team.  And and so so the marginal. The next see depends also on the potential on on on other  Key  So just with the potential and exceed you don't have sufficient information to divide the marginal  Will see that when we go do in France.  And so as third aspect to mention is that we don't need all the clicks.  We can only look at the maximal peak, it is sufficient to consider.  I'll use the annotations see script max which is the set of maximal click  Of maximal kicks in my graph.  And the link is maximal if it's not contained in a bigger click in the graph, right. So, for example,  If this is my pattern so 123 so one two is a clique, but it's not maximal because it's included  In  It's included in 123  Which is a geek.  And 123 in this case is maximal. I mean, it's trivial yet say I will add another node for. Here we go. So that's not just one full clean graph. So, in this case 123 is a maximal peak.  And so  So why can we only look at the maximum or click. Well, for example, suppose that I have a clique see prime which is included in a bigger clique see  And what you can do is instead of having to talk about the potential and see prime, you can just absorb it in in the potential of a bigger teak right so you can redefine  The potential and see  To be the old potential  On see times the potential, which was on. See, Brian.  Right. Because, see, Brian is a subset of see then this is still a valid function on the variable in see  And so then they all need to have a specific potential just on zebra.  And if  See prime belongs  To more than one click.  When you transform things to only do a prime position and maximum to just pick one of the to click to assign it to just pick one.  So a good example is, let's say I had  A tree. So then the maximal clicks or or size to for the edges, but I could still a node is also a one node is a trivial Keke of size one. And so in the  If I looked at all the kinks. I could also have potential a node, and I would have potential on pair of nodes basically called an edge potentials. And so then. And so, in particular, let's say we have this so I could have, let's see, I, J, and so I would have  A potential and I  A potential on Jay and I would have a potential on the two nodes. So I'll call it i j x I extract it. So these would be three potentials. I could have and now  Walk in this case, it's kind of stupid, but let's say I would have another edge here. L. So you could decide  This node potential. You could assign it either to the potential and jail or the potential and you just have to pick one of the two because those two are bigger teeth, which include the node.  JS.  And so, so to not have this redundancy and some sense. We don't consider a smaller set of clicks. We can only look at the maximum click if you want, but we will see later that sometimes it is convenient to still consider smaller teams. So we'll see later.  That it's sometimes convenient.  To consider basically an overall parameter ization of your  Of the potential in your graphical model.  In particular for trees.  Where you will not only use the edge, which are the maximal clicks. You will also look at the node potentials. We have both sigh of x i.  And sigh, J.  Of exchange.  Okay.  And so that's the notes for the GM and let me answer a question that so Ezekiel asked pulling up on Jacob question.  The potential gives us the conditional up to a constant. Correct.  depending which condition are talking about. But again,  The potential the conditional on  Subset of variable and exceed let's say for example,  Let's say I have  See is one, two,  And I would like to look at p of x two given x one, right. So this is so if I have p of x one, two, by the way.  If I have the marginal on on X one, X two, I can compute by marginalizing it the conditional of extra given excellent right  So this is basically p of x, right. So that's what I meant by that. So if I have to see I can complete the conditions.  But I told you that the the PFC is not obtainable just from the potential and it also depends on the potential around it through the interactions, because it share nodes with potential which are use somewhere else. So, for example, okay. So if I have  This graphical model 123 so here I will have a say one, two and upside, two, three. Okay. And so if I want to compute the marginal on x two, x three, this will be summation over x one of, say, one, two times say okay well I can even write it down.  Say one two x one x two.  Perhaps I should have not put it so crowded that go back user avatar   Can I try to do this work. user avatar   Fancy. user avatar   Them. user avatar   All right, here we go. So  So this would be equal  To say one, two, x one, x two and site to three x two x three. Okay.  And unless I want to  factories in a specific way and see as a product of a potential and, next one, next to. So you can see here that to compute  This marginal. Oops.  To compute this marginal I need the knowledge of the potential on X one, X two.  Because x to belong both into the potential and extreme, extreme but also expanding  And so just knowing their potential on two three is not sufficient to give me the marginal. The next  Does this answer your question.  Okay. Well, sure.  Am I missing an opposition constant. Sure, let's put our numbers issue, but that's just a concept.  And this is to contrast, by the way to a doctrine graphical model where let's say I would have  You know this graphical model 123 then I have that p of x two x three is just p of x two given x three times p of x three.  And so the potential an X two X three is not sufficient, but also adding the potential of the parent, then I'm done, I don't care about the potential of X one given X two.  And that's the magic like because of the directionality of things you were able to kind of like just removing those without caring about it and and things are still locally. Fine.  Okay.  So that's for the differences. So let's talk about some properties of you GM  Which you know analog in analog way to the directed graph model.  You also have that when you add edges you add you make bigger clicks. You make bigger potential you have more possibilities.  And so that means that the data under the graphical model gets bigger. So if I have  The edge set of he included in a prime, this will imply that the under the graphical model. According to G.  is included in the one of g prime right  And so in and, in particular, again, if there is no edges if the edge set is you get the smallest under the graphical model.  Which is, in this case, the set of fully factor is distribution.  And if instead I put all the edges in  So I put all pairs.  And so that means in this case that g is just a big click  Because all pair of nodes in my graph or connected. So all my nodes are one big clique.  So basically you can say the joint is just proportional to a potential on all variables. Well, then all distribution can be written this way. So, this implies, same way as the GM that this is the biggest you GM where you have all distribution.  And again, here we're into discrete city.  If we're starting to talk about continuous variable, then it gets a bit complicated, because you need to talk about what's the bays measure and the variable to touch right so it's a bit more complicated. We'll get back to that when we talk about continuous distribution.  By the way, so similar makes a interesting point. So he says, in a way, undirected edges mean Decker being dependence depends in either direction. Correct. Of course, that's why they're undirected for the dependence is definitely not directional dependence.  And also the definition of the you GM and there was no direction anywhere right it's just  The only thing which matters is the is a set of nodes which are all connected. And so there's no notion of one is connected to the other, or in one direction.  Okay, so that's one thing. Another thing I will mention is, I will make a link with something called the exponential family so if  Your potential are strictly positive  Then what you can do is you can write  The joint  Of your you Jim. Bye.  Taking the log of the potential and taking the exponent. So you have exponent of the sum over my clicks of the lug of my potential  And then minus lug of my normalization function right now normalization constant because the some of the log is the log of the product.  And then the X OF THE LORD cancels out. So I'm left with the product of the key right so so that's basically  Does the same thing as before, and x of minus lug is the same thing as one over zero, right, so I've just rewrote  My product of potentials normalized by z as x summation a lug and this is valid because this is not minus infinity because these potential or positive. That's why if you have a zero. It's a bit annoying to have lug of zero, because this is minus infinity. So,  That's why we make this restriction  And why do we do that well because now you can think of this summation here of potential  You can think of.  You can think of this  Potential here as taking the product.  With a parameter which depend on the click and some sufficient statistics which depends on exceed  Were basically  TC XE it will be an indicator function, it will be one when  X prime  Is equal to  X see themselves.  And it's basically just you can think of.  For every exceed I have a potential. Now I just take the log of that. So for every see I have a log potential and I can then  REWRITE THIS AS A can think that my parameter of makes mention of family or just these look potential. So these would be the this data.  And and then the sufficient statistics in this case is just selecting the correct parameter  And we'll see that again when we talk about the exponential feminine more detail, but be a. Keep in mind that undirected graphical model on the screen variable A can be always rewritten in the exposure family can be written in this way when the potential are positive.  And  This has something related to physics which that's why I'm also talking about it. So there's a this model is using physics.  And this thing here.  The sum of potential here could be the negative energy function.  Right. So in physics, you want to minimize energy. And so the state with high priority will be with the state with low energy that's why  You can think as the product of these some of these potential as being the negative energy I take x of negative something  So if it's a small energy export negative small will be big. So this will be high quality. So basically you can model the, the state of physical system and statistical mechanics as the state with low energy will have higher quality of appearing  Okay. And this is actually used for example for spin system or magnets and this is called the icing model.  And so, for example, there's this thing called the Isaac model.  In physics,  And this is a model which can be used for repulsion, or attraction between magnets and you can think in this case the variable. The will be binary 011 would be up zero would be down for the two directions of your spin or your magnet north south.  And  You could have a graph which looks like this where you only interact with your local neighbors.  So this would be a lattice and Xi would be the spin up or down or magnet up or down in a physical system.  And then you would define potential on the node to the node potential will tell you. Oh, what's the energy of being up. We call it AI and you will have an edge potential  Which would tell you what's the energy of having both nodes being up when their neighbor and you would call this API j and just defining  The energy of both up and the energy of one up is sufficient to describe the system, you don't need the other one because the probably teams. Basically, you, you, you can redefine the, the other possibilities by just subtracting the marginal  So this is called the minimal parameter ization and we'll get back to that as we talk about minimal transition of graph.  So here he i is the potential for x equals one and j is the potential for x equals one and x equals one. user avatar   Okay. user avatar   And so this is a model which tells you distribution over distribution of  Assignments spins and then you can ask question. Okay, well, what's the most likely state will, it will be all up or down or mixture depends on this energy, of course, and then you can look at what's the priority of a specific spin to be up, so that would be the marginal of xi calls one  And it turns out, actually, that computing this marginal properties NP hard, unfortunately. So we have to approximate it so we'll come back.  In later class when we talk about sampling and virtual in France. And actually, that will be your homework five homework five you will approximate the marginal properties in there in the model.  So that's an example of a place where I understand Africa all makes sense because you don't have any directional interaction, right, it's, it's very, it's basically, you know, symmetric interactions. Another example is using social network modeling.  And so, for example, you could have  Edges, which represent friendship between people. So you would have a graph which represent the social network and we will suppose in this case that friendship is symmetric. And so you would have  Know this connected to all their friends and then there could be some attributes on these nodes, for example, or voted Republican Overton Democrat.  And then you would want to, you could define a model of like, well, if I vote Democrat was apology that my or how should influence my neighbor to vote Democrat.  Or the opposite. Perhaps. Perhaps you know people like to vote. Unlike difference which I mean that's not really a good model. But, you know, that could be also something to consider. And so you could use an underground model to model that  And so in this case, instead of specifying conditions like you do in a demographic Mo, you just described influence. Basically, if the potential of an assignment is high, it basically mean you're encouraging it versus the potential being low  And so in this case you could if you want to affinity model SAYING THAT WILL YOU WILL VOTE like your friends, you could say, well, if x is equal to one and x j is equal to one and you're connected. This will have higher potential than being the opposite. So that could be one model.  And then you can also learn these parameters, of course, will see that when we do learning in Eugene  Alright, so are directed graph preferred for modeling temporal dependencies, or is it possible to model a time dependent system using a huge em.  How would you model the temporal random variable in the gym. Yeah, so I think  Often people think of time as being directional so director director graphic I'm are more natural for temporal problems.  You can still use a huge em. If you want to have these long range dependencies across time.  And in particular.  Not in a time model, but this was used for a natural language processing. So we're not trying to rush processing. Let's say you have a sentence and you want to tag the elements of your sentence. So this is called parts of speech tagging. So is this a noun, it's a verb.  And the model could be for each word in your sentence. You have a variable with your present. What is the tag of this variable and one of the most successful model I  Guess in the 2000 was to use conditional. Conditional random field for that which use undirected model on the tags. It's not directed mode. And so basically the tag at the end of the sentence can influence it back at the beginning of a sentence and vice versa.  Can you have a mix directed under the Africa model or in such a case, it just doesn't make sense to think of it as directed, with potentially parallel edges.  I think there are extension.  Of graphical models which are neither you GM or the GM, which also handle these by director all edges that gets in colors and Friedman's book, but I'm not familiar with it.  And you can definitely define distribution where one piece is a huge GM in one piece is the GM right so i. For example, I could have  P of x A as  You know, one over z product over clicks.  In a have some potential exceed and then I could say that p of x be  Given X say could be the product over I in be some conditional of x i, given the parent user avatar   Right. user avatar   And so you could now the joint will just be the product of the marginal of eight times the conditional x be given x A. And so one piece will be model as a huge GM and the other piece will be modeled as a DJ.  You can also do that.  We won't cover that to the staff.  So somebody is asking about  Being  Is is a family defined by the gym, always a subset of that defined by a similar Eugen so we'll come back to that. When talking about moralization of a graph. So let me come back to this question.  Alright so let me states the conditional independent statement that we will prove after the break, okay, because  That's something important, and also related to the assignment that you're doing with the  Hammer certain as it called hammer  Hammer slick Clifford theorem.  So what are the conditional independence statement.  implied by the GM factorization independence.  For you, Jay.  Alright, so first we're talking about a graph.  Okay, so we say  That a joint distribution p  Satisfies  The global market property groups.  And this is making sense in the void. It's always with respect to a specific graph.  And so it's understood that there was a graph that we were talking about. So it's with respect  To a undirected graph G.  And so we will say that the Property Distribution satisfy the global market property if and only if  For all a b s subset of the  Ground said v.  And these set of these subsets, or this joint  Such that s separates  A from be in G and this is not a directional. So if you separate A from be also separate D from a because there's no directional at this  Then  We have that x is conditional dependent on x be given access  So that's  Basically the implied  Conditional dependence statement in the GM. And what do we mean by separation. So basically,  Let's say I have nodes.  In a  And then I have nodes in S and I have nodes in be okay. And you're allowed to have edges between nodes and a  edges between us and edges between be and then there's edges between DNS, but there's no edges. So you don't have any edges from A to B, like this. So, this is not a valid edge. Okay.  And so when we say that s separates a from me. It means that any path between A and B has to grow through s  Okay, so when I condition on. So I'm now or actually went to remove s and its edges then I'm separating MB MB are separated. Right. And so, for example, I can also have notes here.  And I can have these kind of connection can have these know the discussion groups and  So I have notes here I have connections between the sets, but I'm not allowed to have connection between here because then I could have a path which doesn't go through s  Okay, so, so, and the nice thing in your jam on like the clock complicated the separation in the GM  In new GM. It's very simple separation is just like is there a path between two to variable in the graph. And when a conditioning variable like access. This is the things I'm conditioning here. All they do is, you know, this is the separator set and it blocks the path.  And if all the path of blocked by. So then we say that the verbal or independent clinician independence. Okay. And actually, that's what we will prove after the break. So let's have the preposition.  will prove that if he  Belongs to my Eugene, so it's factors, according to the graph, then it implied that P also satisfies  The global market property.  4G  And does all these conditional depend statement or planning, there might be more as usual. For example, the fully independent distribution is always in me on your GM. So it's also implied, but this is saying that these conditional depend statements are true for all members of the year.  And will prove that after the break.  Unless there's any question that you want to ask before the break.  Nope. Alright, so let's take a break. It is  333 so that's come back at 343  All right, so let's do the proof. So we want to show  That  If my distribution factor is according to the GM  Formulation so according to your undergrad graph.  Then it also satisfy the conditional depends statement.  Of the sort. So x a conditional. The benefit of XP given access when s separates a from being my graph.  So the first thing we'll do is we will prove it only in a specific setting. So, the proof.  So again, without loss of generosity. So, W. L. O. G. Without loss of generosity, we will assume that A, B and S or actually  All the nodes in my graph is equal to be there.  And to get a statement will assume that A is separated  By the  So from BBS  And  Why are we allowed to say that. Okay.  Well, if we're considering a subset A be an S.  Then what we can do. Oh, by the way. Sorry, let me know if if I'm, again, if my mic is  rubbing against a nice shirt. And it's annoying because I need to move my cable so that it doesn't do that.  Okay, so if we are trying not to look at the conditional depends statement for a BNS which are not all the V. What we can do is just  Do a bigger set. Okay.  Actually, I'm not even sure I  Hmm.  Interesting. I don't. Yes, I just realized. Normally, I always make this argument that oh let's create the bigger set. But the point is,  If I get x A  Conditional independent of x be given access  This implies x A prime  Condition independent of x be prime given access  For all  A prime included in a  And for all be prime included in beat  By the decomposition property.  And also,  If A is separated of be by S. It's all the subsets are also separated, right, because it's even like easier to separate subsets. So a prime is separated user avatar   From the prime by its ok user avatar   And so we don't need by using the composition, we can prove the statement for all the subsets. So we only need to prove the statements for all the possible partition in E, A, B and S, which takes all of you.  All right, that's simpler than what I used to see before. Interesting.  So, alright, so we are so what we suppose, as we have that v is a union S union be  And so it will look something like  I have all my nodes in a or S or B. And then I can have edges.  Within all of these and in between, but not edges between A and B directly  Okay, so now what I do.  Is I'm  Let's consider a clique. So let's see.  Via click  Because of the separation property, you cannot have  That see  As some nodes in a  And some nodes and be  Okay, so either see is intersecting some nodes in ar S, for example, or sees intersecting some nodes and B and S, but it cannot have both in A and B, because if  There's nodes in seeing both A and B, because all the edges between pair of nodes are  In the graph that would create an edge between A and B, which means that there's a direct path between A and B, which doesn't go through. So it's not separated right so that's why because of separation. Right. So this is because of separation.  Okay, so why do I make this point is because I want to split the clicks into groups. Okay.  So basically, by the definition that P isn't the GM. I know that P is written as a product over clicks. Right. So what I'll do now is I will split my clicks into the ones which are included in a union S.  Say see exceed  Product.  The other clicks, which are not in  A union S. user avatar   I'm here. user avatar   And so, I mean, this right now. I've been said anything. But the point is that if you're not in a union S.  If the clique is not fully included  In this piece here.  And it can be  Fully included and be, that's fine. It can also be including a bit of be in a bit of s, that's also fine, but it cannot be both in being a right okay so that implies by by what I've just said.  Here and so that implies that if the clique is in is not fully included in a union S That implies that I have that see prime is actually included  In be Unionists  And actually, because it's not included in a DNS. It's not including today. So it's also, it implies, this is not important. Actually, but it also implies that see prime is not included in S.  Alright and so why do I care about that. So that means that basically I have a product of  Of potentials were either to click or is in a union S or the click or in be union S. So that means I can factor eyes, my  Joint in as a function of only variable in a union S.  And a different function which only dependent variable in be unions, get some something I've splitting the dependence on A and B in two different factors.  And that's a crucial aspect to show the conditional independence day so  And so now we actually the rest of the proof is very simple. So that was the most subtle argument. And so the rest of the proof is basically just showing that the the some property of conditional dependence. Right. And so basically we'll use the the the  Factorization proper property.  And so we need to compute the probability of X a given access right  So we know that this property is proportional to the joint on x a, an access  Okay, so, so somebody is asking the clicks, which are fully in S I would define it in this function here. I mean, it doesn't really matter. You could also put it in this function if you want  You just need to make a choice, but in the notation. I've used. I said, if a clique is fully included in a union S that includes also that it's fully included in s right.  So if it's not included in Alien so then it cannot be fully included in S.  Okay, so yeah, so it's been presented the joint.  And user avatar   Oops. user avatar   And this is just x A union S, by the way.  And so this is the marginalization of my full giant with respect to x be  Okay, and that's because all my nodes are either in a B or s right. So a union S union.  Be  Is all the that's why this is true.  Okay. And so now this is summation over x be I just wrote above f x A union S and G x be union s right. And so there's no there's no be in the first term, so I can push the sum over there, right. So this is the same thing as f x A union S summation or XP of g x be units.  And this is a constant.  With respect to x eight because there's no either  So that implies  That the conditional  Of x A given access  Is just  F of x, a union S divided by the marginal and access. So it's the summation of this over excess of a  Union. user avatar   It's user avatar   Just an Mars ization of it. So now we have a simple form for the conditional  And similarly,  We can use the same argument on G. OK, we can get that the conditional of x be given access is just  G of x be union S.  divided by summation over x be  Oh, yeah. So there's this is a problem. So I was not something over the right variable. The marginal to get access, I need to some x eight so I will do that.  And because I don't want to screw up meditation, I need a dummy variables. Let's use x A prime and then access when. Same thing here, x be prime g of x be Prime Access  And Jacob has already seen my mistake. But I've corrected it before seeing his question. Alright, so I have these simple form. So now if I take the product of these two quantity. So I have that the product of x A given access times x be given access  Is f x A union S g x be union S divided by summation over x A prime  F x a Prime Access summation over x be prime g of x be Prime Access  And actually, I can move the summation over x be out because there's no x be on the left.  And so what I get above this was just the  The joint right, by definition, I just said that my joint was the product of f and g and so  I have that the top is the joint.  And the bottom is a summation of my joint over x A prime minister, the prime, which gives me the marginal. So this is divided by the marginal over access  Okay.  And so by definition this is just the conditional of x A x be given access  And so conditional of x be x x be you've given access is a product of x A given access times x be given access. So that means that I have that x A is conditionally dependent on x be given access as I wanted to shoot  Any questions about the proof.  No question. I mean, or your bit confused. The proof is in clean form in the scribe notes online.  You can also walk it again in more details there. The main part of the proof was just splitting in terms of  The potential. In fact, rising two pieces right  That was the, the crucial part here.  And and and for doing this or I guess it was this part here.  And to do this, all we did is that we use the fact that the clicks had because of separation could only be in one side or the other side.  Okay mode is asking what's the difference between f of x A  So f of x, a union S.  Is the same thing as  F of x A comma EXE s  Know that a an S or disjointed this case as well. Right, so f x A comma access as no repetition of variable. So it also makes sense to write it like that.  Because if you remember, like, we can use set as subscript of the variable to mean  Basically  An expansion of x i, where I belong to the set. Okay.  And so  Here I have split  A union S BECAUSE THEY'RE JUST JOINED BY THE WAY AS just set a and set so so that's why I cannot say and then access. And then if you wanted to write down this quantity in the full you would have  X i. For every element of A and xj for every element of S and then you would have this huge chunk right it's just short in the patient.  And the other question.  Was that the right implication was that right implication from left to right. Yes. So we were. I'm not sure which right you're talking about. But this was to prove that if we assume that the distribution is fact rising according to the GM, then it has the conditional depend statement.  So the converse is also true, but not always. And that's what you're actually exploring in the assignment.  So here's the theorem, which actually look at the converse and it's a very deep theorem. It's called the MSA and mer slee  CLIFFORD  Theorem.  And it only holds if your  Density or a PMS is always tricky bigger than zero.  So if you have that  Then you have the converse hold and thus you have the if and only if you have that P is in a huge GM so factories. According to G if and only if  Actually, I guess.  If not, if, if P satisfies the global market property.  Purple.  Mark of property.  4G  So if your distribution is tricky positive and all these conditional dependent statements, according to the graph hold, then you know there's exists a factorization. According to the GM  If there's some zeros weird stuff can happen and that's what happens in the assignment, we show that you will define you will you will you will define the distribution for which  The conditional depend statement holds, but not the factorization.  There's no factorization, which works.  Records, the graph, even though all conditional statements.  And for the proof. The proof is actually complicated, which is why we like the proof that they just don't. It's actually pretty simple. From the perspective of these kind of proof, but the converse is is very hard and and if you're curious, you can look at chapter 16 in Mike's books.  He goes through it.  And basically, there's this trick where you use something called the most obvious inversion formula.  Which is related to the something called the exclusion.  Inclusion principle.  In property theory.  Is basically when you want the probability of a of a joint of events of the union of a bunch of sets you can rewrite it as the probability of  One set  Plus the intersection of the two sets minus intersection of things anyway. So I'm not saying it correctly. But basically, you will  You will look at the possible like union of sets and then you will double come think so you'll have to subtract them and you need to do it carefully so that everything works. And this is called the exclusion inclusion formula and user avatar   This is actually using user avatar   To basically define so given that you have this realization of conditional independence, you will define the correct factorization, which works in the graph.  By kind of like taking these these local conditional and then recombining them together, but then you'll double counting. So you have to exclude them and so you'll have to use this movie's inversions are from it.  Okay.  Alright, so that's far as the conditional dependence. And so now let me go through  And other properties of a new GM  And then I will finish by summarizing you GM versus the gyms, so more properties of ujian  So unlike the GM  The gym or close with respect to marginalization.  Okay, so let's say I have this graph.  And this is the note that I will marginalize up. So what you do is you need to connect all the neighbors of the node you eliminate. And so you need to connect those two notes to add this edge in the new headset.  But then it turns out that the set of distribution you obtained by marginalizing out this node is the same as the set of distribution, you would get on the smaller graph with this augmented edge. Okay, so if you let the prime to be v minus  And  And you let a prime  And your end could be anything. It doesn't have to be a leaf. There's no notion of a leaf anyway on  I guess you could have these extreme edges for extreme nodes in the undirected.  Truth. But anyway, so the note that you can marginalize out you know you jam is any node.  But what you do is you need to add a bunch of engines. So if you let the prime to be the edges when g minus  N  Plus you connect  All the neighbors.  Have  In in G together. So you form a big clique for all the neighbors.  Okay, so this is a new peak.  Then you have that  The marginal the set of distribution, which is a marginal on the node one up to n minus one.  For some p in the GM  Over the whole graph.  This is the same thing as the GM on the smaller graph.  While the smaller graph. The graph on less nodes, but more edges.  And so you can characterize the set of distribution you obtained by marginalization exactly by another Eugene, which was not the case for the GM. So when you mentioned is a leaf in the GM, it's fine, but not for generic Mars's marginalization.  Okay, so that's the difference.  Alright, so now let's talk about a summary of properties of the GM vs Eugene  Eg and versus you G. M.  So the first thing is, let's talk about the mark of blankets. So what's the mark of blanket. What's the smallest set of nodes to kind of like create conditional depends with the rest of the graph. So the mark of blanket.  For a node i  Basically, like how do you need to cover it so that everything else is independent of it.  Again, this is always for a graph. Je je je je je je. Yeah.  The mark of blanket for I  Is the smallest  Set of nodes.  Of nodes, M.  such that x i is conditionally dependent of everything else I'll use X V to  Avoid  Complicated notation. But remember, we are allowed to just put repeated notes, given the the blanket.  So basically the meaning here is just the rest right so because if I put an eye there. It doesn't do anything if there's an M there doesn't do anything. So we're just talking about the nodes which are not an eye and M.  And so what is the mark of blanket for a new GM versus a GM. So for you, GM  It's actually fairly simple. It's just neighbors.  The mark of blanket is just  Jay, such that there's an edge.  between i and j  IE, the set of neighbors.  Right, because if I conditioned on the numbers of a node.  I'm definitely separating this node with the rest of the graph right any path has to go through the neighbors.  And so that's a way to isolate a note some sex with condition on neighbors.  For the GM. It's a bit more complicated.  So basically, for the GM the mark of blanket is the parents.  Union, the children of the node.  Which, by the way, these are the neighbors. Right.  So, so this some sense is like  The GM because it's the neighbors.  But I also need the parents of the children.  So it's union.  The Union over the children of I  Apparent of Jay  And so basically this is I  This is the parent of AI and the parent can have parents. Parents can have parents, then I have seen a child child of AI.  And then the child can have parents as well.  And these can have parents  And then there's more grandparents grandchildren. Right. And so the mark of blanket of I in the set would be these nodes which are the parents.  This node which is the chill child in this case and the parents of the child, which is this thing. So this would be in this case.  And the reason we need  These here is because  When I because this is the structure  If I observed this node there's a dependence between this node in this note by the baseball algorithm.  And so I in order to really disconnect. I also need to observe the parent. And then what happened is, I'm not going through the structure. So I'm trying to go to get a pass to this. It's actually blocked by this observation.  That's mark.  Okay. So, any question about the market make it user avatar   New user avatar   So let's talk now let's do a recap of these properties.  So if I compare the GM  Versus  A new GM  How do the factories. Right. So the factorization definition for a D GM. It's the product over the conditional  And then the parents are use the parents structure from the graph is used to define the conditional whereas for you, GM, it is one over and over position constant product over clicks of the potential user avatar   Okay. user avatar   Then the conditional independence statement.  The one for the GM or obtained by using the graph the separation, which is a complicated separation concept was you, Jim is just separation.  And these are all the conditional independence statement which are true for all distributions, there might be more as we knew, but for specific distribution, but not for all of them.  marginalization.  We said that  It is for the GM. It's not closed in general.  And so for example I had this this the structure graph here.  And if I marginalize out this node I get something which is not described by you, the GM  But it, it's fine for leaves right  So I can merchandise out leaves and I get a DJ. Mr. Smaller Daejeon  Whereas for you, jam. We said, it's fine. It's closed.  But I need to connect all the neighbors.  Of nodes that  We remove  So excellent has asked, why do we call this the mark of blanket. I mean, Marco because it's it's kind of related to the conditional independence and the blanket is like how do you  Insulate or isolate a node from the rest of the graph rights you so you can think of.  Thinking of the putting a blanket around the nodes so that it's it's separated in the case of the GM  It's very clear because it's only the neighbors, if you, if you, if you condition, all the neighbors, then your conditioning fan of everything else you can think that the neighbors is like the blanket on the note to insulated from the rest.  And if you think we're in Quebec and it's called the snow, the toy.  Alright, so now the last thing so memorization, we just mentioned. Finally, the last thing is about, oh, what about set of distributions. Okay, so there are some family that cannot be  Capture  By the GM so so the GM Canada exactly capture some families.  And an example of that is the set of distribution obtained from this Eugene  So there is no d GM which capture exactly that. This is actually included the set of distribution given from this new GM is included in the GM, in particular the fully connected the GM, but that's also include more distributions.  So that answers a question which was asked towards the being of the cast about oh, isn't it, that the GM always including the new GM. Well, you can always include a museum in a digital as well. Right, so it goes both ways.  By picking a big enough graph.  But will I'm going to continue with  Our final characterization of this inclusion, but here's a set of distribution, which is not included in a in a GM and similarly the the structure  Is not well characterized by Eugene, so there's no huge GM which precisely characterizes the distribution from this DJ. So this is a different set  Any question about this.  Okay. So Jacob is asking  Is there a concrete formulation of a way describing the set of decisions that tended to be more biographical mall.  No idea.  It's probably ugly.  But I think  Most likely has been analyzed.  But in some sense, there's a lot of set of distribution, you can consider right  Yeah, and so I don't. Yeah.  The trick and seem something more meaningful than that.  Because one thing to know is here, we're not even talking about characterization. So suppose now, you, you, you, you specify a very specific pasteurization and you say okay well that's the set of distribution span or are generated by all these factors.  That can easily also not be characterized by a graphical model or of any kind.  But you know, it's also very specific  Alright, so now  Yeah. user avatar ezekiel williams  ask a quick question. Just about that.  Yeah, I'm sorry, I don't fully understand where a digital capture the kind of square the square kind of graph that you have there is that it can't. It actually captured or it can't concisely capture capture this user avatar   So what I'm saying is that there's no d GM such that  You exactly get the set of distribution from this huge em.  And the reason is because here you have a cycle. Right, so, so, so let's say you put an orientation like this that doesn't work. It creates a cycle. Okay.  And so what you'll need to do is like okay well let's put some orientation without the cycle. Well, what happens is, is there will be some distribution in the new GM which are not captured by that. So then you'll add an edge.  Okay, but that will include now new distribution, which was not in the original because now you're allowing a potential on these three nodes which was not allowed to be original, Eugene  That's the problem. And so  There's a D GM, which is smaller and as a GM, which is bigger, but there's not a gym, which is exactly equal to this. user avatar ezekiel williams  Okay. Cool. Thanks. And in the same way for the for the GM not capturing the  V shape up to find like a larger your GM, of course, that does user avatar   Yeah, because, because basically here you have three nodes. So you need  A clique of side street.  Right.  To be able to capture that. But as soon as you put an edge here in a clique, then there's a dependency in general between those two nodes which was not present in the individual  Ok and now will actually  Look at this in more details. And I guess we'll conclude with that.  So this is the concept of moralization  And so basically, how to transform the GM into a new GM  And when is a DJ equality, Eugene  And so  Let's say we we have whoops.  So let GB a DAG.  Now, when can we  Transform.  When can we get an equivalent to VM.  Okay.  Well, the first thing is, in order to make D GM within a huge GM will need to add some edges and this is called demoralization So define so definition.  For hoops.  For G a DAG.  Recall  I'll use a notation G bar.  To be demoralized  Graph.  Of G.  Which is basically all you do is you add edges under. So first of all, you make everything on directed and you add edges between the parents have a note to make a big click  So where G bar is an undirected graph.  With same  Set of nodes.  And we have this augmented edge set  Which is  The undirected version of my original edges.  So you replace the couple by two sets.  So this is basically the  Undirected  Version of G.  And you also add edges between all the parents. So you add an edge, K, L,  For all k not equal to L belonging to the parents of i. For some, I  And this is the  This is what is called the moralization  Okay, so this is a sorry for the religious reference. But basically, I think the the semantic of this comes from the fact that in Christian  Beliefs, the parents have to be married. Otherwise it's bad. Right, so you, you have a best or something so  So in order to make the family moral there should be a bond between the parents, which are these edges. So if there was no edges between the parent, you need to add and that's make it moral. Now the thing is  You can have more than one two parents like you can have like 10 parents if you want. So this is a bit like  It's a bit interesting.  polyamorous family, but that's why. So I think that's where the name come from, but it's just mean at all. They just between the parents.  And somebody is asking is it for all i, instead of for some, I know. So the thing is you need to add the edge, K, L, if P AMP L belongs to the same parents set for some  And for any i, this happens, then you need to add edges but it doesn't make sense. The key and else should be in the parents of all die because user avatar   Usually you're you guess you would have cycles. If this is the case. user avatar   Okay, so basically what I'm saying here. In other words, this thing here is to connect  All the parents of I  With I in a big click  And basically demoralisation aspect was that you're marrying the parent  Making it a nice moral family.  Split a bit of exclamation mark.  And so you only need to add edges if there are two parents at least  In  In a note, that means, so this is only needed  If the number of parents is bigger than one, I if you have a V structure.  So if you have no V structure, then you don't need to add any edges.  And so what it means is, let's say I had  This family here and let's say there was a note here like this. Well, what you do is you will add this edge and this edge, which was missing to get a full big click user avatar   This would be the user avatar   Augmented edge. user avatar   And so an example if I start with my  Here's a nice directed graphical model.  So now I actually have a V structure.  And so this so  Here, there would be this edge which is added.  Oh I guess there's no direction right so  So I have  Oops.  I'm going to go slightly over time. Sorry about that.  Here we go. So that would be my moralized graph.  OK, so now that I've defined whether moralize graph is  We can see that  It's not hard to say. And actually that's I think what you prove  In the assignment.  So if a DAG.  Has no V structure.  With no the structure  So if all the nodes as at most one parent with you basically have is a forest is set of trees.  Then you have that the on the GM on G.  Is equal to the new GM on G bar.  So the, the, the undirected version of the tree actually give the same set of distribution. That's what your proof in the  Proof see assignment.  And so that's so when you have trees or for us the GM and new GM are the same. But in general, all you have.  You can only say  That L of G is included in L of G bar. user avatar   So this was a daiquiri, whoops. user avatar   And in fact demoralized grab the moralized graph so that G bar.  Is the minimal  On directed graph.  Such that you have this intuition. user avatar   Okay. user avatar   And that's it. So that's it for the GM. So the next class I will talk about in France.  I hate how to compute marginal in a generic fashion for any members of the GM or Eugene  And actually we will do it for you. GM because it's simpler. So we will just think of the GM as a new GM by moralizing it and then you can just run the algorithm under Eugene  And is there any other question about the D GM and you Jim. So Jacob is asking about extending directed graph model where you have cycles.  And  You can do it in a perhaps like dynamic system type of way where you would think of defining a transients system.  And and and and and you would have time in it. And so there's no cycle the cycle is when you look across time, then yes, that could be feedback, but there would not be direct feedback at a specific time slide. So that would be one way to do it.  But if you just say or time less with direction. It's a bit complicated. I mean, you could do it with fixed point operators, but get a bit complicated.  Any question.  On you GM or did you  Know,  All right well on this, I'll see you on Friday.
  It's recording. Alright so today.  We're going to look at influence user avatar   As I mentioned last time. user avatar   Will do in France in  Data graphical model and particular will see the graph eliminate algorithm.  Which is basically a generic way to compute marginals in your under it a graphical model.  And then we'll see later.  How to efficiently do it for trees using something called as some product algorithm. But Griffin even aid can be used in any graph, it doesn't need to be a tree. So that's kind of like the most generic  Formulation  So perhaps just before I start in France. If me just recall a bit the general themes in this class.  In this case,  So,  I mentioned that there were three things I will often come talk about. So one is  Oops.  Is representation  How to  Overcome represent a family of distribution. And so we've seen already, the director graphical model family and the under to graphical model family. There's other families. That's the main one we looked at in this class.  And then there's also a question of not just how to have  A set of distributions. There's a question also of how to patronize the individual member of the set of distribution. So this is parameter ization parameter ization  And  This comes, we'll talk about it when we talk about the exponential Femi  Particular how to define features on the graph.  And so that will give us ways to penetrate these these conditions, in particular for these graphical model.  And the whole point of all this was, okay, how do we model.  high dimensional distribution.  in an efficient manner. Right.  So already,  With  The  Doctor graphical model new GM you saw that we were able to reduce the number of conditions or the number of cameras. We need to specify for mothering a joint because of conditional independence assumption.  And  Then we still haven't I apart just saying specifying the whole conditional. I didn't tell you how to penetrate the conditional. And we'll look at that, when we talk about six months off me.  But then the thing we're going to talk about today is in France. Right. So once we are we have some pasteurization and representation of our distributions, we would like to be able to compute say the proximity of  Set of variables which are queried given some evidence right so this is kind of the query.  And this would be the evidence.  And  How do we compute that efficiently. Right. And so today.  What we'll see is the elimination algorithm.  Which depends on the ordering of variables that you eliminate meaning that you actually some of them out in the marginalization.  And the thing about the limitation algorithm is that it will depend on the the efficiency of your argument will depend on the quality of the ordering that you choose and already as  Scoop it's empty. Hard to find the best ordering in general. So, so there's a bit of trickiness there.  But when you have a tree. There's a clever ordering, which is basically eliminate the leaves. First, and that's what that will also bring something called the some product algorithm.  Which is an efficient implementation of marginalization for for trees. And so this is for trees.  And then we'll also. And it's also called belief propagation  Because it's propagating beliefs on a graph propagation  And  We'll also have later approximate and France.  So some product and the division algorithm or exact algorithm. But we can  Instead, to have approximation to compute this distribute this quantities and we'll see Markov Chain Monte Carlo sampling and version of methods. user avatar   Very user avatar   Emotional methods. user avatar   So that's for in France. And finally, another important aspect is how to do statistical estimation  That is to call it a lunch or where I'm having trouble writing  That is tickle. user avatar   Estimation user avatar   Or basically learn it also just called learning and machine learning.  And so then, there were different principle I mentioned in the past like maximum likelihood estimation  Will see soon maximum entropy as a different principle.  And the method of moments.  So we'll see. We'll talk about that after the exponential Femi  And we'll see also later how to estimate the parameters for a graphical model.  As well as to learn the graph itself that's model selection. Okay, so that's kind of good. The big picture of these three three themes. And so today, we'll focus on in France. And so let's motivate a bit the problem of influence.  And its background. So let's talk about influence hoops. user avatar   Like user avatar   Okay and so  It's a bit of motivation. So we want to compute  Either the marginal. So let's say for example, they could be the marginal  P of X subscript capital F.  For some  F subsets of the  Okay, and I use F like the final note or something. The final query nodes.  Or we could also want to compute a conditional  Like  Except for access given X evidence  So as I mentioned, this is the  The nodes, we're acquiring  And this would be what we observe  And we'll see, for example, in a medical diagnostics a scenario that could be interesting and also by the way we also want to compute the normalization constant in a under Africa model. So for for under to Africa model, there's this partition function.  Z right  So z is the some overall XV of the product over your clicks of your potential right so that's the joint something for something to join and this is an experiential some  So in general.  It's intractable to compute. So you need to be clever about how to organize your computation to avoid the exponential some  To be able to compute it. Then why do you need it. Well, because otherwise you cannot compute. What's the priority of any event in your  Joint because you only know the up to a normalization constant, right. So if I say, Okay, here's an assignment of my variable. For example, my Isaac model. I say, what's the probability. Well, what you do is you will  Multiply all the  You will multiply other potential together for this assignment, but then you don't know if it's a big or small number because you also need the normalization conference.  And so that's important also to compute and will see that the algorithm find friends that we define can also be used to compute. In addition, constant  So,  More concrete example of why we want to compute these quantities.  And so an example is to fill in missing data. user avatar   Data user avatar   So for example, I could have a model and then I could compute, what's the probability of my unobserved variable, given my observed variable.  Like my latent variable. And once I complete this proteins, then I can either perhaps make prediction.  Using this policy statement because I don't know what that they are, but I, I could use their expectation. I could use some of their properties to still make prediction about the world.  Or I could fill it in right by just looking at what would be the another variable which maximize this conditional. But before this, I will need to compute this condition. And so an example.  From computer vision is  In filling task. So let's say I have an image this called the in filling  Task where you have something that you've observed, and then there's occlusion somewhere.  Whoops, I'm not very good at drawing  A mathematician and not  A painter. But yeah, let's see, this would be the missing.  Pixels.  Perhaps because somebody took a picture of put their thumb in front of the camera, and then you miss part of the picture, right.  And so if you have a model on images on how what are realistic images. You could compute this property of the mixing pixels, given the observed pixels to try to fill in to perhaps replace them when they're more likely component of perhaps their expectation  Is that could be a way to fill in things that have not been observed  Another example is to do prediction.  And so you could have a model and you want to predict what's going to happen in the future, given the past that has been observed, right. So let's say you have a model for financial  Product the stock market. For example, usually not that great. By the way, it's quite  Random very noisy, but you know, you could have a model and say, oh, is a stock going up or down. And actually, the more information you have about this, the better you can make a prediction as well like you'd say, Okay, I have observed that this is our company was it's an oil company.  That's not great for the environment but spices oil company and you've just found a new all field. So yeah, I think this tech was going to go up.  And so he's asking, can we take prediction, as in France discussion discuss about exactly. These are all examples of inference problem so prediction is also in in France problem is that exactly what I'm giving examples of  And then another example of application would be to figure out the latent  Cause, which explains the observation. So for example, I could have the probability of some cause  I want to know what's the process of quality and cost given some observation.  This is not in a causal really complete causal way but  I mean, if you had a causal graphical model, this would be your true cause, but where does this come from, is for example in this Q Mr network where you have  The quick medical reference where you have all the diseases as on observe. Then you have the symptoms.  Which are observed. Right. So we have a bunch of symptoms like Oh, do I have a runny nose, do I have fever, you have a rash or my teeth falling and then you have  Some experts who came up with these  You know, potentially conditional property that oh, well, if you have the flu with some property, you will observe the symptoms and if you have  coven then with some of these, you have this symptoms. And so now what I want is given the observed symptoms. What's the problem to on Mike diseases because it's important for diagnostic and perhaps going quarantine if it's coming.  Right quite relevant question.  Is always asking about the in filling problem of computer vision. In this case, your graph would represent the lattice structure of the pixels and your potential function would be on  Sort of clusters. I guess you would design them to have some nice image like properties. A year rotations covariance. Correct.  Yes.  So the grid like or the neighborhood structure like graph is the simplest for this kind of image.  Though more modern models will actually use a yard vehicle idea because you can think of a scene that's composed of multiple objects. And then these objects will have specific appearance and  So you could have now the the factors between objects, rather than pixels that make more sense. But yes, that's kind of the general idea.  I mean, also making nicer model and images is a whole field by itself. And it's, it's definitely non trivial and outside the scope of this class, but that's a drink idea.  And alright so that's that's basically some example of what we're trying to do with this and France and also I would like to mention that  Related to in France. So normally in France is talking about competing these properties but also related. And actually, it turns out that  The same algorithm style can be used is to find the maximum the maximum Iser of these properties. So if I want to compute the art max over x F of probability of x F given x E. Okay. And in this case, f  Could be big in terms. You could have a lot of variables there.  So for example, if you do speech recognition.  Right Speech recognition, you could have that the observer variables is the is the sound.  And the sound waves and the latent variable that you're trying to query is what it was said. And so you could have multiple for every  Word what it said. Or you could even have, for I think normally it's using full names instead  And so you would like to to decode you would like to give him the observation. Fine. The, the sequence of findings or words which is the most likely given your observation. So you need to compute this art max.  And you just don't want to compute the project. You also want to complete our max and this is an art max over an exponential number of possibilities. So, you know, you need to be efficient about doing it.  And we'll see, by the way, already scoop. So the sun product or within can be used to compute  All the marginals in a all the simple marginals in a tree graphical model. And then there's the max product which which can actually be compute  Be used to compute efficiently the these maximizers I'm talking about guess we'll see, and then we'll see that it has the same structure in the message passing algorithm, but  We'll see that next class. Okay, this is just to give you a general idea. And then so so completing the art max Israel in France, and also note that in France is needed.  During estimation is also needed  During  Estimation  And so, for example, when you do, Emily.  Emily.  And so for under from Africa model, you'll need to compute the partition function to do the maximum likelihood estimate. And so the partition function. We were crying and friends, or if you do em. So for example, if you do em, then we'll do the hmm learning, you will see that  During the East step you needed to compute the posterior over the latent variable given X PFC given right  So to compute that you need influence  Okay, so that's the background.  Let's  Talk about the first all Griffin graph eliminate  And oh, and just before I present  So the shack is asking is learning also an inference problem as we infer weights, given the data.  So you have to be careful about the semantic here in France can mean multiple things and  So in. So if you say inferring ways from data. That sounds like an inference problem. So if you're a frequent this, you're not inferring weights, you're actually estimating the weights.  So you're it's a learning problem. So it's actually a different it's it's it's seen as a different problem than in France, it's it's statistical estimation  If you're a Bayesian everything is a probability. And so the weights will also have uncertainty. So you need to compute the probability over the weights, ie the posterior over your parameters. And so if you're a Bayesian  There's nothing as estimation, everything is just entrance. So as a beige and everything is in France. So that would be true, but for non vision for frequent is  Learning or estimating the parameters is a different type of problem then computing probably t's in your model. Okay.  So, good question. Alright, so the last point I wanted to mention was that we, I will present  The algorithm.  The inference algorithms.  In this class for a undirected graphical model. Okay. And this is for simplicity  And more generally t  Generic  Li t  But I'd like to mention that  So, but note.  That sometimes it's more efficient.  That sometimes  It's more efficient to work directly on the DGA  So for example, if I'm marginalizing out a leaf in a D GM  I don't need to do anything. I just remove it and i know i don't need to compute because I know I just removed the factor. The rest is the marginal  Whereas if I factor out if I maximize out the leaf in Eugene, it's not just removing the potential. You need to do a bit of computation.  Okay. And so if I know that something is the GM. There are some marginals, which are much more efficient to obtain by using the fact that the Daejeon  But these are very specific cases which is why I'm saying it's a bit more general to to work with the new GM and generality, because I won't make use of the special structure that oh I query the leaf and in the GM, for example, because that's why me  And so how do we do that. So that's actually very important. So we will make  D GM  As a subset of a huge GM  Using moralization  Alright and so  IE. Suppose that P isn't a D GM  Then p will be written as the product.  Of its conditional given the parent  And so this I can rewrite it in a huge em form as just one over z.  Product over I have sigh CI X CI.  Where  Basically you define. See, I see your moralize  The graph, you just defined see as I with all their parents, right, so I've connected all the parents in the underground Africa models, I get a big clique. And so then I can define my potential and see. I guess this is CI.  Define my potential  Over x API, just as the conditional of x i given x by then. So this is  A function of the variables on the clicks and it's positive. So, that's fine. So it's a valid potential  And the magic here is that because it's already normalized like for a D GM, the product of these potential already some to one I have that the partition function with these potential is just one right so I don't need to compute it.  And so I will define how to marginalize out distribution. Suppose I have these potential  Formulation, and you can apply the same algorithm on the D GM by just defining these potential to be the conditional  With the correct size and not worrying about the partition function, the partition function is one. Okay.  Okay. So Jacob is asking, well, now we're using if I moralized there's multiple distribution which could give the same new gym and that's correct. So, so that's why. First of all, here.  I'm putting that it's an inclusion. It's not an equality. So, so in general.  My D GM is a subset. When I moralize if I get something  Which is bigger than the original the GM and as a set of distributions. And also, I am not using the particular properties of my D GM  Okay, so that's why I'm saying also that we will work with a huge GN so if you already have a huge gym, then it's fine if you don't have, if you have a distribution which belongs to the GM, making it find a new GM which works. In this case, we just moralize that the GM  And then run the algorithm on this distribution as if it wasn't a huge GM and this will be sometimes less efficient than if we had  You known that it was a DJ. Right. But that's because presenting algorithms for both the gym and the gym, just make things more complicated and more hairy. So, it exists, but I have presented in this  Module, which was asking again the wise, it's called moralizing I explained it in great depth in the last class.  in the right place at the notes disappointed, but it's basically because you married the parents. So there are now moral user avatar   That's the idea. user avatar   The compensation cost of moralizing the GM is not very expensive. It's basically just changing your structure that you add a bunch of edges to every node. So it's basically  The size of the parents times since the some of the size of the parents, in some sense, that would be seen for every i you basically just create a clique, which is this which is the size of i plus by right so  I so  So such simple as asking that, in this case the gym, we define represent the same P as there is not the gym. Right. So again, let's be clear. Did you have a new gems or set of distributions.  And so here I what I did is I consider  Either the GM as a set of distributions and then for all the elements of them I made them part of a huge GM and so for any specific distribution when I do the transformation. I don't change the distribution. So, that's correct.  But when I moralize I could get a bigger family of distributions, in theory, and so it's not true that the new GM represent the exact same set of decision as the the geometric can represent more  But let's say for example you don't care about working on multiple distribution. You'd only care about a specific distribution and say,  Compute the margin off of this distribution. Well, then the idea is you, you take the same distribution and you write it in a form that is a new GM, but it's still the same distribution. So that's what we  Okay, so, so that's an important point. By the way how to make your distribution, which was presented by the GM in a huge em form so  It's the same distribution. It's just that now I wrote, write it with these potentials and z z is equal to one. In this case, it's not super interesting but still will use the alright so now let's talk about the graph eliminate our give them  A graph elimination  Algorithm.  And this is for instance.  I mean, it turns out that the graph emission algorithm is actually a graph, our rhythm, which has nothing to do with in France. But in our case will use it for instance.  Alright, so  So the setup is we consider some distribution, which is in a huge em.  So g is undirected.  I he does. We have the p of x is a product over clicks of potentials  And clicks and then I have a normalization.  OK, and now we will define an algorithm to compute the marginal  So we don't talk about conditional yet. We'll just do the marginal with see how we can do conditional with a trick but. So say we want to compute the marginal on x F for some f which is subset of V and these are called the query notes.  And so before presenting the generic algorithm. I'll give you the the main trick that the algorithm is implementing and I will provide illustrating examples is just to use the distributive at  This tree booty VT.  Of plus over  prod multiplication.  So that's something I mentioned earlier in the class, basically the main trick of graphical model is to use this to be the beauty of plus over product. So what I mean by that. So if I have see time. A plus B. This is the same thing as c times a plus c times b.  And so for example if I have the summation over x one x two of f of x one times g of x two. So, I have a summation of our quadratic number of assignments, but because it's factor is here. Well, this is just the same thing as summation over x one of f of x one.  Times summation of x two of f of x of g of x two. Okay, so that's just by applying disability multiple times.  So convinced yourself that this is true if it's not obvious.  You can just expand it and then apply this to be the money. So for example, the first term of this  Of the sum  Of x one would be  Summation.  Over x to have f of x one equals to say some value.  Then g of x do  Alright so that by taking the first term of the of the left, plus the whole parent Asus and then applying this to BT. BT. I can just multiply F of X one.  Its value with all the g of x to. That's what I get. So that's the first term.  Of product.  Okay, and then you repeat that multiple times and then you'll get summation of x one submission to execute right so you get back this thing here today said it was equal to  Okay, so that's the main trick. And so when we have factorization.  And we have some nations. Well, we can then move the some around to make things more efficient because note here that I had a summation of our quadratic number of objects that said just k values for x one and key values for x two.  There's case square some summon here. This is something case square thing was here. It's order k to compute this in order  To compute this, and in order one to take the product of these two scheduler, so I can compute the whole thing in order k times right on an older case square. So, I made a big game and you can generalize that to now bigger products. So that's the whole trick. user avatar   Okay. user avatar   And so, more generally,  You would have that the summation over x one up to n have a product over I have if I have excited  Is the same Xing thing as the product over I have the summation over x i have f of x, right. So, buddy.  That's why it's just the translation of these two terms. And so here, if key. If x add K values. The first one has some over a k race to the end. So an exponential number of terms. Was this the second thing here.  Each some is only order k and then product is just order n. So I can compute the whole thing in order n plus key right so so basically  This thing here is this is order basically k to the n times n mean the end, we don't really care that much was this is order k plus and  So each huge game just by moving things around because things were a factor x  And so what we do with the graphing dominion over them is just a juror ization of this for more complicated factorization coming from the graph right  And so, in particular, let's do this concrete example. Let's say my graph is this nice grid.  I would have 123 and four, and let's see my query set F will be just the node for case I want to compute the marginal on x four. user avatar   Okay. user avatar   And so I will have that the marginal p of x four, by definition, will be the summation of all x two, x three, x four off the john sorry, x one, x two, x three of the job.  And so it's one over z summation over x one, x two, x three of the joint. What's the joint here a test for potential  Right as I won't put the index of their clique, because it's not super important. So you have a potential on X one, X two.  I have a potential x two x four.  I have one on X one and X three  And I have one on x three x four.  Ok and now. So that's the some I have. So, normally I would need to some overall but over the, let's say this key values for x one, x two, x three, so it's K Cuba.  summons of this product.  Okay, so each man is saying why is this order k plus n rather than key times n.  You're correct. I have order key. And I do that n times so you're right that is cadence. user avatar   This user avatar   Correct. user avatar   Thank you. user avatar   The other one was when there was only two terms to us to to K and to the constants. I forgot about it. But here, there's an end is not a conference. Thank you for that. Alright, so, so now what we do.  Is we will use distributive et of the some to push the some inside the farthest I can buy reorganizing the summation. Okay. And so now the trick is to think about one variable that that will some first. Okay. And so the first variable I will some in this case would be, for example, one  So what I do is I put all the potential which has the variable one on the right. So let's say there will be five sine of x one, x two, x three, and then sigh of X one, X two. So, these two potentials. Oops.  So the both have  The variable one was all the other ones. So, this one doesn't have one, and this one doesn't have one so I don't need them when I send over one. So then I could put summation here.  Oops. Okay. Okay, well let's keep it in purple. I can put the summation of x one here because there's no x one in the other one. And so actually output, then the one with x two remaining here.  And then I have summation over x two.  And then I'm left with the one with x three, which was remaining which is x three x four and then I was some over x four.  I know extreme because mix for is the thing I'm competing Marshall, and I want to. Okay.  So that's using basically disability and I'm just pushing as much as I can. The some inside  And I need to put all these parentheses. Okay, so that's the reorganization of the computation using the subjectivity and notice that this summation here of the product of the potential an X one, X two and potential of x one, x three.  Had that both x one, x two, x three, appearing in it. I'm sending out x one. So, next one is now not a function anymore. So, this you could call it basically a function of x to an extreme.  Which I actually use em for messages.  And I'll use one because it's coming from eliminating the variable one  And so this will call this message.  Because it will be sent to the neighbors in the graph, okay. And first of all, so it depends on x two, x three, so I he for any possible value of x to an extreme.  I will get a result which is a summation over x one of the product of this potential with extreme, extreme fix. Okay, so you can think of this now as storing this as a table right  So now for every value of extreme, extreme value which is the result of the competition there. I will store that that can be seen as a new potential  Okay, because in the next computation that I do.  I will have this potential times this potential which I call em one x two x three.  So it's also like could be seen as a new potential. And so from the, the, the, so basically, when I removed the the node.  Oops, I don't need this. So when I remove when I marginalized that x one, I'm creating a new potential in my, in my product.  And in particular, this depends on both Extreme, extreme. So in some sense, you could think of adding an edge between the extreme, extreme  Which, by the way, if you remember when you might as well as other Nolan, are you GM. I said, it's still in the GM when you connect all the neighbors. So here you connect Extreme, extreme  And. All right. And so then you can compute the summation over x to have the potential x two x four times this m one of x two x three. Right. And again here.  That's the variables extended I will eliminate with energizing out and I will get a new message which will be called coming from emanating know to. That's why I called him to and it depends on which variable. It depends on the on x three x four.  And so from the graph, I'm when I eliminate x two, I get a message to which depends on the on x four and x three. user avatar   OK. user avatar   And now.  You could say that the summation over x three of the potential. So, summation over x three of the potential and extreme x four times this message on extra next bar.  This is also a new message which is only a function of the remaining variable extreme as being some doubt it will be export right so you could see that what you get at the end.  Will be one of z. And then there will be this message which comes from eliminating know three. Oops, I forgot about this. Let's remove this  So what you get at the end is one of z message coming for a meeting or three which is only a function of export.  Okay. And basically this game when I then eliminated. No three, and it's a message, you could think as being passed to the node for this only depends on export.  And now I don't. And then and now I have something which is proportional to my marginal. Okay.  And so the last message that I'm left. So the message which are coming to the query node is basically proportional to the marginal. So the last message here.  Is proportional  To the marginal  Pls for and to compute the exact marginal. I just need to normalize I need to compute z. And so I can just some over x for the message three of x four.  And by definition, this should give Z because it's supposed to something what right so this gives me. And so by summing the last message I get basically z. And now I got a can.  Define my marginal I got the competition on my marginal is just one of receipt times this message. And that's what I wanted. Okay.  So that's the general idea is, is you choose an order of note that you will sum up you gather all the potential which depend on the node that you're summing up  And then what once you eliminated the node you created a new potential which you will call it a message that you will then reuse in your next computation. So that's kind of the idea of the graph in it or rhythm.  So let me write it in general. And then let's see if there's some question or we can already yeah that's that's me right there are women in general. And then let's see if there's questions.  OK. So the general algorithm.  Which is just a systematic ization of what I've just done.  Which is called the graph eliminate algorithm.  So the first thing you do you initialize the algorithm.  So you will choose a elimination organ. Sorry, and elimination ordering  You need me nation.  Or during  Such that  F are the last nodes.  So the nodes in F or all the last in your ordering. So if there's only one though like I just did. Well, you just found that  The last know the the note that you care about this. The last ordering. If there's multiple nodes you just put them all. Last of the order and the order doesn't matter anyway because you won't be eliminated.  And then what you do is you put all the potentials  In a data structure that we call the active list. user avatar   So the user avatar   The active list basically just keep track of things that will appear in the sun. user avatar   Okay. user avatar   All right. And so then, so that's the initialization.  And then the algorithm where you actually now do the update part of the algorithm.  You will repeat the following repeat in order of variables to eliminate  So what do you do when you eliminate the variable, okay.  And eliminating a variable just mean something getting out like something marginalizing out right and so say for notation x i is the variable. user avatar   To eliminate user avatar   Then, the first thing you do is you remove  All factors.  From the active list which contain exile. Right.  With  X i in it and you take their product.  Okay, so basically you will have a product  Of  Clicks or alphas such that I belong to alpha and say alpha of x of alpha  Will be the so you can think of the activities. These are potentials each potential depends on the clique and when i belongs to a clique, you need to take this and you take all their product.  And that's what we've done here, right, by the way. So I, for example, when I wanted to eliminate x one, I looked at all the potential which had x one together and then I took their product.  So that's and that's what we do in step one and step two we will summit out right. So step two.  You some  X i to guess over x i, to get a new factor, a new potential  Which I'll call em. I have x  Si.  Or you could think of this  As upside of SI X of SI right so to to have the potential notation I would use it the clique as a clique, as the set, but because I want to make this link with the messages that I talked about that also use the message notation.  And so  First of all, what is si, si, si or all variables.  In these factors.  Except I i because the message does not depend on exciting more because it's some doubt. user avatar   Right, so you get user avatar   That the message as a function of x as I, by definition, it's the summation over x i have the product that I mentioned about was product over alpha such that it belongs to alpha  upside of x of of  Okay.  And so this thing, as I mentioned, you can think of it as a new. It's like I need a new click  To some  Over and the click would have SI and then also I  Right. And when I added edges in my graph that's to represent this new click right so in some sense here when I remove x one.  I had to sum over a product of a potential over one and two and one and three. So in some sense, it's a potential over one, two, and three. So it's a click.  Over three nodes, which is why I added this edgy.  Yet, so that's the clicks that are kind of like have mentored and as I said as I was, by definition, the Union over alpha of alpha such that i belongs to alpha and then every move is  OK.  And so  So then the last step is you put back this factor.  And I have x as I in activists.  Or, as I said, if you want the notation, because you want them sigh of alpha, this basically will be a upside of SI x  And so that's what you repeat at the reiteration pick the next node and you're ordering. Look at all the factors which contain this node take their product. Some this product over xi, then you get a new  A new message and you factor which depends on all these variable except xi and that becomes a new potential our new message and you put it back in the activists then repeat user avatar   Okay. user avatar   And then once you remove all the variables that you had to remove. Then the last step is to normalize  So basically the steady. Is that the last factor left  The last factor.  Left.  I guess the last product.  Of factors.  Left.  As only x F in them. And so basically what you get is something proportional  To the marginal and access right so you can sum over all of the acts of value to get the nomination constant and then  This gives you the  The, the marginal  Okay, so that's the graph eliminate all rhythm. user avatar   I guess and took too much space. Hey, user avatar   So there is asking, can we choose any arbitrary ordering. Yes. And the arbitrage during will be valid to compute, but some ordering will give really really bad computation complexity and some will be very good. So we'll look at that very soon.  And so, in particular, if we look at the memory that we need  While running the algorithm, you need to store these potentials  So it's roughly  Related to to see the maximum size.  Of si  Right, like the message basically needs.  Suppose there's only two variables, not so the key and suppose that every variable has only two values for simplicity. So basically, a table on key elements will take two to the key.  Storage. And here I had these messages which dependent I needed to some them. So I needed to compute this product over  Yes. So, si does not have I write this, I said here that I need to store these messages on the active list. And so this takes order to to the  Size of exercise storage. Okay, and so and so in terms of complexity, I can look at the maximum one which has been created and I can multiply that by the number of factors, and that gives me some kind of upper bound on the  Memory, I need to implement this algorithm. So you don't need to see that much memory because some will be smaller, but just as an upper bound, and then the computational cast  Computational cast  Is  Related to when you had to some both over si plus one, right, because there was the  The, the excited dependence and I need to do that for every variables or  Basically times and supposing access is order one.  And so we will see later.  That this quantity  Is related to what is called a tree with of the graph.  Basically the tree width of a graph is the size of the biggest clique, which has been formed during this graph and immediate ordering  Over all possible ordering so you sorry the minimum size of the biggest peak, you can get when you try all the possible ordering. So the best ordering will give you the smallest to gets click  And the size of this biggest key would be the tree with minus one. I think there's there's always a plus one minus one that I forget to my notes later.  Think it's minus one. So that's the size minus one because the truth of a tree has kicked off sites to and you subtract by one. It gives one of the truth of a treatment.  But we'll get back to that. And so to answer your question, indeed, the better the ordering the Lord the both the competition on the memory cost is. OK, so the question is what's the biggest si that I get, depending on the ordering  And that's what we study after the break.  But is there any question about the algorithm. In the meantime,  No question. Or you can think about over the break, see if it makes a lot of sense.  But basically this I think this this running example illustrates really what we're trying to do we choose an ordering. In this case I chose the ordering, one, two, and three, in terms of elimination and  We're basically now just systematized this creating a new factor, taking the product of these new factors sending them out and then creating a new factor digital  Okay. Alright, so now it is three no to 34 to 34. So let's come back at 244 user avatar   Let's post it. user avatar   I will tell my wife.  Alright, so  So keep in mind that this is the generic algorithm. So if you want to compute their marginal have any joint distribution, you can just some over all the other variables. That's like the brute force approach. So the graph eliminate algorithm is a way to be more efficient.  It's usually more efficient, unless you choose us to be dog ordering. But even with the best ordering. Sometimes it can also be not very efficient.  And so that's what we'll talk about now, in particular, using some notions from graph theory. Okay, so let's do that. So the first thing is, we'll talk about the notion of augmented graph.  So the augmented graph is basically the graph, you get during the graph eliminate algorithm by adding all the extra edges which appeared with your messages. Right. So this is the graph.  Obtain  By running  Graph eliminate  Plus for a fixed ordering of course so so depending on the ordering you'll get different method graph.  So fix the algorithm fix the ordering and then run graffiti mean eight  And then keep track  Of all the extra edges added  And that gives you one augmented graph.  So, for example,  Let's do a graph, let's do an example. Here's a graph. And now, suppose that f was this node.  And then  That suppose that I eliminate this one first, then this one then this one and this one and this one. So, and then five. So when enemy. The one I connect all the neighbors, I get this extra edge and then when I eliminate three I connect  For an F together so that gives me an extra edge when you made for  There's only five an F left. So it's already connected. And then when we made five is on the left. So, all right. So these are the two edges that I will have added by using this ignition ordering. OK.  And the augmented graph.  After running graph eliminate  When the last node is a singleton right so this case we're only looking at singleton last node.  If actually has some property. It's always something that we call a triangulating graph.  It's now we're getting into interesting  graph theoretic notion and their relationship with graphical model. So this is called a triangle that graph.  It will see this is important when we talk about the junction three algorithm. So what's a triangle graph. So a triangle dated graph, by definition, it's a graph.  With no cycle.  Of size four or more  That cannot be broken.  By a cord.  And a cord right so that's what the triangle, a big graph is and a cord is an edge.  Between two  Neighboring notes.  In the cycle.  Okay, so for example.  This is a cycle of size for  This cycle have no edge kind of cutting it.  In triangles, in some sense, right. So, so if I add. So this is not trying related  Whereas now, if I just add discord. Now this is a train good graph because either a cycle websites three or if I look at the full cycle of length, for there is a cord here. This is a cord which breaks it. Okay, so that's what we mean by having a quote  So this would be trying to do that.  In some sense, when you look at a graph, which is triangulate it looks like it's made of all these little triangles. That's also where the name come from.  And so you can see here that this graph here is strangulated so if I looked at this cycle. There's a chord, if I looked at the full cycle here.  There is discord here. There's also discord. There's a lot of course actually breaking it.  So there's no cycle of links for more you can build this graph, which is not broken by a court. So this try and get into growth, which is what I've said, you know, when you run graffiti beneath you always create a triangle. user avatar   Okay. user avatar   In this example of six nodes. Why is it for F and not to F after merchandise on those three. Okay, so that's a good question. And so  I marginalizing out x one, I add this note when I remove the node three. Why am I not adding  The this edge or because to was already gone. So these two nodes were already gone when I Mars and extract. So you only connect the neighbors in the remaining graph, not in the original guess that's it's a very important point.  When you add these edges. It's all the remaining nodes.  Okay, so that's the definition of a train good at the graph.  And another notion that we talk about.  And and the notion of triangulation right now. I was like, okay, why did I talk about that. Well, it's just an interesting property after running rest of unique  But when we talk about the junction tree algorithm, which is a translation of the algorithm for trees to non trees. This will be important.  But the thing that is important for the computational complexity of graph eliminate is the notion of a tree width of a graph.  The tree with of a graph.  Is by definition the minimum the minimum  The men over all possible  Ordering so it's over all possible elimination ordering user avatar   Elimination user avatar   Ordering  Of the size hoops.  Of the biggest clique formed  Or the biggest click in the augmented graph.  Then just minus one.  So that's the tree with  And the minus one. It's just a convention, so that the minus one here is a convention saw that the tree with of a tree is one  Because the biggest click in a tree is actually to  There's a way if you are, if you remove all the leaves. First, there's a way to make sure that there is no new edges added. And so then the minute graph is the same as the original tree and the biggest clique, a size two. So the tree with is one because it's two minus one.  And so both the memory hoops.  So both hoops.  Both the memory.  And running time.  Of graph eliminate  Is dominated  By  Two to the size  Of biggest click  So it's exponential in the size of the biggest key.  And and the best ordering  Will give you  Basically to to the tree with plus one.  So the tree will tell you what's the best you could  OK. And now let's illustrate that not all ordering things are good.  Right. So, for example, suppose that I have this star shaped tree.  If I remove  The center node. First I need to connect all the neighbors, then I'm left with a huge click where all these nodes are connected  Alright, so that's really bad.  This is actually a tree. The tree with there's an ordering. Like if I eliminate this first. You know, there's only one neighbor. So there's no the edge. So if I just eliminate all the leaves first  Then the tree with is one. There's no unique form. But if I remove this one first, then everything becomes connected then that's a really bad idea.  That's why some orders are bad some orders are good. And so let me tell you, bad news and good news. So the bad news.  Is both that it is empty hard  To compute  The tree with  Of a general graph.  So there's no  No point in time algorithm for that and equivalent Lee also to get to find the best  Ordering  So the number of ordering is like the number of permutations. So that's pretty big. It's morning exponential  And searching over all the ordering the one which gives you the best the smallest biggest clique is empty heart in general, that's a bit of a bummer.  And related to that, it is also NP hard something sticky related to that. But it's, it turns out it also be hard to do exact  In France.  In a general undirected graphical model.  Okay. And so because it's NPR to do exactly in France in the journal you GM that will motivate the need for approximate methods that will see later in the term when we talk about sampling and virtual instance user avatar   Okay. user avatar   But let me give you a very simple example of I told you already, the Isaac model was an example of that. So,  So if I looked at a grid.  Let me make a grid.  Alright, so this is a grid.  All right. And it turns out that the tree with of a grid is actually growing as the side of the grid.  So the tree with  Of a grid.  Is actually roughly  The square root of the number of notes. Okay, so the size of the grid.  So basically this the size of this grid.  Is basically the tree width of the grid.  So there's no way to eliminate all the nodes without creating a click of size side of the grid.  And so let's say now, you were talking about a an image you GM on an image where you connect all these pixels using a grid. So that means let's say the images like 1000 by 1000 pixels. The tree with would be 1000  And so to to raise the thousand. That's a pretty big number. I think it's bigger than number of the atoms in the universe. So, you know, it's still the intractable to do exact influence in this in this graph in general. So that's a bit problematic.  Already just for the simple grids and so assignment five, you will actually use good sampling and rational methods to compute the marginals in such a great graph and gives good approximation, but to do exact quantity. It's not tractable.  So that's the bad news. What about the good news. Well, the good news is that for trees, things are great.  Alright. So the good news is that the inference is linear.  Time.  For trees.  Linear meaning the size of the nodes and size of number of edges.  And in this case, the tree with is one  So there is a good ordering and not only it is one. It's actually pretty trivial to find it just need the leafs first  And in particular, will see in a few minutes. The some product algorithm, which is an efficient dynamic programming implementation of  It to compute all the marginals integral in a tree. Okay. And so we'll use that for doing in France in the hidden Markov model. And when we talk about Markov chain, etc.  So, at least, there are four trees, things are efficient. So that's good.  And it's also efficient.  So efforts in France is also efficient for small tree with grief.  Right. So if the truth is bounded then by definition.  It's not something which varies in the complexity, then it becomes a constant into your complexity and so things are pulling on you. So, for example, here's a graph.  Where it's basic. It looks like a tree of clicks.  And  And so basically you could actually also run a generalization of the Sun product algorithm for that which is called the junction tree algorithm.  Which will see later, and the complexity of that is the  Truth. It's essentially the truth, but because the truth is small. It's still fine.  Okay.  That's a good news. So first we'll talk about how to do some trees and then we'll generates to these junction trees.  And actually, I need a scribe, I realized that I didn't have a scribe notes for the part about trees. So for gravity me, Nate. I have scribe notes. I just still need to put them online, but they don't have them for the inference entry. So can I get  Two volunteers to describe notes for the part about some product algorithm. So it's basically the end of today's class and  The beginning of the next class until we get to the hmm  With some productive max product. So I have one volunteered. Can I have another volunteer. All right, I have to volunteer, you should matter. And then, and then. Yeah. And then the shortening. Thanks. Great.  So I'll send you an email about this.  Alright, so in France on trees.  So basically if you run graph eliminate  Mean eight  On a tree.  There are some orders which are stupid and some orders which are good because this, this was a tree and eliminating the central node. First is really stupid. So as I mentioned, a good order for trees is to eliminate the leaf first  So the good order.  Is to eliminate  The leaves.  First  And so once you remove leave, then a new node become a leaf and that you can remove it as well and stretch until you get back to  The non leads  And so suppose that your  Your distribution is from a under a tree and I will use this augmented  petrol stations. I don't only use the maximum click, as I mentioned before, because it's convenient. So I will have these  Node potential. And I also have these edge potential excited extreme. Okay, so that's the form of my distribution. And now let's do eliminate in this. So, for example, here's a tree.  Here's my last node. And so what I would eliminate first as far as the leaves. So this is a leaf. And this is another leaf. And then this will be a leaf. Once I removed the other two. So that would be an ordering  And then what happened is, I'm not adding new edges, while I use this graph eliminate so that's convenient.  And it looks like when I eliminate this one I will get a message, which is a function of x three. When I eliminate the know to I'll get the message, which is a function of extreme again because it's to disappear. And then when I eliminate  The node three. I'll get just a function of x.  So these are the  Messages. And actually, I will use a notation one sending a message to three because there's only  One parent to this case. So this is to sending a message to three and this is three sending a message to F. Okay.  And so that's kind of the idea  And so  In more general, what you do is you  The order you choose.  Is you make a directed tree.  Because this was an under a tree. So you make a directory tree by using  X f as a route. And in this case, by the way, for, for now, F will always be assumed to be a singleton  Will explain what can we can do when it's not so good to later but for now it's a singleton. So, for example, here's a tree.  Which is undirected. And now, suppose this is access. So, what I want is I orient the edges all away from the from access. So this is a route. So that means I go away from it. So I mean, do the arrows like this.  That's what it means to make a route you make the pass away from the roots.  And so what you do then is you will first eliminate the leave so 1234 and then the parents of the leaves. So that will be five and six.  That would be the diminishing ordering. And so if you look at how the message will be transmitted in the structure so that the leaf will send a message to their parent and then the same thing here, leave to their parents.  And then once the parent as receive all the message from the children it's allowed to send a message, it's allowed to eliminate be eliminated and send a message to their parent  So that would be the other message or done and the rule, the update rule would be that the message from a child in this directory tree to a parent. So Jay, would be a parent.  This is a parent.  This is a function on the of the parent variable so exchange, by definition, this will be the summation over x i, the verbal of the of the child.  And what will be the potential in the activity. Well, you'll have all the nodes which have X in it. So you will have the node si x i, you will have the edge potential. Next I xj and then you'll have all the incoming messages.  From the children of it. So K belong to the children have I  Have the message from key to I wish depends on pics user avatar   Okay. user avatar   So these were basically the new factors.  Which contained I in the user avatar   Activities. user avatar   So that's basically the update rule for running graph eliminate on a tree in the right order. So this is just the implementation of gravity mean eight, when the correct order for a tree. user avatar Breandan Considine  Simon. I have a question about  We try to understand the motivation here, are we  Removing the observed variables to to and when do we stop the this rewriting process of elimination process.  Yeah. user avatar   So, user avatar   Basically  In order to to kind of like run. These are the more implement is that you find in fashion. I first present how to do just the monetization. So I suppose a nap conditioning and anything and then you can easily run the exact same algorithm with conditioning.  By putting  Basically  A deterministic. How could I say a direct potential under observed variable.  To make sure that the only keep when you some over all possible values for variable. If this variable supposed to be observed. You're not allowed to summon because it's it's fixed and so you can do that with a trick of like using a credit card delta function or what's the  Kind of  The direct delta is for continuous and critical delta is for this week. We use a credit card delta function. I'll explain to that very  But so yeah so so just being marginals is not a super interesting. But it's still also interesting.  But we can also conditioned by just fixing their value.  Why do we need sigh xi.  In yeah so good questions. Good. Eric so filmmakers asking. Oh, why did I use the over characterization  Why do I have this thing. Okay, well,  We'll see that it's convenient for some formalism. So for example, in the hmm, you can think of. Hmm, as I have observed the  The so in hmm, you will have these these these know like this. Right. And then what happened is you could think of.  These observed variables at the bottom as defining a node potential in just a simple chain graph.  Okay, and so I have the standard transition probably T on my, on my Markov chain which are Perez potential, but then the node potential. You could think of coming from the observation. So that's one way to still treat it directly without having to  Kind of like run  But  But, but it's it's mainly just more generally, sometimes it's convenient to just have this note potential  Because it's true. You can always absorb this in an edge potential and then forget that these exist right but it's just convenient also know how it happens.  Yeah.  Okay, so that's the graph eliminate the. Now let's talk about the some product algorithm.  So the some product algorithm.  And this is for trees. This is an algorithm to four trees and the sun product is a bit similar to the graph eliminate but it's also doing a bit more. So this is an algorithm.  To get  All the marginals  Okay, I'll say all the node.  And the edge.  marginals  Cheaply  On a tree by using done any programming. So you will actually do storing  Or cashing, if you will.  And reusing messages.  So if I care about is to compute the probability of X one, for example, then just run graph eliminate like I've just told you, with the right order.  But if I want to compute the quality of x, y and the quality of x to the property of extreme as marginals  You don't want to run Gretchen emanates multiple times, because it will compute a lot of the same messages. Okay. And so here, the idea is basically to use dynamic programming to save on the computation. So you will store some of the message, which are reuse for multiple user avatar   Computation. user avatar   So for example, if I go back to this example here suppose that instead of  Instead of computing the marginal for access. I wanted to compute the marginal x five right so then I say, okay, well the graphical idea would be to have expired as the route which means I would need to slip this edge, the direction. Okay.  And so then in terms of message being sent these two message would be exactly the same as before.  These two message would also be exactly the same as before. This one will be the same as before. The only difference is that instead of having a message. This way I would now have a message this  So in terms of computation all the message or the same except one.  And so if I compute. I wanted to compute both the marginal. The next F animals on x five, I wouldn't want to just compute separately. These two  These two set of messages, because a lot of them are overlapping. So instead, if I store these messages, then I can reuse them to compute multiple margins. So that's the idea of some product. user avatar   OK. user avatar   So the idea of some product is basically to get all the directions, all the message, you need to complete all marginals, and so the idea is to have these two phase. So let's say this is the it is the root of my  Graph.  Of my directed tree. And so what you do is you will have to collect phase where you compute all these messages like if I was trying to compute the marginal on the route. user avatar   This is a root tea. user avatar   Okay, so in red here is called collect phase.  And then the distribution phase is just now to go from the route back to the leaf. So I could compute this messages and then these messages.  So this is call the distribute face.  And so notice now that if I wanted to compute the marginal on this node.  I have, you know, the message in red. That could be us.  And one message in green, which was the missing message. And so by having both the green and the red messages I can actually compute the marginal any node.  And so here the goal.  Is to for all edges in your graph.  You will want  To go  For all edges in your graph, you will want to compute the messages in both directions. You want to compute the message going from i to j, which is a function of exchange and the message from Jay to i, which is a function of excited  And the rule in order for this to be a valid computation, I can only send a message to Jay  Message to the neighbor.  Jay, when it has collected all the messages from its other neighbors. It has received  All messages.  From the other neighbors.  And distribute collect face their collect distribute phase I just told you is a way to organize the competition to make sure that this is satisfied.  Okay, but the general update will be all right. There's a  There's a note I there's a neighbor. Jay, and then I have a bunch of neighbors have I  That's called them with index k. And I want to compute the message going from i to j as a function of extreme  Well, the rule is if all these messages have been computed, then I can compute this message and its value will be as before the day not to invisible ink.  And so the update rule. So I will have the message from i to Jay as a function of xj.  Will be summation over x i.  The nodes, the potentials, which have excited. So that's as before. Excited sigh J x i exchange. And then I have the product.  The product.  Over all the neighbors have i minus j  I because God is in the competition already. So these are the neighbors.  Of the message from K to I, as a function of excited  So that's the update true. Okay, so this is basically the same update rule as I've done here. But here there was a specific ordering and I already had a query node was now what I'm doing is I'm  Considering all the possible direction of messages. And so then I have this generate Trevor's hold of the tree.  And we'll talk about some product schedule very soon. But first, let's just recap what we get. So at the end.  Of this all rhythm.  Then we both have the node marginal right  We can get the node marginal  So we have that p of x i is proportional to all the remaining factors which contain excited, like in graph eliminate which would be product over all the neighbors have I  Have a message going from Jay to I excited because it's a function of excited. And there's also a sigh of exile, which still has not been eliminated because it only has excited  So that gives you the marginal and then you can normalize it.  To compute see  You can just some for overexcited of this thing, right.  But you only need to compute the ones you don't need to compete it on all marginals was this only this is true for all marginals  So that gives you the norm marginal. It turns out you can also get the edge marginal, as I mentioned,  So the edge marginal if you you run graph eliminate you would have that the two nodes on an edge.  Or in the last in the elimination ordering. So you need to get all the messages from the rest. So from a, from a little drawing perspective, you would have an edge between i and j  And then you have all the messages coming from over there.  All the messages coming from over there. And what's the meaning of these messages, it's me, it's what's the influence of something out all the other variables from from this part of the graphics all the potential. That's what its meaning.  And then you would have that the marginal an x and x j  Is just one of z.  And then all the remaining active factors. So I would have sigh of x i say Jay of xj. I would have sigh g of x and x j and then I would have the incoming messages. So it will be  Product.  Product over the neighbors of I minus j  Of message going from key to I, as a function of i and i also have product over key prime neighbor of G minus i.  Have a message going from keep trying to Jay as a function of extreme right so that's how you get  So you get the marginal  The emotional and the edge on to nose.  And so during the sun product or rhythm. You basically compute with the right order. All these messages and once you have all these messages you can compute all these marginals, like a just mentioned.  And so one thing to mention, which is important is that, let's say this is a tree.  Let's say I have this tree.  And let's say I want to compute  The marginal on those two notes right  Then you can do it with just some product.  Okay, because what happens here is any elimination ordering where those two nodes are the end will add new engines. So for example, let's say I choose I eliminate the leaf first  Well, then perhaps I will eliminate this will. This will add this new edge between these two nodes. And so now I'm getting a bigger click so  And some product only allows like edge potential. And so to compute this margin, all you would need. So here you would need graph eliminate that you will need the more general algorithm.  But if all the marginals you want or either on the edge, or no, then you're fine with some product.  Okay, so let me conclude with the schedules that you can use for some product.  Because basically, the rule is you can only transmit messages when all the incoming messages have been computed. So how do you make sure that this work. So I already told you that  You could actually use as above the distribute collect schedule.  Actually or collect first and then distribute. But anyway, you can do both. Either one first.  Actually it's a collect distribute sorry because you need to start a relief.  So collect and then distribute schedule.  So that's what I explained above, but there's something else you can do, which is called the flooding parallel strict schedule.  And so this is a bit weird, but  It's important to mention, and especially I'm talking about it because it's an it's a it leads to an algorithm called loopy belief propagation  And so what you do in the parallel schedule is you actually initialize all the potential also all the messages to a uniform distribution. So all the messages in all directions Ajay messages.  To a constant. So, to a uniform distribution.  Mean these don't have to be some to one, because we don't care about the normalization constant. So this is for all  I j, such that i j is an inch  So you also look at both direction. Right. So because the message is going both directions. So you start to you you you initialize them to a constant value. It's the old table over X i and you just or xj so constant value and then what you do is, at every step.  In parallel, so you can actually implement this algorithm as a parallel algorithm, you will compute  The new messages.  As a function of exchange.  As if  The neighbor messaging than neighbor.  Message were correctly computed  Okay.  So originally, it doesn't make any sense because you started with unit for message which has nothing to do with the potential, but you can actually prove easily  That after the diameter  Of the tree.  Which is the longest path in the tree. After this number of steps.  All messages are correct.  Or correctly.  Computed  And this is only true for freedom.  And you can think of it as Dr. Fixed point  What I mean by fixed point. Well, if I look at  This rule here.  Right, so I have  Whoops.  So I will have that all these are my neighbor messaging messages I have an update rule to compute the value of this message, but this message already had a value before  Which is used to compute the value of the message for the neighbors. And when this is currently computed. It turns out that  The new value for this will be the same as the old garden because it doesn't change, right, because if these measures as unique value up to the magician constant. But if there. We don't change their position constant during the algorithm.  So it turns out that this will have the correct message shape. If I had done the correct order and. And the reason this is true. It's actually not super hard and I will conclude with this, or perhaps I'll mention  I'll yell. I'll do the looky BP after in next class. But basically, here's a tree. user avatar   Teck Teck Teck here's another tree. user avatar   All right, and I'll connected. So the point is  If I would do the the collectors to breed phase I would do a sequential fashion and all the message will be correct, right. So now when you do the the the flooding kind of our rhythm.  The message which will be sent in this direction will be correct at iteration one  Because note that  These No don't have any parents. So while I do the computation of this message. It's already the correct message because New Orleans.  So after one step. These messages will be correct. And now, because all these incoming messages are correct, then the next time.  The next time I will compute this message at step two. It will also be correct. So now in red or correct message which are fixed.  And so then the next iteration, the neighbors of these will be correct. So these will be correct. And then the last at the four step. These will be correct.  And so you see that after four iterations of this flooding algorithm all the messages are correct and the diameter of this tree is actually for so that makes sense.  So that's the explanation why this works.  Okay, well I'm already over time. So is there any questions. So the next class. I'll finish up will be I will mention loopy buddy propagation, as well as how to get the conditional and start to talk about  Max product.  So I need. Yeah. Brendan user avatar Breandan Considine  Can you recommend a software library that implements say graph elimination or  Some product or one of these algorithms.  Kind of, are you aware of any stable ones. user avatar   Uh, yeah, I forgot the name of these libraries, but I can put them in Slack.  So there's there's Unfortunately though, these libraries were before pi torch and this kind of stuff. So you were like in MATLAB or something. So I'll have also to see if there's a more modernized version, which  Can also be using Python but I mean I'm sure there's also Python libraries, it's fine. But yeah, I'll user avatar Breandan Considine  put this up on Slack. user avatar   So Jacob is asking, what do I mean by correct i mean the the  That the represent something that will be used to compute the the correct marginals, right. So, because  Basically this update rule.  You know this update rule was obtained from the graph eliminate are within dynamics.  To actually compute the correct summation over all possible other nodes for computing and marginal. Okay.  Now, if instead of putting the correct incoming messages I put random quantity, then this is not the city of correct message. That's what I mean by that.  Right. But what happens is that when you start the flooding schedule some nodes, what have any incoming messages you will all be have this part because their leaves.  And so then these message will actually be the correct computation you would have done if you did a sequential algorithm.  Like a flooding scan normal sequential scans and so then these message will be the normal correct message. And so then the next step. The, the message computed for the numbers will also be correct, etc, etc.  But originally, you just initialize all these message to a constant or it could be anything. By the way, you don't have to miss it to initiate enter constant  You could entice them to say half and half. If you want, or sorry, a half and half. But that's a half on one though, then a third, a third on the other node is just constant is a more like  regularize initialization. As long as you don't put zeros, then you're fine for this argument.  Okay. user avatar Abdelrahman Zayed  This is very fine. I just have one  From what I understood in flooding. We have to start from somewhere where we don't have any incoming messages. Right. user avatar   Yes. Well, so in flooding. Basically, you're so the flooding our idea is, is kind of a  First wasting computation, because it's competing, a lot of messages which don't make any sense. But on their hands done in parallel. And so you can go kind of like  It's a bit like you don't have to worry how you implement the order of the messages. So in the standard or with them, you need to first just collect  Complete the message and collect order and then you compute the messages in the discovery order.  Was in flooding. You can think of, oh, each node will will have their own CPU in parallel to compute their message and I do all of that in parallel.  And it's actually a synchronize all women, which means that iteration one everybody compute their message from there using the neighboring message which are junk.  That's. Step two. You repeat. And it turns out that the nodes which are dead leaves after one step already correct their message. The computer is correct.  And then I stepped to the their neighbors will be corrected with retro but everybody's always computing new messages and it's kind of stupid because  We're simple believes it will conclude their, their message. And then the next time it will compute the exact same message. So it's kind of like wasting computation. But from an implementation perspective, it's kind of like simpler. You don't have to worry about which one is correct wins. user avatar Abdelrahman Zayed  Thank you. user avatar   And we'll see that user avatar   When the graph is not a tree. This all rhythm can compute an approximation of the marginal which is called the new people. Each publication.  Idea and that's why I'm talking about this because it's kind of since kind of a stupid algorithm because it's wasting a lot of computation, but it kind of motivates the good people, the propagation approximation.  Yeah. So similarly, that's getting the managers of this is that each message can be completed in parallel.  Yes, and also that it motivates the loopy belief propagation algorithm.  Sure you could also make more clever implementation to not have to whisk computation, but then it's just more complicated to implement  Okay, so I think I haven't answered all the questions in. So I will stop the recording and  I remind you that  There's a small get her town, social, and actually I have gotten information and how to exit the gator town got through town, and though I haven't had the time yet so perhaps next time I'll be able to do it. But yeah, so I encourage you to come to gather town if  You're
 Recording. So, today we will  What are we doing today we're going to do a bit of provocation and then I will do something called the max product algorithm, also known as Vitor be  Which is how to find the maximizing assignment for the distribution and then will generate the some product algorithm to arbitrary graph with the junction tree algorithm. And then if we have time, we'll start to talk about how to do in France in hmm  As a specific application of the Sun protocol green  That's the plan.  And so first  Let's talk about loopy belief propagation  loopy  Belief.  Propagation which I started talking about in the last lecture.  And so  So what's this loopy verification. So it's also called loopy BP for vacation. This is to do approximate  In France.  For graphs.  Where you have cycles.  Okay, so we're talking about underrated graph right so we're there. There's no fee structure or anything. So if you're not a tree then one node will have to, in some sense,  Will have a clique of size more than two. So three and the peak of size three is a cycle right  And so you can run some product because it's not a tree, but you can do as if it was a tree, run the parallel version of some product and then you get an approximation.  So that's just recall what we saw last time that we're on the same page. So first of all, if you remember, that was the update rule in in the some product algorithm.  Which is basically  If you recall, this is running graph eliminate Gravity minute algorithm is a way to just compute  marginalization marginals in the arbitrary graphical under 50 Africa model. And then what I did is I I specialize the graphing immediate algorithm to trees with a specific information ordering, which was clever and that gave us basically the these this kind of update rule.  Where  When I when I compute the new potential that I put in my active list in the graph limits our rhythm, which are message basically going from one node to another.  This is marginalizing all the potentials, which have exciting it so it's the note potential the edge potential and includes all the other incoming messages.  Which has already been computed. OK. So the idea of the Sun Protocol version was to do a specific ordering for both the graphics or rhythm and store them in these messages memory so that you can reuse the messages. Oops, you can reuse the messages.  When you compute any marginals right and so  And so in order for this computation to be computing the correct message I needed to have that these message has already been completed, which is why in a standard some product schedule, you need to start at the leaf because the leaf don't have any  Basically children so you don't have any of these incoming messages so you can already just compute the message to  The parent of the leaf by just marginalizing out the the potential. Okay, so basically, that was this this distribute collect phase which way I told you how to organize a competition and a tree.  But then at the end of the class. I mentioned. Okay. Well, we can also do a parallel type of optimization of some product.  Which instead of waiting that these messages are already meaningful. We actually initialize them to some arbitrarily non zero value. So I suggested the constant I use. It's basically like a uniform distribution, in some sense, because  You can think of them as when you normalize them. This gives you a distribution. And so that was the this flooding parallel schedule but I mentioned last time, where you just initialize all the message to a constant.  And then at every step in parallel, you compute a new value of the messages using basically  This formula. So here you have new and here you would have old and you do that in parallel, because you know  The there's no dependencies between the values. So, so there's there's no new on the right hand side.  And so I can compute all the new messages by in any order in this case doesn't really matter. And that's what you can do it in parallel.  And I told you that at the beginning, this doesn't the messages that you compute don't really make much sense.  except the one and beliefs, because the leafs don't really have neighbors. So, so then you only have this piece. And so it's the correct computation. And I told you that as the algorithm run as you do multiple steps of this flooding schedule more and more messages become  First of all this. The become fixed point, they are not updated anymore. The state. The state, the same as they were before, because the basically satisfy the correct  Property that the new message on this side is the product. If I put also the new message on this side. So it's a fixed point of this update and after diameter of the tree number of iterations all the messages are correctly computed. Okay, so that's what I explained last time. Okay.  And  And and I gave you the rational widest Western right so this was basically here, you had the you had the  The display that after iteration one these messages in green or correctly computed  And so I think duration to  The messages which depend on the correctly computer message or also correct so that the message and two are correct. And it's a threat three are correct. And then for after four steps all the messages are correct and for here was a demo. OK.  So now up believe propagation. It does the same thing. The parallel schedule.  But not worrying that perhaps the graph is not a tree so that these messages might not be correct in some sense okay and so  loopy believe propagation update what you do. Let's go back here. So what you said you will. It's the same idea as the flooding schedule so you will have these messages for every  Edge and then I will compute my new message. I will just say, oh, it will be some summation over xi xi xi say i j x i exchange and then product over the neighbors have i minus j  Of the messages.  Incoming to i, which depends on the size and the good thing that the thing here is I use the old value of the messages. Right. So it's, it's an iterative process. So you could think of it as  The message at time iteration T dependent. The message at iteration t minus one, but I'm using old except of t minus one and new for t  So you just compute these  And  If the tree was if the graph is a tree. This would convert to a fixed point and it's all fine. If the traffic. If the graph is not a tree. This could also converge. If you're lucky.  But to make it more robust to actually make the algorithm more robust what you, what you do is you add something which is called a step size.  In some sense in the log domain. So what you do is you will take a convex combination in the log domain between the old messages and the new message right so you'll say,  Instead of moving all the way to this update I will basically raise the old  The old  Message to the alpha and then they will have the update for the new message race to one minus alpha and they take this combination in here alpha belongs to  01 and you can think of this as a step size in the log domain right if I think the log of that I would get alpha log of the old message plus one minus alpha log of the update and so I'm thinking of conducts combination between those two.  And this actually stabilize the updates to make sure that it converts to some fixed user avatar   Okay. user avatar   And this is called basically  This idea of using a step size is called dumping.  The dynamics because instead of making the update to the old new message, but what you could get as you could get this association behavior and it  Doesn't really converge. So what you do is you take a combination between those two things in between. So it actually helps you to to stabilize the dynamic that's intuitively, that's what's happening. There's actually a whole formalization of these from a vision perspective, etc, etc.  Yeah, so this is a product here between the parent assists. So if you take the log it would become a son that's why I'm saying  And so what's happening here is that this gives  Exactly exact answer on tree.  Okay I III, at some point, this will converge to a fixed point  And the fixed point yields the correct  marginals user avatar   Okay. user avatar   Even if you do the damping, it's still, it's still fine for trees and then when you have a graph, which is not too loopy I he doesn't have too much big  Big loops.  You'll get that these  Fixed point that this algorithm reaches which depends on your initialization of the messages, by the way, they will be  approximate solution to the marginals, in some sense there because this update that you compute this marginalisation update  Is something which makes sense from a, from a consistency perspective of like computing the margin, all of a distribution.  But because of the of the loop aspect. It's actually not  Fully rigorous approach, but you can still often get marginals, which are not too far from the true marginals, because this is an empirical  Observation. So people made a lot of experiments, sometimes in the end it's not it doesn't give you that far approximation.  Can you get guarantees on this approximation, it's really, really hard. It actually I don't even know if this  loopy believe propagation give guarantees apart dark guarantees in terms of convergence, but I don't think there are guarantees in terms of the quality of the approximate solution, you get more Eucharistic  And sort of fact asked in general graph with cycle. There can be multiple fixed point. Correct. It depends on initialization. So there is  In the book by  Rain right in Jordan. So there's actually  So we'll talk a bit about virtual methods later in class, but there is an interpretation of the loopy belief propagation as an optimization method basically implementing a very rational approach. And so you can  Show that this algorithm is basically approximating and optimization process, but which is non-complex so the minimization of problem is non convicts, which is why there's multiple story points.  Right, so that's up BB  Alright. So somebody's calling me hit me hung up.  And the other thing I didn't do last time was how to do conditioning right so in some product. I only told you how to get the marginals, so to recall so that we're all on the same page.  You compute all the messages with the rules. I told you. And then at the end of the day to get the the marginal the node you just take the product of all its incoming messages to this node you add the note potential and you read normalize my because this is proportional user avatar   Oops, I forgot the invisible. user avatar   But what about if I want to, instead of getting a marginal in a node, I would like to  I would like to get the conditions.  So that's explained that user avatar   Getting conditional user avatar   And so I told you that, basically, we will use a semantic trick. The idea is when you have conditional  It's proportional to the joint. And so let's use a bit of notation action. First of all, the notation. So let's say I want to compute the margin. The conditional on node x i given some evidence node and I will actually use the bar notation to indicate that these will be  The values that were conditioning.  Indicates values.  We are conditioning.  So why am I introducing this annotation. Well, because when you compute the  The messages in the sub chronic algorithm, you're often something out a variable, right. So in some senses variable should be dummy.  But you want to distinguish when you're sending out a variable versus something which is fixed. Okay. And so that's why I use the bar notation for something which is fixed. And so we have that  The conditional to excited given x bar is proportional to the joint on X i and XE bar right so this means X. He is equal to x the bar right that's the notation here.  So have to know which variables are we talking about when we talk about XE bar. Well, it's just a variable without the bar.  And so  We want to compute this this this marginal and we can do that by just not something out the variable which are conditioning right if we don't send out the stay in the joint. So if I run the graph really mean our rhythm I some overall variables, except the one which are conditioner. Right.  And so when you don't send out. It means you keep them fixed so you keep this fixed  During  marginalization.  For each  Jay belongings to eat. So what happened is when you're on the sun product algorithm. Sometimes you will you will you will complete a message, which should normally some over x j where x g is observed node.  And so what you do is you want that you don't want to some over the observed know because they're fixed. And so we actually use a formal trick.  Just to have the same update you could, by the way, just have an if statement in your son product algorithm will grasp it in that order them, which says  If the variables observed don't send it out. But there's a neat way to actually not have to worry about which variables are absorbed are not in your message update is to just  Add new potentials. And so that's the trick and talking here and that's also highlight a bit the usefulness of having note potential is you will have you will redefine the note potential I'll use the tilde notation to talk about the new potential. So it will be the, the old potential  And then multiply by a connector delta function on xj.  Which depends on the observed value. Right. And so this delta here.  Remember that we're talking about discrete random variables. That's why we have  The connector delta, if you had continuous you would use the direct delta which is a bit more complicated, but the grantor delta function.  It's a function of two arguments and basically  Its definition is just, it's one perhaps I'll use the  Delta A B is defined as one. If a is equal to be and zero otherwise.  So it has two arguments. When the two arguments are equal. It's one when the two arguments are different than zero. So it's just make sure that the two arguments or equal  Which means that when you marginalize say the first argument over all possible value when you somehow one possible value because it's only one when this is equal to the second argument. It's basically selecting your value. Right.  And so when you are summing over x j. This potential  And then press time some function of x j and X i. So, when you compute this message going from Jay to I  Normally you would be something over xj when your condition when xj belongs to the evidence set. You want to fix it. So you don't want to send it out. Well, this is what's going to happen when you use this trick because  This is equal by the because of the direct delta part to say g of x j bar right because it's zero otherwise. And then  I have basically replaced xj, the sum over xj with just the correct value and conditioning.  And so that's the trick to do conditioning is just fix the variables that your conditioning on in when you are sending out all the other variables. And at the end of the day, what you get is the  The, the joint on exile and the observed variable. So at the end.  The result of some product.  Will give  The margin all the next I N on X E bar, which has been fixed.  And so this will be basically proportional. So there's a professional constant which is z which is and then there's sigh.  So I have x i.  And then they will be product. Overall, the neighbors have empty to I have x i.  And in order to get the marginal in the x bar, you just normalize this over exact right you re normalize  Over x i.  To get the conditional because  Okay.  So that's it for conditioning.  So when you run some product or graph or even a just don't send out the variable that you're observing. That's it. It's not very complicated.  So we use a trick here that the conditional is proportional to the joint and then that's why there's no complicated division apart at the end when you re normalize  So is there any question about this.  No.  Alright, so if there is no question  Let me move on to the max product algorithm.  Okay, so some product was to compute the marginal  Then max product is to compute the art max of the joint.  Which I told you was important. Let's say you do speech recognition. You want to decode the the speech given the observed the sound. So if you can. You want to compute the art max over to probably T of the speech given the observation. So it's a big art max.  And it comes from a interesting observation that for some product.  To run the some product algorithm.  The main property I use was that for graph eliminate for that matter, the main property we use was to use the disability law, right. So the main property used was the disability.  Of plus over product. user avatar   Right. user avatar   And  So by using this video I could push. I could to reorganize the competition and push all the some inside and instead of keeping in  At outside where you had an exponential something right so that's kind of a trick.  And so when you have distributed et la basically there's a name in algebra call or ring. So with a ring you have normally two operations and one is actually a group with an inverse and the other one.  Is not to see it doesn't see the habit inverse. And then there's a distributed VT. Okay. And in our case, we actually don't need the the inverse. So  All we need is  That  What you're competing on where you have the plus operation and the product operation is called a semi ring.  So semi ring is a ring where you don't have. You don't need an additive inverse. So the plus operation is not  A group with an inverse. I mean, it's not a full group don't need additive in verses  So okay, so am I talking about the semi ring. So the point is that you can do a some product like our rhythm and other semi right where you just replace the plus operation with something else.  And the product operation was something else. But you read you you reuse the same kind of idea. Okay.  And so that was a neat operation to generalize this trick to other structures. So you can do some product.  On other semi rings.  So what are there are some earrings. It could be if I look at the real number, I could now look at instead of the plus i use Macs and sort of product I use plus. Okay. Okay. So why is this still distributive it well I have that the max of A plus B and A plus C.  Is the same thing as  A plus max of B and C. So there was this a in common in the plus  Which replaced that when you had a times b and a time. See, and then you factor out the A for the distributive de la so here it's it's it's the max which is kind of like  It's the plus, which is factored out  Okay, so that's the example of disability another one you can do is if I look on the positive number. I could have max and then product. Okay. And in this case, I have that max of A times B a time see is the same thing as a times max of the NC  Okay.  And when we run some product on this semi ring structure, what we get is Max product.  Okay, see we get max product.  And to kind of like reuse an analogy that I made last time. So if remember I started by doing summation of products. So if I have a factorization. Right.  I have a bunch of potentials which factor is only over node potentials. Before I looked at the some the exponential some of this thing. Now I can look at the exponential max.  Right. So if I want to max over all my variable. This product by to this to BT. BT. This is the same thing as just the product over I have the simple individual node maximization.  So that's the direct analog of having pushing my some insights. So here I pushed the max inside  So even though the original problem here had the maximization over an exponential number of  Assignments here for every i it's only say order k. And then I have any of these. So this is order n timescale instead of order key race to the right. So it's, it, it's a big game. Okay.  And so when we run max product we compute these messages. The same way we did in the in the sun product algorithm. But we replace the summation with maximisation right so if I compute the message which go from it. Jay, which is a function of  X James case. So why do they flip it.  Normally I had to. I know.  We see it. See if I screwed up at the being in the class.  I know I always had extra  Good, yeah alright so message from it. J to function of x j. And so this will be defined as the max over x i, which replace the subtle Rick's I have my potentials, which depends on the size I have potential over I have x i.  Put the show i j x i xj.  Then I have the product over all my neighbors.  Have I minus g  And then I have message from key to i, which depends on that side. Right. So this is the so I've just replace  The some with the max right so  The some became a max.  But the rest of the same idea.  And so that's the update for the max product algorithm and the same idea you you need to schedule to compute the message, you need to have all the other messages already Pre compute  And so the point, though, is  Unlike the the  Unlike the marginalization over every node, which we want to compute in some product here it's. We don't want to compute the maximization over every node, because the value is. It's just a big max.  Over the whole joint. So there's not there's not actually difference if I compute the max.  For have one message versus the max of the next message will always be the global max because there's no difference. Okay, so basically  We're in a setup where we're more thinking in terms of graph eliminate with a clever order. So let's say this is my variable 12345 I start at a leaf, which could be, let's say x five and then you want to compute these messages like this. user avatar   Up to me one. user avatar   Okay. user avatar   And so to compute  The message from one to two, you would  So one doesn't have  Doesn't have any neighbors. So this would disappear, you would just do this maximization like that.  Over x one.  Or I guess here. I'm doing it the other way around. Yeah. So I'm starting at five. Correct. So I'm keeping the message from five to four. So you could think of maximizing over x for the  This product.  Over  Which this doesn't appear at four x five because there's no neighbors. It's a nice  OK, so now the point is  If I want to compute the max over the variable x one up to five of my new GM  Joint. So this is because we're clique.  Of this thing. Well, this is the same thing as one over z the max over x one of the message to going to one of x one right as before the marginal was the last messages coming to the note here the last maximization is done on the last message. Okay, so let's kind of do user avatar Breandan Considine  A quick question.  I i am i'm curious, you mentioned that it has this this semi ring property. If you had like a non linearity between between the the message passing when it goes from one to the other, would that violate the  The, the dish.  Some kind of like a soft max or another non linear functions are user avatar   Wearing already put the non linearity. user avatar Breandan Considine  After well we're the say with the max is if you wanted to.  Have like a sigmoid or  Logistic user avatar   Function. Yeah, well, well then you definitely don't have distributed beauty right you can have pushed the max inside when you have an ordinary  Yeah, this to be Pvt of max over product in this case is because there's no non-interfering user avatar   Uh huh. user avatar Breandan Considine  Okay, yeah, that makes sense. I guess so.  Okay, thank you. user avatar   Yeah. And bingo. She is asking  Whether what I'm describing max product is for trees on me.  Well, you could still do the graph eliminate algorithm. If you don't have a tree and just instead of replacing having you have an evaluation ordering  And then instead of just summing things you're just maximizing. And so you could also run that for non trees, but the the the update I gave you here, which already only had  Edge potential is only valid for trees, indeed, right. So this is the direct analogy and a log of the Sun product algorithm for user avatar   Max product. user avatar   But the thing I want to mention is that we actually don't care that much on the maximize of the, what's the maximum value is where we care is what is the argument of the art max. Right. And so in this case we we need to store to do a bit of  How could I say in English, you need to keep track of things you need to write down a bunch of things you need to store things so to  To get the art max. So, for  Getting  The arg max.  You need to store.  The argument.  Of this maximisation  The value of the max depend on xj. So the, the maximizing value will also depend an extract right so it's it's a function  Of x g. OK, so for every possible that you have xj. You both have what's the maximum value that you could get by maximizing over excited. And what's the art max. What's the maximizers  Okay, so you said it's kind of another message, but now these message as to do about keeping track of the maximizers  And then what you do is you do backtracking to get our next right so  So let's kind of like, see how you implemented backtracking.  And so  Let's say I want to compute the message from I 2G right  And so  Let's see, this is my variable x i.  And then I will have a bunch of possible values for x i and then I have a bunch of possible values for exchange.  And we'll use the values of x j as indices. So I would have 012 blah, blah, blah. user avatar   Okay. user avatar   And so in this entry here you will start the message from i to Jay, which depends on exchange. Right. So for each value of x, Jane, let's say value zero, you will have  One value and then for a value one, you will have a different value, etc. Okay. And this was computed for a specific xj by maximizing the, this, this, this object here.  Overall excite. Okay. And so what you do is in the other entry, you have another array where here what you store is the art max.  The art max for this value.  OK, so the silver excited, ie what your computing is basically the art max over x. I have some function which depend on xi and xj, which in this case was fixed right  And so you can think here.  This is kind of a pointer to a previous value of x i.  And basically,  Here I would have the message from K to i, which is a function of x i, which was using the computation.  And actually this is because I don't have a tree structure. So this is a sequence structures. If you have a tree, you would have multiple messages, but here I'm just having a sequence structure. And so these are basically the message data structure.  And this would be the art max.  Data structure.  And so what you could do, for example, is run things forward to compute all the messages with all the arg max stored until the end of the sequence, for example.  And then you compute the actual  Maximizing value for the, the last variable. Then once you find that you can follow the pointer back for all the other variables which we're maximizing and that gives you the assignment of all the variables, which gave you this maximization. Right. So in other words,  To get the art max over all your variable of the joint of X one up to n.  And you can do the same thing for a conditional by just fixing some very right. So, and this is called decoding.  So what you do is you run  The max product.  All over them.  And in this case, you only need the forward messages.  So you don't need to. So in some product you were competing messages in both directions because you want to compute all the marginals, but here, like I said that you only need the margin, all of the message at one of the last node or the first node.  And so all you need is just to run the algorithm forward and then you do a backtracking you backtrack.  The art max pointers.  To get the full art max.  And if there's some ties. So sometimes, for example, there's multiple values which give there's multiple variables which give the same value.  Well, you could follow any of these right so you could actually keep track of these ties and so then there could be multiple times. So that's a way to also get multiple  Joint assignment which which give the the art max and and this algorithm, which is basically max product and then decoding by backtracking is also called the, also known as the visitor be algorithm.  So it's an old algorithm. And it was figured out much before these semi ring trick.  And it was actually also rediscovered in multiple setup and signal processing and and other fields. I'm pretty sure  And now basically the idea is you can have a unifying perspective and all these algorithms that oh, there are just special case of this, like, kind of like graph eliminate idea where you use a distributed beauty of one operation over the other one. And then that's some product max product.  Okay.  So is there any question about the B2B algorithm.  Or max product. user avatar Breandan Considine  Can you express this as a matrix matrix product. If you were to use like a different operator for the multiplication.  Instead of here you're defining it like on a node level right that the node and its neighbors.  If you could if you could  Have another algebra on on the matrix because if you have a matrix instead of like a real number here that would also be a semi rain. Right. user avatar   Yeah, so the so so the marginalization operation. So if I  Keep the some here, this could be a matrix.  Product. So this is basically a vector. And then I would have, I would have a matrix multiplying all these to get the some I could get matrix time Victor  And now you're saying if instead of of defining my matrix vector product as some over product development, I would do this max operation and it's I just use a different operation that could still use matrix vector operations. I think so. Yeah.  And actually, so later in the class would talk about the hmm. And then, hmm, normally you do all do these matrix vector product because for the the some product, but indeed you could instead. You could also use the same kind of matrix structure for other operations like use it. Sure. user avatar Breandan Considine  Thanks. user avatar   Any other question.  Okay, so perhaps just before going to the break.  Let me  Introduce a property for the you GN which will be useful for the junction tree perspective. Okay, so let's talk about a property.  Of tree underrated graphical model.  So if p belongs to a  undeterred graphical model over a tree.  With non zero marginals  Then you can actually write p  Only with its marginals in this very elegant form. So it's the product over the marginal of xi and then the product over i, j  In my edge set of the joint over my edge. So it's my edge marginal divided by my node marginals user avatar   Okay. user avatar   And so now I have  For you, right.  Which, by the way, this is kind of like you can think of.  It. See, so you can think of these as being sigh of exciting and these as being say i j of x i exchange. Right, so they're only function of exciting. So, but now I have written my potential in such a way as the only take these marginals in them. So it's kind of a very nice way.  And so the way you can prove that  Is you can show that  Similar to a and directed graphical model.  You can use these factor to define a giant. So for any set  Of factors.  And I will use. So I will have pairwise factors on X X, Jay.  And node factors. If I have x i.  Such that the  They're all positive.  And the satisfied, something which is called the local consistency property.  And by this I mean that the the basically behave as marginals right in the GM we had these factors which behave like conditions.  Here we want these factors to behave as marginals, which means that if I'm marginalizing out over xj. The factor which depends on the exciting xj. Well then I get the marginal overexcited which is the factor excited. Right. So this is true for all excited  And then I can also marginalize out over X science dead on the first argument.  And this will be equal to the other marginals, which is f g of x j for all exchange.  And you have that the node marginals, the node factors are basically something to one.  Okay, so you suppose that you have a bunch of factors.  Which  That which basically behave like marginals, and they're consistent with each other. Because again, when I marginalized out but joining them to variable that should get the marginal in the node.  Then you can say  If you can consider defining the joint or a joint as p of x with these product, product over i, f x i product over edges.  Of these big X i xj divided by f of x i and f g of x j  So you you start from that to say, okay, let's suppose I define a joint like this, then you can show that each of these factors are the correct marginal  And okay and this is actually, whoops. Oh, I can move this thing. Interesting. Ah.  Ha.  Sorry. So I had this annoying.  Bars a button in the middle of where was raining and this has been like a few lectures like that. Each time I was like, Oh, this is so annoying. Now realize that can actually move it around. And so I put it backwards should be which is at the bottom.  And I guess at some point I just moving in the middle, without knowing and I didn't know I could move it back so magic. Now, finally, I can see what I'm writing  Okay. Well, anyway, so yeah, so I was saying that this is only valid for tree.  If you have loops you this won't be a valid joint and then you can show that we get  The correct marginals  IE, you can show that the marginal and Xi is actually just the node.  Potential etc etc.  Okay, so that's a way to prove that  If you have a huge GM, you can just rewrite it this way.  So somebody is saying, oh, it's similar to one of the proof. We didn't in your homework.  Yes, indeed, though, you know, in a homework, you would work directly with the conditional, not the marginals, and because here you have multiplication by p of x. So you have both.  You know p of x i here and you have p of x i there so  That's where there's a problem that if this is zero, you get a zero divide by zero, which is a bit annoying. How do you manage that. So instead, you can choose a direction for your tree and then just work with the conditional everything's  Okay, so that's actually a very convenient.  Form. So each time you have a huge lemon tree, you can just rewrite it this way, with its marginals, okay. And there's actually a very useful when you want to compute with see later, the mutual information between two distribution when they are  The they are in under two graphical mala which is a tree because then you know you can actually work with these simple marginals, and some over to marginals, which is much easier than something over the whole joint which is an exponential. So  Oh, yeah. So Jacob, it's normal that you're confused. This is not the proof I said that  Reusing similar arguments we did for the GM, we could  We could show  That you will get the correct marginals yeah i didn't i didn't show that you've got the correct marginals, I say we could show that you get the correct marginals, which means that, then the the GM has this form right  Because basically, these factors. They're just any valid potential. So, yeah.  Okay, so, and why am I talking about this. Well, because we will use the generalization of this for arbitrary graph which are not trees and that's using the junction tree algorithm, which we will see after the break. If there's no question. Is there any other question about this.  If not, let's user avatar Jacob Louis Hoover  Have a question. Yeah, you talk you speaking of these as factors.  And so in in we speak, we refer to the  Parts of  By definition, the parts of the undirected graphical model as potentials and they have certain requirement. Right. So are these potentials with an added  Like with the consistency property. And so then we call them factors is or  Confused with that. user avatar   Yeah, so, so user avatar   The factors. Let's see, so the fact, those are not potential directly because  Normally what you would have is sigh g of x i. Exactly right. You wouldn't have these extra term at the bottom. So you could just define if you want this whole thing as sigh J excites J. Right.  So, so this combination of factors together gives you a potential in your GM if you want. And these could be your, your node potential, this would be your edge potential and these would be your note potential  The only requirement on potentials. Is that the only depend on the correct variables and their positive was these factors, I'd be indeed these extra requirement that you have satisfied consistency. user avatar Jacob Louis Hoover  So if such factors exist, then we can do this. user avatar   So,  So, so if you define any  Factor any edge factor.  Which  Yeah, that's fair point. So, if such factors exist, then you're fine.  It's not too hard.  Yeah, that's a fair point that you would need also to show that  It's a non trivial set  But the point is, you can start by just defining that the edge factor.  Now you have a constraint that once you define one edge factor and you have a common node you can define the other edge factor arbitrary because they need to agree on the on the note potential, but there's still a lot of flexibility. user avatar   Any other question. user avatar   Okay, so let's take a break. It is 331 so let's start again at 343 to 232  Degrees. So at 342  Zoom recording  So what is the local constancy property that I wrote.  So to be clear what I wrote was not for a D GM. It was for these factors.  But basically  If this is the joint. If this as the semantic of the joint overexcited xj. I want that when I marginalize out over  xj. For example, I get the marginal over x i. Right. And so this is this, this has a semantic of the marginal of excited this as a semantic of a joint over x and x j. So, for these to be the correct probabilities. You have to have this consistency properties. So that's what me  And we'll look back when we talk about version and France, this, this will come back also important.  When these factors. Good. Now the approximate to the marginals, and we'll try to update them and they will need to be locally consistent to make sense.  Okay, so  Now let's talk about the junction tree algorithm.  Junction three  All over them. So I won't give all the details.  In the interest of time.  But I will give you the gist of it and the general idea and a very useful generalization.  Sorry, of this  Expression for general graph. So basically using a junction tree structure, I can write an arbitrary joint into something analog to this kind of formulation  And and that's what we use in the junction tree algorithm.  And what's the junction three algorithm. It's a generalization.  Of the some product algorithm.  To a structure which is called a click tree.  To a  Click  Oops. Click tree, which is a tree over clicks and this click tree has to have a property, which is called the junction tree.  Property which I will describe various. Okay. And so now that we have things organized in a tree. You can do this message passing over them again.  Alright, so let's  Let's see how we get a junction three. So let's talk. Let's say I have this graph here.  And so the problem is to work with a junk to build a junction tree you actually need to triangulate the graph.  So if you remember the triangulation. So let's say these are my nodes 13456. This is not a triangle that graph because I can create a chord. I have here a cycle, which has size for which is not split by  accorded the middle. Okay. So, to try and create it. I can do graphic midnight, and then, the augmented graph is try and get a dead. So for example, if I eliminate one I will add this edge here. And if I eliminate  Three later. Oh, whoops. Let's see.  So if I need one. I will add this edge, and if I only three later I will connect the neighbors, which is four and five. So I will add this edge and actually this is now are trying to get across.  And so suppose I start with a triangular graph, if it's not trying to triangulate first then look at all the clicks in the graph. So this is a clique 123 I'll call it a bit. Click a would be the click B which is 234 C and D. Okay.  And so now what's right click three. Well, it's just I will each node is a click.  And so let's say I put these four clicks as my nodes in my tree. So I have clicked a be click see user avatar   And user avatar   Click D.  And I can also put the content of my click inside it. So the click a contains an old 123 be I said was 234 C is three, four and five and D was four, five and six.  Alright, so now and then a key trick is just a tree on the cake. So that's me. Let's say I put these edges here.  And  There's this thing that we will use later, which are called the separator set. So, we will put  On these edge other sets.  Which contains the intersection of the two neighboring clicks in the tree right so the intersection between click in the, in this case is the note two and three.  And the intersection between B and C is the node three and four and the intersection between C and D is the node four and five. So these two nodes or in both kicks. And so these are call the separator set  You can think of. So in the victory. You don't need a separate or set the victory is just  A bunch of edges between clicks, which form a tree.  But you can we will use it in the junction three already moved us a super receptive and now and so above. So this click tree is a click tree. It's a tree and click  With something which is called the running intersection of property.  The running  Intersection.  Property  And what's the running intersection property. They're running a destruction property means that  If there is a node in the original graph which belongs to two clicks.  So, for example, and what's a good example here.  I'm let's say three  Let's say three here. It belongs in click a and b along in click see  Okay, well they're running into a certain property will say that this is a treat. There's two nodes, there's a unique passed between two nodes in a tree.  The running into certain property is that for all clicks on the path between those two nodes. The, the, the intersection between those two. Click will have to also be there.  Okay. And so here, if I look at the path between A and see there's this this node here, and indeed three belongs to it. user avatar   Okay. user avatar   And it's true here for all parents of clicks. Right. So, for example, there's  Let's see another one which is interesting. This for here in this for there, and indeed for appears here.  So let me give you an example.  Which would not satisfy the running intersection property.  Yeah. So, for example,  Let's say if instead  I would connect  As it. This is not  Let's say I put a concert this edge instead between A and C and the super set is only three  And let's say that my  That the tree. I'm considering instead would be. Oops.  It said this would be these two edge. Okay, so I remove this one.  Okay, so let's say this is also a tree over clicks. And now if I look at the there's two here and there's two  But the path between these two nodes of this click here, which doesn't have to. So this doesn't satisfy the running intercession property. So that's a different tree. Another click tree. I could have built  On my nodes, but it doesn't satisfy the running industrial property. And so when you have a click tree which satisfy the running a decision property you call this junction tree.  As it  Is a junction user avatar   Tree. user avatar   Right, so somebody is asking to write down the running intersection property.  So the renovation property basically says that  If  Jay belongs to see one intersections see to  Then Jay belongs to say Jay for all  CJ along the path from see one to see two okay so i mean i i'm a bit quickly writing it now.  Because the notation is a bit abuse here. But the point is that if there's a node which belongs to two clicks, then it has to belong in all the clicks along the path between those two nodes in the tree in a clique tree.  Okay.  This answer your question.  Simon or civil  Assignment, or is it simple.  Okay.  All the calls you see man like everybody calls me now. Sorry. Call you Simon are  All right, so somebody say, oh, well, is there a trick to build junction tree from the graph. Yes, there is. So how do you build a junction tree. So to build  A junction tree.  On a truncated graph.  So the first thing you do is you use the maximum  Weight  Spanning  tree algorithm.  On the graph.  Where the size of the separator a set set or the weeds with size of separator.  Set  As the weight on the edges.  Okay, let me  Kind of like explain this. So here I have my for kicks the click graph is basically a graph with edges. So the nodes or clicks now and there's an edge between  Two clicks. When the share some notes together. Right. And so for example here. A and B, the share than those three and threw together. So they're adjusting. So there's an edge between them.  And the edge and actually this edge is this one and then the weight on the edge will be the number of nodes and comments. So here it would be a way to have to because it has two nodes and comment.  And A is also adjustment to see because the share three together. So that's another edge that I could have in my key graph. And here, in this case, the weight would only be one because it has  On the one edge and comments. So if I complete the key graph.  So there's also the only missing edge here so Baeza just into C and B is adjustment to D.  With a weight of one because there's only the note for. Okay, so this is the superset. Okay. And so what I've just dropped here. So, all these edges that I've drawn  This is the key graph. So it's not necessarily a tree anymore because you have cycles and everything. And now I want to find a spanning tree I eat a tree which connects all the nodes.  And I want the one which maximize the sum of the weights of the edges in a tree.  In this case, the weights are the number of nodes and comments. I'm just want to find a way. I want to find a tree.  Spanning tree which maximize the number of nodes in common. Right. And so all these nodes, all these edges have two nodes in common. These two have only one. And so the  The maximum weight spanning tree is actually the one I've drawn. So this was the maximum weight spanning tree in this graph. And actually, it's the only one so unique solution. And that's actually interesting. The, the only junction tree.  The only spanning tree on the click graph which satisfy the junction. They're running intersection property. Okay, that's it. That's how you can construct a junction tree on the transitive graph, okay. And it turns out there's a theorem which says that there exists a junction tree.  If and only if the graph.  There exists a junction tree if and only if the train good at the graph is trained related  Which is also known as the compostable  On the possible graph is the same thing as being transmitted infection.  So if the graph is not regulated. There is no way you can construct a junction tree on the structure, you'll have some problems because there's some kinks, which will be missing.  And  You can always get a graph to be translated by adding some edges by running graffiti meaning right so  Okay, so here we're making a lot of interesting graph theoretic eliminate  In the Nate.  So we're  So we're making another interesting relationships between graph properties and graphical model and then Jacob is asking that when you build a junction tree.  Much oh yeah so so basically the missing part is that when you run the maximum weight spanning tree algorithm on the graph. And the graph strangulated this will actually imply that  The  Has the running intersection property.  By using the fact that it's a maximum weight spanning tree with the way defined this way you can show that it has the running intersection property, meaning that if  There's a edge, there's a path between two nodes which share a node where it doesn't belong in some of the clique, you can show that there is a difference spanning tree which has higher total weight so that you can argue by contradiction that it has satisfied that running intersection  Okay, so  So I told you how to construct a junction tree. Now, why do we care about the junction tree. Well, once you have a junction three  You can show the following property when you have a junction tree.  One can show  That the joint over all your variables can be written as the product over your clicks the marginal over the clique divided by the product over disappear eaters. The marginal on the separator.  Okay, where these  Are the separator sets see Ray sets.  In  The junction three. Okay, so that's, I told you the cigarettes. That would be important. That's where they come from.  An exercise to the reader, you can apply this on a simple tree and define the separator set and you will get back the exact same formulation as this.  As this one here. The thing is, you can cancel. Some of these and then you're left only with one node for the separator. Second, so you just need to choose where you will do the cancellation. But basically this kind of form.  Really looks similar.  To this one when the kickoff size through and the separator set in this case is on the size one, right.  OK, so now basically we can show that the germ can always be written this way. And so what is the junction tree algorithm.  The high level idea hoops. So the junction tree.  Hoops it's the junction tree.  I like  The Jungian tree algorithm.  The idea is you actually reconstruct the above formulation  Because if you have the factorization above you have the correct marginal right peel back see  What. So how do you reconstruct the the above situation you start  By starting with  The joint  Is a court because it's a huge GM. It's one of our Z product over see click potentials and then you can divide by some potentials over the separator sets and you only define this separator potential  As one just constant at initialization. user avatar   Okay. user avatar   So you start with that.  And then what you do is you actually do updates on the click trees. You basically do message passing  Message passing  On the junction tree to update these potentials  Like if  They were correct marginals, how you could compute the  The new marginals of the of the neighbors. So you would have you basically get a new potential. And you also get a new separator potential  Using these message passing I'll read them the update. I won't give it to you.  So that is asking is, say s access equals one. It's actually find this case so FI S access equal to one. There's no so so  This is five, the society so so so these were not in the original formulation of the GM. So this is just the standard joint and then I introduced these by just saying there one at the beginning. But while I'll run the algorithm. I will update them and then they won't be one anymore. Right.  So you update these these these these potential by running marginalization and the neighbors on the on the graph. And the click tree. Sorry. And it turns out that at convergence.  At the end  What you get is  So at the end you'll get that, what is the potential new will be the actual marginals, and the potential on the separate set will become the marginal on the user avatar   Super others. user avatar   Okay, so that's the high level idea.  But for the details, see the the pointers. I put on a website.  So is there any question about the high level idea for the junction Thiago  And so the the junction tree structure is to identify the correct supporters right that's the, that's the only place where the  The junction tree structure is is coming from.  And then in the algorithm itself also these updates will use the neighbors in the in the tree to actually  Complete things properly. So that's also the click tree will be important.  And if  You don't have a junction tree with the running into certain property, then this the composition is not valid. Right. So this is only valid when these separator sets are coming from a junction  Okay, so if there is no question about the junction tree algorithms.  I will now move to the hmm  Okay so sad Simone is asking, in which scenarios would you use this algorithm.  So this scenario. So, these, these message passing  Will basically margin. So when you do the the message passing. You basically do the summation over X on the separate or set of like  Okay. Well, anyway, that's fine. So, you will have summation over access, then you will have basically your side see of X see in this kind of stuff. So, so if the separator set are really, really big you'll have these huge exponential summation right so you so as long as the maximal click  When you triangulate in your graph is small, then the complexity of this algorithm is just exponential in the biggest key right and so  So the junction tree algorithm is just, again, a system that's ization of the graph. We made our rhythm. And in this case, you can think that you got a good order by building your junction tree and then using the tree, kind of like structure to make sure you don't create big clicks.  But again, there's multiple ways to triangulate a graph.  And I said, you can get the triangulation by you're running rapidly at once. So some sense you know it's a  It's a chicken and egg problem, right. So you could have just run grapheme you need to do the in France, but the junction tree algorithm gives you all the marginals right in this case you get  All the marginals for all clicks. So that's why it's the analog of some product. So the dictionary algorithm is to do exactly in France in a gruff underground graphic model and get all the clicks.  marginals efficiently using caching of these messages. And so basically these becomes the new messages and  It will be efficient, only if the biggest clique is small. I mean, if the biggest clique is, you know, size 100 well then you need to do something special and hundreds of that will be efficient.  And the other question.  Okay, so for the describe the people who were describing this is where it ends for the describing of last class in this class. Now we'll start the hmm, which actually I also already have scribe notes.  So let's talk about implementing the some product Origanum on for the hidden Markov model.  And that's what you'll do in the assignment for  It then mark of user avatar   Model. user avatar   Alright so I'm reminded of what's the structure. So it's a directive graphical model where you have these latent variable whichever dependency between them over time.  Is it three  Blah, blah, blah. Up to season T and then I have my observation x one.  X two.  X three  And then  60  And so, said it for now we'll be discreet  Random variables. So say things that you want up to. Okay, so I have k state.  And x day XT could be either continuous  So, for example,  In speech recognition XT could be the sound.  Recording so speech signal.  This case, the speed the z's are the four names.  And it could also be discreet in other applications. So for example in conditional biology. It could be the DNA sequence.  And later when we talk about Goshen.  Random network so graphical models where the variables are gushing distributed, we will use ZTE which is a Gaussian  And what we get with the hmm structure is something called a cabin filter, which is basically one of the simplest model on time series model for us for state space model.  So, Zeke would represent the position of an airplane and X would be the observation on your radar and then you want to track where the airplane is right for the object and then you can use a common filter for that.  And as I mentioned,  When we talked about the Gaussian Mixture Model. So the hmm can be seen as a generalization of the mixture model.  Alright, so  In the assignment three you work with the Gaussian Mixture Model, where you had this plate structure, you had the latent variable. Then you have the observation extinct. And now, what you do is you add dependence in time on the latent variable.  And so instead, what you get is now these Z's cities which depends on each other.  So you can still think of it as a mixture model, but now there's some time dependence on the mixture component  And actually, that's what you will do an assignment for whereas Simon for will be the exact same data that you use for assignment three, but instead of using a Gaussian Mixture Model, you actually use hmm model with Yashin observation.  Model and try to fit the data.  Okay, so that's the hmm. So now let's write it as a directed graphical model in the formulation. So I have that the joint on my observed variables and my latent variables.  Is factoring as a bunch of conditional which respect the graph. So I have the marginal on that one, which is a  As no parent, then I have the product over time.  Up to t of the observation. So x t givens at T  Then I have the product over T of my  Transition probabilities between my latent variable.  And so in terminology  These will be called the emission  Probability.  I could probably t given a latent variable. What's the priority over your observer variable. So, for example, given the position of my plane was the priority of my readings on my radar or  Given that I have a coding region in my, in my gene was what's the property of observational specific code Don's, for example.  So that's different models and this is called the transition property because it's modeling how the cluster component, the mixture come the yeah the discrete component various over time.  And  Often, not always, but often we will have model where these emission and transition policy don't depend on time.  So often the emission  And transition  Probability.  Or where we say homogeneous environment.  That's just basically means don't depend on time.  I he  Do not user avatar   Depend user avatar   On time.  And so in this case, that means that the conditional. And if I put a subscription fee. Just to highlight that it could depend on time of x t givens a tee. This is the mission property. We'll just call it some function of X t given ZTE where dysfunction something one  And  We'll revisit represent the  Transition probability  That t equals i. Given that t minus one equals j, because this is the street, we'll just call this a j where A is a matrix which collects all these numbers together.  And so a some sense it's a matrix and it tells me, what's the priority of going from a state j to a state i. And we organize it that each column.  Basic in this case gives you. What's the conditional probably to you over I, given that I started in Jay and so the sum over. So this could be, for example, some I so this, this is a distribution.  Over that  It's attached to some to one, right. So, summation over i have a i j is equal to one for all three.  And this is called a stochastic matrix.  Because its present  A transition probability  I each of its column sums to one.  Note, be aware that in some books or in some places you use at transpose of A. Instead, you can organize the conditional from in rows and set of columns, then you just use it transports. I like this order because then marginalization.  To predict what's happening in the past. I do a victor. I do a matrix vector product. Whereas if you use a transpose, you need to do row matrix product which is a bit annoying. So you work with rose instead of columns and MATLAB and scientific Python were used to work with colors.  Star Trek is asking that. Was there some misery theoretic issue and standard the GM new GM for continuous case, which is why we stuck to completely destroy this case. Yeah, it was to simplify a lot of things because  When you start to talk about continuous distribution we talk about densities. You talk about what's the bays measure and you need to be careful. Now when you start to condition because conditioning.  In continuous world is is really weird object and and and really want to go into all these rigorous details now.  For a lot of specific case. So, for example, now I can see the emission probabilities are dashing in this structure. It's totally fine. I can do that.  What it means is that this is actually density, let's say x with respect to the back measure and that's fine. And these would just be PMS, as usual, and so that's, that's fine. Right. I can do that. These are kind of these weird hybrid the distribution  Okay, so I set up a bit of meditation. So let's say I have a transition matrix which are present my transition, probably to be discreet space and I have my emission probably T which could be a density or PM. If it doesn't really matter, ie  This thing here could be a PDF or a PDF depending on what's x, but x will be fixed anyway and observed in these things. So it doesn't really matter what this is. And now we will talk about how to compute some properties. Right. So we want to do some influence in this model.  So we first will talk about how to do in France, assuming we know the parameters in the model. And then in the next lecture, we will do maximum likelihood in hmm model, which means we'll have to use em because it's a latent variable model.  And then so will will derive what are the maximum likelihood updates in the hmm model to learn the patterns. And that's actually what you'll do you'll implement the assignment for  So what are the infant stats that we could care about. So things and a bit of getting a bit of terminology from signal processing. So this prediction I could see, I could try to complete the polity  Over my Layton variable, given my current observation which are in the past. Right, so I observe  Up to t minus one. And then I want to know, well, where will the plane be at time t. Right. So this is call prediction. Basically, I want to know where next the plane will be. There's this thing called filtering, it's, it's a very signal processing.  Terminology, which is computing the property of mallet and state given observation up to the same time step.  And this is basically asking. Okay, we're now. So where is my plane now.  Given my observations. And then there's this thing called smoothing  Which compute the product you over ZTE given x one up to capital T, where a capital T is bigger than little tea. So basically, here I have observation.  In the future, compared to where I want to know where the plane was so this is basically where in the past was the plane right in the past.  Was the plane.  And it's called smoothing because in terms of getting the value of a signal when you have more observations, you'll get a smoother estimate which varies less because you have more information.  Okay. And so now the plan is  And I'm a bit out of time. Let's see if I can manage to do that. So the plan is to implement the some product algorithm on this structure directly. So it's kind of like a  At the same time, it will read derive standard algorithms, called the alpha beta recursion. And then it's and at the same time, it gives you an example of how to use this product algorithm and the specific structure right hmm structure. So it's kind of a pedagogical example.  So there's this thing called the alpha recursion.  And you can derive it directly if you want and but we will derive it by using the supper. And so the idea is, we'll, we'll run some product.  To  To derive the alpha recursion in the hmm  Recursion.  To compute the polity  And so let's sure graphical model. And if you remember I derive some product on the GM formulation. And actually, in this case I will also run it as if it was a new GM, so I will just first make it as a new GM  But let's see, where does the some product, the alpha recursion come  Alright, so let's say this is my tree.  And this is x t and this is X t minus one.  I'm running.  Okay, so and so this is z t minus one and this is et  And so if I was to run the  Some product algorithm.  I would compute a message going from Z t minus one.  To ZTE and this is a function of ZTE  And I will compute a message going from x t to ZTE as. So as a function of city. Right. And if I want to compute the marginal overs et I would have that the marginal adversity is the product of those two messages, times the potential overs it user avatar   Right. user avatar   And so to compute  The filtering distribution. So, P of ZTE and the evidence from one up to tea.  This is proportional  To the conditional of ZTE given x one up to team.  And I use the bar notation here because the extra observed. Right. And so if I want to compute the filtering distribution front of compute this, this was the filtering distribution. I can instead compute this. And then we normalize right  And so now let's use the some product to compute this marginal  I said that the marginal on ZTE and x one up to t bar observed  This is proportion, it surely it would be  One over a z.  The potential  Which is one the note potential, which is one times the message going from Z t minus one to ZTE as a function of ZTE and the message going from x t Tuesday t  As a function of city. Okay, so I have first made my hmm as a huge GM by just saying that if I go back here so  This arm. This is my potential on ZTE and Zed Zed t minus one. This is my edge potential. I don't have any node potential in this case because these are all like edge potential. So I have a potential here potential there. It's just wretched wretched  And then z is just equal to one, right, because it's a game for me.  So these are the potential that using my message of date.  And so from that I know by the  The trick of the message passing algorithm that to get my marginal I just take the product at the last note. Okay.  So that because of the way I wrote my new GM we have that z is equal to one, right, because there was no no margin constant. Now let's compute the messages the message going from x t to ZTE as a function of ZTE well it's a fake message, right, because this is summation over at  The edge potential, which is observation of St givens et  Sorry, the property of expediency team. And then I had this credit card dub, dub potential because st has observed that, so this is a function of x, TI and XT bar. And so this is just p of x t  Bar given city.  So that's this message.  And now the other message, which is the interesting one. So I have the message going from Z t minus one.  To ZTE as a function of ZTE  This is the marginalization over z t minus one.  Of the edge the edge potential between them, which is a transition polity so if ZTE given city minus one.  And then I have the other product of messages so message.  Z T  Minus two going to see t minus one which is depending on z t minus one.  And the message.  Going from X t minus one to z t minus one, right. So these cycle here. I had a message coming like this message coming like that. And these are the messages I use to compute this message right  You were all the neighboring message and this depends on z t minus one.  And now if I call  This thing.  This joint here I would call it alpha t of the tea.  You notice that it's just a product of these two messages.  Right, because this is one, this is one  And so these two messages here. Oops.  And I'm almost done. And so these two messages here.  Product together is the same thing as alpha but i t minus one. user avatar   As I forgot the invisible ink. user avatar   And so user avatar   What we do user avatar   And so user avatar   These two messages together is just probably T of Zed t minus one and observation from one two t minus one. So,  If I don't care about the rest of the graph. So that's the magic of the the  The DGA I can remove the leaves and then I get just a smaller the GM and then I can still transfer me that as a GM. So I notice that these two messages in the in the smaller graph.  Is just basically the marginal on smaller things. And this is what well this is alpha t minus one of zero t minus one.  So the alpha recursion. Notice that you can just compute this alpha messages in the forward fashion is a recursive to compute that. Right. And so the alpha recursion just tells you that  I'll fatty of the tea can be computed by just taking the emission probably tea.  Given the tea and then something over t minus one.  The probably to transition  The t minus one times the previous alpha messages.  So that's his call the alpha recursion.  This called the alpha recursion.  Also known as the forward.  Recursion.  And it corresponds to the collect phase.  In a in the some product or window.  Where I start that the leaf and then go to the  In some product.  Okay. And I think now we're way out of time, so I will continue covering this in more details at next class, but basically the idea is  The alpha message. So this thing is the probability of that tea. It's the marginal of the tea and the observation up to and in order to compute that I can start with alpha zero of zero, which is basically  Just the  There's no alpha to compute it, you can just compete it from its emission probably t here.  And then you keep perpetuating forward these messages to compute up to the last time step. And that's where you can get the filtering distribution by re normalizing  Okay, so is there any question about that.  So I'll revisit this piece at the beginning of next class and talk about the time complexity of the algorithm and  How to implement this in a numerically stable way because what happens is these properties goes to  Will do under flow very fast.  Because you want to play another number of smiling one  And this is what you'll implement in the assignment. Cool. Alright, so see you on Friday.
  Okay. So today we'll finish.  The message passing in hmm, and then we will see how to do maximum likelihood in the HMM. SO WE'LL USE em for hmm  And that will be all for hidden Markov models and then we will move to information theory.  And the kale divergence talk about its properties. And if we have time, we might start to talk about the maximum entropy principle.  That's the plan. So let's go back to the end of last class. So if I go back. Let's see, to do  Alright, so I finished by applying the some product or rhythm on the hmm and I derive the offer recursion. Right. So what happened is, if I want to compute the marginal overs, et  And including all the things I'm conditioning on, right, so that was the issue. Remember the trick I did for some product is  Conditioning on variables you just fix them to values and then you run miss the message passing as usual.  Still trying to marginalize these variable. But because they're fixed is just does it. Just keep them fixed  And so basically, when we compute the marginal anxiety and everything, which has been fixed. This is a  We, what we do is we take all the messages coming in the know that we've picked a product of that, right, so that was. That's why I said that the  The this distribution over is empty and all the observed variable which are conditioned on is the product of the messages coming to this node.  Normally there would be a normalization constant in an underrated graphical model, but because this is a D GM that we just put it in a huge em form z is equal to one, because it was already normalized, and then  This one I think was coming from.  What was it coming from  Yeah, there was no incoming messages there, I guess.  Because  Because I have the incoming messages and then I have the know potential, but there's no no potential. So I guess, perhaps one is the note potential there was no no potential for this one.  Okay. And so I said, I will define this distribution here as the alpha t  Message and, in particular, what else it is is just a product of these incoming messages to a node right and so at any tea, if I looked at the product of these messages coming like this.  Sorry, this is the alpha message. And when I complete the message I use the simple some product rule. And so the message which goes from  Zero t minus one to city. It's the marginalization of the I take all the potentials, which has ZTE in it. Right. And so I get this transition probability  And then I have all the other messages which were from the neighbors. Right. So basically,  To compute this message I first take the product of these other messages coming to this node I add the potential, which was on this edge, which is a transition quality and then I marginalizing out z t minus one, right, so that was the  That's just a general rule for some product message computation. But here, applied in this specific setting. And then I noticed that I have this product here.  Of message to the previous board previous node which is just the alpha t minus one formulation. Right. And so that's why we can summarize to compute the alpha t potential or productive messages which is this property here, you need to  Compute the previous message. Sorry. The premise of it.  And then marginalize out respect to the t minus one by multiplying by the tradition and then also point wise, multiply by the observation quality, this is, this is basically the the message coming from the observation, right. So this, this is this part.  Okay and this is called the alpha recursion, because it's a recursion to compute this, I need to compute this.  And I said, it's, it's also called the forward recursion, because basically you you could start at  This note here with which has no  Into you GM style which would have no incoming messages from here, it would just have this message. And that's it.  And that's how you initialize and then you just get forward up to the end and then you get the final policy that you want, which is  If you read normalize this over a Z, you get the conditional of ZTE given observation and this is the filtering distribution. Right. That was one of the, of the  Of the quantities, we want to compute in a. Hmm, okay.  Papa.  And I said, this is like the collect phase in some product algorithm where you collect all the stuff from the leaves to the root.  And then now we will talk about the distribution phase which is going back in the other direction and this is called the beta recursion.  Sorry.  But before going there. Let me  Highlight the computational complexity of this and how to implement it. So perhaps what I'll do. Let me try to do something fancy. Can I copy this. user avatar   And then I do copy user avatar   I guess I have a keyboard here copy. And let's see if it works here. Can I do paste  Maggi it works. user avatar   Okay. user avatar   And here is this user avatar   Oh, no. It's a picture. I don't want it to picture I wanted to as text. user avatar   Hey, we go user avatar   Okay, so I just copied the offer recursion.  Let me erase these things. So let's continue the alpha recursion.  And so, how to implement it. And so basically  You can think of this  As a message that you store. And this is a vector, right. It's a function of ZTE minus one, that's easy to minus one as K possible values. So I just need to specify the value of this for k  Possible values. So it's a vector. This thing is the transition matrix, right. So this is matrix. And so this summation here is just the matrix vector product.  And this is another vector user avatar   Okay. user avatar   And so in terms of implementation. If you let as it  There's another vector  As a function of z t  Minus one.  Temporary  Inc said vector as a function of ZTE and this is a vector as a function of the t minus one. And so if you let it to be a as a function of ZTE  To be just the emission quality of the teeth observation given CT  Then you can write the alpha t message.  As taking ot and then doing point wise notification with the result of taking the summation, which is A times alpha t minus one.  And so  And here, this dot notation just means the had Mr.  That's weird.  That's true. I just went crazy like this. So, this is the had Mr matrix product or vector product, which just means point wise but deprecation  And so  And so let's say you want to run the offer recursion in a computer, you would first do the initialization of your  Alpha one message. So basically, alpha one as a function of z one, as I mentioned, while by definition is just the property of z one and then the first observation.  And so this is there's no message coming from before z one and  You just need to introduce the message coming from the absorption and probably tea. And so this is the observation probably T X one bar givens you one times the prior right the that's the little potential. So p AMP Z one  Okay. And so this is actually also just by the definition of the commercial property, you could have derived that but that's how you can get the first alpha message and then you keep propagating forward by just  Taking the the the marginalization using the transition matrix and then doing a point twice vector with the evidence at time t.  And you don't need to store the previous message if all you care is the filtering distribution, you can just forget about alpha one, when you compete offer to etc etc. So the, the, the space complexity of this algorithm. user avatar   Whoops. user avatar   space complexity. user avatar   No. user avatar   Temporary ink again. So the space complexity.  Is order key extra storage.  So I say extra because, you know, just a transition matrix is order k square. So, if you want to store a you also need a square, but to run the the filtering distribution. The filtering algorithm. The alpha recursion. You only need to store the alpha  One outside the time so it's order cake and time complexity.  It's order case square at every iteration, because you do a victor. Maybe you do a matrix vector product and you do that tea time so it's order T case square  And if you want to compute the filtering distribution, you can just normalize your alpha message. I'll use the tail notation, because that it will come back later. The normal the normal normalize  Alpha message. And so this is just the property is empty, I guess, as a function is a T.  Certain normalize alpha message is the  Property of SETI, given all the observation.  Up two step t. And so this is the filtering distribution.  And this is just  Alpha tea. The tea divided by summation over the tea prime alpha tea. The tea time. And so that's the normalization.  So all right, so, so, so some people asking about  What is a  By definition, I said, A was the the transition priority matrix. So, AJ is the probability of Zed at t equals i. Given that t minus one is equal to g  OK, so if I come from Jay, I have a distribution over all the possible next state. So the whole column of A represent what's the distribution over that and it seems to one and we said that  Usually doesn't depend on time so we could also make a depends on time and then we just use it at here if you want  Good put it there, if it varies with time, but we will look at the homogeneous setting where the transition policy doesn't depend on good  So hopefully this answers your questions.  And K is the dimension of the Z variables yet. So, just so that we're all on the same page. Let's go back to the previous class were introduced to the station.  So I said,  This is the distribution for the joint in a hmm I have the property of Zed one, the product of the emission probably tea and then the product of the transition probably tea.  And I said, suppose that the emissions or religious in time and the transition property as well. And then we just have that the emission probably t is just  Some density or PMS on XD givens et and was some See for example, like we could use a Gaussian for that like it's kind of a fancy Gaussian Mixture Model, where this dependence between the component and then this transition polity is given by a matrix.  And  We said somewhere that ZTE was discrete over casings.  Okay.  And notice that this thing.  This normalization factor here.  This is equal to summation over a ZTE prime p of ZTE prime XD  Whoops. It's x one up to t.  So the normalization of my alpha message is just a margin all over my observation, right. This is just the quality of my observation up to see. So this is the evidence probably t  It's called the evidence priority because that's the thing you observe is called the evidence and here we're just competing wasn't probably to have this evidence  And when you do maximum likelihood you want to find a parameter in your model which maximize the log of this property David is probably the right so when we do. Yeah, we will basically doing  The a matrix as an odd resemblance to an unhealthy union operator in  Quantum Mechanics.  Yes, so the link here is that when you have time evolving system usually you'll have this this operator, which tells you how the states in your system evolved over time.  And there's a question of discrete time modeling or continuous time a link here. We're doing discrete I'm modeling and we just say, Well, the way you get the name, the next time step is just  With this fixed matrix A, but you could also have you wanted a more general model where you have continuous time or period or and then use basically the matrix exponential to model that but that's that's a bit outside the scope of this class.  But because usually physics systems are more coven you suppose that the future is conditionally independent of the past. So, these, these Markoff structure like in hmm is  Also using kind of physical systems of spice. There's some resemblance, I guess.  Okay, so let's talk about now, the better recursion.  The better recursion.  So this is to compute now the message in the other direction.  And this has to do smoothing right smoothing, if you remember I mentioned last time, it's to compute the quality of my my current have some  latent variable ZTE but given the observation, including the future. Okay.  And so the little cartoon is I would have  My little verbal is ET and then I have ZTE plus one, blah blah blah, up to the last one. And I want to include all the observation in the future.  X t plus one.  Is I'll use these values.  And and i also want to include everything from the best right and now the point is that in what we did last time.  Like that the message which goes from the left is this alpha TCT just a product of the to message. Actually, the evidence at ZTE and all the stuff in the past.  And this gives me the probability of ZTE given x one up to t bar, right. So this is the message coming from the left to my nodes, et  And if you remember to compute the marginal attendee know dinner inner in a tree using some product. I need to take all the incoming message to this node as well as the potential  And so now when we're missing here is the incoming message from the right for computing them the marginal NCT taking in consideration the evidence from the future. Okay, so this is the message going from said t plus one two is that t and it depends on it.  And we'll define this as will just say this message is just BT Betty. It's the better team message.  Okay, so basically we'll have a message called betta T coming from the right, and then the marginal the smoothing property on that is, it will just be the product of those two  In particular, we have that the probability over city and all the observation up to capital T.  And where capital T is bigger than small t  This is as before, one over z but Z's just one  And then I have alpha t of the tea.  And then I have  The message coming from the future.  And I said, I define this as to be better. user avatar   Okay. user avatar   So now we will end the end. That's how we get  The better recursion by completing this message using the some product updates. So let's do the some product update rules. So the message from the t plus one to the t as a function of ZTE yes I'm using Zoom's and that's kind of cute.  Make your choices to be both the British and American  So you need to marginalize out over Zed t plus one.  Potential there's an edge potentials in this case is the transition probably ti  P of t plus one givens, and T and then I have the observation quality. So that's one of the message, right. So basically, all I'm saying here is to compute  This message here I need to take the product of these messages as well as the edge potential. And so this message here is just the evidence properties. This is probably T of x t plus one bar given city. The city plus one. No, I  Probably have 60 plus one bar givens. It took us one  And then I have the message coming to the t plus one which is a message from ZTE plus two to the T plus one as a function of the t plus one and we said this is just  This is just beta t plus one, right.  And so we have kind of analog to the alpha recursion. We have the better recursion, which says that better T of ZTE is just marginalization over as a t plus one of the transition priority.  Times the observation priority at t plus one.  And then time the incoming message be t plus one of ZTE customer  So this is called  The better recursion.  Or, also known as the backward recursion. Because you go backward in time when you compute  And the role. So the, the role of the alpha recursion is to collect all the information from the past to make a prediction about the present.  And then the better recursion is to collect all the information from the future to help compute the probability of the present, given all the, the, the evidence which include also that stuff from the future.  And  Alpha i said was just the probability. I told you that the alpha was just a quality of 70 and all the observation up to 20 does better as also a probably stick interpretation. Yes. Okay. Turns out that  Whoops. It turns out  That  Better T of ZTE is actually the priority of all the future observation.  From one. It's from t plus one. user avatar   Up to capital T. user avatar   Given as a tea.  Okay, so that's the interpretation and how can you prove that. Well, you can just use a bit of prognostic  Manipulation. So we have that the alpha t message, which is another smoothing distribution which is empty proportional to said tea and all the observation capital T.  Well, this is  By the product rule. This is just the probability of everything, given  T times the quality of the tea. Yes. I haven't done anything there. And now I use the conditional independence in my market model. So because the future is conditional dependent of the past given city. So I will have that this is probably T of x bar t plus one up to capital T.  Given that T  Times probability of x t  Well, everything else, which is one up to t.  givens and tea and then they had p  So, so the this split here of this in this product is just from the conditional independence.  OK, and now I recognize that this product here is the joint between 70 and X bar one of two t. So, this is alpha t of city. user avatar   Okay. user avatar   And we just define here.  That p  The full smoothing property.  This thing was the product of alpha t and Betty.  And so I have here to smoothing. Probably it is the product of alpha to end something. So there's something is better tea. Right.  So this thing, this implies that this thing is better tea. user avatar   Okay. user avatar   Bunch of questions.  90. And so the question about linear systems to process systems. That's a bit outside the scope of this is I would need to think more about it. I'm happy to discuss it in office hours if you want  Darius asking what a real world application in this model I you went to actually, you know, the future. So the idea there is you have a time series.  Of observation and you're trying to make prediction about stuff, which happened during the time series, right. So, so let's see. I've observed  So I have a radar. I have observed the the MIT I made measurement on my radar for like 100 seconds. And I want to know where was the plane at  At 50 seconds. So I want to make inference about the past and well then you should also use the future observation to kind of reduce the noise of where the plane should be. So that's the idea.  I know that will. Another one would be coming in signal processing is that when you have the whole signal, you can actually reduce the noise in the prediction about things in the past. And that's, that's also why it's called smoothing  Because when you use more observation you read we reduce the variation. The noise in your prediction. And so it's smooth as it smoothing the signal.  Look, I guess. Sumo is giving a another practical example using music generation.  Alright, so  This is how we compute the, the better messages. Now we need to initialize it at some point. And then what happened is, if I look from this graph.  I told you that the so the better message are coming like that intensity and so in the last step, the first better where there's there's no more observation to include  Well, basically, there's nothing coming here, right. So. So the message is just a constant one. And so that's how you initialize things. And actually, you can also see it here. If you say that the  Probability of like nothing, given that t, which is basically the, the better capital T is just one, right, because the quality of nothing is always one.  Guess it's a bit of a formal situation, but so the initial ization of the recursion.  Is to let better capital T, have said capital T to just be equal to one for all Zed capital T.  And finally,  So these are for the known marginals, now we also want to compute in some product we could want to compute the edge marginal. So the marginal two variables which are beside each other and this will come into play in in em. So let's, let's write it down so  The edge marginal computation. How do we do that, that we make a little drawing. So I want to compute the marginal on both said t and t plus one.  Okay, and so I would have stuff coming from the observation.  I have the observation at time t plus one.  And so what happens is that all the information  In this blue box is coming from the alpha t message at ZTE  And the message from the future is coming in this betta t plus one of zero t plus one message. So these are the message coming from outside.  So to get the marginal and two nodes, I take all the incoming message from outside and all the potentials on these notes right and so here I have that the probability  Here. So the probability of both ZTE and Zed t plus one.  And all the observation from one to capital T. So this is a smoothing edge marginal. This is equal to the product of the incoming messages I have alpha t of the tea.  I have better t plus one of zero t plus one.  And then I have the other potential. So there's two things missing. There's the transition polity that t plus one givens a tea.  And there's the observation probably T AT ST, press one. So the aversion probably tea at XD is already included in the office. I don't need to include it. But this one here is not included. So I need to include it. So I will have  Probably to have  X t plus one, given that t plus one.  Okay. And so that's just the way you compute and there will be one of these equal to one. So that's how you compute the  Edge marginal as well for any claims that  Is there any question about the alpha and the beta recursion.  And so in order to compute the edge marginal, you need to do both alpha recursion and better recursion.  Or to complete a smoothing quality. You also need to be both the alpha and beta records which are basically the collect and distribute face in a tree under, under the graphical model for some product.  Okay, so if there's no question about this.  I would like to mention something about the numerical stability. So in the assignment for you will be implementing the hmm, which is the EM algorithm for hmm So, which will need to also come in and France.  And  If you don't do numeral. If you don't do any numerical stability trick, you'll get zeros everywhere. And then you'll take lot of zeros will get minus infinities into you have some troubles.  So the issue in these things. And actually when you when you deal with, with large high dimensional distribution. You'll usually have this problem.  Is that the alpha t message and the better team message because they're taking product of numbers, smaller than one and you multiply it like 10 times or 100 times because of the know depending if it's a long signal you get number which are small, very quickly, so you can easily get  For example, to one e to the minus hundred. Okay, which usually you will under flow, the computer because you can normally represent up to each of my 67 I think or something like that.  And so if you use Matlab or sci fi, it will just at some point if you multiply like these numbers by another small number, it will just say zero  Game. This is called an under feel error because it's not zero. It's a bit bigger than zero, but you can have to present it because of just the the  The storage representation and computers. And so you need to do something about that. And there are two possibilities.  And the one who will implement in the assignment is the first one because it's more general. And so the first one it's a more general approach is to instead of storing the numbers will store the lot of the numbers. So you store the log of alpha tea instead. Okay.  Because to present the log you own three in the case of emails hundred. It's basically the log will be minus 109 hundred is a number. You can easily represented a computer. user avatar   Okay. user avatar   So that solves a problem. And so now the trick is you need to be able to compute some of  The numbers when you only have their login representation. So for example, when you will do the marginalization, like the summation over is empty of your stuff you'll need to compute the summation, for example.  Over a AI. AI. AI are just numbers and I want to compute the lug of the result. And all I have access is the log of the AI is I don't have the eyes itself. Okay.  And so I'll do a bit of manipulation here to make sure that things are stable. So, I will write this as the log and I will factor out the maximum value of the eyes, so I will say a tilde time summation of i have a i divided by a tilt. So all these are supposed to be non zero  Then, actually, I guess, in this case, positive because you have to lug  So here a eyes are all strictly urine and zero.  Pop Up, up, up, up.  And I define  A tilt to be the max.  Of your numbers.  And the reason I do that is because this is the dominant number in the sun, right, the biggest number is the one. These are positive, the biggest number will dominate the some and so that's why I was factor it out to do exact computation with it because this is just log of a tilde plus  log of one plus summation over j which are not the maximize sir. So let's say also that i max is the arg max.  Over I have AI.  And then so yeah so I saw everything else. And then I have, I need to take the exponent, because I didn't know what the, the AI is I only have login VI, so I can obtain it by doing expose the exponent of lug of AI and then i divided by  Log of  By at a tilde, which is when you take the the lug it becomes minus minus log of itself.  So this is how you would implement in numerically stable way the  The log, some of the numbers when you store them as know so become a lump sum x and then the trick is the reason I do that is, suppose that AI.  Suppose that AI is much bigger sorry that it till the is much bigger than AJ for the non maximize her then.  This thing will be bigger will be much smaller than this thing. So you'll get X have a negative big number.  Expert and they get a big number is close to zero, especially compared to one. And so often, what you get is, if a field is much bigger than all the other number. This will just be roughly easier. user avatar   Okay. user avatar   Which is why it was good to kind of get the max out  Because otherwise, when you take x of stuff you could get big numbers. So that's why you you're by taking the max, because let's say instead of taking the max I  I would have taken another number factories out  Well then I could get X have a really big number, which is infinite, or if it's over an overflow. Right. So it kind of did. That's the idea.  And somebody asked if you use the same trick for betta yes you use this for anything which might under flow, you can just store it them as log and then you just have to be careful.  So that when you add the number you use this trick multiplication is not complicated because it's just subtract is just adding the lungs. So multiplication is easy because log of A times B is just luck of a plus b.  The summation is a bit more complicated. And that's what I described, so that's Option A. Option B is to normalize the message.  And this is actually simpler, but it's less general right, you need to kind of like  Do a bit of bookkeeping to because the, the value of the normalization constant might matter in your application. So you to do the bookkeeping.  And so, whereas when you do the log some x, you don't need to do i do bookkeeping, right, you're just doing the standard alpha recursion. It's just that you're clever about it.  And so in the case of the alpha recursion, for example.  The standard approach would just be to use the asset tilde message that I mentioned before, so the alpha tailed. It's just normalized version of the alpha, which is the reality of ZTE given the observation.  So the nice thing is that because it's the property over only one variable which has key values. These will be normal, there will be  No. Okay. Numbers was the problem with the outside. It was a priority over like T variables a joint or T variables. So, because there's an exponential number of the  T variables. So each number should be quite small, so that they also do one right so that's kind of what's happening. But when you only have one or two dimension. These numbers are reasonable  And so, so if I use if I store my message I use a fatty  Okay so Dora is asking, Where is my one appearing. So what I've done is, first of all, so you get login this and log of that.  And what happened is that for a tilde divided by a deal when you have IMAX you just get one, which is this part and then you do the some over the other ones which are not IMAX okay  Alright, so I was explaining how to redefine the alpha recursion. Now for the alpha tell recursion. So before where we had was that alpha t was just ot at Mr product or user avatar   Before user avatar   You had a fatty is ot Adam our product point waste product with a alpha t minus one.  Well, you can just write this as ot times A alpha tilde t minus one.  And you just read normalize that. So, summation over ZTE of the thing here.  And this gives you a fairy tale, right, because then it's normalized  And this thing that you normalize is called CT and you need to store it. If you want to compute the probability of the observation.  Which was these and and you can actually show  One can show that. So if you remember I said the normalization factor for  ZTE  When I have  All these variables here was just the quality of the evidence when you normalize the message and you only look at the normalized message, you actually don't get the property of all the variables you only get the quality of x t so you can show by analogy that CT  Which is summation of rigidity of OT that  A alpha Tilden t minus one.  As a function of the T is just the probability of X t given all the previous article  And thus, if I use the normalized alpha recursion. And I want to be able to compute the product. The evidence probably team. So I want to compute the probability of X one up to t bar or, I guess, capital T, let's say, doesn't matter.  Then by the chain rule. This is the product t equals one, up to capital T.  Of probability of x t bar, given everything before  Which is just a product of the city.  Right, I define about. So while you run the algorithm, you realize that each step. Each step you need to store the normalization factor, then you keep all of them or you can just  keep taking the product. You don't need to start them you dig the running product and then from that you can actually compute. What's the quality of the evidence, even though you've read normalize everything  And then there's the better recursion.  So you could have actually normalize also the better factor but traditionally people have considered something else because it kind of make use of the CT and I think Mike Mike's books. That's what he use. So in this case, you define better to have that T  To just be the original message.  divided by the probability of x t plus one bar given x  60 plus one to capital T.  given x one up to t.  And it turns out that this thing is the product from u equals t plus one to capital T of this cities that you had stored and so  And here, this is just probably T of x t plus one.  Up to capital t given is it  And so here, as I mentioned, this is not truly normalize you don't have that summation of of ZTE of beta t ZTE is equal to one.  But you have that both the numerator and denominator have similar scale. So that's why bit that T is actually a not a reasonable number of order one. It's not like into a mess hundred  And then as an exercise if you're curious, you could try to derive the beta tilde recursion. user avatar   From this definition. user avatar   But in the assignment, as I mentioned, you will do the lump sum except  Alright, so is there any question about the numerical stability.  No. All clear or you're totally those  Thumbs up or it's clear.  Thumbs down okay I got that sounds good.  Alright, so let's see. Do I start maximum like you would for hmm, or I take a 10 minute break your choice. You choose.  Then we break that break. Alright, perfect. So it's 224 let's start again at 234  The question. Okay, so now let's  Let's do maximum likelihood in the hmm. So right now we we know how to compute the probabilities. But what about estimating the parameters of our hmm from data. So I want to do maximum likelihood for the. Hmm. Well, as I, as I said,  Either last class or two classes a girl. You can think of it. Hmm. As a generalization of a latent variable model.  Where there is dependencies between the latent variable, right.  And so suppose that your observation models, the probability of my observation, given that my latent variable is k is some density or PM F on XT I'll use effort. It's representation which depend on the parameter of the case component. Right.  And I'll use a tacky for the parameters of the case component  And so this is basically some parametric model.  For the distribution user avatar   On user avatar   XT. And so, for example, we could say it's a Gaussian  Annex.  Right. So that would be the generalization of our mixture. Our gotcha mixture model where now there's a time dependencies between the mixture component  And so in this case at that would be the meaning the covariance of my gosh, and they would have a meaningful difference for each component so that key of them. And so at that now we were present all these parameters that we use at  K for k equals one, up to Capitol care.  And so that will be the parameters for my observation. Then I need the parameters for my transition model. So we said it was homogeneous. So we said that the probability of going into state. I went coming in state j is just represented by a stochastic matrix AJ  And finally, the last thing that we need is the prior distribution over the first component. So, said one equals i will use pie. Right. So this is the same pie that we use for the Gaussian Mixture Model.  So the difference with the Gaussian Mixture Model is we now also need to estimate the relationship between the latent variable which is by given by the transition product.  And so theta which represent all our parameters.  Theta is just all these variables. So I need to know the parameters of my absorption models, my transition probability as well as the prior distribution over it said, well,  Okay, so now the goal is, I want to estimate  To estimate  At the hat. A hat and pie hat by maximum likelihood from observation data. So, we will have  See an independent capital, an independent time series and each of these is actually a whole time series, right. So this is like x one up to ti so we could have that the length of the time series depends on i. And so that's the notation we use  And the beautiful thing here is that you could estimate very well the transition probably T and the observation properties from all the one long time series. It's just this one.  It will be hard to estimate it with only one long time series because you're basically only have one observation to estimate, in some sense, right.  And so by having any time series, you can estimate this property better but for these actually you can estimate it fine with only one time series.  Which is what you'll do in the assignment.  Alright, so this is maximum and IQ in Layton verbal model. So what do you do well. You can do. Yeah, right. So, we will use em.  And so the East step we need to compute the S iteration. So, by the way, t now is used for the time series, I will use S for the interests of the estimates.  So will compute the posterior or latent variable iteration st plus one, sorry, yet so S plus one will just be computed by looking at the posterior over z, given my observation x which is huge and my parameter at iteration as  And then the M step.  We need to make new estimate of a parameter. So data hat.  St plus one is doing a maximum likelihood problem.  Over parameter space of the expected complete like like you, I take the expectation over st plus one of the log complete magnitudes of x and z.  And x here is the combination. So I call this x is this concatenation of all the the time series. Right. So it's a huge variable and z's the equivalent for the latent variable.  So let's derive the complete like like you. Let's write this down. So the complete  Load likelihood  What does it look like what's, what's the log of the probability of all my time series and all its latent variable, given my parameter. And so as I said, these are huge verbal right so  Yeah, and this is I am in this is  So by independence over my time series. So it will be summation over by n time series.  And then I have the love of my hmm. So I have the log of the probability over the first state. user avatar   Is I hear user avatar   That's notation. user avatar   And then I have plus summation over tee.  Up to ti of the log of my observation probably t  Bar T which depends on, I  Given Zed T which depends on I as well. And then I have my transition property right. So, summation t equals true to the eye of the log of the probability of Zed t  I  Given Zed t minus one, I  So that's my full complete likelihood. If I knew disease. And so now I will use the usual trick of writing my disease with one hot encoding. So this thing here is the same thing as something over King  Of Zed one key I lug of pie cake right pie is my the parameters for my prior  This thing here.  Is same thing summation over k of Zed T ki I  Log of my model for emission property which I said was f f of x t  I  Given attacking right so so Zed. Tell me which component I use and then I use the parameter of this component which is given by at that. So it's at that key.  And finally, here I have  I want to get the right trip.  Entry into a matrix, so I can sum over L. M. M.  And then I can take the product of these indicator variables. So it's Zed te l i  Know, I guess I LM about. So let's say okay I get confused. So I want to put a LM so that means I go from em to  Me. Alright, so, yeah. So at time t, it's T it's L and then at time t minus one, it's m user avatar   Okay. user avatar   It's only 60. This is an n. So, when it's when I have observed say l is, I don't know. Jay and m is a k, then this will just get the a JK entry.  So that's the usual trick. And why do we do that well because now when we take expectation over z.  Everything else is a constant, right, so, so, so, so this is a constant all the parameters part is a constant. So I'm only going to take the expectation of the Z variables which give men, which gives me probably teeth right when it's indicator variable, the expectation of a  An indicator variable. Just give me the probabilities. And I guess I forgot to use my invisible ink.  Can so  So, user avatar   And cancel. user avatar   All right, so now when I take the expectation, with respect to que es plus one of my lug of my whole complete look like here.  I will get basically a bunch of stuff, which I want free right  But the important part is that  By the narrative of the expectation, I will have to compute the expectation of the ZTE variable so expectation over que se plus one of z at K AI.  And so because it's an indicator variable, this is just what que es  So it's que es plus one of  Zed t k i is equal to one.  And we'll use again these Tao self counts. So I would define this as Tao of T. K. Ay,  Ay, that represents the probability that the latent variable for the ice time series at time t as the state key. Okay, which you can compute by running this the alpha beta recursion to compute the smoothing probably to where the latent variable, right.  So this is the smoothing distribution.  Groups, because we said that the queues were the distribution over the latent variable, given my observation. user avatar   Deck. user avatar   And here it's just a margin all on ZTE gay  And so you get  The smoothing  Distribution.  Probably to have Zed t i.  Guess.  Said TK i equals one, given all my observation for this time series up to ti right  And it also depends on parameter theta at step s  Okay, so you use. So that means here you need alpha beta recursion to compute this.  And so, and by the way. So it's the same thing for for user avatar   Let's see. user avatar   You also need to compute  The expectation of these it's. It will be just how one K. I. Right. So there's different  Computation. And then we also need to compute  The marginal over the the transition quality variables. And so this is a pair of variables, that's why. Now you need to compute the edge marginal right so this thing here will give you  This will be que es plus one of the property that te l i is equal to one and Zed at t minus one and i is equal to one.  Which is the marginal  Of Zed te l i n  Equals one Zed t minus one i n  Equals one given all my observation. So one up to tea. I  And it's depends on I and it also depends on my trend parameter  And will define this as another stuff counts because they will appear in my ML maximum like in my m step. So this will be, by definition, the stuff counts for  Time step T. When I go from  To state L from state em and it depends on, I so I have one for each time series. Okay. And these represent basically the smoothing  Edge marginal  Which basically look at was the probably t  Given your observation of going from state to state. Hell,  And you can compute by the alpha beta recursion. I gave the  formula here, right. So this is how you compute the smoothing edge marginal you take the alpha message, the better message and then you add a bunch of stuff.  Okay, so this is how to get like a nice beautiful shape for the complete look like good by using alpha beta recursion.  Okay, so, Ezekiel is saying, is it possible to formulate this EMR preliminary and entirely online formulation instead of using smoothing  Not really, because the point is when you do. Yes, you do. Maximum like you're given the whole training set. So if you do stochastic em.  This is batch em in some senses. I look at my all my training set all my training observations all my time series sequence and they update my parameters.  And so if you have very long time series said, say for example, like you're you're you're thinking of a document have a billion word to be just one long time series.  It will take forever to update your parameters. So now you can do something I like another to suggested grey descent, which is stochastic em just no more now like exactly where you would sample which piece of the day that you will look to update your parameter  So in this case, you could start to only look at, for example, prefixes of the data. So if you only use from x one up to t, then you could only need filtering because you're not looking at the whole data.  And so there would be perhaps a way to do that though.  I'm not super familiar with the details of this approach. And so, and then partner in stochastic em. I don't think the do that. And so I can take em actually sample subsets of the data, but I don't really think they will do it in that I kind of like in a online filtering approach.  But to be verified. I think it's also in one of the project paper. Next you want to do is to guess the key him that you might guess  But, good question.  OK. So the point here was to compute the expected complete love last year because in the end, step, we want to maximize this quantity respecting the parameter. Okay, so let's do that now we want to do.  Maximize  With respect to data.  And basically, what we get is like it really looks like a standard mixture model look like you but with a few transition priorities right so what happens is in a standard  Mixture Model.  You would have this thing here.  And you would have this thing here, and you wouldn't have the transition polities right and so  So in terms of how you estimate the observation quality. It doesn't change anything.  The fact that you have some dependence on the Zed, the quantity, how you get the distribution over Deadwood depends of the dependence, because when you do smoothing. It's not the same thing as just looking at the current observation.  But in terms of estimating these parameters. Once you computed the soft counts is the same. And then the difference now is you also need to estimate the matrix. And so that's where you will use these accounts.  But I won't do the details exercise to the reader, but it's very similar to what we've done in the past. And so what you get is that the estimate at iteration t plus s plus one for the prior probability  Is just the normalize soft counts. So it's the summation of all your sequences of the stuff counts for the first Layton state.  And it's normalized, so you some over I, you basically just some the the top thing over what will over the, the state. So it's over. It's called El on up to k to one L. I. And now the thing is  This is always equal to one, because the probably to over all possible states. Some to one. And so the denominator here is equal to n. Right.  So it's just the proportion of time you're in a specific state instead of being 01 is the probability of being in this state. But, you know, it makes sense.  And now.  The other one is the estimate for the matrix. So it's similar in spirit, you need to normalize the count of times you observe a transition  But instead of being account. It's a soft counts, right. So basically, the quality of going to state L from state M will just be summation over all my time series and then summation over my time.  Of when I have observed that I went at time step T to state L from em right this is given by this problem.  And then I read to read normalized. And so I just some OVER THIS OVER YOUR internalized over L because L is this the state where you go to. So it has to some to one. So you have to be careful. So here it's a summation. I'll use you as my  dummy variable for L. So, and it's just a didn't. I mean, the numerator. So it's submission over I submission t equals to to capital T. I. So  Of tau teas and then it's L that I'd be normalizing right so this is you, and then m  There we go.  And finally,  How do you estimate attack a  Well, depends on your likelihood model for x, but basically you can think of it as if you had observed data.  Now you just replaced  The maximum likelihood estimate  With soft counts.  So it's so I kind of put it like a soft counts maximum likelihood. And so, for example,  In the case of a Gaussian Mixture Model.  If it's Gaussian  It's similar  To cash and mixture model, but using a weighted empirical mean sort of using the empirical mean for the mean estimate. Use a weighted empirical mean where the weight comes from these accounts.  With the weights.  Basically Tao Te K. Hi.  Right. So, for example, the, the case.  Will be obtained by taking  Summation over my training set.  Summation over my time step because I have multiple observation.  So it's kind of making more observation of the observed value.  But I multiply by the soft count the property of being in the state key.  And then I realized  So when you read normalize it's summation i want to end and the number of observations per time series, which is ti because the summation of the over key of the soft counts is equal to one. And so basically, the number of observation. I've seen is just summation over the tea ice.  Okay. And you can do the same thing for the empirical covariance, the usual thing you will have the the X t minus the mean that you are estimated and then x t minus the mean transpose in front of that you will add this stuff counts. Wait.  Okay.  So, for example, so you do that at every iteration S You're basically. So the EMR rhythm is compute the sub accounts which depends on the parameters. Then re estimated parameters then be computed the stuff counts by using alpha, beta, etc, etc. If that's the EMR.  And as usual, you initialize it to something you can do the same thing for like digitization for the em in GMM a big diagonal covariance matrix for the Goshen and say, the proportion after key means for the pace.  And so that's for the estimate of the parameters. And finally, if you want to predict if you want to make prediction in your sequence, you can use Vitter be to compute the most likely  Sequence of latent variable, given the observation. Right. So if want to compute the art max. Overall, the latent variable of the property from one up to t.  Given the observation.  You can actually use the max product algorithm, which I described last us  Yeah I confirm that capital N refers to the total number of sequences, where each sequence as ti licks ti next  Alright, so that's it. Is there any question.  So Dora is asking, why do I start at t equals two.  It's just because it's a transition matrix, right, so there's there's two times step and so  Basically  This part in the log likelihood  Because it's the t zero t minus one. And the first time step is the one  There is no transition at t equals one.  So if I, if instead I started at t equals one. I could have written ZTE to z plus one. And then I would stop at Ti minus one, right. So it depends on how you want to organize your sequence.  Yeah, I started XE one because here this is a XE one I  Depends on how you which conventions.  I prefer and like index one it's simpler because then also like it goes through the, the actual length of your thing. Otherwise, if I want to links Ti, I would need to go up to t minus one. So you all these assets is a bit confusing confusing.  Any other question.  Alright, so if there's no other question. So we're done with latent variable model. As I mentioned in homework for you'll be implementing in  Glory details. This alpha beta recursion on the time series with a Gaussian Mixture Model.  And then feature B and the cool stuff, by the way, is that you're using the exact same data as homework three  So it turns out that the true generative model for the the the Omega three was a time series. So there was dependencies between the  Component of Z's. So I didn't tell you so you so in Homework three you just did a GM and model you thought they were all independent variable. It turns out that the latent variable we're dependent  And so the hmm model will be much better to estimate from the data and you see it will be a better fit to the data.  So it'll be interesting to compare both  Alright, so now I'll do a little segue. In information theory.  The plan is to talk about kale divergence, etc, etc. And in the next class. I'll start about entropy. The maximum entropy principle and we'll get to one of the coolest results in this class, which is the equivalence between maximum likelihood in exponential family and maximum entropy  Of with moment constraints. Okay, so that's a super powerful results which actually use Lagrange enjoy it and convicts analysis.  But yeah, but before getting there. We'll start with a bit of a crash course on information theory. So it will be very succinct. But, you know, just to present. What's this key divergence. And what's a bit of information theoretic interpretation of it.  So,  As I already defined earlier in class when I talked about, I think, em, so the kale divergence between to distribution. I will define it here for the screen distribution.  So let's say  P Q R discrete  P and Q. So that's the PMs the kale between P and Q is by definition.  The summation over all my observation in the discrete space probability of x. So that's the PMs right log of x divided by q and x  And so you can think of it as the expectation over P of the lug of the ratio of P over x.  And here I use the capital letter X because I'm taking an expectation respect to x.  And so that's the definition  And it's fine to have sometimes the property over p. We can put P of x equals zero in this. So, the convention is that if I have zero times luggage zero by convention will say this is equal to zero.  And it's meaningful convention, because if you take the continuous limit as x tends to zero plus of X log x you actually get zero by little rule, you can have fun with the Beatles rules. This gives zero. So that's what that makes sense that zero times log of zero, we say zero  All right, so where does this thing come from, well, you can get it from the difference between the the marginal likelihood and the auxiliary lower bound in EM  Alright so let me finish what I was saying. But now I will do. I would give to motivation, one from density estimation and decision theory and the other one from coding theory. But before that, there's a question. So the support of T rex must be a subset of the support of qx user avatar   Px must be a support of the qx user avatar   So you cannot have. Yes, correct. So that means. Well, I mean,  It does not have. You could say that if q equals zero, but P is not equal to zero, then you get so  If there exists x such that q of x equals zero, but p of x not equal to zero.  What you get is you get  Into some you'll get basically p of x.  lug  There's a minus here q of x. And so this is minus infinity.  And so get plus infinity. Okay, so this is equal to plus infinity.  So either you say you don't allow that or you just say the kale divergence between those two distribution is plus infinity, which is a meaningful thing to say.  So if, if the support of p is not a subset of the support of Q, then it turns out that the kale between PQ is infinite.  Then you have the, by the way, you also have the continuous analog where you will replace supposed to be an X or now densities respect to say the bag measure, then you will just replace this some with an integral here and then we will have a dx.  And you will have the same issues here if sometimes there's some set for which there is a measure of zero for the distribution queue, but not for the decision.  And moreover, if P AMP Q OR NOT, the density with the rest of the same base measure. So for example, let's say Q has  Is a decision on the line and please distribution.  Know, so I want q of x equals zero.  But peanut zero. Yeah. So let's say Q is a decision on the square was piece of distribution on the line.  So then the for any interval on this line Q will say the property zero because it's doesn't have the right dimension.  But the decision of repeat. It's fine. It's aligning and so it will say nonzero. And so these two distribution will have an infinite killed averages.  And so what and that happens a lot. When you have data with different dimension or different support that say you know when this machine is on the manifold and the other one is on the different manifold. They always have infinite kale.  Which is one of the probably the difficulties with the kale, by the way, is that it's often infinite, which is not super useful. So there are other this notion of divergence between distributions, which are better behave like the Wasserstein one is always finite  Or the healing your distance I think also will be finite. So, but, yeah, but let's focus on kale here.  And yes, so I guess here to rephrase what is it kill said so if support.  Of Q  Is not included.  In the support  Of p  That implies  That limits. So it's the other way around. So you want supportive P, including the sport of Q  Okay, let's do it this other way around so supportive P not included in support of q. Now that's the right direction. This implies that the kale P and Q is equal to plus infinity.  Okay, so let's do a motivation from density estimation  So if you remember, a long time ago, I talked about  Density statistical decision theory which formalize how to evaluate statistical methods.  So we recall statistical decision theory.  And in this world, we needed to define the last which tells you how bad a decision is. And so if you want to do density estimation  You want to estimate the parameters for distribution. And then you want to say how bad the parameters that you're given competitive true parameter. How is this bad. So this is defined by a statistical less the statistical us  Statistical loss.  Is user avatar   Let's see. user avatar   Which tells you how good  Your action is when the trial is pure theta. So, this is the  The world and a the action here would be  In this test because it's the, the, the task here is estimating a distribution. And so we'll say, for example, we'll use Q hat as rotation for distribution that you've estimated  And when you do maximum likelihood  You can think of doing maximum like you. There's minimizing the negative loveless guess to the standard  Maximum Likelihood loss.  So the ML. He gets so  Or should not have machine learning, I'll use ML he the standard Emily last is the last  IE. The last between P of theta and Q hat.  Is the expectation over the test data.  Of minus the lug of your density  Okay, so when you do em maximum, maximum next you from empirical data, you just take the summation over the training set of minus log of the probability that you give to this observation.  And  As somebody just mentioned, this is what it's called the cross entropy  Is called the cross entropy which appears everywhere and  As a surrogate loss for learning and neural networks or whatever. This is a pretty standard one. This is just maximum IQ maximal magnitude  Or maximum love likely or dislike minimizing the negative log euless over the training set and  The, the, the true statistical us is is not, it's not like, how you doing a training set, because you don't care. It's how you do on the test set.  And so that's this expectation over to test distribution. And that's where this thing is coming from.  So this is like the hell doubt test log likelihood or negative like like us. It's like the how well you're doing in a test set, or you want us to be small, because it's a loss, and it's the cross entropy. So this is a cross entropy loss.  And by the way, if you use Q hat is equal to p of theta to use you predict the correct distribution.  Then this cross entropy becomes the entropy. So you get summation over your sample space of minus p of theta of X lug of p of theta of x, because this is your prediction and this is by definition.  The entropy of pure failure.  And that's the best you can do right if you predict the correct distribution, then the thing you get the smallest cross entropy which is actually the call the entropy here because you take the expectation respect to the same distribution as the one you predict  Okay, so that's the entropy. Now where's the kale. Well, the kale comes from the access loss of making a wrong prediction. So the access loss.  When you predict Q hat.  Instead of the true pure theta is the last P Q hat minus the best you could do. And I told you the best you could do is just putting the right thing, which is L.  I guess I put the minimization. So the men overall Q of the loss of P and Q. And it turns out that this is the same thing as just the loss of P and P which is the entropy. And so if you compute that you'll get  So minus summation over x.  Px  Right, because this is there's a minus here. That's a summation over x.  Today, not use my computer ink again and then. You'll hear. You'll get a Q hat.  So here I have a Q amp and then I have a  P and so it will be log of oops log of Q hat of x divided by p of x. And so there was a minus here. And so I could flip these two to remove the minus. And so I just get the same thing as a kale divergence  P Q AMP.  Okay, so you could interpret the kale divergence as the excess loss in the density estimation problem that you get by predicting the wrong distribution you had when the true distribution of p of data.  And so if you predict the correct thing, then the kale is zero because there's no access. Plus, if you break the wrong thing. Well, the access of the negative log last that you get is the kill divergence. Okay, so it's pretty natural because the Douglas is the standard  Last that we use for density estimation  So that's the density estimation motivation. Here's now take coding theory. So coding theory has a bit of a similar flavor, you'll get that the kale divergence  Is also the access cost of something. So the coding theory motivation.  So in this case,  The idea is if you took if you use prefix code. So if you use a length of a code for a character or a word which is proportional to minus log the probability of seeing this character.  And this is now we use because, you know, when you say binary code so log is actually log in base two. And so when you measure things in base two, you get something called bits.  Was when you use log in base. He  You get something called NATs so  It's just a different unit.  And so normally the entropy is measured in knots. But when you go back to coding theory, you might measure them entropy in based to instead. And that does this is bits.  And so if I use as my length of my code something proportional to the negative lug of the quality of seeing this word, which makes sense right so if if a word is very frequent I want to use a very short code.  So the protein will be high. So the negative log. Probably it will be small, so it has a short code was the word which are super rare. I don't need I can use a very long code for them because it's I see them very rarely  So the expected length of my code.  Of code when I get like  random words coming will be the sum over my possible words the priority of this words time that's length which is minus log of px and so I get basically the entropy  measured in bits in this case because I use  I get the entrepreneur. My distribution measure in bits.  And then the kale divergence will be the expect them length of my code when I code using the distribution. Q When the truth submission is P. Okay, so it can be interpreted  As the excess cost.  As the excess costs.  In terms of length of code.  To use the distribution queue.  To design the code.  Versus the optimal distribution, which is the true institution.  Just give you basically the best code.  Okay and Dora posted the link on this topic.  Interesting, thank you.  Alright, so that's the another view of kill divergence. Okay. And so now let's look at some examples. So example. user avatar   Oh man, I'm always late. user avatar   So,  Let's talk about the entropy of a bear new distribution.  So the. This is basically  P minus P lug p  Plus  Guess.  Minus one minus p plug one minus p  When the parameter of your burner use p  And so this function actually looks like this. So this is my parameter which is the probability of x equals one.  And here is the entropy of my distribution.  And  The, the parameter can only be between zero and one and it as this nice concave shape like this with maximum value at one half, and then it has entropy log of two.  OK, so the more deterministic your burner you is so that this bias conflict, either. It's one or zero, then it has low entropy  And the maximum entropy. It's a bit in some sense the, the maximum disorder is minus one half, one half or uniform distribution and, more generally, the entropy for a uniform distribution.  On key states.  Is minus summation X equals one, up to capital K of the property of any state, which is one overcame when I have key states log of one of our key.  So it's a constant respect to x. So the minus log of one of our key becomes legal Ricky and I'm something one of her key work a time. So I just get lug of capital Kiri and here it is. Yep, that's it.  And it turns out that this is the maximum entropy distribution.  Over case states.  So the maximum entropy distribution of a discrete variable over case states is actually the uniform distribution. And that's something you would prove in the assignment, by the way.  And so to help you with the assignment. Let me mention some properties of the kale.  So you have that the tail between P and Q is always bigger than zero.  And you can to show that you can use Jensen's inequality.  To show this, you use the Jensen's  In inequality.  Which as you recall, says that the  If f is convex, f of the expectation is upper bounded by the expectation of f of x. And the way to remember that is just to dry it. So this is  Expectation of X.  And this is  Expectation of FX  And this is here f of expectations so it's below.  when f is convex.  And negative log, which appears in the entropy is convex, because love is concave  It turns out that K L is strictly convex in each of its argument.  convicts  In each of its arguments.  Okay, so by this I mean that  The KL  For example, if I fix P, and I very the other argument and the other argument is  I can represent the distribution of our key numbers by  Vector in dimension key, which are positive.  And I take this as a function from busy the property simplex to this.  And KL dot q. So both of these are strictly convex function.  And kale.  Is equal to zero.  P P equals zero for all P belonging to practice simplex for all distribution, you always have the killer of a decision with itself is equal to zero.  And finally, a property of the kale is that it's not symmetric. So that's important to keep in mind.  You have that the kale of P to Q. It's not equal to kale of Q to pee in general.  Okay, so is there any question about the kale divergence or the entropy or these things. So now you should have everything you need to do the assignment for  No question.  Is it possible to have a distribution over a high dimensional space with a non zero probably to have some lower dimensional object.  Not if it has a density. Okay, so if you have a distribution, which is a density over c a p dimensional object any p minus one dimensional subset will have zero quality.  Because then the big measure basically give zero mass on these things. Now if your decision doesn't have a real density. So for example, let's say you're in dimension one  You could have a distribution, which is a Gaussian everywhere except you put the direct mass, you could say like property one half of being a Gaussian everywhere and probably one half to put all the massive zero  Okay, so there's this division does not have a density respect to the big measure. It's a mixed measure, but this one give a nonzero policy on the lower dimensional object name Nia zero dimensional object which is the single turn to zero.  So this is possible, but if it's not possible. If you have a density user avatar   On the full dimensional object. user avatar   Is there some easy way of extending the kale divergence to make it symmetric. Yes, you. It's called the symmetry kale and all you do is you take the average of the kale in both directions. So you define it as the kale of people. So basically this where's my pen.  Yeah, so you have that the symmetry is version.  Is you use one half of kale p q plus kale have to pee.  Right. And so this by construction is symmetry.  And I think this call. I think there's a another it's not the junction shine divergent. That's a different thing.  And then is there a nice relationship between the was fine distance of the kale divergence. I think you can bound one with by the other one. I think there was a sign his upper bounded by the kale in some way. Do I don't know the exact  Relationship and when would we use a symmetric variant.  When you want something symmetric  So for example, I don't think this gives you a metric. So it's symmetric, but does it satisfy the triangle and the quality. I don't know.  Does anybody knows whether this satisfies the triangle inequality.  Because if you need a metric on distributions, then you would like  To have something which is both symmetric and said sided triangle and quality. Now it's symmetric, but I'm don't think it's a two sided triangle equation.  So we'll see later when we talk about exponential family and  These beautiful geometric relationship where there's a there's a triangle inequality in this and there's something equivalent to the partner in theory with the KL  But it's getting quite specific  So actually is yet so  Yeah, one can minimize the divergence, even though it's not a metric crisis. It's the thing is here, it's, it's something the kale divergence is something which is  Always bigger equal to zero. And it's actually only equal to zero. When p equals two cute, so it's as a as a notion of  Something is not is it's kind of, it's not a distance, really. But it's, it's, I think, the name is divergence. It's mean that something is not equal to something and it's quantifying how much something is not equal to something. And so if let's say two  Groups that meet console. And let's say for example I want to approximate p with a simple distribution to like we will do when we do virtual in France.  You will maximize the scale where you belong to a smaller set of distribution which don't include p. And that's a meaningful way. And basically, you say, I want to find a distribution queue, which is closes in KL to the true distribution.  But then you'll see you'll get different answer, depending on which direction used to kill. Okay, so one is called mode seeking the other one is called matching moments, but we'll get back to that when we talk about exponential family and  Personal influence  Or their fun use case of the submission that don't have a density  Will anything which is discrete don't have a density you. I mean, it has a density respect to the contact measure, but it's, you know, if I have discrete data. I don't have density respect to the limbic measure right so  And then if you have this mixture model like if you have a Gaussian with an A massive zero this is  There's this thing called spike and slab prior they use a Bayesian inference. When you want to have sparsity wanted parameter to be  Often zero, well then you want that the mass at zero is non zero. So, to make sure that the parameters are encouraged to be zero. And so people can use this kind of like mixed type distribution which don't have a density, respectively.  Okay. Well, I think I went quite over time. Happy to answer more question in the in the social get a town so. Have a nice weekend if I don't see you and get it out reminder that now we have poker tables. See you.
  In this recording. Alright so today.  As I mentioned, we will do a bit of deep elegant mass. So, we will look at maximum entropy  And also Lagrangian duality.  And we will  Start looking in more details.  On these Patrick family which are called exponential Femi which are very elegant  Pro stick families for which you know Gaussian Blur new eat the fish. They all standard distributions are belong to it. But then we can prove general properties.  And as I mentioned earlier, one of the the deep result will see today is the equivalence between maximum likelihood in the expansion of me as well as the maximum entropy  With woman constraints.  And it will be a good excuse to review a bit of leverage and Dorothy. So that's the plan. We're in some sense.  More in the big picture here. We're in like topics in machine learning and high dimensional to sticks. So this is really to maximum length you that maximum entropy and this kind of thing, as well as learning interesting tools like language and quality.  So just before we get to maximum entropy. I just want to finish something, I didn't have time to finish last class, which is a relationship between maximum likelihood  And  Killed minimisation right so last time last class I mentioned, what was the callback library divergence and I did a bit of information theory Crash Course  And so it turns out  That there is and then again also reinterpretation of maximum likelihood  So consider that you have p of theta.  You have a parametric family.  Of distribution. So you have a parametric  Family  And to avoid a bit of  Formal problems will use a discrete  Observation space.  The relationship is also true with continuous relationship space, but it's  You'll see that  Where it breaks down and where you have to be a bit careful. It's more like a formal argument, rather than  The exact same standard definition. So when you do maximum likelihood for theta in this Patrick family. It's equivalent to minimizing over theta.  The kill divergence between the empirical distribution and your model distribution. And so this pn hat here is the empirical distribution.  So it's PMS is defined as  The end the hat of X is by definition, one over n summation over your observation of the chronic term delta  On the observation, right. So this is the chronic, chronic or delta which, as you remember, it's equal to one when it's two arguments are equal and it's zero otherwise.  And so basically this, this tells you the number of times I've observed a specific X in the training set and the proportion of time I've observed it is it's probability that I assigned to it. That's the empirical distribution.  And you can see here already, why I made the assumption that the position is discrete, because if this patient was continuous instead of a PMS would have to call  We have to talk about the empirical PDF and that's a bit annoying because the the empirical distribution is doesn't have a density respected the big measure when you have a  continuous distribution, because what you formally get is instead of these cracker delta, you get the direct delta which are basically infinite spike at zero.  Sorry, on the data. And when you integrate it, which is what you do when you do continuous distribution.  computing. Computing quality of events in a conscious decision then it select a value. Okay. But I didn't want to go into these kind of formal detail of, like, what is it,  Direct delta. So, to avoid that. Let's just stick to discrete. It's much simpler, but be aware that you can also make an analogous statement, but  Careful with the mathematical details for a continuous distribution and so doing maximum likelihood is the same thing as minimizing the kale divergence with the empirical distribution as this is user avatar   Not user avatar   Temporary yeah user avatar   And so in some sense.  You tried to find data for the for the distribution, such that it minimizes the kale divergence with the observed empirical distribution. So it's kind of a interesting  Principle. So another a very natural way to look at maximum next good and also the kale and the proof is trivial, but let me just spell it out so you have that the kale between the empirical distribution and p of theta by definition is the expectation over the empirical of  P and X lug of p of x divided by p theta of x and here this is discrete, some because it's a discrete observation space.  And so the first. So the log of the ratio becomes log minus log. So, the first term is just minus the entropy of the empirical distribution. Right. So this is so the summation of x pee pee.  pee and lumpy. And this is by an efficient, the entropy and they get of entropy sorry there's a minus sign, which is missing. And this is just a constant respect to data. So what we're left is just summation over X pn of x and then lug of p of theta of x, right.  And if you recall  Is one over n summation i have the direct as chronic or a delta x, x i.  And so when you some over X these credit card delta will only be non zero when x is on the data points, right. So basically, what you get is you can flip the some so this becomes one over n summation over i submission over x delta x, x i.  And then you get the the selection property of the  Of the clinic or delta. So, this gives you basically  I guess and I multiply by stuff.  So, P.  Of x, blah, blah, blah. And I know there's no pain of x. This is what's piano. So this is lug  Of p of theta Vic's. And so this all thing here just become lug of p of theta of x i. user avatar   Right. user avatar   And so basically I rewrite it. This is equal to a constant which is the entropy of my vehicle minus one over n summation over my training set of log of p of theta x of AI.  And this is the usual like us with the product over i.  have the luxury of p of fate of excited  So, you know, this is a standard  For the log likelihood for ID data. And this is just a constant.  With respect to theta and thus minimizing the kale is like minimizing the negative log likelihood. So it's the same thing as maximizing the log activity. Right. So that's what I want to user avatar   Say that user avatar   And user avatar   As it is, this is not temporary think so we're done. And if you did the argument with continuous data, you would replace now the sun with an integral  And then, oops.  Okay, so  So you would replace the some with an integral over x. And then you would have these direct delta which when you integrate still select the date the function. The problem is that the kale is not formally defined  Between the continuous density and the empirical distribution when it's continuous data so so  So what happens is when you make this argument for continuous distribution. It's more you will define you'll say, oh, I will define the kale as just the integral here of the of this empirical in this weird sense user avatar   And yeah. user avatar   So yeah, so that's the maximum likelihood and the ratio with kale.  And now we will see the maximum entropy principle which will which can also be interpreted as a musician of a kale. But then the other direction.  And so and so instead of between instead of being the kale between the empirical and your model, it will be the kale between your model and the uniform distribution. Okay, so we'll so let's go there. Let's talk about the maximum entropy principle.  Alright. And so the idea between the maximum entropy principle and then I will formalize this idea  So you will consider  A some subset of distribution.  Over x  According  To some data driven constraint.  So because you made some observation of the world, you will decide that the distribution for satisfy some constraints.  And so we will get  Some subset, which would call a capital M, and we'll see that it's because of of the moment constraints that will see later. So this is a subset of all the distribution over my let's do the discrete up with again for simplicity  And let's say it's a finite number of observation, again, for simplicity  So all the distribution over a finite number of observation is represented by a elements of the property simplex. So I need to specify the PMA for every element.  And that instead of adding all possible distribution over key elements. I'll consider a subset of that which satisfy some constraints.  And so I concern I have this subset of distributions that I selected. And the question now is, oh, well, how should they select the distribution in this subset. And that's where the maximum entropy principle come in. So the maximum entropy principle.  Will say  You will pick  p hat in the set, which maximize the entropy  Okay, so if you don't know which distribution to pick among a bunch of this division for according to your mind they're all equivalent, they all satisfy the constraints you care about.  Well then pick the one which has maximum entropy in some sense which maximum entropy is a bit like maximum uncertainty. So you're committing in some sense to the minimum amount of structure in your distribution. So that's kind of the intuition.  And so and so from an formalization principle, you'll say, okay, well, the way you will estimate your distribution will be to do the arc max.  Over a set of distribution.  Of the entropy of these decisions entropy of cute. Okay, so that's how you choose this mission.  And it turns up well that  The entropy of Q is the same thing as minimizing  Over your distribution, the kale divergence between q and the uniform distribution.  Right, because it's called this view.  So the kale divergence between two and the uniform by definition is summation of x q of x lug you have x  Divided by UX right and so you have x here is just equal to one over k  When is the uniform distribution. And this is just a constant, right.  And so when I minimize the spec to Q you is just a constant. I don't care about it. So all I'm left with is basically minus the entropy, because the entropy is summation over a que le Pew with a minus. And then I just have plus some constant, right.  Yeah so minimizing the entropy maximum sorry maximizing the entropy is the same thing as minimizing the kill between cute and the uniform distribution and capital delta subscript absolute value of x. This is the probability simplex  Over  Let's see key elements rights were key is a number of observation of in my sample space script x  Okay, so that's also a thinking of the entropy of a distribution as the kale with the uniform distribution is in the discrete case, it's actually also quite neat.  Because you know the uniform distribution in some senses, the one which doesn't commit anything on any observation on any possible  Value of the random variable, right. So you just say, well, all they're all equally like in  And now, what you do is you made some of you made some constraints. For example, you can say, oh, I made them the observation of a proportion of, like, let's say I'm looking at a coin flip, I observed that the coin flip, has been  Eighth 10TH OF THE TIME OF ALL MY OBSERVATION to be head okay and I want to say, okay, now I want to distribution, such that it's mean is a 10th.  But which maximize the entropy which what would be the maximum entropy principle. So the closest to the uniform, which would be one half on this case.  And so that would be the maximum entropy. And in this case, actually, because this is an essential for me, it would be the same thing as maximum an accurate. By the way, in general, it doesn't have to be the same thing as maximum IQ, but we'll see later today that  For this concrete example of the Bernie ran a variable, it would still be the same thing as maximum IQ.  And I will mention also that there's this thing called generalize maximum entropy  maximum entropy  Which instead of minimizing the kale.  Between Q AMP a uniform will just minimize the kale respect to some fixed distribution H of zero.  Which can be seen as you know your clinical know observation distribution.  And so this is basically your preferred distribution to be biased towards right  So for example, you could think that you like Gaussian with zero mean and unit variance and then you made some observation enough some continuous distribution and then you  Some consistent and variable and you want them to minimize the KL between your, your distribution and this scholarship. So that could be another example. And we'll see this age zero will appear later and the expansion of me.  I know somebody is asking, Is this some similar to having a prayer. So it's similar in spirit to having a prior though it's not explicit right so we're not Beijing here. There's no notion  Of a distribution or distribution. But indeed, it's a inductive bias in our learning principle which says that there is this based distribution that's called H zero  Which, in the case of maximum entropy is just a uniform and I will try in some family of distributions. Find the one which is the closest to this one. user avatar   Yeah. user avatar   Alright, so let's do a concrete examples.  Which I picked from Martin rain rights class.  So imagine rain. Right, by the way, is Canadian if you didn't know, I think it's from Toronto.  And he likes beer. So  So,  By the way, my rights if you don't know is a faculty at UC Berkeley in the electoral in in the CCS department, very famous machine learning person.  And  I think I took it from from this class. And so basically says, okay, you're going to Australia and you observe a bunch of kangaroos.  And you will have observed the proportion of kangaroos, which are left handed seals you observe that three quarters of the kangaroos in Australia.  Or left handed.  How do you know that they're left handed. Well, because they're carrying their beer with the left part  And then you also observed that the proportion of kangaroos which like Labatt beer, very famous Quebec beer. So I don't know why do you have it in Australia, but why not the drink Labatt beer. user avatar   Okay. user avatar   So these are marginal observation.  So you know that when you look at marginally on on how many kangaroos or left handed, it's three quarter and the marginally the number of them which drink beer is to third  Not want to ask us, the joint, what's the proportion of kangaroos, which are both lifted and drink beer. Right. So the question is,  How many kangaroos.  kangaroos.  Or both.  Left handed  And drink.  Fear.  It's an important deep question.  So,  And so here, if you use the maximum entry principle is you'll say okay concert. Now, the set of joint distribution.  On both the observation of whether a kangaroo is left handed and whether the drink beer.  So there's like, you know, there's four possibilities, right. So it's a discrete space of four possibilities. Either kangaroos left handed.  It doesn't drink beer that whether they're left handed and drink a beer, etc. For both object. So it's like to. It's like observing to conflict in some sense.  And over all these distribution you have constraints on their marginals, so because you observe the marginal. So you would like that the joint distribution satisfy the marginal which you observed  Now there's still an infinite number of distributions, which satisfy these marginal. And the question is which one among all these should you pick it. So the maximum entropy principles as well, just pick the one which has maximum entropy. And in this case, the maximum entropy solution.  Exists is to the reader after way we'll see later is actually the one which just says that these two observation or independent. So, the matrix maximum entropy solution.  Here will say that  The joint on whether you drink.  Why is it be okay Labatt beer and you're left handed is just  The proportion the product of these marginals, right. Okay, so you just assume that they're independent  So, so basically B equals one and left Labatt equals one. So that's just mean the proportion of kangaroo, which both drink beer and our left handed.  Is just a product of their margins, because its independence in some sense by doing maximum entropy. We decided to be as less committal as possible.  Which we suppose there's no correlation between these because it makes things more complicated and, in particular, actually reduces the entropy of the distribution.  Okay, so that's the general idea. And so then if we formalized more specifically  How to get these constraints.  So how do we get the set em.  From data. So, by the way. So, the maximum triple principle is just very general like that is I have some subset of distributions. According to some whatever that that driven constraints and then I speak the one which maximizes entropy  And what the most standard way to get constraints.  But it's not the only one. But that's the one we would consider is, is actually to look at empirical moments.  And  Empirical  Moments.  And so when do we defined by the feature of the empirical moment. Well, so we'll define some functions that we call feature functions.  Which makes some measurements of the observations. So we'll have  An actually here I'm using here the notation of the exponential family because that's what we see when we do the duality. So I will have say d feature functions. So the features.  Which are measurements of my observation.  And so, for example, a measurement could just be x, right, or x square or x cubed.  And that's why I call them moments because when you take expectation of this you will get expectation of X exposition of experience within your excuse but these function could be much more complicated just x or x squared. Right.  In the case of the lab at beer. It was an indicator function. So the it was like, oh, is x equals two is the the the bat observed piece of x equal to one.  So that was the nice feature function and then the given these feature function which are just measurement of the data. You will define the set m  As the set of distribution, such that when you take the true expectation respect to your expect to have your distribution of these feature function.  They are actually equal to the empirical observed  Mean of these features. So the observed empirical case. Okay, so I take, I use the P and hat notation for the empirical distribution and here it's just the empirical expectation  And you want this to be true for all your user avatar   Feature function. user avatar   And so you want that the  Model. So, so this thing here on the left.  This is the model.  Expected  Feature count.  So that, what's the model would predict would be the expectation of these feature. And these are the empirical feature count.  Okay, so that's why we call this like moment constraints, because if the feature.  Would be like something simple like x or x square, whatever, then you would have the expectation of X matches the empirical average on the data of x, for example. So these would be this kind of constraints.  So Jacob is asking the constraint set em in the kangaroo cases that submission, which have those marginals, yes. So in the in the kangaroo case. So the kangaroo case.  We had that  T one of x was basically the indicator of  X.  So can do x  Drinks Labatt  And tea to have X would be the indicator of x.  Is left handed.  Okay, so I would have those two indicator function when I think the expectation. The becomes probably t. Right. And so, and and and PL  And PB here. We're I guess I could have used perhaps to make it simpler. These would be hacked. These were empirical observation and let's put that here that will be cleared and  And so, as this is temporary.  Teck Teck Teck and so I have observed the empirical counts of these observation and I, why did I  And now all I want is that the true model marginal  would match the empirical observation.  There constraint that p hat is an m is hard, right.  depends which P at we're talking but but here.  This p hat, which is our prediction by definition will satisfy the constraint is  But you have to be careful pee.  pee and hat this hat does not have to satisfy. In general, the constraints. But here, by definition, it will because if I take the expectation here respect to pee and hat, then you see  It will satisfy the constraint, because it's on both on both sides, but in more general type of constraints we could have that TN hat doesn't belong to our family because, for example, we could  We could regularize I don't know we we we might want to have a distribution which which doesn't put all the math somewhere but  Are these moments in any way similar to, for example, central moments. Yes, so central moments is the expectation of X minus mu, for example. So, or x minus square. That's the variance, the uncensored moment is this expectation of X and X squared to throw  Then, and here these are more like generalized moment because there they don't have to be the standard moments of x, x squared, etc. But  We call them like basically still moment constraints, by analogy.  So Dora, you're confused why we look at counts here as the feature function.  Can you explain more your confusion.  Talk to us. user avatar Dora Jambor  Okay, let me on spot.  Yeah, I'm just not following, I guess, in general, like why do we use these feature functions.  So, user avatar   So the idea is we're designing now. Right now I'm getting I'm giving you a general formula to design the learning principle.  Okay. And so the idea here is, I will make some measurements on my observation.  And these measurements will be defined by these feature function. So for example, in the case of the the kangaroo example.  It was also trivial because there was only two possibilities. But let's say instead of talking only about Labatt beer.  And left handed. I could also talk about eye colors and other features, but I'd say we don't carry about Eichler. Okay, so then we say is like the two measurement will make is  Is the kangaroo drinking beer and is the kangaroo left handed. So these are the two feature function. Okay, and now  We will restrict ourselves to the set of distribution which make as their their their model expectation of these feature function.  The same as the observed empirical moment the empirical expectation of these feature function. And so in the case of a kangaroo. We want that the so then when I take the expectation of the  Indicator. It just gives me my probability  And  That's the model expectation and when I think the empirical expectation, it will just give me the proportion of time I observed something right so then we will say, okay, we want that the policy that we predict that something  At the kangaroo drink beer should match the empirical proportion of the movie with drinking beer. So these are the constraints.  So, and so that's a way to design constraints on the set of distributions from data and then the maximum entropy principle was, say, well, there's a lot of distribution, which is why these constraints.  And we will choose among this infinite set of distribution, the one which maximize the interview.  And then we'll see in a few minutes, that this is equivalent to defining a parametric exponential family on the data and doing maximum likelihood in this parametric explain as much of me okay though. In general, if, if  You have  In general, the moment the maximum entropy could be different. And we'll see where  One day be cases where the true distribution doesn't have some moments, but the empirical moments always exists, then the transmission won't even be part of em.  One there be cases where the true description doesn't have some moments, but the political movements always sure let's say for example,  But I mean, here we're talking about the discrete space. So the true distribution is pretty simple. But let's say we would had a continuous space which is also you can do this principle for continuous space.  Let's get to the solution is a cushy and it's saying that we do maximum entropy, then the meaning of a cushy doesn't exist, but the empirical mean will always exist, right. So then there would be a problem. Indeed.  OK, so now  Let me  formalize this maximum entropy problem as an optimization problem. user avatar   Alright. user avatar   So the maximum entropy problem.  As an optimization problem can be seen as minimization.  over q  And Q can be represented. When you have only K observation K possible values for the random variable for the sample space.  Is just  A subset of art of decay.  And you want to minimize the kale between que en the uniform distribution.  Such that  Q belongs to em en que is also a correct distribution. So I would use this this notation quality siblings.  So it's basically a constrain minimization problem and actually it's it's a convex musician problem because the key is convex. I told you, and Q belonging to men can be written with linear constraints. So it's summation over x.  Q of X T g of x.  That's the true expectation of TG I want this to be equal to  One over N summation over the observation training set of TG of x i think  And we've called this alpha, Jay.  Will just give a value for the empirical  Feature counts for every dimension of my feature.  Map, which is called them alpha g again. So you can actually see this as I take the inner product between Q seen as a vector because you know it's a it has key values. So it that's key components. And there's some to one and they're positive, but is it is just a vector and  TJ.  Could also be seen as a vector. What's the TJ value for every possible X, which I have key possible observation X and I want them to be equal to alpha g  This thing here is just a product  summation of x could be seen as each of the component  That's why I'm saying it's a linear constraints over q. And so this is actually a convex constrained optimization problem.  over q  Which is belongs to  I guess which belongs to the poverty simplex  Which is a subset of our to the X Men.  And so now  There's some questions.  Okay, so somebody is asking. Oh, what happened when you mix maximize the entropy when the  When the, the true distribution is not part of your constraints set right because the empirical moment because the the  For example, like we define here, the set of distribution, such that the true moment mesh network, a woman. So if the true moment doesn't exist, and it certainly doesn't match the empirical  Well, I think we're getting in very weird len anyway because usually most distribution would have their moments to exist. I think this is a bit like  A corner case because of like we are the solution that cushy I think also, one thing to keep in mind is that will you're minimizing the kale between your decision Q and A uniform right so  Even though the true distribution doesn't belong to it, then you will you will just try to find  Yeah, so in by and, more importantly, I guess the  The true distribution doesn't appear anywhere in this formulation. Right. So you use the empirical observation to define em.  But the ratio between the empirical observation and the true distribution or not direct  And so yeah, so I would say, let's not worry too much about this case, it's, it's a good question isn't killed, but I think  First, find a concrete example where this is the case, and then analyze it and then we'll see what will happen, but it's I can actually do that online unfortunate.  Okay, so  Perhaps. All right, let's do  It and then we'll take a break.  So,  Before showing the equivalence between maximum entropy maximal magnitude in expression of me. Let's do a quick recap.  On Lagrange enjoy it.  And I'll basically just present  The part that we need. Okay. And I will give you pointer for more details. If you want to go deeper on this topic.  All right, so  Because if you remember when I did the when they use the lead grants multiplier method when we did maximum likelihood in the multi Neue  Problem. For example, I just introduced language multiplier and then take the gradient equal to zero and that's right. So I didn't talk about the equivalent dual problem. And so in order to talk about the dual problem. I need to talk about like engine royalty and in more details.  So let's talk about the convex minimization problem.  Let's say I have a convex minimization problem.  Men over x of f of x. So now I'm basically immunization. So x is no more like an input x is just a variable optimizing so you can think of it as the parameter of a model. user avatar   But user avatar   And basically, we will make these assumptions. So f and f j. Oh, yeah. Okay. Sorry. So I need, I need to talk about my constraints. Alright, so  So, such that I will have basically constraints which I'll express as inequality constraints like this. So this is for j equals one, up to em and I will have equality constraints.  Which are expressed like this. Je je je je je X. Oh wow, that's complicated for  I guess I'll use Kingston for these  GK there we go that's problematic for all key.  Guess perhaps notation ways this is clear, like this.  In one up to  And I guess so. I have and here x belongs to say RD. Okay, so I have a two dimensional problem.  I have em inequality constraints and N equals the constraints and basically you will assume that f and f j are complex function.  In GK are a fine function. So I find just mean linear plus perhaps a constant. user avatar   Okay. user avatar   And so when the constraints are of this farm Fj small equal to zero and g k equals zero and all the functions f and f g or convex NGK is convex, that implies. That's what you call it convicts constraint musician problem.  And so if GK is not just linear. It's like convicts a then it wants to be a complex problem. So, for example, like if you have  X square equals one, which is basically x squared minus one equals zero. So this is a complex function, but x squared equals zero will give you a set of possibilities which is  Minus one in one, which is not a context for the physical set on the convex. So that's why it's what we call a convex musician.  So this gives you  This implies that  Context problem.  I am having trouble writing  Why am I having trouble writing  convicts  Problem.  And this constraint.  Problem will be called them primal problem.  Because we will derive now through the eventually T and the equivalent or potentially equivalent do a problem.  All right, and so in Lagrange and duality, you define the Lagrangian function.  With the Lagrangian multiplier. If you remember, but now it's a bit more general because I also have inequality constraints. Before I only talked about equality constraints. Now also talk  About inequality constraints. So it will be a function of the original variable plus  You will have also the Lagrange multiplier, which are some sense will see later called dual variables and by definition this is the original function that you minimize and then what you do is you take a linear combination  Over all the constraints.  Of  The Lagrange multiplier times the constraint function and then the same thing for the inequality one equals one up to n.  New K Ji Ki, Alex. Okay.  And so, and so these  Variables are called the Lagrange multiplier.  So somebody is asking F and G should not be interchange  Can you elaborate on your question finish.  And  So, so I mean that you could name, anything you could name, F, G, H, I, J, if you want, but the important is that the the  The inequality allowed to be convex, but the equality cannot be convicts function, you have to be i mean the the next function, but they're very specific complex function, you have to be a find function for it to to work.  Hopefully that answers the question.  Okay, so let me talk about the magic trick.  Of leverage and do it.  So the magic trick of ingenuity.  And this is called the subtle point interpretation.  Because we will express the original constrain minimization problem as finding a subtle point, we'll see that  So let's consider the function h of x.  Which will be the maximization.  Over positive language multiplier for the inequality constraints and any new  Of the Lagrangian function.  Okay, let's look at that. Well,  If  F of x, if x is feasible.  So if x is visible. This will be smaller than zero and this will be zero.  Right, because these are my constraints.  Did I forget the temporary ink. I did. Oops. user avatar   Oops. user avatar   All right, so  So I said this is smaller than zero and this is zero.  If x is feasible, which means that  When I maximize respect to lambda is positive, this is negative. So this is always negative.  And this doesn't matter any new because it's multiplied by zero. So for physical  Then the optimal value of this problem is to set lambda equals zero. Because if I put a positive lambda, this is negative, it will just be a smaller value. Right. And so I set lambda equals zero, new doesn't matter. So the soup of this when x is feasible is just f of x.  So this is equal  To f of x, if x is feasible.  Or he satisfied all the constraints. user avatar   Feasible user avatar   And if x is not feasible. So consider that there is some inequality which is strictly bigger than zero.  Then I can have the corresponding lambda j and blow it to plus infinity. And then I'll get plus infinity.  And for any constraints which is not satisfied, I can always do that when I do so. Yeah. So somebody is asking what's the stupid Superman.  Superman is like a maximum, but it's more formally defined because maximum sometimes are not reach with Superman always defined, even if the actual maximum is the maximizers is is not reach. Okay.  So,  So if x is not feasible. This is actually plus infinity.  So the cool thing here is by this language and function.  And take the max over these negligent multiplier. I kind of implicitly defined the constrained optimization problem, right.  And so let's look at it. So an equivalent  Problem.  To the primal problem.  Which was the constrained optimization problem.  Is to take the minimization over x.  Of HR X, which I told you was the soup.  Linda positive and any new of the Lagrangian function.  Okay and this is he looks user avatar   Right. user avatar   And this h of x is a fancy  Non smooth function.  None smooth means that it's not doesn't have niches gradient and, in particular, it has plus infinity outside the feasible region. So it's a really ugly function.  But the nice thing now is I can think of minimizing this crazy function over X with no constraint. Right, so I got rid of the constraint.  So that's the the by using this Lagrangian function and maximizing this nitrogen function I express my my original problem is an unconstrained musician and then the duality trick is to swap the ends with the soup. Okay.  So I have an inferior and I have a super sorry  And the duality trick.  Is to swap these user avatar   In and soup. user avatar   Okay, so instead of doing instead of soup of the engine and not consider soup of info the languages. And that's the jewel problem. Okay. So let's write this  Soup.  Soup.  Lambda bigger than zero, new of my leg and then in  Of x  Of the leverage infection.  And so that's a different problem in general.  And then somebody asked  Till now, we haven't tried to use convexity or find right yes right now. This is true for any problem.  There convexity will be used to get strong do it and to have equivalent dual problems. That's what we'll talk very soon about and it is asking what it means in physic infamous which is the opposite of Superman, which is basically like minimum, but in a more formal way.  Alright so this end.  Of  The Lagrangian function is what we call the dual the Lagrange dual function. So this is a function which only depends on lambda and new and this is what we call the Lagrange.  Dual  Function. user avatar   Okay. user avatar   And the Lagrange dual problem by definition.  The Lagrange dual problem.  Will be the maximization of this function which is the soup over lambda positive and new of the function as London. user avatar   Okay. user avatar   So that's the Lagrange dual problem.  And  These variable here or whether we call now the dual variables. So they'll have their government supplier are now seen as the dual variables. user avatar   Okay. user avatar   Alright.  So now why do we do that. Well, there are multiple reasons.  So the first thing is  We started with a problem where you had fancy constraints on x.  And now we get a problem where the constraints are s s s kind of disappear, right, because there are a simple constraint on lambda, but that's it.  Okay, so we have kind of simplified the constraint set and, moreover, to compute the dual function. It's unconstrained minimum of this function.  So sometimes this can be obtained in close form. And so actually you can get a much simpler problem than the original problem when you do that. Okay.  And  I will explain very soon that this central problem sometimes can give you the answer on the original problem. There was an equivalence between them in right now. This is just a different problem. But I will  Talk about some relationship between those problem and then instead of solving the primal, you could decide to solve the duel, and then transform the solution to the primal and explain quickly how to do that.  And so that explains why we had this soup. Right. So, so the the so somebody is asking, okay, why did I define the soup. Well,  Basically, it was a way to rewrite or constraints in a in a in a kind of like specific form because by defining this language and function and taking the soup. I basically guess this function h year which is equivalent to the original function on the feasible set only  So it's a way to encode the physical constraints and then  It turns out that min max is often equal to maximum, which is why, you know, we said, Oh, what about if we interchange the order  If we didn't charge order, we get the soup outside in inside and then I will talk about when is this equivalent when are dumb in Mexico. Mexico, but one thing which is important to mention is that also the dual problem has very nice probably is that this dual  Dual Function is always concave  As  This function.  Is always concave  Okay. And that comes from the fact that when you take minimization.  So you have here that  Each  As a function of  change color. So there's a function of lambda in new. This is basically linear in lambda and new and the linear function is concave  And the minimization of a function is always concave. Okay, so basically the way you define a dual function is by minimizing some function which are actually concave  And and the plot here is here's a concrete function. Here's another key function. If I take the minimum of these to function, I get this function and this function, even though it's not smooth, it's still complete ok so the immunization.  Of concave function. user avatar   Is concave user avatar   And that's why the nitrogen to all function is concave, even though their original function could be anything. It could be non con comics. Okay.  Alright, so now let me fill in the last remaining details and then we'll take a break and then you can ask as many questions as you want. So in general, it turns out  That the soup in  Of x lambda new is always smaller than the soup of L x lambda  And this is called week. Do it.  Some sense when you solve your dual problem you'll obtain a lower bound on the primary problem because the primary problem is this thing here.  And a dual problem is maximizing this in which is a dual function and we say that the dual problem will always give you a lower bound on the panel problem. And this is always true. This is just buy properties soap, and if it's actually pretty easy to prove. And this is always true.  And now.  There's this concept called strong duality.  Which will say that in this case the soup in of L will be equal to the end soup of L. So it would get the same optimal value.  And  There are constraints on your problem to be able to prove strong do it.  And so example of sufficient conditions to get  Strong do it.  Is when the primal problem is convicts  Which is why I talked about convicts  Earlier. So the primal problem.  Is convex  And you have something called some constraint.  Qualification condition.  Which means your constraints are well behaved and they're not weird such that strong do it hold and a name for that you can look it up in Wikipedia outline Boyd's book is called Slater's condition.  And I won't go into these detail in the interest of time, and because also, it's not the focus of this whole lecture.  But it's just to give you the keywords so that you can look it up.  And so then you get strong do it so you so that means that gives you a way to get the optimal value of the original primal problem by the solving to do a problem because it's give the same value.  Now, what about the ark, Max. And so that's a very important question. So how do you get a star.  And so there's this thing called kick et condition.  Which allows you to read derive the prime the optimal primal variables. So the optimal primal variables. So, x star, which often can be derived as a function of the optimal do our variable lambda star new star.  Using the kitty conditions.  And so basically the kitty conditions they are necessary conditions for optimally to have the primal in the dual problems.  And they will be non linear equations, which depends on both the primary and the dual variables. And so if you saw them. And if you've solved the dual problems to get  The do all optimal to a variable. Often, you can find uniquely. What's the corresponding primal problem which satisfy the kitty condition which, in some sense, gave you. Now the solution of the primal problem as well.  And so for more details and all that. See Chapter five of Boyd's book. I'll put the link in the note.  But it's really well to explain in this book on both the subtle point interpretation, as well as other interpretation of language and do it. Okay, but the main takeaway for you that we will use is that  To get the dual problem. We first compute the dual function.  Which is by minimizing the Lagrange and function like this respect to x, which is unconstrained minimization, and then the drug problem is to maximize this Lagrangian function.  Okay, and that's what we'll see that the maximum entropy problem will be equivalent to the maximum likelihood inexpensive family is that when we compute the dual problem in the, in this case will get maximum likelihood inexpensive.  Alright. So Jacob is asking that. Is it true that in the dual problem f g n g k or like the language multiplier as lambda doing the problem, problem. I don't think so.  Because user avatar   You know, user avatar   I don't think it's symmetric. So to Lagrange enjoy it, unlike, unlike financial duality franchise conjugation. It's not the symmetric thing. Like if you take the language. Do all over again to do it. Honestly, get back to the exact same problem depends how you massage things  But I think there might be a way perhaps to make these relationships, but I'm not familiar with.  Is there any other questions before we go to the break. user avatar ezekiel williams  We asked a question. user avatar   Yeah, is it kill user avatar ezekiel williams  Um, I was wondering is, is this at all. Similar to this framework like useful or related in some way to  To like the zero sum game, kind of framework that you would see in the optimization of like general or generative adversarial networks. Yeah. user avatar   Yes, it's related though it's not exactly the same. So in a  min max problem you have this kind of settled point structure where you will have  Was up all this is a min. This is a max. Right, so, so, so again, some sense can be written as a min max revelation, so it's it's written as a min and max of something  Okay, and  And you can see here that I can transform the men of a problem to a max of another problem using leverage and do it. And so  So okay, so there's two things I want to say first is that the link the the most straightforward link. Is that a min max is also a central point problem because you have this min and max which appears in language and  The second thing you can do is if if you had some complex problem, ie, you could do the language and do all of your problem, respect, some of the variables. So,  Let's say have men over my parameter W and then max over alpha something of L M w enough of it. Perhaps I'll use  Non temporary Inc here. So this is side.  For it again. user avatar   So, user avatar   Side note, let's say I had men over W max over  That say alpha have some L of W alpha  Okay. And actually this I cover that in my advanced structure prediction and optimization class next year what you could do is you could do allies this max by using language ability to get basically an equivalent minimization respect to lambda of  L tilde, which is not a function of w n lambda  Will LT Lt is the dual function of the original problem. Okay. And so the. Now the nice thing is by using the maximization problem.  I got the minimization problem and I have men over respect to w men respect to lambda of something and it's just a standard musician, so I can transform my complicated min max structure.  Using do it to a simple Min Min structure. And that's, and then you can just use standard musician technique.  And so, so you can use language and duality to transform and Max two Min Min and then use standard tools and that has also be done for again but  Because then you need to convict city to get these kind of tools. It's not super that's useful but I mean it is useful, but it does not super applicable for again because these are usually non comics.  Okay.  Good question.  Alright, so let's take a 10 minute break. It is  340. So let's come back at 350  And I'll take more questions after the break.  Resume recording  Okay so Dora asked a question about a quote from Boyd, which said that dual function is the point. Why is in film have a family of a fine functions of lambda new  Thus it is concave, even when the original problem is not convicts. And so that was what I was explaining here. So if I have to come K function.  And I take the min of these too clunky function everywhere. I get this function here, which is also concave so  So this was the main between two functions, instead of having two functions, you could think of Avid an infinite number of functions. This principle is the same.  And so as long as you, you take the, the minimum is a you define your new function as the point wise immunization. That's all it means. I look at this point, I take them in between my two function.  And that's what my value of the function is defined. And so that's what you mean by point wise immunization. And so as long as all these function or individually.  Convex concave sorry in the variable that you care about. In this case it's lambda in new. Then there are the new function defined by taking the men.  Over all the possible function. So some sense here. You can think of x as an index. So for any fix x, this thing is a function of London and New only x is just fixed right  And so to get the function. The dual function as a function of lambda new I take the men. Where's that I take the men respect to x. So respect with this index of these function of lambda and new  And these function as a function of alumni new or concave because it's linear in London, and this is a complete function. It's connecting. Okay.  Okay that clarifies things door.  Is there any other question about like ingenuity. user avatar ezekiel williams  And crappy question about a point kind of further up in the entropy section. Sure.  There is it possible to scroll up it user avatar   Where do you want me to go user avatar ezekiel williams  Up so just be under or just above where you started talking about the grass duality.  I'm Jeff Rick here.  I didn't follow on the right, when you go when you have the box on the left with Max entropy inside it. And then there's an arrow over to the equation on the right.  Yeah, I  Guess it's probably a silly question, but for some reason I didn't see where the queue X went from the left hand side of this to the right hand side. user avatar   So, Q is a distribution which satisfy some moment constraint. So now I'm explicit sizing the moment user avatar ezekiel williams  Constraint. user avatar   We said that the expectation respect to queue of these  Feature function. So the expected expected queue is summation aroma possible that you were X of qx TG of x, right. So this is the same thing as so this thing here is just the expectation respect to queue of TJ of capital.  That's a rewriting of that.  Okay. And then on the right is the same thing, but with the empirical distribution.  And so these give me constraints on cute.  Okay. And, and I use a bit of meditation here, I says, Well, if you think of Q as a vector in our to the k. So, for every possible x, you have a value of x.  This thing here is just a duck product between the vector q and the vector TJ, where each component of TJ is what the value is at x right  And so there's user avatar ezekiel williams  These are linear constraints. user avatar   Which is why also you get a you get a. These are equality constraints. So he has to be linear, and other to have a complex problem. And so indeed, this is believe  Okay. user avatar ezekiel williams  Great. user avatar   Alright, so, so let me write this down. Because now we will derive the dual problem.  So let's do that and have your mind blown  So,  That's derived the dual problem.  For the max for maximum entropy  Using migrants duality.  And here we have actually a strong ability. So it will be any equipment problem.  Alright, so let's consider max and in the primal form.  And then will derive the dual form.  So we call the problem P for primal  So we want minimization over queue, which are basically in RK  And we have the kale between q and you right and so you have x is the uniform distribution.  Which is just one over the number of observations, but I'll keep you as you because we'll see what would happen if I replace new by age, how it will change a solution in the end. So that's why I'll keep it at you.  So the KL by definition is a submission over x cubed x log of x divided by UX right  So that's what I'm trying to minimize  Which we said was the same thing, minimizing the kale between Q AMP a uniform is the same thing as maximizing the entropy of cute.  And I need to write my constraints. Right. So I have the constraint q of x is bigger than zero for all x  And I have summation over x.  Of q of x is equal to one, because this is so all these is just saying that it's probably the simplex constraints.  And then I have my moment constraint which is summation of x q of x t g of x is equal to alpha g for LG  So this is my primal problem.  Okay.  And all these constraints here specify the set, em, right, the set of distributions, which I consider and then I want to find the distribution among in EM which maximize entropy or minimize the kale with a uniform  And to simplify the derivation. I will use the usual trick where I won't care about the inequality constraint.  So we will absorb  This constraint in the domain of definition of the kale.  Definition of the kale.  I eat IE.  I can say that the kale between q and you will be defined to be  Plus infinity if  Q of x is negative.  For some mics.  And otherwise, it will be the kale between Q AMP you  And the thing here is that the kale act as a barrier function right as Q become closer and closer to zero when you is not zero.  The kale would become increasing it will increase it more and more. And when you set any q equal to zero because you is never zero, then it will set  It will really be a repulsion point. So that's why you will never be equal negative. But anyway, even if you wanted to put formal the negative Q, you could just define the kale in this case to be  And so the same way as you know when we did the mid to normal, we also didn't care about the inequality constraint we just looked at the equality constraint and use the language multiplier method on that. And when we  Solve the greeting condition we had that by definition or solution, actually. Well, no, it isn't. But by  The property, the problems the solution already satisfied and equally concerned. That's why we didn't need to include it. And we've do the same thing here, though, if you wanted. You can also introduce the inequality constraint is just that you will remove them later anyway.  Alright, so now, so that means I have  These equality constraints. user avatar   To user avatar   Consider in our  Lagrangian function, right. So let's look at the language and function, I will have its would be a function of the primal variable which is q  Just a vector in arc of the game and then I will have a dual variable, one for each equality constraints and actually explicitly separate the some to one equality.  And you'll see why soon because I would get rid of. But so normally I only had like new for all my political science, but here I will separate it first. So this is for  Summation over x q of x equals one and this is for the other equally T constraint.  All right. And so, and these are my my my dual variables right new and so by definition this is the original objective. So I have the kale. So it's summation over x.  Q of X lug of q of x, you have x. That's just make yell. And by the way, right, if you're uncomfortable with the functional notation, because he was just a final vector. You can think of q of x as just queue X right  Which if x was an index between one and Kate will just be that that that that ice component of cute. And so it's just the usual vector notation is just that.  This argument could be generalized to also infinite set X and integrals and stuff. So that's why I keep the Qx, but think of qx I just the the X X entry of the vector cute. Okay.  Alright, so  I have the kale and Omar is asking, I don't understand why the dual problem should be zone and primal in general, it's not the case. Sometimes it is  So so so for this, you need to look at a lot of examples and constrained optimization, which I don't even have the time, but I already gave you two hints for why this would be the case. One is that  Suppose there is no inequality constraint in particular. Suppose that I don't have any quality constraints in the original problem.  Then the dual problem is actually unconstrained right so if you don't like constraint example like you don't know about content optimization  Well, you can just use standard unconstrained optimization and a dual so that's often simpler.  And then the other aspect is that the dual problem is always concave and so even if the final problem is not  Convex so that's also because it's concave, it will be better behave. So that's usually will be nicer okay but but there's a but I think this is done by doing a lot of examples of  Like orange jollity which I encourage you to do if you're curious and and and there's a lot of an invoice book.  All right, in this case will see that it's not really I think it's, yeah. Because maximum. They couldn't express with me. Seems a bit more direct than the original problem. So we'll see that the dual here will also be quote center.  Alright, so this is the original problem. And then I need to add my language with the players. So it's summation over j of new J.  And then I have my equity constraint which has to be of the form something equal to zero. Oops.  And so I will massage this I just put this alpha G on the left, so that it's equal to zero. So, I basically get and the site doesn't really matter. So I can actually put this on the right by putting a minus. And then I get something equal to zero. So I get basically I want alpha g minus  Summation over x as  alpha g minus dimension x x  T g of x.  Oh, I would write it as the expectation just simpler.  Well, it's improved so that is faster.  Okay.  And then, so that's the part about the quality constraints for with alpha j. And then I have this some to one constraint. And I said, I use the the see dual variables because I will get rid of it. So this is one minus summation over x. QX  Alright, so that's my language and function.  So to get my leveraging dwarf function, I need to minimize this language and function respect to X unconstrained  So if I minimize this respect to x, I will find stationary points. Actually this is convicts in queue. It's beautiful. So if I said that are equal to zero, I have the global minimum. And so let's take the derivative respect to the x entry.  And I want to set that equal to zero. So, this is equal to. All right, derivative of the first term, if I take the derivative of  Log have to have x, I get one over q and so it will cancel this ques will give one right so I can get basically one and then product rule. I need to take the route of respect to queue of x and then I just left with log. So I get lug of Qx, so that's derivative of the first term.  And then  Oh, sorry. I also had to you have x because q of x multiplied. So when I take the route of of this part this is I just get one. And then I kept this whole thing that's this piece and then  Here.  Perhaps I will rewrite it in clear form.  This here. Yeah, I should have done that. So that's right here. This is summation over x q of x t g of x.  A we go. And so now when I take the derivative of this respect to your next only one will give me for specific x i will just get T g of x. user avatar   Right. user avatar   And so, and, and so then I will have that for every j. So, this term will give me plus sorry there's a minus, because of this thing.  To get a minus here so I'll get  minus summation over j new j of t g of x. This is for X fixed. Right. And then same thing when I think of this here. I'll just get minus one on for the correct x and then I get see  So then I guess minus c  Okay. And I want this to be equal to zero.  OK, so now you just sold for q of x, like the only variable is Q Right. Everything else is fixed. So if you just manipulate that you get that Keith star of x.  To get the unconstrained minimization of the aggregate function is actually the you have x x of new transpose to have x user avatar   Trying to get my user avatar   Color. So this is new transpose T of x.  Plus c minus one.  Is just algebra and. And just to be clear, this this product here is summation over j new J TJ of x, right. So that's the meeting. So I put t t of x as a vector and dimension.  D i guess had D feature function. So this is it a product. And so we see here that to get the dual function, the distribution which define a dual function isn't expression of me right. This is an exponential family that's where the exponential federally appear  Which, by the way, one of the key to condition when you look at primal dual relationship is that you know the the primal variable that has to  Minimize the Lagrangian function to define the the dual variable, right, sorry, the  The language and function there like Grinch dual. Sorry. Yes. And so in some sense, if I knew what new star would be, I would know what the correct optimal function ear. He is right. This the optimal function here at depends on new user avatar   So I guess I could put user avatar   The notation here. It depends on new and it depends on seen this case.  All right, but so this are the, the variable which minimize the language and dual function. So by plugging in back. I can get. What is the dual function. So to get the dual function I just plug back the art Max with the argument. Sorry. So I plug back  Q star, which depends on new and see in the Lego engine function.  And so I get now a function which depends on both new and see  Which is just l have to star which depend on you and see new and see  So let's plug it in, what do I get  So this thing here.  It's this is the expectation expect to queue. Right. So, that's right. It's like this. So this is expectation respect to Q star.  Of and then I think the lug of q of x divided by you. Right. And so now the magic is is when I think this ratio. The you will cancel out.  And I have an x, and then I take the lug. So I'm just left with the thing inside the argument, okay.  And so what I get here is  New transpose T of capital X.  Plus c minus one as it temporary  New transpose T of capital x plus c minus one. And that was the log of q of x divided by you MX. So that's the key part of the objective  And now.  Bob, Bob. Bob, what do I have  I have  This piece, this piece here, which is basically V transpose alpha that's how the right kids for this is a constant. So, V transpose alpha  Oh, sorry. That's of these new principal alpha. And then this thing here.  I can  Take the some outside and take the this some inside  And I can rewrite it as  minus expectation respect to Q star. So that's the subtle respect to x.  Of  New transpose to mix. That's the some over j of new Jay TJ of x, right, which was basically  This piece here and this piece here.  Alright, so that's that's how I got this piece.  That set  And then I'm left with see. What did I forget to use temporary see if this works. user avatar   Okay. user avatar   And so, not unless we see. So I have plus c.  minus expectation respect to Q star.  Of seat right so this is the summation over X right so i just rewrite this summation of x as expedition respective Tuesday.  So expansion in respect to the star here was the shorthand summation over X of Q star six  Okay, so why do I do that well now it's clear. I can cancel a lot of stuff. Right, so I can cancel.  The see here, appearing in the expectation with this one because it's minus and then I can cancel the V, the new transpose TX with this one.  Okay.  And so what I'm left with is new transpose alpha  Plus C.  And then I have minus the expectation respect to Q star of one, right, which is this piece here. And so now I will rewrite it back as a some because this was just shorthand notation. So this is some over x Q star. OK, I will just write it in full because Q star of x.  Was in this form here. So I will just I want to know the dependence on new and see of everything right. So that's why I will just substitute the deck I get you have x  X of new transpose to x.  And then I will separate the sea dependence is X. Oops. user avatar   We user avatar   X of C minus one.  Alright so this basically is just this minus one here expanded  And that will use a bit on the station as well because this would come handy. So, summation of x of the UX expert new transpose the x is just the normalization of makes my family. Right, so I'll call this  And Zed of new because it's a normalization factor.  Okay. And so that's my dual function, it depends. Both of new and see. And that's what I'm trying to maximize respect to do and see  And to make the link with maximum like you that I will get rid of, see, right, because he's just to make sure that the the distribution. Some to one sort of a distribution. And so it will see, it's basically just the normalization constant  And then we'll get back to just as much of me, but let's do that. So first I will do max.  Of G new and see with respect to see  And it's actually, it's concave and see. No problem. I think the greatest respect to see  And set it to equal to zero.  That implies when I think that the rid of, of this thing over there. I get one for the first term in see. So this piece here.  Let me just finish that, and I'll answer your question, Ezekiel. So one and then I have minus  Y Zed of new  This is just doesn't. There's no CNN. I think the really the X of C minus one. I just get x plus c minus one. Again, times one, so x plus c minus one and I want this to be equal to zero.  Okay. And so now, is it get a single, how is the expectation of minus one, not just minus one.  Okay, because here. To be clear, this is shorthand.  For summation over X of Q Starbucks as  This piece here is shorthand.  For  Summation over excuse Starbucks. So if q is normalized, then it will just be one. But in general, Q star didn't have to be normalized right because  The constraints of see that the sum over  So for a specific see this thing.  Is nothing to see normalize but when you use the see which maximize a dual function, you get your input. See star, then it will actually satisfy the constraint and that's the relationship between the prime on that door.  So exactly so.  So basically by maximizing respect to see we get that the expectation of see star minus one has to be equal to one over A Zed of new  IE.  This piece here, the value of see here  Was just to make sure that the whole thing was not nice. As I mentioned, okay. So if I plug back this that UFC star.  Plug back see Star in over there. So I get that the maximization respect to see of G new and see is just new transpose alpha plus see star minus  Now I have said a new. I said, and x of see star minus one is just one overs Inuits I get one of those a new so these thing cancels out.  And  This thing is see star minus one.  And we said that experts, the star minus one is one overs at New. So this is actually equal to minus log of said you  Buy this thing, right.  And so the dual problem.  All this manipulation. So the dual problem. Whenever move. See, I'm just now left back with new is maximisation respect to new of are called detailed new when ca has been  Has been  Removed where g tilde of new is just  V transpose alpha minus lug of Zed new  That's my dual function.  So, now where's the link with maximum naked right so now I just need to put back all the elements. So the link  With  Maximum likelihood  I said if the constraint alpha was defined as the empirical moment. So, summation over i have to have x i.  Guess I'll use parenthesis notation. That's what I use before  So this is the expectation, as I said earlier, respect to the Emperor call up just to have x  Now, T is a vector function where each component is TJ.  Then  Detailed of new  Is just one over n.  Summation over my training set.  Of  New transpose T of xi.  Minus lug  Of Zed new  And this is just a log likelihood of the bunch of ideas, right. So this is lug of p of x i.  Given my parameter new plus some constant  Where  P x  Given new is basically  The expansion of feminine. So it's you x user avatar   X. user avatar   Of new transpose to have x minus log of Zen.  So the logins that new plays a role of a of new which is the log partition function. Okay.  And so I III, I have the dual problem.  Is max over New of detail that new which is max over New have one over n lug of the quality of my data.  Given new IE maximum likelihood estimation  Pretty cool.  So to summarize,  To summarize,  Maximum Likelihood in exponential family.  With  To Vic's  As sufficient statistics because I remember this T function was what was defining the essential family.  Sufficient statistics.  Is equivalent  To  maximum entropy  With mom and constraint.  On to x.  Were basically alpha  Is expectation empirical expectation of to mix.  And this equivalence comes from the fact that they are actually Lagrangian do all of each other.  And so in other words, Emily. He in the exponential family.  Is equivalent to moment matching  In exponential family.  Because the solution.  Has the distribution which which is a solution to maximum likelihood  Estimate has to satisfy the moment constraint, right, it has to satisfy that user avatar   Okay. user avatar   And actually that's kind of a beautiful results here which and we can read derive it also explicitly  So let's do that here.  So note.  So you have that  When you will do the maximum likelihood an expansion of family, whereas that  But  Yeah, so this is the function. You want to try to maximize respect to new so I need to take the gradient of lug of Zed of new as a function of new. So let's do that now.  After the gradient  Respect to new of lug of Zed of new. This is one. So remember that new the Zed is just a summation over X of you, x, x of  New transpose to have x. And so when I think the rid of, I get the exact same thing.  But it's a log. So I have one over is that new  That's from the log. And then I have great respect to new of summation X x X of new transpose to next.  And Papa, Papa, so I can move  The  Zed.  Sorry. So I take the green and respect to new so I will just get a tier of x, which will appear. So that gives me summation of x t of x.  By taking the derivative of this piece here and then the rest is the same, because of the X which kept everything else. So then I have one over as a new  Exp of new transpose to vex and then they have a Unix right  And so this all thing.  Is just P of X given new special family. It's the normalization of makes much of me.  And so I have that the gradient respect to new hoops.  The gradient respect to new of lug of Zed new is just equal to the expectation of p of x, new have to have x  I'm using the tool. user avatar   X here. user avatar   And this is equal to what we call the model, model moment.  And I'll use mew of new to call the model moment. So this is the the web, the model predicts the expectation of a sufficient statistics to be  And so now when I take the gradient respect to new of G detailed of new  I will get alpha, because it's linear, and in in new right so this this first p s at this piece here.  So I will just get so this becomes gets when I think the retrospective new I just get the alpha piece and the alpha was the, by definition, I said, was this empirical moment.  So let's do that. So I get expectation respect to pee and hat have to fix.  It. This is my alpha  And I'll give a name to that. I'll call that the empirical moment you had n because it's the empirical expectation  Of x  And then the derivative of the lug of new give me just new right so it's new of new  And I want this. So I want the gradient of this thing to be equal to zero.  So that imply that we want that the true model moment for a new star matches the empirical moment.  So that's what we meant by more than magic.  Right. So when you do maximum like Unix machine family to set the gradient equal to zero of the likelihood  You actually just need to find the parameters of expression families, such that when you take the expectation of the sufficiency of mystics respect for the distribution with these parameters you get the empirical moment. Okay.  Which actually we knew because by definition the distribution which has the maximum entropy  which satisfy these moment constraint will, it will have to satisfy them all and constraint and we said that the only distribution we certify the moment constraint and has maximum entropy is the exponential family which satisfy the one constraint. Okay.  And there's beautiful geometry on this, but unfortunately we're a bit behind in this class. So I will not cover that. But if you're curious, I say see lecture.  16 in  See lecture.  16 in 2017 for something called the kale pie Tigger in period theorem by so  Go Rhian CRM.  Which basically have this beautiful relationship between the between the uniform distribution, the empirical distribution. I think the exponential family, the moment matching in the in the right order.  All right, but this is a bit beyond the scope right now. So is there any question on this duality.  Between maximum, maximum potential for me and maximum entropy with woman constraints.  Can you come in a bit on doing optimization by solving by for one variable and sitting back in the form of other variable and so on with concerning partial the respect to all the variables sitting up to zero.  Right. So let's see. Where does this come from. So the question is,  Okay.  Blah, blah, blah. So here, one of the variable was Q star.  And then what I did to get the optimal function as a function of new  So this is a function of new and C and then I just plug it back in the original objective like this and then I get something as a function of new in. And so in some sense.  Indeed, when I think the third aspect of G respect to new. It's like taking the derivative aspect of this respect to Q star. And then the rest of us to start respecting yeah so you can, you could use this chain rule. That's fine.  This duration makes me think of the link between entropy and free energy in physics, the free energy is illusions transform of the entropy. And they also said to be 12 each other is that link farfetch. There is also a function coffee energy and ML. Is it related  Okay.  So, good point.  So it's a bit complicated, because the the genre transformation as is not using actually Lagrangian do it. It's actually using something called the genre or sorry, financial duality.  And the free energy coming in statistical physics, it's indeed coming from the genre transformation of some variables and, indeed, these are, have to do with Leo Zhang Dooley sorry and she'll do it.  I mean, essential oil at the same thing as the John's really T, but with perhaps infinite dimensional space.  But  Yeah, so I don't think so that the free energy coming off me is also coming to the fact that when you have statistical physical system you can have this Boltzmann distribution, which also has this exponential family form. So there's a lot of links with physics, indeed.  Now with maximum likelihood and user avatar   maximum entropy user avatar   There might be other links, but it's hard for me to, to, to be able to think about them on the fly.  With it. See, to be explored for you, Sean. Jacob is lust. So I'm now last in terms of what's amazing here cut out for a bit. What is moment matching. Am I supposed to already know what this is.  Kind of  So moment matching is the principle of estimating the parameters in my distribution by finding the distribution which satisfy the moment condition, which is that the true expectation  Of of your expectation of your model of some function is matching the empirical expectation. So that's the moment matching principle.  And what I said is, when you do a woman matching for distribution which are in the expression of family. It's actually equivalent to do maximal magnitude. Right.  And  And now I was talking about moment matching. It's both because I can just drive it, but also because I made the equivalence between maximum likelihood in the exposure family with maximum entropy with moment constraints, right. So, a woman constraints, by definition, will do moments matching  Okay, though, in this case, there's an infinite number of distribution which satisfied the moment. So you could not just do moment matching to find the distribution, because there's an infinite number of them. That's why you had to add the maximum entropy  Principle to find the distribution and the beauty here is that this is equivalent to maximum academics, much of me because  When we do maximum entropy. We have all distribution. We don't have to be exponential family and it could be any type of distributions.  All but the only thing we we specify is all the have satisfied. Some woman concert  And now it turns out that the maximum entropy distribution has a very specific form. It's actually an expansion of me.  It's an explosion of 70 with social business statistics which are coming from the moment constraint that I okay so that's kind of like the what's so beautiful about this result is both that it makes interesting link between two different learning principles as well as use these powerful  convicts analysis tools which are leveraging Dorothy.  But, you know, this is non trivial thing you probably have to go back over the notes to kind of like let it sink in. But I think that's a very powerful result.  And is it God is asking, can analog the result be formed when the support is comfortably or uncomfortably infinity.  And we probably can, yes. Yeah. So, so the math is a bit more complicated because now you're working with internet dimensional object, but  You can actually have this similar formulation. So if you if you had if you're optimizing over densities and you're trying to find the density which maximize the entropy  You would get that the solution will be also an exponential family. And so it will be the same thing as the maximum likelihood estimate in the corresponding special family.  And know that if you replace you.  So all this was I do the kale respect to you and you got that this was the base measure or the multiplier and exposure family if you replace scale.  Respect to age zero, you will just replace this you with ages zero. So that would be just a different expression of me this generalize explain a maximum entropy problem and user avatar ezekiel williams  If I may ask, I was. I was just wondering if if you move to the continuous, the case of continuous distribution, then you can you can no longer be  Basically minimizing the kale divergent with respect to  The uniform distribution. Right. user avatar   Now, and on this case you just maximize the entropy directly  So the kale. I kept the kale here because I want you to see the role of the uniform distribution and, in particular, that if instead of minimizing instead of just doing maximum entropy. You do.  Use generalize entropy and replace this you with h zero, then the H zero will appear in the in the sequential  But, indeed.  In the case of like continuous distribution, you will have to be careful because this scale is not defined because the uniform distribution oversee all the real is not defined. user avatar ezekiel williams  And correct me if I'm wrong, but then this basically shows that if we're finding the maximum entropy solution to any kind of an inference problem, it's always going to be an exponential distribution. user avatar   All the if  Your constraints are specified using this sufficient statistics.  If there's other types of constraints in particular, you could have constraints which are not to see linear in queue and then you will get something different. user avatar ezekiel williams  So so effectively with using the moment constraints.  We're always user avatar   There's always be correct and that's user avatar ezekiel williams  Kind of user avatar   One of the beauty of this result. user avatar ezekiel williams  Cool, thanks. user avatar   And we'll Mara is asking, does that result kind of justify the beauty of explanation of me distributions, in some sense, yes.  That's kind of like, give it another place to to get it.  Right. So you like you could say, oh, I don't like maximum accurate. I prefer maximum entropy. Well, if you do maximum entropy with moment constraints, you're basically if basically doing maximum next year. The next measure of me. So that, yes. user avatar Sarthak Mittal  And someone I had a follow up to the question.  When we did in for example of previous assignments invaded MLA, etc. We took the gradient with respect to all the parameters and said that to zero.  You're in debt of doing something like Emily in caution. You could also do take the derivative with respect to sigma square will get a term in terms of meal we put it back and then to segregate with respect to meal. So what's the difference in both the user avatar   Settings. user avatar   As well it's equivalent. So, so basically you're saying that. So what you're talking about is  Like there is  I can do min respect to say  New and sigma square have some function.  Of new and sigma square. And so the solution will have a degree in respect to new and sigma square has to be equal to zero.  And you say, well, I can just define this as men with spectrum you have F tilde, which is just a function of new where I have already optimized respect to sigma square as a function of new and plug it back in the objective right  And now I take the gradient of that perspective you and  Yes. That should give the same. I mean, when things on unconvinced, it's more complicated, but in this case it should give the same results.  Okay. Oh, I got to wholesale because who appeared, because I have a meeting with with Christina. So, all right. So, uh, that's the end of the lecture, I think I already went a bit over time.  Hope, hopefully you have enjoyed the this this school duality result and I'll see you on Friday. See
  So,  At the end of last class I  Basically presented you this powerful result of the equivalence between maximum likelihood in the expansion of family with maximum entropy with woman constraints. And so today we will now present properties of exponential family as well as its formal definition and much more details.  And  I will also talk about how to estimate the parameters in prophecy graphical model. It's actually pretty simple. But it would go. We'll see the exponential family in glory details and  Estimation  Of the parameters in a PG AMP.  And we'll see that a lot of things are intractable. And so that will motivate also to do approximate in France, which I think I'll be able to start with sampling as an example of approximate indifference.  So that's the program. Let's start with the exponential family.  Spinning show  Family  And so, again, the motivation of talking about the expansion of me is that it's kind of a way to to present pretty common Patrick families in a unified way and show cool properties about it.  So what is a bunch of family.  It's a parametric family of distributions and I will see a flat or canonical  Explain the show family.  On some sample space script x  Is a parametric  family of distributions.  Defined  By two quantities.  So,  There's two things we need to specify to get the Patrick Femi, the first thing is called basically a reference measure. So it's h of x.  The new of x.  So this is called the  Reference measure  So this is fixed. It doesn't depend on a parameter. So this is coming into all the family and it tells you. Also, what is the support of your distribution.  And so this is called so this he is called the reference density  Because it's coming to all the members in your distribution and demo of X is called the base measure  Which tells you  The density is defined respect to which measure and I will  Mention two possibilities here.  To simplify like we want consider any other possibilities, but either it is the counting measure  Which basically means you have a PMS. Right. It's not a density to PMS.  So if you don't know what I'm talking about when I see the Come, come, counting measure, do not worry.  The important thing for you is that this means it's a discrete random variable, and that age of x for all possible values of x in the discrete possible set of  Facilities will tell you what's the base was the, the overall PMS for each point and then we'll have the expansion of family peace to modulate according to parameters and nature of x. In this case, usually just one, for example.  And then, but yeah. So, so this is just to deal, both with a discrete in the continuous. In the same way, and then it could be the bag measure when we have  In multiple dimension. When we have a continuous random random variable.  Alright, so that's basically something you have in common for all the expression of me.  Yes, so each man is asking what is meant by flat here or there are other types of expansion Geminis yes there is something called a curve as much of me. And so I will define it later. So for now, flat basically means actually the set of parameters is actually a full  Euclidean space, in some sense, it's not a curve manifold, but we'll get there.  So first component is this reference density second component is the sufficient statistics. So the sufficient statistics, it's a function from your possible observations to  Win us RP  So that means we have p dimension in your submission statistics. This is called the sufficient statistics Specter. user avatar   Sufficient statistics. user avatar   Vector  And I also use the terminology earlier and especially when we talked about the the maximum entropy framework. You can also think of it as the feature vector  The feature vector, which tells me what are the features of the possible observations that we use in our model feature vector  And  Then given those two pieces.  The family is uniquely define and so the members.  Of the family.  Will have  Distribution and I'll use annotation.  P of x.  Which depends on the candidate called parameter at that. That's where the flat and Kennedy call coming in.  And I will use annotation with  The base measure, just to because it will appear.  And so it is exponential of at transpose to x.  Minus the log normalization constant to make sure to distribution. And then I have HM x. The music's right and so the defining pieces.  Of your  Expression of family.  Oops. Are these two pieces that I mentioned. So it's the reference measure that to us as well as the sufficient statistics.  So these are the defining pieces.  And in some sense, implicitly, I assume, also, that I had like a sample space right for x, where these age of X is define and  The base measure and give you a mix.  But the other pieces are coming just from the Patrick family aspect. So basically,  The, the way the parameters entering the parametric family when it's flat. So that's where the curve aspects, if it's curve. It won't just be at that  It could be more complicated, but for using just the parameter directly here in front of this efficient the six. So this is called the kind of Nicole parameter 10 the call.  Parameter  And so when you think about the gash in where the parameters are the meaning of coherence. These are not kind of default parameters, because they don't appear just in front of this official statistics directly  And  This thing here is called the log normalization right and it's it's defined by just normalizing, your family, your distribution. Sorry, Norma lazing  Norm normalization.  Or it's also called the log partition function.  Our. It's also called the Cumberland generating function cumulus and generating function. These are all synonyms for the same thing.  And so that's the definition of an expert flat exponential family, let's go here. I guess when I zoom in doesn't fit. user avatar   There we go. user avatar   Alright, so there's a few questions here. Are there any scenarios. When, when will not use accounting other big measure. Sure.  If you would like to describe it density respect to something else in the bag manager.  And also will will have mixed distribution like if you have a mixed distribution. So, for example, you want to you want your base measure your base reference density to be a point mass at zero and see a gallon  So this is not a density respected a bag. It's a density respect to both in the bag, plus the contact measure on one. So it's a kind of a mixed  Density based measure and then you could have you could put this an Ex Machina family if you want it. I can and then you would just specify. What's the effect of the of the  Sufficient statistics.  What is the point of writing out the mule of x. If it gets cancelled. I'm not sure I understand what the value of x represent. So this is actually the formal notation for integration, when you have a respect to a measure  So think of it here as just a way to both specify something as a PDF or PDF. Okay, so if you know about measure theory, you would know what it means. If you don't know about measure theory. Well, it's just  To be kind of rigorous here and clear that we're def expansion a feminist specifying  A series of densities with respect to some base measure. And so I need to specify, what's the base measure. And so by writing it.  On both on the left and the right, you know, I've included this information.  So somebody is asking whether a demo of X define the support of the distribution.  So do you have the X define the, I mean, so the whole  So this whole thing define the support of distribution rights because if I said ah of x equals zero, even let's say them you have access to the bag.  Well, the support is all real life, but I could specify age of x is zero and negative numbers. And then my support is only the positive number. So it specified by both did you have x any tricks.  So for example, if I want to have a PMS on the positive number. I have the content. I could have to contact measure on all the natural  All in the sorry on the call z with Z that's wrong numbers are natural and positive I forgot. Anyway, the number is minus 101 etc up to infinity, all these numbers. I guess it's the natural numbers.  And  And then you could boost our the integer. Sorry. Okay, thank you. I was like, what are these  Yes, that's wrong numbers are positive and integers or can be negative. Thank you. Wow, I can't believe I forgot these things.  Present and getting old. But yeah, so this so so the integers. So they're putting the cutting measure on the integers will give you  A part of the support, but then putting h of x zero on all the negative integers, for example, will restrict the support, even more so you need both to specify the support  Ticket Jacob is asking at that as a parameter, along with G and H. Right. And for me, what is the reason that is called clinical and nothing to do with specifies the family.  Yeah, so T is not a parameter. So, T is something which will define the properties of the family, the parameters in the parametric family or at that. Okay. And it's called candidate called because the way to peers in the in the distribution is is  There so that now I will specify also example of non clinical family to clarify the rule.  Okay, so some people are realizing that Jacob probably copied from somewhere else.  To have it appear in the comment as a real Greek number. Okay, so let me just explain a bit more properties and then it will answers this question. So if  Or a simple space is discreet  Then  P of X eta is a PDF  And if or a sample space is continuous, then this for present a PDF  And so now  The other thing I want to define is that we want this density to be or PMS to be normalized, so we want that one is equal to the integral over the sample space of p of x at the debut of x.  And if new is the cutting measure just replace that with a some over x. And so that means that we want the integral over x of x.  Transpose to have x  And then I will put e to the minus A attack. That's the look partition function of x, demure x  So we will all this to be equal to one. So, if I now solve for A of data that implies that for everything to be normalized Ayurveda is equal to the log of the integral over x of x.  Transpose to have x h of x dim Unix  So that defines our luck partition function. That's why it's not part of the family. It's defined by if I if I specify to have x and h of academia of X. Everything else as well defined.  And by the way, this I use annotation already last class. So this is  Also what I use a z of ethics right as a function of the parameters, it's the  It's done.  The normalization. It's also called the partition function. This normalize your of your, of your distribution and the z will be the same, which will appear in Eugene when we make links with under a graphical model. So that's also why you use, use the notation.  Alright, so now where the Kennedy call will come into play. There's this thing called the domain of my exponential family.  So the domain, unfortunately, it's called Omega. It has nothing to do with the sample space. So let's be clear.  This is not the same thing as the sample space for my random variable. This has to do with the parameters. So the domain of my exponential family of my clinical expansion family, by definition,  It is the set of clinical parameters.  For which you can normalize the distribution. So for wish when you could compute this integral over there, it will actually be finite.  So, and these are basically the valid parameters.  For the expansion. So set the parameters. user avatar   For the user avatar   Guess. There are called Kennedy called  A vetted Kennedy go  Parameters.  And so I will mentioned already that this is a  The lug one can show  So note that the log partition function, one can show that it is always convicts  In  Africa.  And so, that implies that the domain of its of the function Ayurveda is also comics.  And by the way, this domain is something which comes from context analysis. So in context analysis you if you use your work with function which might take the value plus infinity.  And then the domain of the function is just where the function is not infinite it's finite gain. So here you can think of a of it as being  The thing which we use to normalize our distribution. And so sometimes it will when I will  When basically computing this this summer. This enter goal would be infinity. And then I can normalize it. There's no way to make it meaningful this division. So then  I could just define a beta for needs to be plus infinity. And so this would be the standard Convex analysis domain. And it's actually a convex it  Alright, so now let's talk about this curve expansion of family. To clarify, so more generally.  Instead of talking about the Kennedy called parameters we could use another set of prime position, we could consider  A repatriation.  Using a subset  Of the family.  So in some sense, the chemical parameter ization is is the one which gives the, the biggest number of distributions and my family because basically, I consider all the possible distribution which are normalized evil.  But I could decide to specify a subset of this family and to use it as my family. And so what we'll do is we'll, we'll define some mapping  And I'll use data to represent the mapping because the image should be the one we use this chemical parameters and so we'll use some set of parameters which we call capital eta and will map it to the set of possible parameter for my family, which is a subset of my domain.  And so this will be my new set of parameters.  And in particular, in the gash in this could be the mean and the variance to be, you know,  So this is a new set of parameters.  to index the element of my family.  And then how we define  The density on x, given the power of theta, we will just use, what was the density. When we for the corresponding clinical parameter associated with data right  And so that's our new family.  That's a new family.  And then  Sometimes it's the same set of distribution, just with a different parameter ization sometimes a different set of distribution. And then what is called a curve, especially fnb  Is when this set of parameter  Is actually not a nice Euclidean space.  It's a curved manifold.  Exposure family.  If  The image of our mapping in the clinical space.  So if  If  If I looked at all the possible chemical parameters associated with my my parameters data.  So if this image is a curved manifold.  In  Omega. Okay.  I want define formerly what occurred manifold is but basically you can think of it like, oh, it's a surface that say you in 3D, instead of being the full three dimensional object. It's actually now.  Surface, which could be a plane plane is not curved. So now it's more like a curvy surface, for example, so  So that's not really described with a flat object that's why it's not flat. So that's the position of flat clinical family versus a curve. It's my shop me. Okay. And so what's an example of a curative expansion of me well. So here's a concrete example.  It's a bit abstract, by the way. I mean, it's a bit. It's a bit. Not sure how many people I've ever used that in. I think people might have used that in statistics sometime, but you could consider gal shins.  So let's say you could consider. So the sessions are a subset of X much off me and and in this case where you will use  The parameter for you. Gotcha. And you will only have one parameter, which would be the me and then you would use the variance, you will say the variants will actually be the main square user avatar   Okay. user avatar   And so  It turns out here. So the chemical parameter or related to the mean and the variants. But in this case, you can, you can see that it's a one d  It's a one. The curve manifold into the space which is the set of possible clinical parameters for this family.  And where does this matters is that  Some properties of the expansion of any only valid when you have a non curve. It's much of me because you need you to be able to take the route of and and move in all directions. And so if there's a if there's a manifold, then you can do it.  So small demand says so they said, I'm all points in Omega given by at that have failed. I guess is not, for example, using equations. Correct.  Does it mean that, for example in 3D space that curve is locally 2D  Yeah. So I think, I mean, so, so you could have. It's okay to have a full dimensional subset  Of the space because indeed like it. We will see that in the case of, say, the Gaussian will need the variance to be positive, right. So, so in this case the possible candidate called parameters or a half plane.  So, so this is in. And so we do dimension and a half plane is two dimensional, even though it's not the whole space. So that's fine. This is not curved. This is the flag.  But to have something curvy, you will need. You will need to be a lower dimensional object in the full space and this lower dimensional object cannot be just flat because, you know, you could have one. The line is still flat was a curved line is not flat.  Okay.  So that's a charisma chef me and I will mention a very important point here.  To note is that  The same way that when I talked about the graphical model right I said a graphical model is a set of distribution. user avatar   So, user avatar   So a specific distribution is always in any in some graphical model, in particular the graphical model, which is the fully connected graph contains all distribution. So, any distribution is in the graphical  It's the same thing for an excellent show me any distribution can be made in six months, if any trivially by just using h of x as a distribution, right, so any single  Distribution.  P of X can be put  In  An exponential family.  By using  As a reference measure h of x px right and so in some sense, this would be dysfunctional family where when you set at the equal zero. So it doesn't matter what the sufficient that this is  You said it equals zero all your you get his age of x. Well, then you get p of x. So, this PR x is always in is in some expansion of me.  So things whether some some specific distribution is an X much family or not, does not make much sense. What you want to know is, is a parametric family of distribution and exponential family is a set of distributions.  Can be characterized by an expression of me. So that's the meaningful question.  And here's some example of me, which are not indexed much of me.  Or not characterize exactly by X much money. So two examples.  Of family.  Which are not  exponential family.  So a mixture of actions.  So one mixture of gushing with specific parameters is an extension of how many because you just said he likes to be the mixture.  What I'm saying is when you take a mixture of two options and you very demeaning the covariance, you get a lot of different distribution and there's no way you can  You can put this an expansion family because it's functional family always have kind of this nice union model structure in some sense.  So you can have multi modality from each of x, but you cannot have the multimodal at varying with the parameters which is what's happening when you have a mixture, Mom.  Okay. And we'll see later that like maximum next to the next question family's always convex optimization problem in the chemical form. And so the fact that when you do maximum actually a mixture of Goshen. It's not convicted also highlight that you can pull in next bunch of them.  So mixture of guns or, more generally, like latent variable model. They are not user avatar   Inexpensive me user avatar   And another example is the uniform distribution on zero, theta.  Because what's happening is when you change the parameter theta, the support of your distribution change according to your parameter was index functional family, the support is fixed. It doesn't depend on a diet specified by  This thing here. So the parameter here is always within an x. So, because it's within the experts will always be positive, it will never be zero.  You're not allowed to put plus infinity parameters and substance or minus and fee.  So Jacob is asking, Is it true that requiring the variance to be larger than the main square would still be a four dimensional space that's still isn't that user avatar   You user avatar   Think we're getting into topological question here.  Because so so you have a full dimensional object. In this case, which is 2D, but it has a curve boundary. So that's your problem.  So for me this is definitely flat is just, yeah. So, so I would still say it's fat because it's not the boundary which matters because in particular will see that that this space here is convicts, you know, a ball, for example, is convex.  It's a complex set and  I could have that I that say use a full dimensional ball, I would still say this is like a flat object. user avatar jacob louis hoover  And this is the convexity that matters not the dimension. user avatar   So no, it's more like the user avatar jacob louis hoover  So, user avatar   It's the. So if I have an object.  Let's say, let's say I'm into the right, for example. So this is a 3D object.  And locally in the interior of the subject it. I'm always full dimensional so and so I look Euclidean everywhere and decide inside this object. So that's why it's still flat, right. So, so the flat versus curved is more. What's the local structure around any point in my object and so  So that it's not the boundary which matters. It's the inside of the set to say if it's flat versus curves.  Yeah, so will I will answer. Perhaps these questions. A fifth you aspect, this question about services that also. I will get to the multimedia because we see also stuff happening.  weird stuff happening when the multimedia. And actually, I guess it's right now. But let me just be a yes so make sure I've got shin.  Is not in uniform is not because the support changes with a partner. That's why they're not as much but Gaussian expansion of our next bunch of family bear new You betta the Irish lay  loveless you know all your most of your standard name distributions.  And alright so let's do an example which will hopefully start to clarify.  So let's do the multi know you  As our first example.  So let's say I have x, which is a multi Know ye  With parameter pie. So what's our sample space, I will use here the sample space encoding where it's zero. It's one hot encoding there one to the k  And the possible sample space is the intersection between the property simplex and X okay so I use one hot encoding. So this is just a one way to quickly say it's one hot encoding.  I only one of the  Coordinates can be non non zero. They're all zero, except one which is equal to one.  Alright, so now or parameter pi.  Belongs to the poverty simplex right so it has to some to one, it's positive.  So now to put an expression for me. I need to already. Suppose that all my parameters are strictly positive, otherwise I will have some issue because I will take a log of something  So suppose not. Now I look up my all my parameters and they're strictly positive  Then I can look at the distribution for the multi multi Nui  And you can think of pie here as theta, right, think of as so i'm not i haven't given the chemical formulation yet. So this is just  Given as a parameter that you had to Patrick Philly from the material and I will try to see what, how can I make it an expression of me.  So this is just product over my possible  Observations probably to have this observation and then raise to whether I have observed it. So that's the trick. And so what I do now is I just put that in the exponential form by taking the log. So I to exp summation over j  Of lug of pages xj. So the exchange goes in the front, lot of pages. And that's where I had to assume that page by Jay was positive because log of zero is minus infinity. And that doesn't work, you can just put minus infinity. Some we use any final numbers there.  Alright, so what is this. Well, this is x  Of summation of OJ xj lug PJ.  Minus zero. So it's already normalized because I started with a distribution, which was already normalized by the parameters. And so what do we have so here we have  That the mapping  You can think of the chemical parameter associated with pi.  Is basically login of pay. G.  Right, because the thing in front of this deficient statistics here to sufficient statistics is just x, right. So, so you want X have some function of x here was just each component, then some parameters and then the log partition function.  Okay, so we have that this, these, this is the mapping from the parameters to the clinical parameters. My sufficient that they six is X my base measure here is just a contact measure  On x user avatar   squared x. user avatar   The reference density is just  Basically one or it's basically  The indicator of x belonging to the true observation space.  IE.  One x  Belongs to property simplex  Intersection skeptics. So you hear. Here you can see the difference between  The base measure and the the support. Like, I decided to just put  To count stuff on all possible vectors binary vectors on zero k, right.  And then I you, as I said, the polity, hmm, x to be zero for all binary vectors which are not correct one hot encoding.  Sorry, it can soak and so  So basically I use HIV next year to really define the support of valid one hot encoding.  And now, what happened to our luck partition function.  Well, so what happens here is that or parameter space.  Is  To do this transformation we suppose that all the pie were strictly positive. So it's the actual interview interior of the property simplex  And then  It turns out that the partition function.  The luck partition function evaluated for the parameter  Output of this mapping is just equal to zero for all pie in my  parameter space.  Okay, but that's weird. So why why why is my luck partition function zero. Well, here we have a problem of dimension mismatch. Okay. Because the candidate called parameter  Here because there's like K dimension. So because there's key components of x. So my kind of called parameters dimension key, but my parameters space theta.  Is known dimension kids and they mentioned k minus one i mean it isn't dimension k, but it's dimension as as as a set is k minus one.  So here  Where's my blue. So here, it turns out that if I use this as my sufficient statistics my sample space is actually all of our key. Okay.  Here.  I could use  I could put any value. user avatar   Okay, so user avatar   So I could put any value here, I can always be normalized because it's only have some over key objects. So it's very simple to normalize user avatar   And so user avatar   So theta.  As I said, because it's probably simply excellent key element as dimension k minus one.  As a set user avatar   Okay. user avatar   And  At have say that will have the same dimension. So this is k minus one dimensional object, whereas omega my sample space, as I mentioned, key. So there's a mismatch here. So basically the when I looked at this personalization. I don't have all the possible Kennedy called parameters.  Even though I can actually get all the possible distribution that they care about. So here, what's happening is that are sufficient statistics is as properties that this call. It's not minimal so we do not have  A minimal  exponential family.  So what does it mean to have a minimal expansion of me.  So,  It's there is no linear represent no relationship between your sufficient statistics.  Because here.  What's happening is that for any x  Such that h of x is not equal to zero. So, any x which is on one hot encoding.  We have that the summation  Of of the component of your sufficient statistics.  Will be equals to summation  Of x g, which is equal to one.  So there are some constraints between your sufficient statistics coordinates.  For all the in your support. So for for all the elements which are valid, which has an under polity, you have that some linear relationship between your sufficient ethics or  Are satisfied, which means that some says there are redundant. I don't, I can have very different component that my sufficiency physics independently. If I specify the value of x j for Jay smaller than cake.  The cake component is automatic specified by just doing one minus the some of the other ones. So the case component doesn't tell you anything. So that's why it's not. I have a redundant representation. Just why it's not called  Minimal so basically if the definition of being minimal is that you don't have any a fine linear dependence.  Between the component user avatar   Of tea. user avatar   Okay. user avatar   Because what's happening is when you have this finding their independence. There are multiple at as  Which will give rise to the same distribution.  And so this is what we call over parameter ization  Because you know there are multiple parameters which gave the same distribution. So in some sense, I use too many components of parameters that I needed to  And so when you have this overcentralisation or these linear relationship between the solution statistics you have something which is called not a minimal  Expansion.  Last week, there is asking, what does it mean to be what's, what's the interior of a set  Not important  These look at Wikipedia.  This is the key. The the  The interior of the set is  That's when you start to do metric space and everything that goes topology. It's the point where you have a neighborhood around them. So, for  And so that's one way to look at it. So any point in the interior of a set, I can put a ball have small enough radius around it and it's still in the set. user avatar   So that's one way to define the interior user avatar   Um,  Alright, so now  How do we get  Minimal exponential family for the multi. Well, we just redo we just remove one of the dimension in our sufficient statistics. So for a demo to eat.  One way to get  A minimal next bunch of me.  Is to define the sufficient that this takes as just being the first key component  That k minus one component. Sorry, because the case one can be always because it's one hot encoding the case one is just one minus the some of the other ones.  And so then the log partition function.  So in this case, then all kind of capacitor always give unique distribution.  And so you have that I can get the  partition function by just normalizing my distribution. So I need to some over my sample space of exp of at that transpose to vex  And so  This just select which entry have observed. So this is summation over j one of two k minus one of  Exp of f g  Whereas when I observed  The case, the case possibility, x one, two x minus one would be zero. So, I will just get expert zero, which is one. Right. So the last possible observation gives me one. Okay.  And so now, that gives me. So the log of this is my my normalization function right so now it makes more sense in this setting, so have that my  Clinical version for the move to New he will be exp  Summation from Jake will want to k minus one to two k minus one dimensional me at j exchange. And then I have minus the log partition function which is log of summation over j  E to the letter G plus one.  So Dora. You want me to explain why the fact that we have redundancy in our sufficient statistics is important. So, so what's important is that some properties of expansion feminity require minimal exposure. If I need to be true.  And moreover, if you want your parameters to be identifiable. You want to be able to estimate a parameter from data. Well, you need to have that each parameter is associated with a unique distribution.  But when you have a non minimalist much of me when you have redundancy or over prioritization.  There's multiple parameters which give exactly the same distribution. So, then these parameters are not identifiable and then. Moreover, some of the perhaps to political properties of the parameter space.  Will make things we're so so we'll see later when we require a minimum explanation of how many for a property to be true. Okay.  And so in particular.  So this is our. So this is a beta here by the way a beta  In  If you recall, I mentioned when I talked about the  The  maximum entropy and taking the gradient of the log partition function as much of me. I had computed the gradient respect to have a of data and I showed it was the expectation  With pick two p of x of the sufficient statistics. user avatar   Right. user avatar   And this is actually valid.  By the way, this is valid for when it is in the interior of the domain.  So when you're at the boundary weird things can happen. You can have treating to see take this gradient, but in the interior of the domain, things are fine.  And so  If I compare to what the A of it. I have here. So for the multi new G.  Let's just verify this property routinely  We have that the derivative of a of it as respect to energy  It's one of z. Right, so  Basically this thing here is z. And I have a log. So when I think the review of the login will get one of z.  Hips, so I get one over zero, beta. And I think the derivative inside of z, which is a son of e to the editor. So then I'll just get basically he to the energy that's the only one which is non zero  And what is that, well, that's just by definition, the probability of x being close to Jay right this is a discrete observation, even after  But what is the property. This is the expectation  Of p of x.  Of T g of x, right, because TJ of x was basically just a zero or one.  And  It's one. When x is equal to Jay. And so when I think the expectation. I just get the probability that X is equal to specific value.  And so this is as required. Right. So basically, I've just re verified the properties of these of these things.  So start attack is asking if there were some other restrictions and Tina defining error with would still be enough minimal exposure. How many  Say if if the if the ratio between the services that the six is nonlinear like let's say the first component square plaza second Copeland square is equal to one, for example.  It's not, it wouldn't be a problem because the fact that it becomes a problem is that when you can really scale things. So you can use the linear representation of the of x and then just change the parameters.  At it in a linear manner, and then you just get exactly the same value. But if it's nonlinear, then it will it will change the priority distribution. So the it's so yes, you will still have that every eta is associated to a unique user avatar   Distribution. user avatar   Okay, so that's our first example.  Somebody is asked. Oh, why is this as required. So I said that I want.  So, good question. So I want that the derivative of my luck partition function is equal to the expectation of the sufficient statistics.  So here I explicitly computed the derivative or my luck partition function and I got that the solution was this thing. But this thing is the same thing as the expectation of my sufficient statistics and thus I verified.  You know, I have verified this property, basically. user avatar   That's what I meant. user avatar ezekiel williams  Um, can I just follow up on that question.  Sure. I'm sorry I maybe I missed something earlier on. But why do we need that the derivative of the partition function is equal to this. user avatar   But something we proved and the last class I just  Basically it's not too hard, that if you take the definition of your, of your leg partition function.  So this is a definition of your luck partition function. So when you take the derivative of that you get one over z. And then you get  To have X, which comes out in front of that. And so you get the expectation of the  Of the sufficient statistics like the tix so just just by definition, you can show that the gradient with respect to  This give the expectation of to vex respective distribution and now we've just verified in a specific case for sanity check, you know, just to show that things are all consistent user avatar ezekiel williams  And the first the first only hold for exponential distribution. user avatar   Well, so, I mean,  Ayurveda was only defined for the exponential family so  So your site, depends what you mean. Right. So, so, which function would you define for something which is not an expansion of me so user avatar ezekiel williams  I guess will be the log, you could find a lot partition function for an exponential family couldn't do worse, but not even because user avatar   Yeah, and I don't really see what how I mean you could define  You could define that for any distribution, which is not normalized, you could say, well, I define this log partition function to be the some overall the states of this thing.  Or the integral over all possible X of this thing. And then you take the lug but it doesn't have to be in speed in exponential  Then I have a then then that this gradient might have any property. I'm not sure. So yeah, so I know this holds for six months with me.  And we'll see you know the link with the log partition function. So after the break. I'll also show that any discrete undirected graphical model with positive distributions can be put in a bunch of empty right so  And it will be the same partition function as the one appearing in the  Z will be the partition function appearing in the gym.  All right, let's take a break.  To 29  So let's go back at  239  Alright, so let's do a another example with the one the ocean to put in a bunch of me.  So example to  The one the Gaussian  Alright, so now I have a gash in with mean new in sigma square my sample space is are my parameters are basically new and sigma square and we call this the moment prioritization, because these are the  Expectation of X and expectation of X minus new square. So it's is it centered moments.  Parameter ization  Whereas the candidate called pasteurization is a different one. So let's put it in next month. I'm the forum to identify what are the chemical parameter. So this is one over two pi sigma square, square root of x.  Minus x minus mu squared divided by two sigma square  That's my density. So now I want to kind of like isolate stuff. So, this is x  I will look at my minus x squared away two  Times one over sigma square  Then I have the plus two x news. So let's write it as two x or guess there was a two which from the denominator which cancels, I get plus x times mute very by sigma square  And then they have stuff, which doesn't have any x. So, this is basically the log partition function. So it's minus then I have new squared or by two sigma square, then I have my normal user in front, which was  Basically nights in the exponents. I took a log. So I get to one half in front of it lug of two pi sigma square. That's one way to write it.  Here we go.  So from this formulation we can now identify the sufficient statistics. So this official statistics would be x and then minus x square divided by two.  And we can also identify what are the corresponding clinical parameter mapped from more time physician.  So basically the first clinical parameter  Associated with the this x is this thing here. So, this is  New divide by sigma square  And the other one is one over sigma square  And so we would call these two as at that one at the two. The two component of mechanical parameter  And so we have that at a two is one over sigma square the inverse of the variants. We already talked about it in the past. This is called the precision. It's called the precision.  And it has to be bigger than zero otherwise.  You won't be able to integrate the gusher to give it plus infinity and at one is equal to two times view, basically, that's their relationship.  And so if I looked at the domain of maximum and family here. It's the set of at one at a to such that at a two is positive, but at that one could be anything right to them the normalized mean it could be anything but the inverse of our and switches the precision has to be strictly positive  And in this case,  You already had a minimal so so there's no difference compared to the new you already had the correct space of dimension for the parameters of my, my gosh.  And we'll see in the last few lectures at some point we'll talk about Gaussian graphical models. So we'll talk about the booty right Gaussian. So we'll see later that for the multivariate Gaussian  The clinical parameter is the precision matrix.  And  The it will have both and precision metrics and something which is which we call just at Abbott, it is actually the whole thing. Normally, but that's the standard  notation for the clinical position of the expansion of how many of the Goshen. And then what you take is you take the precision matrix and you multiply them, you  Right, so here  In one D you had this is the precision and to get the other parameter you take the, the precision and you multiply them you right so that's the analog of this, but in one day.  And so this is inverse of covariance times. So that's how, from the moment, Pam position, you can get the chemical composition and in this case the clinical sufficient sufficient statistics is both x and minus x x transpose that by two.  So these are just some statistics.  Here, but we'll see.  Any question about this.  So you remember when I did the multivariate Gaussian  And I showed that the maximum likelihood parameters is the you said the covariance to the empirical currents  I took the derivative of the log partition function. But I took the derivative of the love partition function with respect to sorry the love that they love the French version, but the the love determinant but respect to the inverse covariance, right, which is the precision.  So one of the reasons because it's indeed much better behave to work with the precision matrix because it precision matrix is the kind of thing called parameter in the, in the midst of regulation.  And as I said before, it turns out that the log likelihood is the negative log like you. There's always convicts in the clinical parameters.  And so in some sense, it was a way to kind of like have that the convicts city in this formulation  So it's not complex in sigma, but it is convicts in  capital lambda which is a personal matrix.  Okay, so let's talk about the last example for today, which is  Now I told you that basically any discrete  Under graphical model can be put in the expansion family.  Okay, so  Under the assumption that  You have a full  Well, it's the subset of the of the GM, which has full on these tricky positive potentials. So let p be a distribution in a huge GM where g is undirected.  And we suppose that or potential are all strictly positive  But then, Charles tricky positive. This is for all clicks and all possible assignment on the clique.  So then my density  What am I didn't see my PMS is one over z product over my colleagues.  Of my potential over exceed because they're strictly positive I can take the log of the potential and take the x, x is equal to the same thing as their potential. And so this is basically the same thing as x summation over my clicks of login of se si, si, si  Minus log of z. Right, so  That's where my partition function went because x minus log is like the same thing as one of the z.  And so now it's trying to identify the sufficient statistics. So, this is x  Summation over my clicks.  And then I can think of something over all possible assignment on my clique.  And then I will have the indicator function of whether x is equal to this assignment.  Log of size see YC user avatar   And minus legacy user avatar   That's kind of a trick to identify the dependence on x.  And so  You can think of.  This entry here.  To be one entry of my sufficient statistics. So for any click and any possible assignment I will have the indicator feature which says, Oh, is this x  Is the the click assignment for this joint X equals to YC. So that's basically one of our features.  This is just a translation of like the the multimedia example right they maintain the example I have keep us at ease.  Now I have replaced may keep us to meet these with oh well for every clicks I have all the possible assignments and they look at the integrator features of these assignment and these are my suspicions that this  And this will be the associated Canada called parameter with this sufficient statistic. So I will index the clinical parameter by the clique and the possible joint assignment YC  And so if I looked with this T of x looks like jointly well it has one dimension for every click and assignment. So it has a bunch of stuff.  And every dimension. It's just looking at the indicator of whether exceeds equal to I see. And I have one of these for every YC and for every case. So every YC  In my assignment on the clique and for every case.  And basically what's  Script X. See, it's just the possible set of joining us, I mean that they can have on a  Little bit confused. Now why did they write that.  Oh yeah, exactly.  Yeah so exceed is just a possible  Assignment on why  Basically  Why i for i be looking to see such that  For which wise belong to next.  And the mapping  Of mechanical parameter is simply log of size see  I see with the same dimension right  And so I see  Why, see for every possible. Why seek could be your which was the potential Pampers ation you can map it now to the love to the clinical position by taking the luxury of these  Okay.  Was there a question they hear a question.  So let me mentioned some notes.  So the first thing is that the multinational multi new example is a special exempt is a special case of that.  Where the complete graph.  Is used right so you just have one big clique.  By the way, so what I just showed. So this is here. This is not a minimal, not a minimal representation  Because they are relationship between the sufficient statistics, right, particularly because they have these indicator fate feature if I some overall possible assignment you will always be equal to one. So I have a dinner appreciation. So this is not the minimal  Representation. I'll give you the minimal representation in the case of a binary variables, very soon.  But the other thing I wanted to mention, which is important and that's also why we're talking about these international family and why his attention in Mike's book is that you can think about. Now the feature perspective to penetrate your family.  So instead of having all the possible indicator  stead of using  All possible  Indicators  As your features are your sufficient statistics.  Right. So I had this x equal to I see  You could actually use a subset  Depending on the task. Right. And so, for example,  Suppose  X is a sentence.  And x i is a word.  And so let's say I had a vocabulary of possible words.  Then  If you read, you could have say a edge between two words in the sentence that so you have a sequence model, for example. Well, here in  In general, you would have for every possible pair of words, you would have a feature. But let's say this 50,000 words in your vocabulary that will be 50,000 by 50,000 that's a lot of features and and so  To have a bit less number of features because also you might not care about all these pair of words which are called by ground. By the way, you could have a smallest to the feature like  You could have a feature on x and x plus one which could be  The indicator of x i is a verb.  And x i plus one is  A noun, for example.  So that could be an example of features which  Collapse, a lot of possible observation to the same value. Right.  And so for certain tasking and appeal that will be sufficient to do things you don't need the exact identity of the word.  And so from the expression of family. You can define these feature function which are busy. The sufficient statistics that you will use in your model.  Okay, so instead of thinking about all the possible drink observation, you could now say, okay, what are the properties of observational don't want to use it. My prospect model.  And then is it kale is asking if this representation is only for discrete random variables, a  Yeah, because the problem is here. I started with all the possible assignment. So basically, there was really no restriction on my distribution, a part that I put non zero mass everywhere.  And and now you couldn't do that for continuous random variable at because just random variable, you will have to decide already. Okay, what's your base measure what's, what's the shape of my distribution. So,  There's a lot of possible distributions in continuous world and you cannot 3D have a parametric model with fine number of parameters which covers all of them.  Whereas a mutiny young key objects with key parameters you cover all the distributions. When you have a graphical model, you have to to the key to the key you have pop up, up, up, up.  So if you have any variables. Each which are binary, you have to, to the end possible. So you have key calls to to the end.  So that's another parameters which is why you might want to use a feature parameter ization  To kind of reduce which is done here.  But in come to this world it's it's  You don't have this this relationship between generic and specific as as precise.  So let's talk about the Isaac model.  So the binary Isaac model.  Oops. So I already mentioned it in the past, I think.  The binary  Icing model.  So in this case, my component of my observation will be binary variable, so Dr zero or one.  And was suppose that the clicks are smaller equal to so basically you have kind of this  You never have more than  You never have these triangle, but so you can have this kind of grid structure that's a pretty standard Isaac model.  And I told you that a tree with of a grid was unfortunately scaling with the site, the site of the grid. So, and because in France is exponential in the size of the tree with in the truth. So that's why in France is intractable. And that's why we do approximate in France, but  Basically  We can change that to the minimalist much money. So the first thing is, suppose that we use nodes and pairs.  As geeks.  So I am also adding the note potential in this case.  This means that the dimension.  Of to vex in this case is, you have one dimension for each possible assignment on the node. So that's two number of nodes. And then you have four possibilities for an assignment on the click of size two and so you have four times number of edges.  Parameter okay and this is  Basically the overpayment tries  Formulation, or eight over metal pan transformation.  Because you have relationship between the submission statistics, as I mentioned, right. So you have in particular that the summation over YC of TC YC of x is equal to one for any kicks.  Okay, so you don't have a minimal expansion.  Because you have this linear relationship between the components of the surgery. So there's x. And so it turns out that a minimal representation  Can be obtained.  Is to have X, which just look at  The value of x is so instead of looking at all is x equals one over x, y equals zero and just look at what's the value of excited  For all nodes.  And the product of excited and xj for all edges.  So basically, you only need the indicator is a node equal to one and the integrator or those two nodes, who's the two variables in an edge equal to one and this is sufficient to  The basically  Deduce all the possible value all the values of all the other possible edge and it's minimal. So this is basically the indicator of x equals one exchange equals one.  So the dimension here is  V plus he  And one way to see why this is actually a minimal  So is that  Let's say, look at the  The possible joint on two variables, right. So I have x is Jay and I could think value zero or one exec and think the various 01. And so I want to know what are the, the probability the marginal probability of all these  Possible pairs. Okay.  And so from  The parameter  In front of xi.  It will  Guess it's not really clear. What am I giving  Oh, yeah. So basically,  So if you remember we have that  The derivative of the lock partition function will give us these probabilities here. So it's basically that's what's going to happen here.  And so without going too much into details.  This representation will give us the probably T for each nodes. The marginal property for each nodes.  As well as the joint for x equals one and x equals one, which I would call a big. Okay, so I will have these three numbers specified by my parameters.  But these three numbers are sufficient to give me all the other  Properties that that are not explicitly given because. So this is the property that exciting. It's very cool to once I put it here.  Then the marginal tells me the probability that  X the property that x is equal to one.  So x is equal to one is the sum of the joint of x equals zero and x is equal one and x equals one and x equals one, right. So, so the some of those two entry SPI, which means that from from the marginal and from the joint of big I can deduce back this joined here by doing p minus PhD.  And similarly, if PJ here so I can reduce your PG minus P Aj  And now I can find this value here by having that the foreign trees as to some to one, so I get basically one minus p i minus Pj and then there was plus two big minus big which is plus one pH.  So basically, with these three numbers.  Are sufficient  To specify  These in some sense for a tables. There's four entries. There's only three degrees of freedom. And these were the three degrees of freedom.  Okay, well this is, I think, I don't think I made a great job of making this clear. But that's to give a bit of intuition why this is a minimal representation user avatar   But yeah user avatar   So Jacob is asking is it saying that knowing the edge potential. And one of the note potential till the other note potential  So what this is saying is that, first of all, if I can specify arbitrarily all the nodes potential and the edge potential there's multiple assignments, which give the exact same distribution.  And what I'm saying is that instead of saying what's the node potential for every binary value. And what's the node potential the edge potential for any pair of values, I could just specify. What's the potential for  For the node when it's equal to one, was the potential for the node which is equal to for the other nodes when it's equal to one.  And what's the potential for a pair of nodes equal to one. And with these three potentials I specified the joint unity or whatever the other potential because it will all be kind of like  Absorbed in the partition function.  And that's convinced that this is true. Actually, in terms of potential, but it is true for the clinical expansion of for the expansion of family formulation  Which has to be because I said that any way that their corresponding was lug  Think it should be true getting confused myself.  Yeah, so this is definitely using the fact that they're all binary  So I think, more generally, if you have keep us to be tease. I think you will need k minus one on the nodes.  And I'm not sure they props k times k minus one on the edge, rather than t square think it's that would be the dimensional UT  But I'm not completely sure replicate them ski minus one. user avatar   Minus one or something. user avatar   What I forgot these details.  Okay, so that's the amino representation for the pricing model. So perhaps before doing estimation. The last thing I will say, I don't have time to tell to you about the company generating function because we're kind of getting a bit behind in this class. So I will just say that  So the some properties of a  You'll see more in the scribe notes, but you have that. So I already said that the the gradient of a of it was the expectation  Oops, I'm having difficulties. It's the expectation of the sufficient statistics.  And I said, I called this like view of  Like the moment vector  When we talked about the expression family and the maximum entropy. And this is valid when it dies in the interior of the domain, because of the derivative. It's tricky to get rid of our honorees  And then there's also the it's true for the history of to. So basically, if you look at the history of the league partition function with respect to at the i n at J.  And if I looked at the  Entry of that this is basically the the entry of the covariance have to have x. So it's expectation p of x at have to have x minus, you have  To have x minus mute of at a transpose  This is for all possible it right so this is as a matrix. When I take this question. This is basically the covariance have to fix.  And the proof is in existence.  And so it turns out that these are called components. So the first component is the mean, the second command as the covariance. The third component is something which is just called the third component  That's why a of it is called a commitment generating function, but they they taking its derivatives, you get all the commitments.  Okay, and that will. All I will say about the expansion of family things today.  Let's move to estimation  Of parameters in the GM  A big M.  And  Let's start with the GM  So now, suppose I have a parametric family.  I'll use P capital theta.  Which will be specified by these PMS for PDF on x.  Which are in the form of a DGA so I will have probably, at the product over my nodes and then they specified the conditional  And these depends on some parameters.  And I will have a different parameters for every piece of my  Daejeon  So every note, I'm not doing parameter sharing in this case.  And so then  This is for theta.  Belongs to  Yeah so theta basically  As  One entry per node.  Sorry. So, so it has one parameter per node, which could have. We could be multi dimensional, but the importance is that each of these entry, I will see  Will belong to theta, where there's no constraints on these parameters. I will say I have a parameter set for first know the parameter set for a second node and there's no relationship between this parameter set  Okay, so that's kind of like the simplest way to have an independent transition. So, because both. I have one parameter per node and there's no relationship between my constraints on the parameters. So this is what we call an independent  Parameter ization  And that makes estimation, much simpler because basically  When you do maximum likelihood in this thing. It will separate in  The number of nodes as independent problem. So, this will be couples in V independent  Emily problems.  So you can basically estimate every little family independently.  So we're not tying the parameters and so that simplify things  And so, for example, suppose my observation are I have an observation of these joint multi dimensional variable.  So if a computer likelihoods I have my data given my parameter, this will become  The product.  Over my observation, probably to have my observation.  Given data and then becomes product over i.  Product over my nodes.  Of probably T of exchange I given x by G. I. N.  G. Right.  And so if I take the log of this  I can first compute the sum over my nodes.  And then I get  Basically summation over my training example.  Of the lug of p x j i  X by Jay, I  Data  And so you can just call this some function which only depends on data, J. There's no other theta there because this is data G.  And so if I try to maximize the lug respect to my joint data of a some of only function of theta. Jay, I can. And because my constraints.  Here are separated like their products. I can independently optimize over every parameter for every element of myself. So that's why it means that the couple  So let's say for example X one given expired. I was a Gaussian with some mean and coherence and perhaps another one was originally right doesn't really matter.  Then I can just, you know, look at this data independently to estimate my mean in my current. Oh my gosh. In and in the same thing for the other family for member to do so.  So if you so basically it's reduces to problems, you already know if these pieces or standard piece that you've looked at before.  So it's pretty easy in a fully observed so fully observed demographic Amal is actually not super hard. Now if you add a bit of constraints, then it becomes a constraint of division problem and then things ties, but also when it's fully observed, it's still not too hard.  When it is not fully observe. So basically, for the observed  The GM is relatively easy.  But if you have latent variable.  So you have some unobserved variable.  Then  You can use em instead  Then you use em.  Like we did for. Hmm. Right. Hmm, was an example of the D GM where we did explicit parameter estimation that you're doing for think for this assignment, right, which is you in two weeks, think that this Tuesday. Tuesday night.  And so in general into each step of. Yeah. And you'll need to compute some probably tease on your on your variable and your latent variable.  And how do you compute the properties of latent variable in the DJ, DJ, and will you do basically some product or junction three algorithm that's how you do it. Exactly. And if you can do it. Exactly. Well, then you can use sampling or virtual methods which will see next. Okay.  So that's for the GM  So that's what about you, GM  You GM as other difficulties here.  And so  We can think of in the case of discrete variable to simplify will look at, for example, Gaussian random networks later in the class, but like, we can then think of the exponential family.  Because I said that basically any you GM could be putting the exposure family when you have positive potential  So I want to do maximum likelihood  When my distribution or of this form.  And so it's a bit more complicated than in the D GM part. Oh, I guess I didn't finish something in the DGA sorry. And so in  One thing which is a very important special case.  Is  It's moved this down.  Sorry about that.  Is that  Oops, I also want to move you down. Mr.  Fully observed the GM is relatively easy.  And so as an example. Suppose that all these conditional or discrete or discrete random variable for discrete  Random variable, you can think of them each of them as routinely  So you'll have that the maximum likelihood estimation for every of your parameter will be basically the proportion of user avatar   Just me. Or we lost Simon user avatar jacob louis hoover  I've also Boston user avatar Breandan Considine  Yeah, I can't hear me. user avatar Abdelrahman Zayed  Or I think he hasn't felt good. user avatar   So who was the host while I was gone.  Nobody  Anyway, so sorry about that. I guess there was a electricity surge which stop my router. And so I had to wait for the internet to come back.  Fortunately, the recording.  Then stop.  So let me share my screen again.  Too. user avatar   People user avatar   Okay, so am I still recording. Yes, it's still recording  Yeah, so I was saying that  To compute, for example, in the case of a discrete random variable. I want to know what's the conditional probabilities for every possible assignment of  value for x j and for the parents and I can just compute that by doing ratio of accounts. Right. So I looked at the proportion of time I've seen  A specific value for xj in a specific value for the parents divided by number of time I've seen the value for their parents with any value of exchange. So that's just a total comp. Okay.  So it's pretty easy to to do for these. So it's basically ratio of counts of local accounts, right. So that's how you you would do is I'll make you an A. D. GM on discrete random variable.  Alright, so let's go back to the UGA em.  So basically, I said that you would have this shape. So you have sufficient statistics vector for every clicks I have my log partition function.  And so unfortunately here because of this log partition function which ties all the parameters together.  The log likes you. It does not separate like in the gym. So unlike in a Daejeon  Because of the normalization. Unlike in a the GM  The log likelihood does not separate  In  Individual optimization problem.  As summation  Over clique see FC of  Deceit  So it's a bit more complicated to do maximum naked in this case.  And so there's this algorithm that is in Mike's book called iterative proportional fitting, which is kind of a cute algorithm, but I don't think it's super relevant in modern machine learning.  So basically, the more standard approaches, would you, you would do gradient descent on the log like yet, right.  And so it's a bit like logistic regression. This is there a generalization of logistic regression, you know, when you talk about the special family.  There's just a progression, you have this x for two possibilities. Now we just have to keep us in. He's in 72 and there's no close bond formula for the maximum likelihood for logistic regression. That's why you need numerical methods like gradient descent.  And as I mentioned last time you had  So if I looked at the likelihood. So, summation over my train example log of p of x i given a fair  So I can separate that as a summation of critiques. I have a parameter for every click and then I have one over n summation over my training examples of the sufficient statistics for dis clicks of the trainings simple right  And then I have minus n times A of data divided by n. So the end cancels out. So that's the normalized log like here over my old training set. So you can think of this  Empirical. This is an empirical expectation of my sufficient statistics and the key right so I could call this my empirical moment on the clique.  So when I take the gradient respect to enter  Of my normalized like like you and I'll get so basically  For every clicks. This will just go away. I will be back with this. So I will get  I get if I take the greeting respect to FRC to make it more  Specific with respect to block at the sea, I will get basically  The empirical moment on my click minus the gradient of a meta respect to the clinic.  Which is the expected suspicious that this sticks.  Which is basically just a problem, right. So, so I call this the  The moment they  Have so that's basically Newseum, so that's by definition this is expectation over my whole distribution of TC of X. See user avatar   Okay. user avatar   So I want that the model moment on a click matches the empirical moment.  And so to compute this just to complete the grading to do log likelihood as a center that look like you're the I need to compute this expectation. So to compute this, you need to do in France.  This you need influence  And, and, more specifically, if I looked at the Isaac model to make it more concrete.  Then we had that the kind of sufficient statistics we had  Was basically just the product between xi and exchange.  Were exciting exterior binary variable. So, if I take the expectation of this  All and get all I get is what I would call me i j, which is the probability that X is equal to one and exchange is equal to one, given my clinical power.  So I need to compute this edge marginal  To get what the gradient would be with respect my parameter and unfortunately in a grid Isaac model. I said this is on tractable to compute. Right. So, we will need approximate inference.  Approximate in France.  And so that could either be  Sampling, which is what we will see next or virtual method.  Which was where we'll see after sampling. So we'll see both these approaches and actually your assignment five will be to implement good sampling, as well as a simple  Mean field approach for for  Computing, a practice for competing these marginalizing ma  Okay, so the so the next things we will look at will be approximate in France.  And then in the in the big scheme of thing we will go back to model selection, like how to choose the graph, how to choose the the models or the Patrick family.  So we'll come back to that right now in terms of just estimating the the parameters in a in a graphical model that's most of the things I want to say is here.  It's a bit related. It's just a generalization of what I've we've seen in the first lecture is in this in this course right where we talked about like maximum likelihood principal maximum entropy this kind of stuff.  So the next class will start to do talk about sampling to approximate these properties. So is there any question about  Estimation in graphical model. user avatar ezekiel williams  And I have kind of been a question.  I was just wondering, are there any situations where you can have a latent variable model where you actually have  Where  We're like a maximum likelihood kind of estimate approaches actually concave  Or is it is it for me latent variable model do always have  non convex user avatar   Good question. So I think if  If the latent variable is discrete, I'm pretty sure that it's  All on in non border cases like I think like with measure zero mission like with poverty, one over the or like the big measure one over the parameters. My guess is that it's done Kochi. Now, if, if the latent variable that is continuous.  You might have things which are still fine, I think.  depends on how you define your joint  For discrete usually I think it wouldn't work now.  Spit inexpensive honey is clear. I think the next bunch of me these mixture model don't work. There are, they are not in mixed martial family.  Because it's always because I said, the next question for me, the maximum likelihood is always  convicts as the property will be convicts  Any other question.  Know,  Well, in this case, I'll see you on Tuesday and joy. Hopefully we can, which I don't know what's the weather forecast but ENJOY YOUR WEEKEND. It's important to take a break. Sometimes  Alright see everyone  Oh, and don't forget that there's a gatorade down social now if you want user avatar   With poker tables and
  Okay, so for the people who are not in the class right now listening and we'll watch the recording later. I just said that I read all the project proposals and they're great. So, good job. Quite looking forward to read the project progress reports and then the cedar project presentations.  So today what are we going to do. I started to motivate last class when we did estimation graphical model, the need for approximate in France. So today we'll look at sampling  As a way to do approximate in France. So the first. The next two lectures will be about sampling. Then we'll see virtual methods, which is another way to do approximate in France.  You could have all and an entire class itself like a two on sampling. It's quite a rich field. So I'll just give you the most important elements in this in these lectures.  So,  Let's start. So this is in the general category of approximate in France.  And let's look at something  simpler approach.  And so I  As I mentioned in the Isaac model. So, for example, as a motivation. It's empty hard to do.  Exact inference in grid graphical model.  So, for example,  It's NP hard  To  Do exact influence  In Isaac model.  So what does it mean, it means that you know there's no polynomial time algorithm known to do that.  To say good accuracy. I mean, you need to put it in decision problem. So it's a bit complicated to say okay, how is an MP hard  But the main bottom line for us is that well that we need approximation. Right. We need technique to approximate the marginal property in icing model or approximate the partition function, these kind of quantities.  Alright and so sampling is actually not just use for approximation do approximate in France. So let me just give a bit more  Motivation for something in general.  And so suppose you have a random vector which has component x one to XP.  So you can think of here as a graphical model and P nodes. That's why you have p components.  So, you know, and peak would be big. So the first  Use of sampling is just to do simulation.  Okay, so I want to sample ID from my distribution p  Okay. And that could be to, you know, have a look at what are the properties of the data points.  Coming from this distribution right then, then you can make measurements on the samples to kind of get a sense of how things behave. I mean, in some sense, doing measurements of these samples is a way to do approximate in France, but it's not mislead direct  And  So for example, let's say you have a traffic model.  And you would like to know, you know, you would like to know what it looks like when you run your traffic simulation model. So that's why it's called simulation. Right. It's just like, basically it's running your model getting example of observations from your model.  And now, in the context of approximate in France.  You could use sampling to approximate  The marginal on some notes on some variable.  And so this is a special case.  To compute the marginal on some know this is a special case.  Of  Computing expectations.  Okay, so I will put it in a more general framework to talk about Monte Carlo integration, which kind of motivates how you do these approximation.  And we'll see why the marginal is a special case. So if you consider  Some function from our to the P to are some real function.  And you considered a problem of approximating the mean of this function.  To approximate  CMU, which is the expectation respect to p of F of capital X. Okay, so we have a random variable in high dimension. I will. I want to make. I want to approximate the, the, the average of f over this distribution.  Okay. And so, and we'll see how we can approximate that using sampling. But how does it relate to approximate in France. Well, you can just specify some specific function. So if you define your function to be the indicator  On whether see  Your variable on a subset of nodes is equal to some value.  Then when I take the expectation of this function.  What I get is just the probability that X is equal to little excitement  Suppose that this was the discrete during a variable.  So for discrete random variable computing the marginal is the same thing as competing the expectation with as with respect to the indicator  Functions. Okay.  But if I can solve the problem for other proximity expectation to function that can also do other things and just marginals  Okay.  And Dora is asking P here. Is that true data distribution.  Yes. So in this case, we're not doing statistics were doing inference. So we're in the context of approximate in France. So we're told that  We have this unknown or I mean is, I know we have this true distribution p and we want to be able to compute. It's marginals or to approximate expectation, a function of this distribution, but we assume that this distribution is known, but we will see later on.  That we don't see no need to know the distribution we need. We don't need to know the distribution itself in great detail. So, for example,  There are some methods which only require the this to the, the, the density or the PMs  Up to a normalization concept which is very useful when you have an underserved graphical model because the numbers in constant which is the partition function can be NP hard to compute  Was to get it's the, the, the distribution up to a normalization constantly just take the product of the potentials. Okay.  Alright, so let's talk about Monte Carlo integration.  Which is a method to approximate these expectation  Monte Carlo integration and we call it also estimation, because it's a way to estimate the mean user avatar   Okay. user avatar   And this is actually one of the most fundamental tool of applied the numerical analysis and it appears everywhere. It appears in physics. Actually, that's probably where it comes from.  It appears in applied math.  When you want to compute see complicated integral that you don't know how to compute machine learning statistics, etc.  All right, and so Monte Carlo integration.  And so the expectation. When you have a density is an integral right so that's where the integration come from, but it doesn't have to be an integral, the calls will be used for some, but so the goal is to approximate  Mew, which is the expectation respect to p have some function of x.  And so what's the Monte Carlo estimation technique.  It's pretty simple.  You will simple, you will generate an samples.  X i, which are ID. According to peak to sample and times independent samples from the distribution p. And then you say that your approximation.  To the, to the true mean is the empirical mean right so it's you hat is one over n summation over your observations of your samples, the function evaluated on these functions on the samples.  And so this is just the empirical expectation respect to p n hat as a distribution. We've seen that a lot when we do machine learning.  Of x the f of x, right. So, and that's your way to estimate the true mean user avatar   Okay. user avatar   And that's it. Well, there's a question of how do you generate the samples. But let's say you had the samples, then you can use the samples.  To approximate the meaning and particularly to do approximate in France, right, because if f of x would be any particular function, then you would get the approximate marginals right  So let's talk about some properties of this approach.  So the first property that it has it as it it's unbiased, if you remember properties of estimators so if I take the expectation over the possible observation I can use in my own in my samples of new hat.  Well, what I get is just one over n by linearity of the expectation, I can put the expectation inside. So it's expectation of f of x i.  And note that this expectation here is respect to  The possible data set, right. So it's expectation over x i.  Because x of AI is simple. According to be. This is just  Mute right and so I get end times view divide by and and so I just get  It, and I would like to mention here that even if, instead of sampling independently.  It say that the exile are dependent. This is still true. This is still true.  True. If the exercise.  Or dependent  As long as they're sampled from the correct distribution.  For example, like what you could do is simple ones and all the other one lead them equal to the first one, then you only have one simple instead of and simple. So that's a pretty bad estimator, but in average and expectation. This will be the correct me.  And somebody asked. Oh, what is a happy of n. So as I mentioned before, this is the empirical distribution.  Right, so p and hat is basically one over n summation over i have an indicator of whether so you know it's a PM F, like when we talk about discrete data. It's simpler. So it's the indicator of whether x is equal  To x  Okay.  So we've used it. We've seen it in other places we've seen it. When we talk about the equivalence between minimization of the kale diversions with the empirical distribution and maximum likelihood  We seen it when we talked about moment matching. You know, when we tried to match to find the parameter in the truth is in the in the model of distribution which max which match the empirical moments and the empirical woman or the expectation respect to the empirical distribution.  If you have continuous data the way to actually write that is a PDF, but it's actually not a true PDF. It's a generalized PDF. It's called, it's called a general distribution, actually. And you would instead of having the indicator, which is a credit card delta, you would have the  The direct delta function which is basically when which integrate which selects the value when you integrate okay but I don't want to go into these technical details so prefer to just stick to this big data to present the formalism.  He. Alright, so we have  The expectation this estimator is unbiased so you know that's already a good property. And now if we want to know.  What's it's expected air.  So the expected or the variance and actually. So the expected error of our estimator.  In LT norm.  Which is a way to quantify how good or estimator is I want to know an expectation  All far is new hat from the true value in Altoona. Right.  And know that because the mean of new hat is new. This is actually the same thing as the summation of my cover and stuff. So this is actually the trace of the covariance of new hat you  So you have new what  Is new  So there's no variance by itself, because it's, it has a new is a random vector  So what it has it as a covariance matrix. If I some the diagonal entry of the current matrix, it's, it's kind of like the notion of variance in some sense when you talk about bias variance, the composition four squared error. And actually, that's what we're, we're looking at here.  So this error that's compute that. So this is the expectation  Of  Basically pump, bomb, bomb.  So what happened is  So we have. Okay, that's actually computed more carefully. So this is the product.  Of one over n summation over i.  F of x i.  Minus new and some issue one over n summation over j f of x j  Minus. So all I've done here is expanded the Ultra norm as a product  You know rewrote what you had was, was  Empirical average and I using here different indices, because now by linearity of the product. This is the same thing as  Expectation of summation one over n square submission over i n j  Of f of x.  Minus new and then f of x j  Minus b.  It's I just use linearity of the of the inner product I taking the some outside and know that one over. And some of you is just new right so that's why I could I left the new here in the with the sons, and now I use independence.  So by independence.  All the off diagonal terms or zero.  Okay, so why is that so I'll explain here, just some of the thing. So I will have by linearity. I can move the expectation here.  And when two variables are independent. So if x is the benefit of exchange the expectation of a product is a product of the expectation. Right. So then I can do the expectation here and then the expectation there.  Because of the independence and then I get new minus new which is zero.  Okay, so all that's left is  The diagonal term. So when x and x I are both on the left and the right. So then there can be nothing but dependence, so I can expect expectation  So, what I get is one over n square summation for i equals j. Right. So when is not equal to Jay, oops, their independence with 01 over n square summation i equals Jane.  And then I have expectation of f of x.  Minus f of x i minus view right  And so this is just  The expectation of the norm of f of x.  Minus new for one that point.  And let's define this to be sigma square right it doesn't depend on it because they're all in  Identically distributed  And so, what you get is. Oh, and by the way this is also, as I mentioned before, this is a trace of the covariance of f of x.  So it's actually property of your distribution of your function.  And so  You have  Any terms of the diagonal. Each of them is sigma square I divide by and square. So, I'm left that the actual expected error for your monthly Calvinistic estimate estimator in Altoona arm is equal to sigma square divided by  Okay, so that's your  Your  Convergence results. So if as NGOs and at your estimator will converge an expectation to the right quantity. And actually, so you'll you'll basically you'll get LT convergence for estimator.  And so that kind of makes sense. So the more and more samples you have, the more you are concentrated. We could also look at the, you know, there's a central limit theorem for that. But this is telling you that the scale of the air. So if you. This is the elsewhere square. So, in terms of like  Kind of STDs style. It's one of our square with 10 right so whenever screwed an error is something which is very standard and statistics for you you you might have seen that a lot of places. So here it's pretty  Elementary to prove that it's just coming from the fact that I have an independent observations and that when I take these expectations off diagonal disappear.  Okay. And so something to mention is that this is there's no explicit dimension in this  In this rate.  Game. So a part sigma square  Which could depend  Implicitly  On the dimension.  The dimension doesn't appear  The dimension doesn't appear in this rate, which is kind of nice because if I want to approximate quantities in the high dimension.  I don't want to have a dimension appearing because it's in high dimension. Okay, so that's actually one of the nice properties and Monte Carlo methods.  Is that these still behave fairly well in high dimension in contrast to other approximate integration techniques. So because this can actually be used to compute an approximate integral right expectation is an integral  For continuous data. And so you could actually use numerical integration like Simpsons rules or whatever.  But these actually will have the dimension appear in the era that you make. So in high dimension. The are there.  They're really expensive. They will have hired or you need a lot of computation to actually get good there was one thing on integration actually works fairly well. Also in higher dimensions.  And somebody asked  Why does the n squared is appeared is because you have n terms.  In the some you have enters. Right. So basically you get one over n square and sigma square. And so then when and cancels.  Is there any question about this.  No question. It's clear comes up. user avatar Jacob Louis Hoover  I'm sorry, can I just ask quickly. I'M SURE WHAT IS SO THE take you to the point was, it was unbiased and at the expected error has this term one over and in it. I mean, the sigma squared is not surprising, but the one over and was both sort of the takeaway from this user avatar   Yes. Right. Okay. And so that this estimator.  So first of all, it's an unbiased estimator.  And as n goes to infinity, it actually gives you the right quantity. So it's also consistent in the sense, right, so it's a consistent estimator.  And it tells you. Also, what's the. Not only is it consistent, but it tells you how fast it converge to the right thing. So it gives you basically a one of this growth and convergence. user avatar   Okay. user avatar   So in particular, if you want to use this method to get a specific error certificate on your estimate. Well, so here's only an expectation. So it's obviously I probably but that's already one way to get some kind of notion of how big your errors.  You can also get high quality bounds for the estimator by  You know, using standard tell balance, but that's a bit more fancy user avatar ezekiel williams  Um, quick question.  Yeah, I guess I was just wondering. So if the samples are not independent, then of course we don't have this one over n, because you  End up  Over n squared sample. Yes. Okay. Cool. user avatar   Yes, exactly. So, so what's happening is if we don't have independent sample, we can still use the estimator, as it is. But then, instead of having n as you're efficient sample size, it will be smaller.  And actually, when we talk about diagnostic diagnose sticking diagnosing your chain. So we'll see later on.  Now we need to get the sample. So to get independent sample is actually hard in general. So we'll go will relax the independence of our samples to go to something called Markov Chain Monte Carlo integration.  Where the samples will be produced by Markov chain. The mark of chain will have some dependencies and on the samples and and then basically will see that the effective sample size won't be the number of samples we get from markup change, it will be smaller because of these dependencies. user avatar ezekiel williams  And effectively, you would need like a memory list property for your Markov chain because otherwise you like I guess you need a memory list property so that you don't end up having like  And standing over n squared samples. If you because if every sample was correlated with every other sample, then you would you wouldn't be dividing by anything so you wouldn't have  You wouldn't have any form about to conversions. Right. user avatar   So we'll get back to that when we talk about particular integration.  Okay, so, so the bottom line and  That's something important to keep in mind is usually  You always keep all your samples and divide by that when we talk about Markov chain. It would be t. So, t will be the time in your chain. And so you let's say you have after t steps of your chain you have T samples.  After a burden phase where you you remove the things because these samples have are actually not from the correct distribution. After that you need to keep all the samples.  And even though they're dependent and and and we'll see why later when we talk about the the errors of the simple because the more simple. You have the smaller the error, but  The more simple. You have also the, the, if you don't trim the chain, the more they will be dependent, but there's. But it turns out that having more samples is better for these estimator, then even though there's a bit of dependency between  But we get back to that.  Alright, so that's multicolored integration. So we can know if we can samples. If we can sample from our distribution. We know how to approximate quantities. So now we need to think and think about how do we sample. And so I will talk about different sampling techniques.  And so, and there's also a question. How do you simple numerically, right, because we have computers like if if you had a through sorry a true  physical phenomena like for example, you just flip a coin. Well, that's when that's when the way through simple right to you. You flip physically a coin and then you you record your result. These are like up with a new years which are  Normally independent and this you do crazy stuff with your finger. And so that's one way to get real samples, but now we talked about how to use a computer. And so the first easy thing we want to sample from is the uniform distribution.  Let's see the uniform on 01  And actually there's a field of numerical methods which are called pseudo  One computer science called pseudo random generator  And  This is what you get when you call the rand function inside by or in MATLAB or these kind of like scientific libraries.  Basically they're calling a pseudo random generator and absurd a random generator is not completely random, because it is it does produce a deterministic sequence of numbers.  But the properties is that the sequence look random  And then what does it mean to look random. Well, that's what the whole field look at basically do we look at a bunch of sub sequence of these sequences and he is the you know the distribution of any subsequent uniform  Is there any correlation between these things. And so that's the kind of statistics you want your pseudo random generator to satisfy. And there are  Some so the modern so the random generator actually better than the one which were proposed a few decades ago. So they actually have something called a longer period. The period is when you start to repeat the values of your method.  And basically you have a seed and then after the seed, you get this kind of deterministic transformation and  The numbers when you look that there's actually no pattern. Like, if you look at it, you wouldn't see any pattern, which is why it does look like random, even though it's actually fully deterministic. Okay.  So you already know that when you do scientific computing, because if you fix the seed. You can reproduce your experiment, even though they're supposed to have our random aspect into it. Right. So, but if you just change a seat, everything changed. So that's a way to kind of get  Different samples for your, your experiment. Okay, so we'll suppose for for a lot of like sampling mechanism will suppose that we have access to this random function.  To generate from a uniform distribution and we won't go into how to design these two random generator, because that's that's a whole specific field of itself just suppose you have a good one from cyphy or matchup ok  So now. Well, the good thing is, once you have this, you can do a lot of things. So, for example, is now if I want to Bernie.  If I want to flip a coin.  With quality P. Well, what get where they can do is just define X as the indicator of you smaller equal to pee where you were simple  Uniformly from a from the rand function right  So now, what happened is the property that you is smaller than P is P does the polity that my indicator gives me one will be p. So that's how I can get the correct Bernie. So from a uniform a continuous journey from distributed random variable, I can define now the brewery.  Okay. And this is a special case of a trick which is called N verse transform sampling  So in verse  Transform.  Sampling  Is a trick to sample from any  PDF when you have this inverse city of. Okay. So how does that work. So let f  Be  The target cumulative distribution function.  Of the distribution  Okay so CDF has been, as I mentioned, it's a cumulative distribution function. And as I might have mentioned in the early classes. There's a one to one correspondence between distribution and CDF right so the CDF determines uniquely the distribution  And and what's the CDF that's  Here will be in one dimension.  First, so that's the probability that X is smaller than little x  OK for x belonging store.  Can and and if you remember the CDF for a continuous random variable is a nice continuous function.  And it goes up to one.  So I have x  And this will be f of x.  And that's the integral of the density from minus infinity to x.  Whereas if I have a discrete random variable, I will have  Say like  Jumps at my possible observation. So, these, these tix here are the sample space for my random variable. And what happened is the property that my random variable is smaller than some value is constant and then it jumps when I pick a new possible value. Right. And so it's, you get this staircase.  Shape.  Twitter. Twitter app and it goes up to one.  And the size of the jump is the value of the PMs add this value right and so under open circle means it's excluded full circle means include  Okay, so why do we talk about the CDF well because I will show you how I can define a random variable starting from a uniform distribution so that this random variable will have the the correct distribution that I care about.  And first, for simplicity we will suppose that f is convertible.  And so this is not the case, f is not convertible. When you have a discrete random variable because it's peaceful it's constant but for a continuous distribution.  It's actually convertible usually  And so now what do we do, is we let x to be defined as the inverse CDF apply to the continuous the uniform random variable with you, which is uniform  Is that's the trick. That's the inverse transform sampling trick or the inverse city of trick. And now I claim.  That x  This new newly defined random variable as the CDF f of x, which is the CDF of the distribution p that I care about. And so  If I sample you uniformly and then I transform it, I get an x, which is distributed according to pee. So I get x, which has a correct distribution. So it's a correct sample from a distribution.  Center to the proof. It's pretty simple.  So I want to compute was the probability that capital X is smaller than why right that will give me the CDF of capital X. Well, this is the probability that f minus one of capital you is smaller than why by definition of x.  And because f isn't variable.  This is the same thing as the probability of you being smaller than f of y. Right, so I can apply F on both sides of the equations. It's convertible. There's a one to one relationship between every element so that won't change anything.  And so, and because you as a uniform random variable. This is, by definition,  The policy that you is smaller than something which is and there's something being between zero and one is just this something. So this is just equal to fly.  And that's it. That's what I wanted to show I wanted to show that the property that capital X, Y and Y.  As actually the correct CDF which is given by f capital F of why user avatar   Okay. user avatar   And if f is nonverbal  Then you can generalize the argument by defining  By picking one value for the inverse of f c will define X to be the minimum  Of all x such that f of x is bigger than you.  So basically, there might be multiple value such that when I take f of this value.  I get you.  And so what I do now is, I'll just take the smallest one.  And the reason we do this is you can one property of a CDF  Is that it's continuous from the right  So because it's continuous from the right, it means that when  You look at the value  When you approach from the right  F will be included. Right. So meaning like here if I looked, I take the limit on this on the side.  The point is included. It's not continues from the left because here I I I could move in this direction.  And then not only the value should be this, but there was a jump and it went to this value. So that's not confused from the left, but it is confused from the right. And so that's also why this is actually a min rather than in so the min is achieved.  Okay, but if you're a bit confused by the non and variable case, don't worry too much about it.  You could work it up later. It's, it's actually non trivial to think about it. But the general idea is given by by, you know, this proof here that I ok  OK, so now that i. So, basically. Now, if I have a CTF and I know how to invert the CTF I can sample from any one distribution. Okay. So, for example,  Another example is, let's say I want to compute  As a user avatar   Group. user avatar   So example.  So if I want a sample from  An exponentially distributed  With mean LAMBDA, From the distribution with from the exponential distribution with lambda know that the exponential distribution is not the same thing as the expansion of family. It is an exponential family but you know there's others myself me, which are not the exponential distribution.  Well, what's the density  For an exponential  distributed random variable. It's p of x is lambda x of minus lambda x.  And then they have the indicator  On positive r plus right because it's only defined on the positive numbers.  So the nice thing is that this you can easily integrate it has an anti derivative. And so the CDF can. Oops.  can integrate the CDF as a cross form. user avatar   I wanted to do. Undo. Undo. user avatar   So the CDF here is just one minus x of minus lambda x. And so you can verify that if you take the derivative that you get the PDF  And. And not only can you compute the CDF. You can also easily invert it in close form. So the inverse  That you can verify is applied to say some variable new you. Sorry.  Is minus one over lambda lug of one minus you  Okay. And so now if you want to generate a if you want to sample from an experiential variable with mean lambda, you just take a uniform variable you take log of y minus the uniform minus of that and divide by lambda. That's it. You got a sample from the explanation. user avatar   So, user avatar   Yeah, so start back ask it.  Basically because of the inequality. There was also the one that does the city, which was us. Yeah, you're right.  Yeah, so the CDF is always monotone  It's obviously strictly monotone, but then when it's convertible. It is strictly monotone  Only of you who saw ask what's the last sentence, I guess I just explained that to obtain a sample from  An exponentially.  distributed random variable with me, lambda i can do it by sampling uniform random variable on 01 and then doing this transformation. user avatar   Okay. user avatar   Alright, so that's the one. These actually something in one day is not hard. As you can see, I mean, the problem is that  For some this division, like the gash in the CDF is intractable. It's actually called the error function. So there's a bit of numerical methods that you need to work with there.  And actually, I will tell you after the break, that there are more efficient technique to sample from a Gaussian and using the arrow function with the University of trick.  But let's talk already about multivariate  Random variable because right now. It was in one day, how you can actually generalize this trick to a military distribution. user avatar   So, user avatar   What about if I want to do district.  In multiple dimensions.  So that's a bit less known, but you can still do it. So you can generalize the trick.  using the chain rule.  Alright, so let's do a bit of notation. So now we have a random variable with P components. So it's in dimension p  And in the case of a CDF in in multiple dimension.  It's the same idea as the one d CTF you just define us as the probability that basically you get like this, these kind of like hyper rectangles. So basically this property that x one is small, smaller than little x one blah blah blah and capital XP smaller next be so  So it's minus infinity to x i. For every dimension.  That's the definition of a CTF and it's still unique still has the same properties and in one knee.  So now, what's this chain rule. Well, it turns out that this chain, the chain rule and distribution can be generalized to CDs. So I have the CDF for X on all these dimensions. And so this takes as input a point in dimension p  Is actually equal to the CDF of the first component  At x one.  Times the conditional CDF. So that's the CDF for the conditional distribution of X two given x one.  evaluated at x two given x one.  And blah blah blah, to the conditional CDF of the last variable, given everything else.  Evaluated at XP and all these conditioning point  And what do we mean by a conditional CDF  So this conditional CDF here, the meaning is the conditional c, d, e, f of x two given x one.  X two given x one is basically the CDF of the conditional  But conditioning on big blocks right so this is conditional of X through smaller than x two given x one smaller than x one.  Okay, so it's not like I'm conditioning on a specific value for x one, I'm saying x one is smaller than little x one.  This really what I'm  Trying to do  I have a doubt. Now, I'm not sure if it should be x one equals x one here. So let me verify that during the break.  So to be verified.  But suppose that we have this relationship.  Then how we would use the uniform random variable, I would simple you one up to up  All from uniform  On 01  So in some sense, it's like simple multinational point in the uniform cube.  And then you define X one to just, you just apply the inverse ETF trick on  X one from you. One  And then you define X two.  By applying the University of trick on the conditional  And then you evaluated YouTube but then you needed to reuse X one. So that's it's kind of like you need to use it in a chain.  Etc. Etc, etc. And then you get the last one XP is f minus one XP given x one, two, p minus one up and then x one up to p minus one.  And so that's a trick.  And supposing I have the correct definition of my conditional CDF there has to check over the break. This should give you the correct samples.  The problem though is that this function here.  Is a very complicated function.  Because to get these kind of CDF I need to integrate in multiple dimension and then into the inverse this and now this is condition and a bunch of pieces.  And so you get normally you will have not a close form formula for that. And moreover, you won't be able to compute discover inverse city of  And so that's where the curse of dimensionally D come in.  Is that these kind of trick is totally intractable in more than like one or two or three dimensions. Usually, so you get the curse of dimensional you teach  It so you can actually apply it.  In more than like dimension one or two.  And  Except when these functions are very  Simple like the Gaussian. So I think you could do kind of like you could document, even the gash and actually I told you there was an error functions, it's more complicated. We will see after the break. How to sample from a gash in directly  And I will mention as an aside, that there's this thing that you might have heard, because of the finance crisis. It's called cupolas  So a couple as is a model.  For multivariate data.  With uniform marginals  So basically, the idea of a computer is that it tells you how the covariance structure of your data works that's given by the computer, you don't care about the marginals, because you can always use the University of trick to transform any uniform marginals to a non uniform  With the kind of like the component by component in diversity of trick.  And then somebody is asking, can we say that YouTube given x one is equal to YouTube.  Yes.  YouTube given x one. It is indeed equal to YouTube because YouTube is simple, independently of you one and x one is only determined by you.  But the point of writing this year is that you need to specify, what's the thing your conditioning on to talk about what's the function. I'm talking about. And then you will have to evaluate that you to to get the x to value.  Okay.  Is there any question about this.  No question. So let's take a break. It is to 29  And user avatar ezekiel williams  A question just about the inverse. The short proof of the inverse ETF  Concern. I was just wondering, and the comments about about requiring monitor in the city as well. The do we actually require minutes in the city there. Is it just  Yeah. Cuz, from my understanding, it looks like we just require it in credibility. So maybe I don't really see what's going on properly. user avatar   Hmm.  So basically,  So here, what I get is a bunch of numbers, which are smaller than why  So now if I looked at a  such that f of a is equal to you.  There's only one of it.  Well, it's clear that from one of the city, then because of this inequality, it will work, whether you really need to city. That's a good question. I will need to think a bit more deeply about it.  But let's do a break. I'll try to figure this out during the break.  Alright to 31 user avatar ezekiel williams  It's good back at 241 user avatar   Okay. Let's resume recording  So as people discuss in the chat. The convinced themselves that monitor city was indeed.  Important for the arguments, but the CDF is always monotone, as I mentioned,  But yeah, so here it's isn't a strict move to cities will be used.  And then  Here in the definition of the conditional CDF that I was using it's indeed.  These conditioning like this. So, so it's pretty obvious from the chain rule that these conditional distribution when I will multiply them, they will give me back the  joint cdf right so now it's much less if it then that. OK, I can just plug in x one as the value of little x one here for everything to works. So I'll try to need to find the reference for that.  But I think that's correct. What I just wrote  But anyway, the the main so nobody use that in practice because it's too complicated. And here, as I mentioned, is usually it's actually hard to simple in high dimension.  And they'll give you some specific examples.  Where you can use the structure of the graphical model to do it otherwise you'll need to use sampling  But there's an exception were sampling from a movie right gushing is easy. So, so one exception is Mitzi very good ocean.  So exception to what what the, what I'm saying is usually when you simply in the high dimension. It's difficult.  But for a Gaussian distribution. It's easy. So if I want to sample from  A distribution with me new a gash in with me new insurance sigma. What you do is first of all you do.  Is get the composition of your covariance  And so, more specifically. So, sigma is is his PhD. So you can do. It's spectral the composition  So you basically get a matrix which is orthogonal. So Mumia transpose is equal to IP and lambda is diagonal  So you can do that physically is ready. But in this case with  Positive different matrix. And as I hinted at, so this is actually, you can use this to do a chilly ski decomposition of your covariance matrix. And that's all you need to do to sample. So you will define L as to be you.  Lambda square root. And then, so then your conference matrix is actually LL transpose  And so now, if I can generate whoops generate  V, which is coming from a standard normal. So I have p ID standard normal  So VP is ID.  And 01  Then you can define X as you lambda one half v plus new and this is just L, by the way. That's why I said, you only need L. You don't need actually to you in the lambda one half.  And then what you get is a variable where it's expectation is mute because expectation of V zero  And the covariance of X.  Is actually you sigma one half covariance of the  Sigma one half to transpose this is identity. And so then I am just back with you. Lambda you transpose which is sigma  And that shows that this is a gash. This is the Goshen with the correct meaning covariance. Why is x a Gaussian. So there's all these properties that when you make a linear transformation of aggression used to get a gash in random variables. So,  That's why I just needed to look at the meaning of coherence and and I was done, I didn't need to look at the actual distribution of mix.  So if I can sample from a standard Goshen, I can sample from any multivariate Goshen by doing the chilis kidney composition of the covariance  How do you sample from a universal. Gotcha. Well, you could use the inverse ETF trick with the arrow function, but that's actually not the best way. So there's this trick called the Bucs Mueller transform  And this trick is to sample from a 2D  2D  Standard standard  So you get to independent sample from standard Goshen.  And the idea is that  If you look at the radio, the composition, if you, if you look at the radial  Formulation of your coordinates. So, r and theta. So instead of having x and y, you will have a radius and an angle from the origin. Okay. Well, it turns out that in 2D. If  The point is some sample from a Gaussian standard Yashin with means zero and covariance identity you have that the radius square is simple. According to an exponential  Random variable with me one and the angle is simple. According to a uniform on zero to two pi.  So the angle is pretty simple, because a gash in his eyes will tropics, we need the angle is just uniform and then the radius. It's actually from a gamma. But, you know, a special case of the gamma distribution is actually the exponential distribution.  And so now  So then I can simple our data easily using the University of trick. So, theta. It's trivial. It's just uniform and then our I can. I already told you how to get it.  Sorry, r squared told you how to get it because I told you how to get the exponential random variable from uniform. So then once you have our data, you can get back x and y by just using the the the radio coordinates. So this is our coasts of theta and y will be our sign of faith.  And so then it turns out that x and y will be distributed according to zero and identity.  So that's kind of a neat trick.  To get a 2D question if you only need one. The guy should will simple from a 2D and just discard one of them.  Okay.  So now you know how to sample from Embassy read Yashin there is another example where it's easy to simple and it's from a director graphical model.  So sampling  From a DG M.  Is I'll say relatively easy.  And what you do is you use ancestral sampling  Alright, so what's the ancestral sampling algorithm. So I have  So suppose that my random vector is this video. According to pee.  which belongs to some the GM  So that means that our PDF. PDF  Is the product.  Over my nodes of conditional x i given expire.  And then suppose  Without loss of generic it  Without loss of generality.  That one up to p  Is a topological sort.  Of  Okay, so basically  That means that I only have edges going from lower index to bigger index. So here's a D GM, for example.  And so an example of topological sort will be 12345 so I'm making sure that the leafs are idea.  And so what's the ancestral sampling algorithm. It's actually fairly  Natural from the fact that we're defining or the GM from conditional  Is you start  So for i equals one, up to pee.  You will simple  X i, according to the conditional  Given by your  Node in the gym and then  Conditioning on the already observed parent and because because of this tub sort these are already  Observed  Because of the top sort. So the idea here is, I found an order such that  I only simple of a variable when already have simple all the parents. Right. So for example, in this example here, you would sample this node first than this node second  And then you can sample extreme and you already know the value of the parents so you can choose the correct conditional to actually find what's the, what should be the conditional for extreme  And then same thing, you get x four and then you can simply x five, when you have the parents. user avatar   Okay. user avatar   And that's it. That's ancestral sampling. So it's pretty natural you just simple according to the CGM order.  Why I said relatively  Easy is because, well, it's easy, from the ancestral algorithm, but it's still assume that you can sample from the conditional. So this is supposing I can sample from these conditional the CD and usually that's the case because it's this is only a one dimensional object.  But, you know, it could be complicated, because there's many things like conditional, but it doesn't matter. These are fixed. Right. So there's a question of, you know, can I use the inverse city of trick for that or  So that's kind of the idea  And so then you can show by induction. Oops.  You can show  by induction.  That  The simple we get  As actually distribution p  And so if you look at the scribble notes from last year I have proven formally  How I explained how you can prove formally that  This argument works for two variables and then you just drives by induction, because it seems pretty obvious is like, Oh, well, it's obvious you sample from the correct conditional. So it should work.  But it's still require a bit of proof and and this uses property of how you show that to the submission on the same  And particular to show that to the submission of the same. You can do it by showing that when you take expectation  Of any function of these random variable, you get the same value for a rich enough set of function. That's how you can show it. I won't go through the detail here because I'm already a bit out of time for  quite late in this class. But if you're interested in the in the in the formal rigorous detail, it will be in the in the scribe notes as well as the scribble notes from an SEO.  And what I will mention, though, which is an important side note,  It is that when you sample.  From a giant  You're also  Sampling  From the marginals  By  Just  Ignoring  The  Joint aspect.  Okay, so what do I need. So, what I mean is, suppose that I have x y which is from the joint on x and y.  Then if I just look at x i forget that x and y was a couple  If I just look at x itself.  Then you have that x is simple. According to the marginal annex.  And so if I have pairs a variable that I simple from the joint. If I just forget the association between my two variables and it just at look at one variable. Well, this variable comes from the marginal. So that's what I mean by that.  And more if you have multiple samples that say, have it simple of excited. Why I by looking at x i for multiple indices. These are ID sample from the marginal  But even with one sample that works. So that's good to keep in mind. So it's better to have like the sample from the joint because then you have all the marginals  Whereas when you only had the marginals, you definitely don't have to join because you need to know. Here I sample from x, you have a sample from why. Well, you need to associate x and y to get her to get the correct relationship between  OK, so  Now we'll talk about  All these were exact sampling  Where you know you directly simple something and then you get the correct  The simple you get as the correct distribution. Now we'll start to define tricks, where  The first simple you get done this, you have the correct distribution, but by manipulating things you'll get the correct distribution.  And so the first example is called rejection something  And  A special case of this will enable you to sample from a director graphical model when you have observed variable. So that means that I want to sample from the conditional of see x given some evidence variable.  So when you condition on some variables, you can just do ancestral sampling  So what's rejection sampling  So the idea of rejection sampling is I will reject some of the samples to make sure that  The sample. I don't reject as the correct distribution.  So let's say I have p of x which is it's on normalized form divided by some memorization from  Vector that sorry normalization conflict.  And let's say  That we can find  A proposal.  Q of X.  This called the proposal.  Distribution proposal.  That we can easily sample from  Which is easy to sample from  Such that  If I multiply  Q of X by some big constant I dominate the normalized p of x. user avatar   OK. user avatar   So the, the plot here is that I have x i have some complicated on normalized distribution detailed of x.  And then I have a simple proposal, which could be a garden. For example, and I will reschedule it by a such that I always am the mediating the anomalies p of x.  And now the problem is because I, you know, Q doesn't really match exactly detailed. There are some I am overshooting a bit the mass at any point. Right. And so the point here is that if I looked at some, you know, let's say there's some x here.  And so there's this extra density or PMS that I'm using, which is too much.  And to get the correct distribution, I will need to reject.  With this probably to kind of reduce the probability of this x  So that's why the rule for rejection sampling is that you sample.  X. According to q of x, the proposal.  And then you will accept  This simple  With probability  Which is  P tilde of x divided by a queue of x.  And we know that AQ of x is always bigger than people of x. So, this is a number between zero and one. Right. So this belongs to zero and one.  So we reject, with priority one minus that which actually kind of represents  This PC. Right.  And so we reject otherwise.  And what we reject what it means is, well, we have to start again. We don't have a valid sample. So if we want a simple. We need to re sample. OK.  So the problem is that if the acceptance property is low, you will need to sample a lot of times before you can accept so that could be very expensive. So you don't want to have a low acceptance quality.  So that's the algorithm and and now I will show that this sample from the correct distribution.  So let's show  That the accepted sample.  Has the correct solution.  Distribution.  IE.  Its distribute according to Pete  Even though I sampled x by q  marginally  It will have the correct distribution.  And again, for simplicity will do the thing for when X is discrete, to not  Have annoying PDF issues.  Alright, so  Basically what I want to compute is the conditional on x, given that X was accepted.  So to compute that I first need to look at the joint.  So there is asking, what is that P and P tail right so the point is, I don't need to care about the normalized value. So said p  Zero P here is just the sum over X of details. So it's to normalize distribution for at some to one.  And the property. We need to have is just on this on normalized value. And when we compute  The acceptance ratio we only use the normalize the unrealized value PTO right so it was more general like you could think of people as just being p of x and and Zed is equal to one. But the point is that  We don't need access to the on normalize to the normalized value of p to to do the argument we only need this detailed and this eight user avatar   Okay. user avatar   Alright, so to compute the conditional that on x, given that X is accepted. We'll start with a joint. So the probability that X is equal to little x and x is accepted.  And so this is the property under the sampling mechanism. Right. So this is a joint on their dissenting mechanism.  So this is equal to the probability that X is accepted.  Given that x is equal to little x  Times the probability that X is equal to little x  Right. Alright, so the probability that X is accepted. When capital X is equal to little x is  Detailed of x divided by a q of x, right. That's by the definition of acceptance property.  And the property that capital X equal little x this is marginal. I don't know if it's accepted or not. This is just coming from  Qx, right. So this is the proposal.  Which is qx user avatar   Right. user avatar   And so the queue of x cancels. So I'm just, I get that this joint his PTO of x divided by eight.  And so in order to get the conditional. I need to compute the marginal that P is accepted.  Sorry, the X is accepted.  The marginal that x is accepted is just a some overall x  The quality, the joint of x equals x and x accepted. So that's P tilde of x divided by eight.  And so that gives me Zed p divided by right  And so now if I compute the conditional that capital X equal little x, given that X is accepted.  Which is basically the samples, whichever accepted. This is the joint which is p tilde.  Divided by a  And then divided by the marginal which is Zed p divided by  And so the eight cancels out and I'm left with people that are bi z p zero P which is just pure mix.  As I want it.  And so, by the way, while doing this proof. I also computed the acceptance property right. So this is the marginal probability of acceptance.  And so we want this to be high.  So if this ratio is very small.  Will need a lot of sample before we can accept it. So it will be expensive.  And is it gathers asking a good question. It says, so the sold benefit of rejection sampling is that we don't need to calculate the partition function know because we're not something from P where something from queue.  Right. So look at this graph. For example, this queue is super like weekly and complicated. There's not me see a reason why I'm able to sample directly from it, even if it's a normalized on them realize, so I could have a much simpler. Q. Also, I was appalled at Nice. You need me to model cute.  So it's both that a simple from a simpler distribution. And also, indeed, I don't need to know the normalization constant of  Which could happen when I have an undirected graphical model.  Okay so rejection sampling is very general. And now let me show you how you can apply it for conditioning in a dark graphical mode.  So application.  To conditioning.  In a the GM  And and this is actually some kind of algorithm that you might have heard of already  Which is basically you just sample from the node.  And you reject when the node are not equal to the evidence.  Right, so you do ancestral sampling. But the problem is, like,  There's let's say you start from the roots. Let's say, for example, I have this graph here.  You have this graph here.  A new computer. Here we go. So I have this graph here and let's say this notice observed right so now I simple this know that simple this node.  When I need to make sure I could simple extreme but it has to be consistent with the observation. So what happens is if extreme is not equal to the observed value, you will reject and start again. Okay, so that's the algorithm.  And it can actually be seen as a special case or rejection sampling, which is also why it is a correct sampling algorithm.  And that's one way to prove that this natural algorithm.  Will sample from the correct conditional  So say you want to sample.  From the conditional p of x given some evidence observation. user avatar   Alright. user avatar   Alright so here  We could use  P till the vics the on normalized distribution as the joint on px he  Complement  And x E bar.  And the clinical delta on x, right, because the because I'm conditioning on X e  I need to fix XC so so you can think of it as this is the joint on Etsy complement and he actually has to be fixed to the bar XC component could be anything.  And this is this is now a normalized because I need to make sure to some to one when I changed all the X. He compliment. And so the normalization constant here is the summation over this overall x E, which is just the probably the marginal quality of the observation. Right.  And so that means that the condition. The on the normalized distribution over x is just the conditional on x. He compliment given x bar, and I've allowed also to evaluate this on.  Non you know on on invalid up evidence. No, but I will put the property of this is zero.  So Jake about come back to your question. Once I've finished putting  Describing a rejection something scheme for for this setup.  So if we simple from POV next year.  I get the conditional on the unobserved nodes, right.  And so then we need a proposal, so let qx  Be the original joint  Okay, which you can do so if we sample from the original joint I can use ancestral sampling for that. So you can sample.  Using  Ancestral sampling  Right.  And so then I have that you have x  Is just p of x. He compliment and X, he  And I have that q of x is bigger equal  To pee till the vics  For all x, so I can just choose a equals one.  Alright so here I had that my on normalized distribution, all I've done is I kept the joint and I just set all the invalid evidence. No to zero. Right. And the queue of x which is normalized in this case because it's the original joint doesn't have this 01 term. user avatar   Okay. user avatar   And so that's my proposal and so to do rejection sampling. I need to compute the rejection property or the acceptance property. So the existing property.  Acceptance probability his PTO of x divided by a cutie build that sorry AQ of x.  Boo boo boo.  Have x  And and so that's my detailed, that's my cue. So the only difference is this delta right so  The acceptance probably T IS JUST credit card delta of x E x bar. Okay.  Which means that if my evidence node match the observed value that I should get then I accept with quality one so I accept always if it doesn't match it. I always reject. Okay, which is why the algorithm here is  Do  Ancestral  Sampling  And then you accept the joint sample if the evidence that you got matches the observed value that you want it. user avatar   Okay. user avatar   And so this is basically rejection sampling for the gym.  So Brendan is us asking how do us here to to extricate lemonade speed tilde of X without assuming access to capital X true distribution will hear it was an example where you know this is coming from the the GM right so  So, so, I mean, it depends. Like there's just a lot of different examples where you can look at to kind of get a sense for how these are applied.  You know, like you could have, let's say, for example, I have a Gaussian  Well, you know, for anything which has full support, this will dominate with a as long as you have, I guess you need to have a tale of the distribution you something from to be sub garden. The garden, will you get them, you need anything, as long as they have sub gosh until something  So you can have properties of the distribution that you're thinking about, you don't need to have an exact handle  It starts, I guess, saying just would be impossible and continuous distribution, um,  Think that still works.  Okay, so yeah, so the problem is  Yeah, so that doesn't make any sense as an algorithm because  The quality that you exactly get the evidence will be zero. Right.  And so here the property of acceptance.  Quality of acceptance.  Is Zed p divided by A, which is, is it a p of x E bar, but this was indeed using discrete data assumption. And so if you actually have a continuous random variable.  There's probably zero that you will get the correct value by sampling it because it's probably zero that it has a specific value. So you will always reject if it's a continuous distribution. Yeah, so this is only used for a discrete random variable.  And so the problem, by the way, is that if  The quality of the evidence is small, according to your, your, your marginal  For example, if you know you're sending a lot of variables. Usually this property will be exponentially small, which means you will need an exponential number of sampling before observing it, which takes forever. So it's super inefficient.  So we'll see there other more efficient technique for sampling from a conditional unity GM  When these problems. You are a small  So let me talk about important sampling  Which is a way to make rejection something a bit more powerful.  Important sampling  So this is in the context of computing  Me mean  And so because now we're not just sampling from the distribution. We're actually approximating mean we'll now consider the idea of waiting the samples.  And how you use the weight. Well, because you will approximate the, the, the mean as the weighted  Sum of your of the function evaluate them the samples.  And so that extra flexibility can give you better estimates and we use a simple trick. So let's do the simple trick as usual will multiply by one. So the expectation respect to p of f of x.  So this is a  Basically summation over x of f of x.  Px  And so this is a summation over x.  Of f of x p of x.  And then I can divide and multiply by qx  For some distribution.  And because I don't want to have zero divide by zero, I need the support of que  Tu to include to be included in the support of Pete user avatar   OK. user avatar   So now this is just the expectation respect to q  Of f of why p of y.  Divided by q of why  Were Why is simple. According to cute, so I've, I've transformed my expression respect to POS with an expectation respect to queue of f. But then I need to use ARE WE WAITING, which is the difference between P, Q, right to the ratio of pinky.  And so now you can do Monte Carlo approximation of this you can say this is approximately one over n submission over I have g of why I wear why I  Is  ID coming from queue.  And the function, you are actually using g of why is the original function f times await W.  Where  The weight.  Is just the ratio of p  And cute.  So these are the weights. user avatar   Oops. user avatar   Okay. And so the importance sampling estimator of the mean so mew hat important sampling, it's one over n summation over your samples of F of why I  Wi  Where why I has been sampled from queue and the weights or define as p of why I divided by q of way.  And so these are called  Importance weights.  Yes, Dora. We need the support assumption to avoid to have a division by zero.  These are called importance. user avatar   Weights. user avatar   Well, bomb, bomb.  So I have two minutes left. Let me do the extension to our normalized distribution. So this is OK. So this is unbiased. So the expectation of new important sampling is equal to view.  The variance of mew is actually equal to one over n.  The expectation of a p of f of x square p of x divided by q of x.  Minus new square  And so you can see here that you have a high variance. When this ratio is is big. So when Q  When Q is small.  But P is big, then P over Q will be a really big number. So, this will create a big variance  And in particular, it turns out that  That indeed.  Sometimes the variants of the estimator can be infinite. If you're not  Careful.  So let me do the are normalized extension. Because right now, I have to compute the weights, I need the ratio of p divided by q which means I need to know their  Pm, okay. Simon is saying, how do you pick the best few will actually, that's an art indeed intuitively  You want  Cue  To be proportional  To f of x px  Then it will minimize the variance. And so it will give you the best estimator.  Alright, so let's talk about the extension to our normalized distribution.  And then I will be done.  And I will have caught up and then next class will talk about Markov chain. What they call them.  To fix these issues of very high variance of important sampling when you're sampling from from  A low mass. So basically, you know, this thing happens. For example, like let's say I have a multimodal P is P of x.  And then let's say q of x is a simple Goshen, so it will be simple Martin is, you know, it will have a simple mode.  Well then what happens is that in the in this region here q of x is exponentially small but p of x is like order one. And so then you have super high variance because of that.  The weights would be super big here exponentially big and you simple them very like very, very rarely because of qx and so then you get a super high variable.  Estimator and we'll see how to fix that using Mark Markov Chain Monte Carlo next class, but let's see how to run this over them when we don't know the normalization constant. So now we suppose that  P  Is I have access to through the are normalized distribution que, same thing. I will have only access through the normalized distribution.  And so then  My mean, which is the expectation respect to queue of  F of why when I simple according to Pew P of why did it take you away.  Well, you can think of this as  Expected respect to queue of F of why and then I take the ratio of the on normalized value, which I can compute when I have access to them.  And then outside I get Zed Q divided by said  Okay, so then the trick is, we will estimate the ratios at KU Zed Q divide by that P with the weight so we can estimate  Zed p divided by q  With this estimator, which we call that hat.  P divided by q  And this will be one over n summation over my samples of P tilde of why I divide by q  Field of why. And this is just the empirical average of my weights.  Because the weights which appears here are p divided by cubic  But PT all the way back. You tell, sorry.  And so then we plugged back in  This ratio because  So I so I have the, the original on normalize estimator. And then I need to divide by zero P divide by city. Right. So the, so basically the all normalized important sampling estimator. I'll use important sampling or normalize I will just use the usual weighted mean  Of f of why I W I but instead of n on the numerator, I will just use the average of the weights.  And here why i is from queue and the weights are actually defined as the ratio of the on normalized user avatar   Distribution. user avatar   So that's the  Hoops  The trick. So basically,  This went here.  And this ratio here when there  Was the idea  And so now what's happening is that unlike the normalized important sampling, we have that this is bias estimator because of this ratio of normalization constant in expectation. It's actually not the ratio of the number one constant. So this is slightly bias.  But it is a synthetically unbiased.  Saying that the Kelly.  Unbiased  As an goes to infinity.  And  What's important is that it also has lower variance  This estimator.  Oops.  This estimator.  As  Often  Lower variance  Then the normalized one  Even when you know the normalization constant, even when, as I said, p equals that Q which is equal to one.  Because basically what's happening is that the normalization.  From the denominator it stabilize the estimator.  Right. So basically, the new weights.  That you're using  That's called them wi tilde our Wi divided by one over n summation over j of W j and this belongs to zero, n  For any value of the weights and so  So they're bonded for a fixed n your weights will be in the bonded range, whereas the original important sampling estimator might have weights which goes to infinity. Again, particular if we go to this. Oops. If we go to this.  Drawing here and of the multimodal case, here we go. So  When I simple points in this region, though the ratio becomes bigger and bigger. And so it's actually  That's it. I had some tail here. So I have a nonzero quality of getting an arbitrarily far point here which will have an arbitrary big ratio. So the way it could be arbitrarily big so they're unbounded.  But by using this this normalization here you stabilize the weights, which is why you reduce the variance  Okay.  And last thing is that I've skipped some things that, but if you're curious, so you can look at the 2017 notes.  For Lync two variants reduction.  Which is something use for optimization called Sega  So there's a notion of variance reduction which are used to in in like estimator of these mean  And you can use it in optimization to get better optimization  And also something called rattle Blackwell ization  From Blackwell, and that's a statistician and row. And that's the idea that if you can compute  The margin all have some variable analytically in close form, you should do it, rather than sampling it, it will reduce the variance user avatar   So that's kind of the general idea. user avatar   Alright so I'm a bit over time. So is there any questions about important sampling and rejection sampling again in the big picture so  These are just  Example of something schemes as I told you, like sampling is actually an art and you can get, like, a lot of lectures on that, in particular, and how to design the correct proposal.  Where we'll see next fast is to use monte-carlo Mc, Mc to to also tells you in some sense it's dramatic way to to give you a good propose a better proposal than these important something  Because that's more adaptive. But then we will get rid of the ability to have independent samples. Like here we get independence.  Which book do you recommend reading about this. I think I've put some in the in the lecture.  Brendan You have a question. user avatar Breandan Considine  Yeah, I was wondering, Well today we're kind of talking a little bit about  The, you know, like these.  sampling techniques that is that the data generating process is like a black box.  So, so this f  Is is kind of, you don't have access to the implementation of it.  But at same time we're all we kind of just care about these these these parameters like the mean the variants and so on. If we have some knowledge about the the data generating mechanism like the dynamics and how that works.  But  It seems like  Well, do we do, we can we get away with.  Without doing all this sampling work. We are we, we have access to the the analytical implementation of this. So then we should be able to propagate like uncertainty and covariance through this function.  Is that something  Well, user avatar   So, I mean, the short, the short answer is yes and no.  So basically if you have more information about f in terms of unethical properties of f, you might use some trick to kind of simplify things, but in the general scheme of thing.  We're talking about how to compute  high dimensional integral. When you take an expectation and high dimensional integration usually don't have tools form formula. So you will need to use numerical techniques for that even if you have good properties of F in under, under your hat. user avatar Breandan Considine  Hmm. And so user avatar   What I expanded the beginning was that Monty Coleman integration usually would have better error properties in high dimensions and say numerical integration. user avatar Breandan Considine  Yeah, yeah. I mean, it seems like that, that if you if you just have some uncertainty about some variables in in a function, then that should be  A little different than the high dimensional case, right, like you have some observations that you'd maybe don't know the exact numerical values, but you have a some gal shins over these variables. And then when you combine them.  Then you should just there should be like a closed form, even if they're very large number of variables right then, it seems to me that if you're just doing simple combinations of these then  Then some of these estimates should be tractable. user avatar   So it's true that you could generalize this ideas of competition or graph these use for neural network.  To also how to compute forward propagates in some sense in the graphical model with simple units, these, these expectation with simple expectation. When you have F. Also the composer's in a nice way.  But you have to be careful, right, because even if you use Gaussian  As your noise, the integral of Gaston isn't tractable. It's a CD. It's a it doesn't have a close from from the arrow function, but it's still you need numerical methods to compute. And so you have to be careful also about like what do you mean by having close form formula or  But it could be that indeed using these iterating arrow function evaluation will be more efficient than simply that's user avatar   Possible. user avatar   To actually can we, so let me stop the recording.  Because I think that we're
  Okay, so last class.  I started to talk about sampling as a way to do approximate influence. Right. And so, towards the end of the last class I mention several groups. That's the wrong one.  I mention  Several sampling schemes and finish with important sampling  And I mentioned that the issue in important sampling is that these weights, which is the ratio of the thing you're really trying to sample from versus your proposal distribution.  Can get really big or can vary a lot in size and, in particular, if your proposal look like like this bump and my true distribution is multimodal  Than the ratio between this is probably to hear and the value of q here will be tight will be really big could actually be expansion of the big so you'll have  Some product small relative value of your proposal, you'll have huge weight when so it actually increased the variants of us to mater.  And so for important something to work. Well, you need to have that your proposal matches the shape roughly of your, of your distribution, but this is hard to have proposal which satisfy that  And so today we will now consider a series of technique which kind of address this by having an adaptive proposal. Okay. And so the plan today is to talk about  Markov Chain Monte Carlo.  Today we'll talk about Markov Chain Monte Carlo Mc, Mc  And so this and why we will will have to talk to, to, kind of, analyze and make sense of them will have to do a bit of review or just present the theory of Markov chains of finite Markov chains, and we'll talk about the metropolis a sting algorithm.  Just one of this standard Markov Chain Monte Carlo.  So let me start with Mc, Mc  So this stands for Markov chain.  Monte Carlo.  And so the idea is  To  Relax the independence assumption.  The assumption between the samples that we will use  To do our Monte Carlo estimate of our average  Right, so some I talked about Monte Carlo integration Monte Carlo estimation last class where you basically just simple independently from the distribution multiple times and take the average  Problems, it's, it could be difficult to sample well independently. And so in the Mc, Mc approach approaches, instead of having independent sample will have samples which are  Defined them with a Markov chain in time. Right, so we will allow basically an adaptive and this will allow us to have an adaptive proposal distribution.  Instead of a fixed queue, we can tailor, we can design a queue which varies depending on where into space. I am okay and so what we'll do is we'll run a chain.  Where we'll have a simple at time iteration t and this will be given by a proposal which will take the previous state in my chain. I will condition them that. Okay.  And we'll do a chain in such a way as that as t goes to infinity.  Or random variable will converge in distribution.  To the target distribution p like converge in distribution to the target.  Distribution.  P. A. And this limiting distribution which will be something that we want is actually called the stationary distribution of the chain.  This is called discretionary  Distribution of the chain.  Meaning basically that starting in any States or in some sense like  When your random variable gets has a specific distribution which is p then doing one more step up the chain won't change the distribution anymore.  In but I will formalize that very soon. That's just to give you the high level idea and so  And so before we would simple the samples independently from a fixed proposal. Now we will use transition kernel. Basically, we will have a proposal which depends on the previous day.  And suppose now that we have this chain.  And that's the Monte Carlo part we will approximate  Or  integral. So the expectation respect to pee of FX  As  An average  Starting after some burning time, so it will be t minus t zero. They don't t is equal to t zero plus one up to capital T of effort 60  Okay, so will will will take a an empirical average of our function on the samples, but we will skip the first few sample because the first few sample or not coming. THIS IS LIVE ON THE RIGHT distribution, because they're influencing by where I started my chain.  And so we, I need to forget the initial condition.  And so t zero  Is called  The burden period.  Which is basically how much time I need such that I can guarantee that I forgot my initial condition and I get a sample which is close to the stationary distribution.  Which is the correct distinction that I care about.  And so, and this burning time how long you need to wait with depends on something called the mixing time of the chain. Whoops.  So this will depends  On the mixing time  Of the market change.  And we will will  Study in bit more details the properties of Markov chain will get back to, to what a mixing time is and this but didn't practice, you need to you need to  Do not consider the first few samples in your chain, otherwise you'll get like artifact from your initial ization  So that's the way you use your chain to do the Monte Carlo integration and an important comment that I mentioned in the past, is that you don't need actually to thin  The samples so thinning a chain in the parlance of sampling means that you you you skip every few iterations in your chain.  To use in your empirical average because you want to have more independent sample, right, because when you have to subsequent sample.  Because of this conditional there's there's correlation between them. So they're not independent.  So by skipping a few of them you ensure that the the two samples are more independent but and so often that's used to have more independent sample.  But it turns out that actually a as an estimator of you, you get higher variants by doing that. Okay, so you there's no need. Oops.  So there's no need to thin the samples.  And what that means is you will basically use a delta t between  Samples  To get more independence.  And the reason you don't need to sit it is that is turns out if you look at the book by kasler  Was it burger.  Burger Monte Carlo statistical methods that I mentioned in Slack, so they will they have a theorem which says that  If you sin. This will yield.  I mean, for more  Reasonable distribution. This will yield higher variance estimate this will yield higher  Variance user avatar   Okay. user avatar   So in practice that's important to keep in mind, it is better.  To use all the samples.  After the burden after t zero  To compute your to compute Mew.  Unless it is too expensive.  Okay. And so what what I mean by unless it was too expensive. So the phenomenon, which is happening here is that  If you remember when I analyze the variance of the Monte Carlo estimator. I use the independence between my sample to show that the variants will basically  Grow as a signal square divided by squared n were endless number of samples and I told you. Well, if you have correlation between your sample.  You won't have entered into the nectar, you will have something else which is basically the effective sample size, which depends on how much correlation, you have  And so by you, having more independent simple you'll have  A bigger number, but by thinning. You also have less samples. Right. So for example, if I use if I skip one step every  If I take a simple than stick skip one and and take a simple then skip up you have half the number of samples that you would do if you would take all of them.  And so there's a question of like, well, I have something with say to end or let's say with twice the number of samples, but they're correlated  Versus something which has have the number of samples, but they're more independent which one has lower variants. And then there's a fear much says, actually, that it's always higher variance to  Not use all the samples. And so there's not enough gaining independence compared to the actual just smaller denominator you. So that's why you always keep all of them.  And Adele is asking, How come they are independent independent 60 I didn't, I didn't see I didn't see their independent right I said that  When you do more Markov Chain Monte Carlo. Sorry. When you do multiple integration, you don't need to have independent samples. That's something I mentioned here, blah, blah, blah.  Was this. Here we go.  So I said that you had the the unbiased this property.  Even if the x i are dependent. Okay, so in in average your estimator will do the right thing.  And then the question is what's the expected there and there. I use a dependence to get S, basically. And in the denominator. If you don't have independence, you will have a smaller value.  Which will be basically could be a definition of the, the effective sample size. Okay.  And then there is asking if thinning will be using the previous simple algorithm be covered up to now.  Well, so in all the previous are so that's a good question there. So all the previous organ. We mentioned right now as independent simply right. So basically, when I did. For example, let's say I do rejection sampling. Where's my rejection something  Yes, originated sampling is you sample independently from the proposal. So every x is independent of the proposal and what happened is you  Sometimes you will reject which when you start again. But it's always independent right so so you wouldn't need to do sitting in all these other islands, you only need to do thinning when you have dependencies between your sample and  Something is expensive because as I said like for estimating the mean it's better to use all the samples. So you don't need thinning. But let's say computing F on a sample is very expensive. That's a computer. So you need to, you know, run the simulation for like hours. Well, then  It's better to have more independent sample because  Like for example, in the extreme case suppose that it takes like let's say that you have a chain, which is stuck, which basically every simple is the same. You only have one sample. So then you would just lose computation to use all the samples.  And so, and, more generally, if it's super expensive to do f of x t  Even though using all of them will reduce a bit the variance, it will be so too expensive. So it might be more effective to divide by 10 take every 10 samples. And so you divide by 10 your competition costs and you want to see increased by 10 the variances will increase on these lately. user avatar   Okay. user avatar   Alright, so that's a practical idea. And now here's a bit of a little cartoon of the motivation.  So suppose that this is my  target distribution. This is p of x that I'm trying to sample from  And so  The idea now is we will use an adaptive proposal right let's see my previous samples my paper my previous simple is here.  That says have a Gaussian proposal around my current simple and this could be cube of X given expression. Okay. And so this is what we call it an adaptive proposal because the proposal on X depend on x prime, ie where I was.  And then what's happening now is that  Perhaps I didn't draw my thing well enough to let me see the user avatar   Areas that user avatar   Yeah, so basically it's. Oops.  I want to get ocean, but the wide gushing  Okay, it's not very nicely drawn, but they do the main point here is, you can see that the quality of being here is  Higher with this proposal that before when I was doing this, like small bumping  And so what happened is, then I will have, let's say, move there.  Then I would have another proposal, like this, and then there's some probably two of being there, it's your turn to try and so I will, it will basically allow me in a few steps to move between those two modes.  Okay. Whereas if I had only a fixed proposal, which I need to kind of choose a bump. The quality of being here is like almost zero acquaintance division. So, so that's why I'm not a. That's why when I do important sampling, I would have a super high variance estimator.  So by having this additive proposal, I can actually move in between my two modes slowly and so I can sample from the correct distribution.  And so in other words before  Where we did with two samples.  We're basically x t or ID from q  Was when we do men Mc, Mc  Instead,  Is we have x t  Given X t minus one.  Is coming from some proposal.  On XT which depends on AX t minus one.  And this is basically a mark of  Transition  Probably D.  Because we basically have a chain dependence right when that condition on AX t minus one. Anything which happened before. Doesn't matter for what extent should be because it's it only has four disempowering because then you only you only condition and the previous step.  So Simon is saying, Well, I'm not sure if I understand your question, but let's say instead you have a distribution, which is like this as your proposal so that you cover both modes.  What will happen is that  You will still have high variance, because your proposal is not well matching the actual target distribution, I could say, like, if we go back to rejection sampling as a good example.  Let's say I do this as my proposal. Well, there's a very high profiteer of rejection and lots of places right  And so there's, there's, there's, there's a because it's hard to to match the target distribution, you will have to sample a lot of time before we will have to reject a lot of time before you get a correct sample. So that's also the problem here.  Doesn't make sense assignment. user avatar   So I guess what I have in mind is like what if, like at each iteration you you sample from like a new option and you define like the variance to be like inversely proportional to whatever you sampled from your function.  So if you get like really low values you have high variance. We have a higher likelihood of landing somewhere else where maybe the value is higher. And then because the value is higher, it contributes more to  The overall like expectation, you're trying to compute and  That makes sense. user avatar   But to your, your proposal is to change your proposal, depending on where you are in the space.  Yeah, which is what Markov chain multicolor does user avatar   Okay. user avatar   So yeah, so I think I didn't get the details of your proposal, but indeed.  Already allowing ourselves to  To use where we are to have an adaptive proposal will be very powerful. All right. Yeah.  And so  So before I tell you how to designs. And so, for example, the metropolis Hastings algorithm is a very generic way to get these Markov chain with the right property which will converge to the submission. We care about  So before getting there. I will just now talk about properties of Markov chain. So we will do a little review.  Or teach you, for the first time of finite state.  finite state space.  Market Markov chains.  And you know all this can be generalized to infinite space and to continue space, but it gets more complicated and it doesn't give you that much more insights. So that's why I restrict ourselves to finance space. And let's say the number of state is key.  And so let's look at our Markov chain.  We can look at it from a GM perspective as a directed graphical model you can think of a big like each simple as a node.  And then you basically have x zero, x one, blah, blah, blah.  And then you have X t minus one. Let's see next. Okay, so that that's would be a simple that's simple Markov chain.  But I also, there's another way to think about a market chain, which is to look as a finite state of the matter with transition right there's also  The transition  Probably probability point of view.  Which gives you a different perspective here, you don't care about how the time various things vary in time, but more. What's the ratio between different states. So this is the probabilistic finite state automatic point of view. And so you use one node.  Per state.  So, for example, suppose  K is equal to four. So I have four states, I could now depict my force state.  Um, let's say this is state number 234  And one  So these are not now we're presenting random variables, right, there's this. We're not talking about a graphical model. Now we're just saying each notre business state.  And then I can have directed edges between state when there's probably people transition between them. Right. So for example you can see when I'm in the state.  Three I transition to state for with quality one one in the state for acquisition to state one with probably Q1 and when I'm in state one, I can position to stay three with property one half, or to stay to with property when enough  And then what stage to stay to then I circle back to one. So that's an example of a transition property that I could define I just need to make sure that all the arrows coming out of a note that sums to one.  That's also useful to identify our dare like cluster which happens, can I go from any state to any other state with a number of iterations and this kind of stuff. And actually, we will use these properties to talk about the convergence of markup G.  Such something to keep in mind. And in our case, we will consider homogeneous Markov chain.  And so the  Probability of going in state i, given that I was in state j at the previous time step is just some fixed matrix a big right so there's no time dependence.  And a as before, is a case by case matrix like for the hmm. So it's the same thing as the hmm, such that the some around the column is equal to one. So I'll use annotation.  One key for the vector of size key with only entries which are equal to one. And so if I take the column, some of my matrix, I get basically when you user avatar   Can this is user avatar   The vector  Of one's  Size key.  So this is called a left stochastic matrix.  Left.  Stochastic  Matrix. Because when you multiply the with the vector of one on the left, you get one. user avatar   Okay. user avatar   So it's very similar. It's very similar to  The hmm  And so now you can say suppose  That the probability  At X time t minus one is equal to j. Let's just call this A. J.  As a vector. So this is from part of the property simplex  Then if I want to know what's the new marginal. So a time step, I have ever distribution of restates which is by now if I propagate this with my Markov chain. What's the new marginal at the next time step. So the quality of St being close to I  You need to marginalize out the joint. So this is the summation over all possible states, the probability that I was a went to I from Jay  Times the probability that I was engine, right.  That's just the marginalization of my joint of two variables. And so this is just a Jay. This is pi j. And so you have as a matrix that you know the new distribution. A t plus one.  Is a applied to it.  Gets here there's a bit of notation issues.  I'll use the annotation here like this.  To make sure that it's really the j component  Okay, so basically if I have an exhibition at time t, the effect of the transition colonel in my chain is to transform to the submission by multiplying by eight.  Right. Which is, oh, by the way, this is also why I use a matrix as I define it because I said it's just vector matrix multiplication.  If instead of a left stochastic metrics. I had us rights, the Catholic matrix, I would have used need to do the transports, it would be piety transpose A. And then I would get it plus one transport.  And so every time I apply my chain I multiply by A. So that means that if I want to know what's the distribution. The marginal distribution over x at time t is just my matrix A race to the tee times my initial distribution. user avatar   Okay. user avatar   So that's the effect of, you know, of the property transition happening in my, in my  Chain. So now you can see the relationship between the initial distribution pi zero and the distribution after t iterations and my chain.  And the idea will be to design a in such a way as a race of the tea will converge to pi, the target distribution.  Irrespective of pi zero  And so the first thing is we will call  So a definition is the discretionary  Distribution.  That's the note it pie.  Doesn't have to be by or it could be any variable. But this decision distribution by have a  Is a distribution pie, such that when I apply a on pi i stayed I get exactly by  So that's why it's dictionary. So once I am in by applying the changes and change anything. I stayed by  Okay and so already, from a linear algebra perspective, you can notice that pie.  Is a right  I get vector  Of eight  With I get value.  One. Right. So from an email Juba perspective, you know, eight times a matrix. Sorry, eight times a vector which gives  The same vector times the scale or lambda. That's an eigen vector right and so here pie as a species is eigenvector, which it's called a writing and vector because you applied on the right.  And it as I get value one.  And so if you could do the you know the diagonal ization of a and identify what's which I get vector as I get that you have one and is positive.  Then that will give you the social distribution. So, from the algebra perspective, you can already have a tool to find this mystery distribution of a matrix.  Now there's a fact that  Every  Stochastic  Matrix.  has at least one stationary distribution.  Okay, so you can have multiple than one by the so you could have to distribution. Yeah, it's not unique. So sack. There's at least one, but you can have more than one in general.  And the intuition why you have one. It's actually the sun it's related to the browser's fixed point theorem.  Okay. And the idea is in one D. If I have a mapping  Let's see, let's say I have a function which map the interval 01 to the interval 01 right so it stays  In the same interval. And it's a continuous function.  So here's an example of a map continuous mapping which maps the  The intervals are one to the intervals, or one. So it's supposed to stay in the interval. So let's do a bit of drawing here.  Let's see. So here's a random function which satisfy which map. The map the interval to itself. Well, you want to find a point such that you mapped to the same point. So what you do is you you draw the identity line. Oops.  You draw the identity line and then you notice that boat, you know, that's a fixed point  Where the identity line cross your function that gives you the the fixed point because every point in identity line as the same x and y coordinate  And there's no way you can have a function which maps all of 012 all of 01  without crossing this identity line that's basically the fixed point there. And that's why is always a fixed when things are continues.  And and we can generalize this argument to mapping the polity simplex to the property simplex with a continuous function and then there's, there's no way you can build a mapping, which is continuous on the whole project simplex without crossing the the identity. Let's come to you.  Okay, so there's always one session distribution.  But then, when is it unique. Right. So that's the question will ask. And so now we will end because the algorithm basically will be  Designer Markov chain such that when you apply it multiple time you converge to its special distribution and  You hope that this session distribution is the correct target distribution. And if there's multiple session distribution, you're not sure to convert to the right one. So you need to design your chain such that, especially this mission is unique and is the one that you want.  It. So now we'll give some properties of change to ensure that you have a unique special distribution. So let me define  Something called which is an irreducible Markov chain.  There's, by the way, there's a lot of ways to define this thing.  Yeah so multiple question are asking whether it's possible to come to construct a weird function, which doesn't happen.  No. Right. It's a theorem. So I encourage you to try, you'll see user avatar Namyeong Kwon  It's it user avatar   Always works. So the condition is that your function has to be continuously their function is not continuous, of course, knew I could, for example, here's a function which you know  I go here and now. Oops, I go there. So I made a big jump. So I'm allowed to skip the intimacy. But if the function is continuous, we will never be able to construct a couple example of that. That's why it's a theorem. That's the magic of math.  Which, by the way. Also, when trying to find counter example is also a good way to get intuition why something is true.  Yeah. So Jacob is asking, what am I talking about here. Yes. So A is a mapping, which takes the distribution and gets another distribution.  Right, so, so you can think of it as a tertiary linear in this case, but it's just a mapping from the property simplex to the property simplex. And so I'm saying that a will always have a fixed point which is essentially distribution by the browser 6.0 on the property simplex. Okay.  Alright, so I was talking about the decision of the irreducible Markov chain. And I was going to say that there's a lot of equivalent way to define these concepts and just oops I'm just giving you kind of like  One of the simplest so that you get the gist of Mc, Mc  So you see that the Markov chain is irresistible.  If there exists.  So if for every  Pair of states I NJ.  Okay. All right. It's like, there exists.  A positive  Probably t  Path.  From  Any i to any G.  States.  So that means  That  After a specific number of hops are playing my chain, I will be able to go from any state to any state.  Okay. And why is it called irreducible because sometimes if you have a reversible markup chain you could spit your chain in two pieces of change, for example, that could be a chain where everything is reachable in this piece and another piece where everything is.  Is reachable in this piece, but you cannot go from one to the other, because you know you don't have this path. He said that. So then you could think of it as a transition pro transit transit to every time. And then you get these cluster of irresistible markup piece.  So that's why it's called like this. And another way to state it is that for all i and j, there exists an integer.  Em.  With i j. So, it will depend on i j, such that when I have a plot. When I done M AG steps of my Markov chain. And I look at the property of going from I 2G from a, I guess it would be GI in this case.  To be clear,  Because I say I go from i to Jay. So that would be Jay i is strictly bigger than zero. Okay.  Because if you recall, you know, to see what's the quality of where I am. After teacher ation is just, I take my matrix race to the t. Right.  And so I could put an indicator on on the state i. And then I want to know what's the priority of being on stage. Jay, Wilson, I need to have something bigger than zero, to make sure I can be there. Oops. And I guess I didn't put my temporary user avatar   Okay. user avatar   What is the symbol of the integer, it's me.  Let me rewrite this.  I am not sure why I'm having trouble writing  And  Alright, so by and then it turns out that by parent for business theorem.  So that's a theorem. You can look at it on Wikipedia, which talk about properties of matrix which are which only have positive entries. And so it's the classic matrix have always non negative entries  And it says that if the market chain is reversible. user avatar   So if user avatar   If a markup gene is irresistible then it actually has  A unique stitch redistribution.  And basically,  Here it will say that you get the unique  You will have that the multiplicity.  Of eigenvalue one  Is one. Okay, so basically the parent for me to steer him. It says that if you have an irreducible Markov chain. The  You will have only one I get vector with the eigenvalue one which is positive, so you don't have multiple century distribution that's just another way, but it's kind of stated as you know your proposal.  OK, so now if we have every possible Markov chain. We know there's a unique specialty distribution. So the question is, can we ensure that will converge into it because it's not clear. We could have a unique distribution. But still, we're not sure to always conversion to it.  Depending on where we start. And so that's the other property, we will need. So, in order  To converge.  To it to this district distribution.  You need something which is called a period, a city a peer DC  As well.  Which is to tell that you won't get stuck in cycle like you could happen when you don't have it when you have a periodic Markov chain.  You can get like after a few iterations, you get there. And after tuitions you get back to where you were. And you always keep cycling never converging to a fixed distribution. user avatar   Okay. user avatar   And so I won't define a periodic by itself. I will just say that irreducible.  Irreducible  And a periodic  Mark of change chain when you have a finite number of states.  It's if and only if there exists a fixed integer em.  Such that a race to the m is tricky, bigger than zero. So all the entries are bigger than zero. So, when it was irreducible.  For every pair of states, there was a different integers. It could be that sometimes it's too hot, sometimes in three ups, etc, etc. If the market changes also a periodic. You can find  An integer which worked for all the pair of states.  And another word for it eras, boy. And if you're a tick mark up chain. It's called a regular Markov chain.  Or  It's called a Gothic  Markov chain.  And again, these are all for finite state.  So if you if you have infinite state space. There's a bit more properties for a regular market chain ethic.  Again. And to be clear, this, what we mean is that a race to the m i j bigger than zero for all i, j right so it's  All the entries of A matrix are positive.  Okay, so that's a. So when we talk about organic market chain we talking about their resistible in a periodic markup chain and we will see  Very soon, that this will guarantee that we converge to the district distribution which is unique.  So, bingo, she's asking, Why am I saying that is always  Is saying, well, is always positive, right. So why does it say a racism is strictly bigger than zero, so he does not have to be strictly positive, you could have that the quality of going from one state to another state is zero.  As long as the sun is equal to one. You don't have to move between all states and I'll give an example actually in  Two minutes but just before I give an example I will mention an important note.  Because we will use it when we talk about the metropolis Metropolis as thing or remember. So a sufficient  It turns out, a sufficient condition.  For in irreducible.  Markov chain.  To be a periodic  Is that there is some state.  So, such that the property of staying in the state is strictly bigger than zero.  And so the the a periodic means that you don't have this period property again. So, and it turns out that some sense that the M you find is basically the greatest common divider of all  Of all these different MIT or something like, I think you use the GC D for all this emoji. And then what happened is that if  They're, they're like prime with each other and the GTD is one some sense then you won't be able to have this cyclic behavior.  And so by making sure that you have some quality of staying in the same state. Well, you can always screw up the periodicity of the chain. So that's kind of like, why, if I have some state where I can stay it, then I will get  A periodic Markov chain.  Supposing it was already reducible  Okay, so let me give you an example of a  Let me give you an example of a regular Markov chain.  Where basically you just make a transition. And so this is on K states.  Will just define a which transition on all the other States except the one where you are with probably t  uniform policy. So let's say for example, if we had three states, we would have this matrix here so I stay in the same state. We probably zero and then I go in all the other states with proxy one half.  Right, so each column. Some here is  Something to one. And basically this is one divided by k minus one. If I generalize that to K states. So k is equal to three. So it's one half. And then what I do is I take the  The matrix with only ones, which I can write as one one transpose and I subtract the identity matrix, right. So that will make all my diagonal to be zero, then I did and all the other one will be one of our K minus one. Okay.  Alright, so that's a matrix, which doesn't only have strictly positive entry. So, m is not one in the definition of the organization.  But if I look at a square that's complete a square, this will be one divided by k minus one square  And then I will do one one transpose one one transpose, that's my first term, and I will have minus 211 transpose. That's my cross term and I'll get plus identity. OK, so now one transpose one. This is just key right it's something  I have key elements. So that's key. And so I get that a square is basically one divided by k minus one square and then I can take k minus two factoring out the matrix of one one transpose and then I add the identity. Okay.  And so  If k is bigger equal to three. So if I have at these three states so 4K bigger equal to three, this is strictly bigger than zero, right, this is strictly bigger than zero and this I have an organic  But  For key equal to  It is not a periodic  In particular, you get here that a square is just equal to the identity.  Okay, so after two iterations. I'm back to where I started. And so you keep cycling in this case to between multiple different distribution.  So Simon is asking whether we should always force all the transition to be positive. Know it helps to make sure that things will work. But you don't necessarily have to do that.  So for example, this change.  Didn't allow transition in the same state, but when i squared it, I will still find Mr. T zero. So it was OK.  Okay, so let me  Give you the theorem and then I will take a break and will prove this after the break. So the theorem is  If a finite  Mark of chain is our garlic.  Or, also known as regular  Then, there exists a unique  Story distribution.  Pie.  And  For any  Starting distribution.  Five, zero.  Will get that the limit as t goes to infinity of array a race to the t zero will be equal to pi. So I will converge to the correct distribution for any starting distribution.  Okay.  And basically the speed of convergence.  Is related  To the mixing time  Tau  Of the chain.  And basically,  I'll have to verify this over the break, because I'm a bit less convinced now, but the mixing time of the champions actually can be shown to be  The inverse of one minus the second eigenvalue of our an absolute value of our  Matrix. So this is the  Six second  Biggest  Again value of a note that the biggest eigenvalue will be one. So that's from like parents for Venus theorem. And so the second one, which is a bit smaller than one because there's only one with the eigenvalues unnecessary distribution.  The second eigenvalue will determine how fast things go and basically you have this thing, which I'm less convinced now, but let me write it down.  So you have that after to ration the lol norm between my true distribution my target special distribution. And what I obtained after t iterations of my chain is basically bounded by some constant  Times x minus p divided by tau. Alright, so the time here, the mixing time tell you how long it takes to kind of get an constant decrease in the distance.  So, so this kind of like the exponentially decaying function is very standard in physics, that's why. Also, this is really called the mixing time in physics, sorry, this is kind of a DK time in physics, but now for its support chain. It's like, it's called the mix.  Okay, so  So let me take a 10 minute break. It's 232  And  I'll give a bit of intuition of why the second eigenvalue appear after the break. user avatar   At 242 user avatar   No question.  Are you totally lost or  Things are cured and that's why you don't have any question.  OK, so the thing I mentioned last time on mixing time is that  Because your, your, your error between the distribution that you get after teeth relations and the target stuttering distribution decrease with an experiential where the number of iterations divided by the mixing time right and so  Basically after Tao.  After tower steps.  The error decreases.  By a factor of one over, he right each one minus one right so you divided the Arabic eat. So that's so it gives you the scale of how many steps it takes to divide the error by  Something which is a in this case, whatever he is like one divided by 2.7 something so it's like a third or something. And so  And it turns out that  Usually the mixing time is exponential. So it could take like 10 to the 23 iterations be be before you divided by two, which is kind of users, which means your takes forever for your chain to mix and so it's not super useful.  But user avatar   We'll get back to that. user avatar   So then, and then what I said is, it turns out that  The mixing time, you could get is related to the second. So a quantitative way to get your mixing time for these finite state chains is to look at the second biggest eigenvalue of your  Matrix transition matrix. Okay. And so I'll give you the intuition here.  For this, which is not fully correct. Unfortunately, but it gives the right intuition. I think from linear algebra.  And this is informal, so I'll stress that this is an informal argument and the formal proof needs to be done. It's not what I'm doing and  And so already, I'll analyze a simpler case.  Where we suppose that a  Is symmetric  And it's diagnosable  With a orthogonal matrix.  Okay, so here we will assume that a symmetric. So basically we have the spectral theorem for it.  And so we'll have that A is equal to you.  Diagonal matrix you transpose  With sigma  With sigma is equal to lambda one, blah blah blah, to lambda case or lambda has or the eigenvalues and I will order them in decreasing order. So, and it turns out that by the Quran.  For Venus theorem.  Will have that the first and biggest eigenvalue is one and it's, it has a unique again, vector, which will which is in the matrix you and it will be pie.  Right, because we have that the special distribution is a unique again, vector and  I want them to be normalized, so I need to divide pie by itself to norm and then I have that this second I get vector is strictly smaller than one that's why there's a unique distribution.  And so I will have lambda to which might be, it might be complex. So, I will just put it up. So, don't you, and then I have all the other eigenvalues up to the last one, and I might have more than one against values which are equal.  So you is a basis of again, vector for my space.  And I have that you is basically you one blah blah blah to UK so I have each column, I will call them you i n us orthogonal. So I have you transpose you is equal to you you transpose which is equal to the identity.  And Jacob is saying saying a dynamic system is mixing is stronger than organic is this mixing time it into this concept of mixing, was it something else. Yes. So when we say that the chain or a dynamic system mixed it's  More specific in regarding are good. It means that it will mix. Eventually, but, you know, we want to quantify how long does it take to actually mix in particular to forget the initial conditions and this is given by the mixing time  Yeah, or good, it gets mixing in the limits. Yes. user avatar   In some sense, user avatar   Papa, Papa, Papa. Alright, so this is just from my spectral the composition or my metrics. A I get these eigenvectors. I told you the first eigenvector, because its associated with the like eigenvalue one  We know that basically a pie is equal to pi, right. So that's why we have that the first I can vector associated with the value one is pi reason and but you need to to make things are going. All I need to have a unit L to norm. That's why divide by the  The alternative right here.  Okay, so that's the part where  I'm a bit less convince is that  We will now consider alpha zero be the coordinate  Of pi zero  In the EU basis.  IE I express my initial distribution.  In the you coordinate  And so alpha zero will be this corner. And so how do I get the coordinate where I can project.  Every component of my basis by  Inner product, right. So, so each. So basically, alpha zero will be you transpose pi zero and you can see it by the fact that you you transpose is the identity.  And so for example the first component of alpha zero. So the first coordinate is just that product between pi zero and the first eigenvector, which is you one, but you want is pi divided by Altoona. user avatar   Etc, etc. user avatar   Okay, so, so this is fine. The problem is  The problem I have with my proof is that this coordinate, I need to show that this is actually  That if I divide by the norm. I get basically one which is not really always the case. But let's see. Now what's happening when I take  My matrix race to the T and apply it to pi zero well because of my spectral the composition. I have you sigma you transpose and then I have another use sigma you transpose and I have that t times  And then I have pi zero, which I will write as you alpha zero, right. So this is my pi zero  Somebody is asking is style always negative. So Tao is a positive number here.  And so I have that this thing is tricky. Small and one. So this is positive, and it's the inverse  So Tao is always positive.  Power. And so what happens here is is this gives me the identity. And so all these  You transpose you becomes identity and when I'm left is you sigma race the t, which is the diagonal matrix. And then I have basically alpha zero  Okay, and what sigma race, the T. Well, it's one and then because one race in it. And then I have lambda to rest of the team, blah, blah, blah. Lambda key rest of the team.  And so  When I looked at a recent t zero, I get  If I take the diagonal matrix multiply by you I get each of the eigenvalue multiplied by you and then times the coordinate officer. Right, so I get basically  alpha zero first coordinate one race to the t you one plus alpha zero, second coordinate lambda to raise the t you two plus blah, blah, blah. And the last one have a zero K lambda key or a city you okay  And what happened is all these  All these eigenvalues converse to zero s t equals infinity because there are strictly smart one.  Okay.  And we said that this is pi divided by  L to normal. Bye.  And so if  You could show that alpha 01 divided by L to norm of pi is equal to one.  Which is a bit weird. That's the part I'm not pretty convinced, then what you get is  A race to the t zero minus by  You get the first term cancels and you're just left with  You know, basically alpha zero to lambda to raise the tea you two plus blah, blah, blah. So you just get the dominant factor is lambda to raise the tea which is right, you basically get this is see lambda to race.  So the dominating factor is the second again.  And so now how do I transform something race to the T to an expansion. When you use it very standard in the qualities you have that  Let's see. I see that lambda two is one minus epsilon one, where epsilon one is the Eigen the first Eigen gap.  Right. And I told you that lambda to is tricky spine one. So, lambda, what I've seen on one is positive. So by basically by definition. Excellent. One is just one minus number two.  Okay, why am I talking about that. Well, because you have the inequality that one minus x is always smaller than x  Minus x. This is true for all x, you can show that from the terror expansion of the  To expansion or no by using convexity of the expansion.  And so, that implies that lambda two is smaller because number two is one minus epsilon one smaller than x minus excellent one.  And so  Lambda to race, the T is smaller than x minus T excellent one.  And so excellent one.  Is one over the mixing time because we wanted to T divided by tau which implies that the mixing time is one divided by  epsilon one episode one was one minus number two.  And so that's basically the thing I wanted to show  Okay, so  So this part here was the fishy part which I'm not really convinced  I didn't have time to really figure this out because I was not able to find the  Correct proof for this in time for this class, but I'll try to fix this for the scribe notes, but at least it gives you kind of the intuition.  Okay, and an important aspect here is that, which is why also in practice these guarantees are bit weak. It turns out that the mixing time is often  Exponentially big  So people use sampling and statistics, all the time. But these guarantees are very weak meanings that to really sure. Make sure that you're not far from the true distribution. Usually you need to wait forever.  But it doesn't mean that it prevent people to use it in practice. And actually, it's still works fine, which is a bit similar to a situation which happens and you know minimizing  non-complex neural network where it's done convicts so you have almost zero guarantees, but people still make them work in practice.  But this is something important for you to keep in mind when people say, oh, you know, I just use the same thing. I'll read them and I can show you convert to the right thing.  And then they think they're done well, they're not misleading because it can take forever to convert to the right thing and so it might still get practical argument.  Okay, so with this caveat, let's talk about how do we design the chain. Right. So the question now is how do we design.  The transition Colonel such that a race, the T pi zero converge.  To buy  And  And so there is one easy way. It's not the only way, easy way. And it's a bit that the heart of all the standard sampling Monte Carlo Markov chain logical center technique is to get a reversible Markov chain.  What's the reversible Markov chain.  It's Markov chain which satisfy the detailed balance equation. So, there is a distribution by  Such that a i j phi j is equal to AJ I pie I for all user avatar   Okay. user avatar   And this is called  This condition here is called the detailed  Balance equation.  And so what does it mean, it means that when you started distribution pie.  You can look at the distribution. So there the distribution of starting in the state and finishing in a nother state is the same as the other direction. Okay, so this means  That when the marginal on AX t minus one.  Is just by  Then  The joint on X t equals i X t minus one equals j is the same as  Two in the other direction. So, to start to go and Jay when I was in, I  OK, so that's why it's called detailed bands, you have this balancing thing where you design the chain is such a way as that the mass that you start is conserved in some sense. Right.  So there's there's as much mass going in one direction as the other direction, taking in consideration. What was the distribution pie.  So that's a bit kind of like the intuition behind a name. Now, why do we care about detail balance. Well, it's because  This is actually a sufficient condition to get  The correct  Status redistribution. Okay, so this is  This is a sufficient condition.  To get that a pie is equal to pi. So, so this pie that we use in the detail balanced equation is the redistribution of a thing.  But it's not necessary so so difference between necessary and sufficient so it's sufficient. But there's other ways you could have  Paid to be correct. This is such redistribution when things don't satisfy the detail balanced equation.  And then the proof is pretty trivial to the proof because it's for tell us where the detail balance comes into play. So if I looked at the ice entry of A to the by the a play to buy. So I need to some over j, ay, ay, J. J.  And then by detailed balance.  But detailed balance.  This is the same thing as a Jay I pie I right  So this is true, by the tail balance. I can flip them around.  And so now I'm something over j. So, by I. It doesn't depend on Jay, so I can just put the pie in front of it, then I get summation of energy of AJ i and this is the left stochastic matrix. This is just one. And so I get that, indeed, a pie, the ice entry is just pie is  Very simple.  My target distribution satisfy the detailed bands equation, then I have that it is a special distribution. And so that's fine, an algorithm which actually does that. So this is the call the mitropoulos is thing our rhythm.  And good sampling is a special case, by the way, so you get back to that. The next class right now we're talking about the general one. And so the goal.  Is to construct  A Markov chain.  With  Sasha redistribution.  bionics  And that would be our target.  And for now, to avoid division by zero, and just defining weird state. We will assume that p of x is strictly positive for all x user avatar   Okay. user avatar   And the, the, the algorithm will define some proposal.  So it depends on the proposal.  Some proposal.  Q of X prime given x  And then it will modified the proposal to correct it so that detailed balance is satisfied. So proposal could be sale simple equity or gal ocean or a uniform or whatever.  And then this one this is this is fine detail balance. So you will you will have this acceptance probably D to make it work. Okay. And then what happened is you will accept  New state x prime  With probability  Given by an acceptance ratio. So I call the probability a which depends on where I went and where I was from. So a of x prime given x  And how is it defined. It's the men.  It's a bit like rejection sampling. Right, so it will be the men.  Have one and  The ratio of cute X given x prime  P of x prime  Divided by cute x prime given X px  And so that's a bit weird. Why is it like, well, this just to make sure that the detail balance is satisfied.  Okay, so this acceptance. This is called the acceptance ratio.  The acceptance ratio.  To satisfy  The detailed balanced condition.  Okay. And note that because you have the ratio of px prime in px  So it does not depend on the normalization constant, right.  So like in in the on normalized version of important sampling. You only get  The if I had a normalized formulation for P i could just use that instead of p  Normalization user avatar   Of user avatar   Alright, so basically you propose a state with your proposal Q prime Q of X prime given x, you will accept with this acceptance ratio probability. And then if you reject.  Unlike in rejection sampling, you will stay in the same state.  You stay  In same state. user avatar   Okay. user avatar   And so this is still a new sample.  So you will just repeat the same state where you were.  So there's definite correlation  But in your Monte Carlo average you will just keep it and use it in your article average. Okay, so this is in contrast to rejection sampling  Where in rejection sampling. When you reject you just need to sample. Again, you still don't have a new sample.  Were only  Accepted states.  Or use symbols.  So the point here is that  This define a transition, probably to you from x two x prime. So a matrix A, which will satisfy the detailed balance equation. So there's some quality of staying in the same state which by the way is kind of a created to the goodness of it right so  Okay, so in more more formally, you have the mitropoulos a sting algorithm. What you do is you start at some state.  X zero doesn't really matter where and then the four  T equals one, up to you're tired.  You propose.  X prime of t, according to your proposal Q and x prime given X t minus one.  And then you flip the bias coin.  With probably t  Acceptance of x prime  t given X of t minus one.  To be one.  And if you accept  It if the coin is equal to one.  You let your new sample.  To be x prime of t.  Otherwise, if you reject you let the new sample to just be the previous state X t minus one.  Okay, so that's the mitropoulos testing.  So there is asking, how do you ensure excuse the q by the way is doesn't change. Right. Q is a proposal. So the question is, how do you choose. Q. And that's a very good question. That's all in the art of designing good Markov Chain Monte Carlo or Williams and we'll see next class that  The good sampling algorithm is a very specific way to design a proposal which gives an acceptance ratio of one actually. So it's kind of very natural way.  But I won't give you that much details on how to choose a proposal because you can actually have an entire set of lectures on just this. It's kind of an art.  But the idea is, you know, the proposal, the closer the proposal is to the target distribution, you know, the more and I keep kind of  As a similar shape than the better it will be. But then there's a question of are you able to sample from this proposal, right, because you are not able to sample from the original distribution. So, if  You're going to just use the original distribution of the proposal because you don't know how to sample from it. And so this kind of target of like how do you can efficiently sample from your proposal. user avatar   Yeah. user avatar   Alright, so something to mention is that  If your proposal is symmetric.  Right. So if q of x prime  Is equal to secure of x prime given x is equal to q of x given x prime  Then when you look at the acceptance ratio.  You have that these cancels out right there are the same. So you just look at whether p of x prime is divided by p of x.  That will be your acceptance ratio. And so, if p of x prime is bigger than p of x, then this ratio will be bigger than one, and so you take them in. And so you will always accept and so in the case when you have a symmetric proposal you will always accept  If the property of your new state is bigger than the property of the old state. Okay. So in some sense, the metropolis hasting algorithm will behave like a noisy.  Hill Climbing  Algorithm.  Right. So when you have  Symmetry proposal. So, so I guess I'll say  Yeah. So basically when you have a symmetric proposal.  You will suggest states if the State gives a high quality you accept. So you went up in the state is lower, you will sometimes accept depending on how much lower. It is  And so in some sense of what it's kind of noisy hill climbing. Right. So overall, it's kind of going up, but sometimes it goes down. So that's kind of the algorithm is doing. And when Q is symmetric. This is actually called the mitropoulos algorithm.  And so the hasting part was to add this modification of ratio of queues when it's not symmetric  And so I will tell you that you can verify as exercise. It's not too hard, that  It satisfies  Detail balance.  With the especially distribution, which is equal to pee right a target.  So that's our metric policies thing all over them. It depends on this proposal. So does it basically this is a design choice.  Which is very important.  Right, so it's, you know, depending on how you define queue, you get different flavor of the metropolis tasting algorithm.  And now let's talk a bit about convergence.  So for convergence.  So we need to show that  So we already have that the by design. The chain satisfy the detailed balance equation. So we have that discussion distribution.  So we have that as district distribution is a target solution to make sure that we converted the right thing. We need to make sure that it's unique and that we can converse to it. Right. And so if the mark of if the if the metropolis hasting chain.  Is or Gothic  Then we know that we will converge.  Getting some trouble. user avatar   Sci Fi. user avatar   Okay, I'm back. So then  We converge.  To  Unique  Stationary  Distribution.  Be okay  And so what do I need to have an organic Markov chain. Well, we mentioned before that a sufficient condition what to what's to have  We want to. So, organic, we need to be a periodic and reusable right and so sufficient conditions for era disability.  A reduced ability  So let's go back to what was era disability and so irresistibility meant  That I can go from any pair of states right  Will it is I'm getting confused that will it disappear. user avatar   Doesn't look like it. user avatar   Okay, good.  All right.  And so to make sure I can go in any pair of states. One way to do it is to make sure that my transition probably T says that I can go. Where is it, boom.  So for disability, I can just make sure that q of x prime given x is strictly bigger than zero for all x prime not equal to x in my sample space.  So I can go from any pair of state.  Then that will make sure that you, you can go in multiple apps here, you can go in one hop right but you can also do it in multiple hub and then to be a periodic  So either  You use the fact that you can stay in the same state.  For some  X in skeptics, right. So, because if remember for the AP redick I said a sufficient condition.  That's why it was important. I said a sufficient condition is that there's some states for which you can stay in it. Okay, so one way to stay to make sure you stay in a state is you can propose the same state in your proposal.  And because the probability of of all states is always bigger than zero, you will you have some nuns or quality of accepting  So either that or another way is to make sure that the acceptance ratio.  Of  X prime given x is strictly smaller than one for some  X in this simple space.  And guess x prime  I mean you need you need to have  For some states for which you can travel  Explain.  such that q of x prime given x is bigger than zero, I mean,  To be the to have the ERA disability. We already assumed that so but I'll just to be really clear. I will keep it here. Now, what happened is  If the acceptance ratio is strictly small and one for some expression that mean there's a property of rejecting which means they have a policy of staying in the same state, which means that I will have basically  You know, both of these are I have that AI is bigger than zero for some it right  That's why I'm a periodic  Okay.  And so by making sure the proposal have this property. I can make sure that my chain is organic and so it will mix it will convert to this tissue distribution.  And so when you design these proposal. This is something to keep in mind. And actually there's a lot of these algorithms and machine learning were which were proposed.  Where sometimes, it didn't satisfy this condition. And so it actually wasn't even sure that you converse. The right thing and indeed some of these were actually wrong algorithm. It didn't convert right so that's one thing to be  To be careful with and then something I will mention, which is important because we will use it in the  In the give something example. So we have that we're also allowed to change or proposal distribution, it is still okay  To change.  The proposal.  With time,  And so instead of having a homogeneous market chain, we can have a homogeneous.  Or in a magennis, I guess.  In homogeneous Markov chain. So, we will have a proposal which depends on the time  But to make sure it's still converse fine.  We need that the choice.  Of the proposal.  Does not depend on the state.  Okay, so you kind of fix in advance the choice of the different proposal.  And then it turns out that the convergence theory.  That we talked about will go through can and importantly, the good sampling our women will be an example of that, because we will simple only one variable at a time, one piece of the joint variable at a time. So, the proposal would change with that.  And you could think of it as look at  The, you know, you will have a fixed schedule of how things mixes and then you could consider like having  A super chain which consider all these possible change proposal.  So Jacob was saying was an intuition that qx x implies a peer to cities so cute x x bigger than zero means that you are  Proposing yet. There's another possibility that the new state, you will transition to the same state as you started with.  Which means that the the overall chain defined by the metropolis tasting Ireland, which is both proposed and accept  It means that there's a non zero quality of staying in the same state. And I said earlier that when there was a when you have the chain which had the non zero quality of staying in some save state, then the chain was a periodic  It kind of broke the pier. The city because you add you know you could you could have either. It takes two states to get to the state or three or four, but I could always do like  Add one by staying where I am. So you can have the number of path. So the length of the past to go from one state to the other could be any number, in some sense, that's kind of me, which is why it's not periodic anymore. I could break deputy city.  So somebody is asking, what are the states of the Markov chain to be specified will be, for example, the parameters of a Patrick distribution which we would update with every acceptance or proposed simple, for example, so  If you do Bayesian computation you want to compute the posterior over the parameters. So you could perhaps  You if you wanted to say. Use your posterior to compute an expectation. So you would want to sample from the posterior so indeed, you could use the Markov chain multicolor algorithm to define a sampling algorithm from the for the posterior  But more in much more sophisti if I have a graphical model. And I want to compute the marginal distribution over some know I need to simple right from this distribution. And I told you some example in the last class to do either important samplings rejection sampling etc but  In the example like the icing model is a good one. So, where you had this grid, and I want to compute the marginal and one node.  But I'll tell you that. I'll give you in the next class. The give sampling example which is a special case of the metropolis testing which will allow you to approximate the marginal over the no denying model.  Oh, okay. And so one last thing before I stopped. I wanted to mention an example of why will the chain be slow mixing  Okay so slow mixing  Example,  And so  Suppose  That the target distribution is a very normal.  So,  Normal with menu and covariance sigma and that q of x prime given x, we will use a symmetric. Sorry, a spherical. Gotcha. So it will be a Gaussian on x prime with mean my previous state and then I will have sigma square I as my covariance. So it's like a spherical. Gotcha.  And so if my true distribution.  Is Isley non spherical  If I use a big variance. So for example, let's say I'm here.  And then I use a big variance. So this will be my proposal.  Sigma big  Now what what the problem is when you will use in your chain you will actually have a high probability  Of rejection  Because because I need to navigate this this kind of like kind of like narrow Valley situation, but I will. I have, you know, not a very big priority of lying in this low quality region. And so these, I will have a small acceptance ratio. So I will reject. Okay. user avatar   So, user avatar   And so this is my p  And and so you say, okay, well, instead of using a big I will use a small  Value of  Of the variance. And so let's say I'm here and I take small value of the variance. So this will be sigma small  And now I will. What happens is, well, I will make very small step. And so, you know, if I make independent simple of my music very normal. You know,  Getting a priority here versus there. They're very high probable, so I should be able to ease quickly go to this.  Far away points right and so because I'm always only making small step from where I am. It will take you know a lot of step before I can get to this point. So the mixing time will be slow. Okay.  And so it will take you know this. These make small moves.  And play many steps needed  And so it turns out that the  If I compared the  The  The biggest and the  The biggest and the smallest singular value for my current matrix, it turns out that the mixing time. You're the best.  mixing time  Is related to the condition number of my matrix. So it's the ratio of sigma max divided by sigma. So, if I have a spherical. Gotcha. This is one, so it's a, it's not a. It's not a switch will have a small mixing time. But if I have a very esoteric  A symmetric or I none as well tropic Gaussian. Then using ISO tropic Goshen as a proposal, it's not it's not nicely working so I will, I will, I will slowly mix. Okay, so that's kind of a difficult distribution to to mix with  Okay, so let's give you a bit of  Intuition and so is it kill this thing. I'm a little confused. Our x and experimenting states and the random variable values that we are simply yes  X and X prime. There are two states, there are the values of the random variable that we want to sample from user avatar ezekiel williams  A third but they're also the states of the Markov chain. user avatar   Yeah. user avatar ezekiel williams  Okay, just start at the beginning that you said we were only dealing with like finite state spaces, but I go user avatar   Oh, okay. You're saying, whoa. Well now I'm talking about the gash in proposals and  When it comes to the space.  Yes, you're totally correct  So I gave you the convergence theorem and everything indiscreet space because it's simpler and to give you the intuition, but it also works in continuous space.  So, for example, making like here, my gosh, and distribution gives you a non zero quality of going anywhere.  And also being in the same state. So it turns out that this is so if I do be triple A tasting with this gushing proposal. This is definitely are guarded markup changing the continuous sense and in the chain will mix so it will still work.  Yeah. Okay. Any other questions. user avatar ezekiel williams  Um, actually I had another question, if I may ask, I was also wondering, so  I guess we we've proved the the theorem relating to the mixing and the case of where a diagnosable matrix. Yeah. Could you give some insight into into what it would be like if we didn't have a diagonal matrix or suggest where to look for for References user avatar   Yeah, so  I think  There's a very nice document by Yuval Paris that I found online think from Berkeley tells you about mixing time of Markov chain. So I'll put the link in the scribe notes.  The idea is, like, it's kind of related to the eigenvalues. But when you don't have this spectral theorem. It's a bit more complicated, but it's still kind of the this notion of like how how separated the next eigenvalue is  Because in some sense you always getting a race to the teeth. So you want to know how fast the convert the matrix converge to the right thing.  And the other question.  You know when a bit over time. Sorry about that.  OK, so the program is next time I will now tell you about the good sampling algorithm, which is basically a specific case of the metropolis tasting, which we know will have an acceptance ratio of one. So it's kind of like it makes sure that you you move more  And it's something you can run in the pricing model. And that's what you're doing the next assignment. After the one that is do for sure.  Alright, so. Have a nice weekend. See you next week.  Oh, we have a don't forget, we have a getter town social
  Cloud. user avatar   Yeah, so  Today we're going to see the good sampling algorithm.  Which is an example.  It's a specific instance ation of the metropolis tasting algorithm with a clever proposal.  Okay, and that will conclude our coverage of  Of  Sampling algorithms and approximate in France, and then we will talk about another approach to do approximate in France, which will be the virginal methods.  And somebody asked so mode is asking about the project, what is this supposed to be in a one page progress of the  The one page progress reports for the project.  It's just a very high level quick overview of, you know, where. What's the status of your project. Like, what have you tried so far. And what's your plan for finishing the project.  So it's, it's basically the idea here is just to force you to have started on the project.  And you can also ask questions there. I will quickly skim through all of them to see if there are any question.  Or if their problems and can give you a quick guidance, but there's 25 projects so  This will be very sparse feedback.  But yeah, so the idea is just for you to kind of like  State in explain what at what is your plan for the final project and what have you tried so far. user avatar   Okay. user avatar   Alright so good sampling and  So yeah, so give sampling our rhythm.  You might have heard it. This one of the most standard something our rhythm in machine learning.  So the idea was to do mitropoulos hasting with a clever.  Choice of proposal.  When you have  When you have a multi dimensional random variable. For example, coming from a graphical model clever choice of proposal.  And which depends actually on the timestamp. So it's not a fixed proposal. It depends on the iteration of good sampling  And why is it clever. Well, because you will always accept your, your set your new sample. So the acceptance probability of x prime given x will be actually one, so it will always accept your, your move. So that's why it's a good  If you remember at the end of the last class I mentioned like user avatar   Where am I user avatar   Some issues of trying to do Metropolis tasting or it's explaining why sampling is difficult. In general, when you have a very  Curly different variables. So you have a long gated Gaussian and when your proposal is actually a sphere. So it doesn't match the shape that you're trying to match and so either you will need to make a lot of steps to explore distribution, because you have a tiny  Variants in your proposal or you make big steps, but most of the time you reject because you you get to lower probability region. Right. So when when you have accessibility of one it  Usually means also that you know you're getting  Better calibrated proposal, but I mean, the good thing is nothing. See the best also sampling our rhythm.  And we'll fix it because you can think of it as a cornet approach. So when you do corner descent for minimization. It's not also this silly always great if if the variables are kind of related to each other. We'll get back to that. But so  Alright, so before presenting the algorithm. Let me mention some application where this is coming from. So example of applications would be either  If you have a undeterred graphical model. And so in this case you want to sample from the unrealized giant, which is just a product over your potential. So it's easy to compute the the or normalize joint  It's hard, perhaps to normalize because you need to some over any special number of states or you could have  A directed graphical model where you need to compute some difficult conditional  Okay, so, you know, for for standard direct graphical model and discrete data, you can just use ancestral sampling. But let's see. Now you want to condition, then  You could also use rejection sampling. But that's not great. If the quality of the observation is small. And so you need something more clever.  And you could use give something for that. So in this case, the distribution, you would like to sample from is also on normalized distribution. You could think of it as just  The I want a simple from the  I wanted a simple from the conditional. You know you want p of x given X evidence that's what you want to sample from. And so you could think of it as the anomalies joint when  X. He is fix. So this is just the reality of all my variables which are not in my evidence and then I make sure that my  Evidence variable or fixed  Alright, then I could have here I could add the credit card delta on my index evidence node to make sure that they're fixed to the evidence. Okay, so this is proportional to p of x given x bar.  So if I and so an example of that would be. I have observed some  Like in the queue, Mr network. I have observed the symptoms of my sickness and I want to compute with the quality over my my  My diseases, given the symptoms I have observed  Okay, so let's look at in the context of  Of a undeterred graphical model.  So let's say I have a huge em.  Let's look at the cyclic  Give sampling algorithm.  And so I will have the nodes in my you GM for my rent a variable. So I will index my nodes from one up to n.  So suppose I have end nodes in my  You GM  And so the algorithm. How it works is you start at some  At some initial as joined assignment and all the variable. Let's call it x zero  And then for t equals one, blah, blah, blah. So this is the mark of Chen Monte Carlo setup. I will sample new state.  And then what happened is in the cyclic setup is you change your proposal during Yarborough them and it's basically a cyclic  Visited the every n iterations, you'll have the same proposal again. Right. And the idea is you will pick the know that you will sample.  In a cyclic fashion. So one way to write it is will be t mode and  Plus one, right. So basically, you look at the remainder of dividing and by T.  And sorry T by n, and then you add one. And so at the beginning, it's the first node then second note the law, up to end.  Guess T probably need  Yeah, okay. So, so t equals one guess perhaps to you will start at zero. In this case, it doesn't really matter. Actually, you can start up  So in this case, what happens is you're starting up no to doesn't matter. So you go to know 2345 then and and then it goes to 12345 etc. So it's just a cyclic. Wait, but if you want, you could also start at t equals zero. If you want to start with node one.  But so you pick which node you will sample and then you sample the value for this node.  According to the conditional  On this note given  The  That all the other nodes have the previous value that you had assigned to them. So use the notation, not I.  And I'll say, not I. So negation of I, at t minus one.  And so this means not i is just a shorthand to say it's all the nodes minus i.  Excluding the node i  And so the proposal we use is the actual true conditional on this node.  And so that's how you get the new value for this node. But then what about for this sample. What are the values for the other nodes way to just keep them constant. Okay, so you'll say that x j at time step at iteration t is just you just recap the the previous value you had  For Jay not equal to i think it's a bit like when you do optimization. When you do coordinate  Method you fix all the cornet and you only update one of the coordinate right and then you you you pick another one corner that you want to update you fix everything else and you have did this one.  Here, it's the same idea for sampling right so you fix the value of all the variable except one. And then you will simple this the new value for this variable according to the conditional  If you can compute and if you can  Sample, but that's we'll see when we can do that.  So yeah, and that's it. That's what you do. You just keep doing that.  That's the cyclic give something over  And so let's see a motivation for this algorithm which is  So you can think of give sampling  As  Its Metropolis hasting  With a time varying  Proposal.  Because at every iteration you pick a node and which know you pick the determines which proposal, you're using  On the other hand, the proposal you choose does not depend on what's your state. Okay. If remember when I talked about convergence or guarantees for Metropolis hasting I said you're allowed to change the proposal with time. If you want as long as the choice of proposal.  Does not depend on where you are, then if the proposal satisfy those usual property of convergence for a fixed, then it will it will work even if it changes. Okay.  And so here every any duration, they get the same proposal and which know they choose does not depend on my current state. And so this satisfy this the the  The conditions. I mentioned in last class. And that's why it's it's still a it was to converge. I'll explain why converge later but  That's already as a high level. That's the idea. And let's look at what's the proposal because I want to compute in Metropolis as thing I need to compute the acceptance ratio to to have the correct guarantees.  Okay, so there are some questions already. So conditional on X i.  Yeah, so, so, so when I say here, it's the true conditional and Xi as proposal. This is condition on the value of all the other nodes, right. So, so it's a conditional an exciting given X, not I.  So that's the other variable.  So the thing is asking, what's the point of picking it that way versus just random or a fixed ordering. So this is a fixed ordering right is the ordering is 1234 or five  You could mute this ordering and using fix. It's not a problem at all. And you can also do random sampling of I instead of doing a fix ordering if you want. That's also a bit better behave. It won't have the bid the cyclic kind of annoyance that  This property has so  Your did. There's nothing magical about this fix ordering. It's just the idea is just to have some order of this, the idea is also you want to make sure that you will you can explore. Well, this state. And so by using a fix ordering you're ensuring that each variable will be  Sampled whereas if you did random sampling of which variable, you will choose, you're not guaranteed that after any durations, you actually have sample all the variables. So, you, you have a bit more stickiness in some sense.  We paid your rent to initialize excite using its marginal if we can compute it.  Well, okay. Excited is is simple. So, so the marginal is not the same thing as the simple right so if you could sample from the marginal  Wouldn't make sense to initialize it to the marginal. Sure, why not. But usually you can compute the marginal, which is also mean you could use in particular in the in the assignment five, you use give give something to approximate the marginal in the Isaac model.  But indeed, if you have some prior information, you can get a bit of better initialization if you want  But note that it doesn't matter where you initialize like eventually you get to the true sample of the of the distribution. user avatar   But, good question. user avatar   Alright, so it's a it's a metrical assisting with the time varying proposals, so let's specify this time varying proposal. So, suppose  We pick  I at time t j. So, let's say that we're going to say we're sampling x i at time t, then the proposal.  Which are we are implicitly defining is I'll quite cutie. So it's a proposal on all the joint and I'm conditioning on the previous simple right t minus one.  And, this by definition will actually be  The correct  Condition conditional of x i.  Given all the other nodes. So it's not I, at time t minus one.  And I'm this is not a proposal on all the nodes. This is just assembling a simple one node and then he tell you how simple all the other nodes were all the other nodes.  Are actually not simple. They're just fixed to the previous value. So you can do that with a credit card data right so you can do creditor delta on all the nodes which are not I  I need to fix it to their previous value. So I'll use t minus one, not die. user avatar   Yeah. user avatar   So this basically enforce that.  For X, not I.  To stay constant  Some sense here. This is just a formal way to say that in a proposal.  All the other variables we said to them, deterministic me to the previous value and only the node x i. Are we sampling their value.  And then  Eve is asking with with the sampling the observed nodes. Let's say we were trying to come to, to, to get a sample from the conditional. And the answer is no, we're not sampling the observed nodes, because there was of nodes are fixed in a in a conditional right so so if I was in  In this application here right so  I would not be sampling  The evidence note. And another way to look at it as well.  When you compute the true conditional on X e  In this distribution.  Well, it will have zero when x is not equal to x bar. OK, so the true conditional in this a normalized distribution that we use any way for sampling  Would not change the value of the evidence that  And so it's kind of wasteful in this case to pick the evidence notes are actually in this case, in practice, you would not pick the evidence, those would just pick the variable which you actually need to sample. user avatar   Okay. user avatar   Alright, so this is the proposal and so that's compute its acceptance ratio. So the metropolis hasting acceptance.  Ratio. If I use this as my proposal.  So the acceptance ratio. When I sample x prime and I was in X t minus one. It's the ratio of the proposal in bold direction.  As well as the distribution that I care about. Right, so I will have Qt of X t minus one coming from express x prime time the quality of x prime, when I use a true distribution that I care about.  And  The other direction. So, x prime given X t minus one.  And then P of X t minus one.  Okay, so now let's just expand all the pieces. So, this I said was the probability of x i.  At t minus one because that's the value I would sample according to this notation given  All my other nodes and here all my another dodes I am using the x prime value, right, because I'm conditioning expand  And then I also have a clicker which fix all my values. So, not I.  prime is equal to  Not i t minus one.  And here I have the same thing, but in the other direction. So, it will be probably T of x i.  Prime given X, not I.  T minus one.  And then they stop my current occur.  And you know it's symmetric, so I'm just putting it in any order.  Okay, so there are some questions. So Cal is asking  From which distribution we start at zero anything doesn't matter so  If it's a discrete state. It could be, you know, any you could just use a fixed initialization. If you want, it's a continuous state space. Well, you need something which nonzero mass on  On any states, I think.  But that doesn't matter.  Because the idea is that to make the change will mix and us when a chain mixes it forgets the initial conditions. So that's kind of, yeah.  So Jacob is asking, is there a reason to use the notation X Brian rather than x t as above.  Know, I could have used t is just  I need to put more parenthesis and stuff. So, yeah.  Yeah so. Alright, so these are the two pieces I had expand so far. Now what about the  This joint well so this joint, I can write it as the conditional the marginal on, not I.  Times the conditional of x i prime given not I plan.  And same thing here. This piece here, I will just write it as P of x not i t minus one times the conditional of x i. user avatar   T minus one. user avatar   Given X, not I. T.  Alright, so why did I do that. Well, now I can cancel all the things to get right. So I have that  Excite prime given user avatar   Oops. user avatar   How's that work. user avatar   Alright, so we know  We know that  Excite x  Not i prime because of this proposal.  Because of basically this thing.  Make sure that excite prime x not a prime is always equal to x not it minus one. Right. And so these two cancels out because it's the same. It's the same argument, okay.  And  Here I have  And and so when I have an x not i prime. It's the same thing at x not i t minus one because of the direct  And so here I have, for example,  Not i prime in the condition  And here I have X know i t minus one, but it's the same thing.  And the arguments like I have x it minus one on on the left part of the conditioning on both sides. Right. So basically, this thing is the same for both and this  And this is supposed to be the same because of the credit card delta. So, that's why. So I have one here. And I have one there, so they can sell that right so numerator denominator  Right, so I'm using this notation of I'm using a bit of a pattern in my crossing. So you can see which terms cancel out each other.  And so the same thing here. I have x i prime on the left. And here I'm excited prime on the left, so they can select on the right. It's not the same thing, but because they're supposed to be equal. It's still work. Oh, and this was temporary. Sorry. Sorry, this and this cancels out.  And I think I'm done. I've cancelled out everything. So I get that the acceptance ratio is just one, as I mentioned,  Okay, so that was a  That's why the by sampling from from this conditional on one though. That's actually pretty good proposal because  You have acceptance ratio of one. Okay, so you always accept  OK, so now we made the link with the metropolis hasting algorithm.  Though it's a time varying proposal. So let's talk about convergence of good sampling  And how we could prove the convergence of good sampling  So, let  A be a mark of chain.  Our mark off transition  So the, the  The representation of the mark of transition colonel in our Markov Chain Monte Carlo have one full pass  Of give something  Okay, so I N steps.  Okay. And so basically you think of it as  Anyway, there's no there's no rejection here. So the proposal that the transition Carol is just say pick the first node sample its value according the conditional then  Pick the second node simple it's going conditional and then do that n times and overall, this gives you a very complicated transition, Colonel.  Which tells you, from a specific joint state before these any updates, what, where, where could I go, okay. And so in this case.  It doesn't because you've done the full pass. It's this every iteration of this is the same. So this gives you actually now a homogeneous.  Market chain. So it's a bit nicer than the time varying Markov chain. So this image in this market chain, so we can just use the convergence result of images market chain.  And if, for simplicity we suppose that the thing I'm trying to sample from is bigger than zero for all x  Okay.  So you can still converge. If this is not true. But then you need to be more specific about using the correct conversions results.  And so one advantage of that. It does mean that if I looked at any conditional  Not I.  Then this will be bigger than zero, right, because all the joint are bigger than zero. So this is true for all xi and X, not I.  And so that simplifies a bit properties. And so, because p of x is bigger than zero and all these conditional will be bigger than zero for any possible values then you have that this transition Colonel is an irresistible chain. He reduce it will  Reduce Civil War, is it irresistible well for any joint state. I can get to any other joint state by sampling each node, because for every node. I have a  Positive probably to go to any other possible values. And so you can, you will have a positive probably need to go to the correct value of the next jointed state. You want to go and you do that for for n variables. So you can really go from any joint state to any other state.  And it is also a periodic  Because  You also have a non zero probability of staying in the same state.  Because AI is bigger zero, actually, in this case, if you have that  AJ is bigger zero for all ages.  I mean this is stronger and being irreducible because they're just a bowl. You don't need that.  The transition in one step, you could actually do multiple step to go to any state, but in this case it's true that you can go from any state to any other states with one step of this transition kernel, which is an updates of gives me  The data. Yeah. And so here, basically.  You can get  To any state.  With and  So because we have an irreducible in a periodic Markov chain. So we have an organic chain. So, that implies that with our conversion serum, we have that a race to the T f 30 durations of these and flips for any starting point. I will converge st goes infinity to the target distribution. user avatar   Okay. user avatar   And as I mentioned before, it turns out it also works.  For Random Scan  Where, in this case, you will pick I uniformly  From one to n at each step.  Okay, so instead of being cyclic. You're just randomly something which node you will sample at every iteration.  And so in this case the analysis, a bit more complicated because you can just find a simple  End steps.  Like you can just define the the mark of transition like I did with an iterations and everything is positive.  But here, you still have the or go to city, it's pretty clear that you have your go to city because you always have a nonzero quality of picking any node and flipping its value and then with enough  Iterations you will always have a chance to have picked all the correct nodes and have flipped to their new value. Okay.  The problem is that, you know, it could be a lot of these iterations before it happens. But, you know, it will happen eventually. And so you do have that a race to em i j is bigger than zero for some em. user avatar   In user avatar   Actually, I think in this case with n will work. So a race to the end will work where an AI is just one step of this transition Colonel IE randomly sample one and then simple its value.  Alright, so is it gathers asking  That they don't see how the proposal doesn't depend on the state.  Because the thing is like when we say what a proposal distribution is is is a conditional right so basically free go back to our, our monthly Gallo thingy.  To do.  So where was this  Yeah, so the proposal.  Is actually a transition kernel, which for any possible value here and gives you another any possible next step, give you a transition probably T, but it also, it's also true for for different assignment here. Okay. And so it's true that  The actual conditional depends on which state I use right so to where where I was simple. Next depends on where I am, because I use a conditional  But the choice of for conditional does not depend on the state in cyclic good sampling. Because the only thing I'm choosing is which variable that was simple, but given that, then the conditional and using in general would be Q of expression, x right  So it doesn't depend on the, the actual state or the type of conditional will use one depend on the actual state, even though that indeed, what you're doing is to use the trigger states as input in your conditional  Does it terrify user avatar   Isn't yeah user avatar ezekiel williams  So I guess the thing that I'm not sure, but the serious thing, but it doesn't depend on state because after you go after you go through a full kind of n steps you'll have gone through  All the coordinates. I guess I just don't see how, how, when you haven't gone through before and steps. It doesn't depend on states. And if you're using the cyclic nature because your update always updating the same coordinate  Yes, so the previous one. user avatar   So you just have to, I mean, I think I understand your confusion because not depending on state. What do I mean by that. Right. So what I mean by that is that in this case I have q which depends on T.  But which QM using doesn't depend on what my state, it only depends on where I am in my, in my  In my, in my algorithm here of T ma n, so there's there's no state there. This is just, you know, fix visiting. Okay, this would tell me which cutie. I'm using  Okay, now acuity takes as input state you condition on and then it will sample something. And so, of course, this depends on the state right but but how I choose cutie does not depend on the value of this X t minus one. Okay. user avatar   Okay. user avatar   All right. Yeah. So it's really terminology here so  Okay, so that's a. Any other question about good sampling  I mean, also it will become a bit more clear. Now today, apply it to the icing model, which is what you will implement in your assignment. So let's do as an example.  The gift sampling  For the icing model.  So,  The Isaac model that's remember what it is, it's, it's basically  Under to Africa model with binary  Random variables. So there's 01 valued and you have some new GM  Some kind of grid structure.  And because of a large tree with its own tractable to do exactly in France. Right.  And I told you the exponential family representation for the gives some for the sorry the icing model. So if I use that. So it's basically one over the partition function.  And then I have x  And I had as efficient statistics. I told you you could only use the weather a node is equal to one and whether an edge as both nodes equal to one. And so basically you have one candidate called parameter for node. And I just put the value of x.  And then I have for all my edges.  In the edge set and their undirected edges. So that's why I use segmentation. I have a chemical parameter associated with that. And then I want to know whether x i, an exchange or equal to one. So, I can do that by just multiplying  And that was the minimal exponential family representation. So this is a minimal  exponential family representation  I don't need more features.  For presentation.  Okay.  So now this is my joint and now  If I do give sampling. I need to compute the conditional and one node, given all the other nodes, right.  So I want to compute. Oops.  Why want to compute  Property of x i, given all the other node.  And so we know, by the way, by conditional independence.  That we only need the mark of blanket. We don't need all the other nodes. So this is the same thing. Sp of excited given  X the neighbors have I user avatar   Right. user avatar   And actually you could also have seen it.  From the joint.  And so this is basically proportional  To  Exp  Of at the I  X plus summation over j in the neighbors have I  Have a big X i xj and then plus rest. But the rest is a constant which don't depend on exile, right. So, so for example here will have x  j and l for l not equal to i.  And these when they condition on x not ID or fixed value. So that's why I don't care. This is just these are just constant  For in terms of exile, because all I want. Here is the conditional in terms of excited. Right, so I can compute the, the, I can easily get the conditional by re normalizing right so you  But you read normalize to get the conditional  So you get that p of x i.  Equals one  Given all my name. Everything else, but I actually all i care as the neighbors is exp of at I because x equals one plus summation over my neighbors.  Have at i j x is equal to one. So I just left with Exchange, and then I have expert rest.  And then I divide by  The summation over the two possible values. So when I set x equal to zero.  Then this becomes zero. This becomes zero. So I just get x of zero, which is one. Right, so I get basically one. Whoops.  So that's when x equals zero, I get one. And then I get x of it i plus summation  As  One plus  X of the i plus summation over j neighbors have I it Jay Jay and then I still have the X reps.  And these cancels out. As I mentioned,  And why did they explicitly right it will because you also it's a very simple form the conditional is a sigmoid  So it's sigmoid  Of at the I  plus summation over j of my neighbors.  Of  X. user avatar   J. user avatar   Okay.  And so you could think now if I add you know  Iteration T in here I have t minus one, right. So here we have t minus one.  And so when you sample you basically compute  Depending on your chemical parameter for your Isaac model, which are given by these values, you just compute this some given the previous values of the neighbors.  And that gives you the the bias for your coin you flip a coin with this probably T and then you will set excite to one if you get head and excited to zero. If you get tail, for example.  And then you repeat on all the notes. And so that's how you will flip things you will keep sampling. Okay.  And importantly, if you remember I mentioned that  You should use all the simple in the market chain.  For computing average right so if you want to compute the marginal probability over a node, you should just use all the possible  Values that you observed, which is kind of like a waste computationally because the value want change will only change once every n iterations, so most of the time, you just keep that constant  So, you know, you could also because you know that you could also just  Look at the value every any iteration, because any way that you will have a cancellation. But in theory, just to remember how you do things you can every new iteration of gives, gives you a new joint sample, even though, like there's a lot of correlation between the simple  It to compute average, it's fine to use all of them.  But it's also, if you want to save a bit of computation. You don't want to worry too much, too much about it. You can also keep every an iteration, a sample for for a value.  Okay, so that's for good sampling  And the last thing I will say about oh, also the other thing is, so when you have discrete random variable.  With a small finite number of possible states, you can always do give sampling for under graphical model or Dr graphical model because all these conditional only depends on the neighbors and the node that you're sampling. There's only  A small number of possible value. So it's easy to normalize and so you can always sample from it easily. It's, it's basically flipping a, like a bus.  It's basically sampling from a discrete dispersion of our case states where it's determined. So that's easy.  So you can always do give something if you have a continuous random variable. Well, then you will be able to do give sampling when you can compute this conditional fine and so depends on whether you can do the integral or whether you can sample from the specific conditional  And so we'll see when we talk about Bayesian method in in next week or next lecture.  That  When you have a conjugate families. So you could have conjugate prior for node and then you have  Different conditional. So when you have configured relationship. Usually the posterior is easy to compute and then you can usually sample from it. So then you can also do give simply  An example of that is called the latest allocation model for topic modeling.  So that's where even though it's continuous random variable, you can still do give something because you can sample from the conditional. Exactly.  If you cannot sample from the conditional exactly, then you need to use again, you can do the same thing you need to do mitropoulos tasting style approach by need to reject things. It should register.  So there's a question here.  Can you repeat your question.  Okay, so. So the question is,  You use t minus one.  So basically when you look at the algorithm of good sampling  So,  When you condition your condition on the previous value that you have right but know that you set all the other variables to their previous value. So you could, you know, if you did this assignment. First, you could still just put here T. It doesn't matter. It's all the same, right. So,  But you know to be a bit clear it's t minus one.  Okay, so I'm not sure about. Can you explain your question orally.  Or you don't have a mic. user avatar Blain-Montesano Yves  No, no, I have a mic. I'm not sure what the best way to explain it as differently. I guess I mean  At some time t with an R and iterations we would sample conditionally on the exes that aren't I at time t minus one.  But  Suppose we've recently sample that time to different  xj. We wouldn't use that new value, but we would use the t minus one value. user avatar   Alright, so let's okay so I understand your question now. So, T is outside and I is inside. Okay, so what happens is  The so team changes very often. And so what happened is  Within one pass you always use the most recently sample value.  So when I say t minus one here because T here did not for present the number of full path. I've made it for present, you know, the number of time. I just chose one note to sample. user avatar Blain-Montesano Yves  So, yeah, yeah. So  I understand where the confusion comes in. Yeah.  Thank you. user avatar   Okay.  So let's talk about  Diagnostic of chain.  And then I will be done with sampling  So,  I'm not saying that, you know, here, you've seen everything about something right, something is extremely rich and particular I didn't tell you how to  Build a proposal distribution for Metropolis hasting I said it's kind of an art.  Give something is one example. But it requires the ability to sample from the conditions which is not clear that you can do that.  And general, you want a proposal to kind of match the shape of the true distribution. But, you know, what does it mean, so it's a whole you know it. So you could have a lot of lectures. But the general idea. I gave you already gave you the main things right. So, so you want  The Mc, Mc idea is you want something you want to change to be organic, you're free to use any proposal which maintain this or goddess city and then you will be fine. You'll get a correct convergence.  And then  The question is, well, okay, you want a proposal which is efficient to sample from and matches kind of nicely. The shape of the distribution. You want to really simple. That's kind of the gist of it. And now let me give you the, the high level topics on how to diagnose  A chain. How do you know that you've mixed right because I told you. Well, you need to do burn in, you need to remove the first few iterations, because these are not representing the current distribution.  But then how do you know how many to remove right so you need to kind of diagnose your chain and you want to know whether it's it's mixing  Okay. And again, that's also somewhere where you could actually, you know, have many lectures, but I'll give you some quick high level ideas. So the first thing is you can actually monitor mixing  Up so simple idea is to run multiple chains in parallel.  By running  Independent chain.  Right. And so what happened is, let's say, here you have time, which is basically the iterations of your Mc, Mc algorithm. And what you will compute is some measurements on your chain.  So the measurement could be, let's say you want to estimate the meanwhile, you could be just a mean but it could also just be like, oh, what is the value of  What's the average value. What's a running average value of one of the variable in your in your gift center.  But the point is, you had different chain which had some different initial values for this measurement because he just started in in different part of the space.  And  When your chain has not mix. You'll see this kind of behavior here where you know the different chains have very different values for these measurements which depends on the initial condition.  And so this is what you call a sticky chain because it has not mix yet. It's kind of stuck in. It's the dependence on the initial condition. Okay, so this is slow mixing  Whereas  If I have a chain which mixes. What you'll see is it will go quickly to the century distribution value of this measurement. Right.  And so here  At the beginning. And so basically here. You could use this as your burning time right because you had that devalue of the measurement over there dependent on the initial conditions, but  In this region here you get that you're just visiting the possible values of your, of your state space, according to this test redistribution, roughly, and you know, there's no difference that you can see between the different chains. So you forgot the initial condition.  And so that's one way to went to plot these kind of like the behavior of your chain for a different initial condition and comparing the value of measurements for these different things is a good way to make sure whether your are not your changes.  And then the slow mixing  Could come from a multi modal behavior.  So slow mixing  Comes from the difficulty.  To move  Between  High probability region.  So an example I had before was you know that here of this would be p of x.  That's what you want to sample from. And so here you have a low priority region. So it's hard to cross this path.  Right, you need proposal, which are really, really big. But then, because they're really big. They're kind of like not well fitting the, the, the shape of your distribution and then you reject the law.  And so one way to to work with that, by the way. As a side note is called a kneeling  So a kneeling  Methods.  Help this  Okay, so basically the idea of a kneeling is you will have a proposal which change according to time, which becomes  More picky with time. So it starts very flat.  And it can becomes more peaky  So it looks like  Basically, one of z x  Minus  One over T  Some energy function.  And this is the temperature  It's not the time anymore stem for richer.  And basically, when you have high temperature. So this kind of like coming from physics. So if you have a  High temperature  You'll have that the proposal.  will divide by a big number. It's arguments. So it's x of a something which is almost constant. And so it's very flat proposal, so it's will explore a lot so it can make big move in the space with this proposal.  And then slowly you will reduce the temperature and then you, you start to have more precise moves, according to the current distribution. So, this is the high temperature idea is just to do more exploration  And I just give you the buzzwords, so an example of method, which is that is called a kneeled  Important sampling  And so the idea is to have this combination of like big move with high temperature and then more precise move with low temperature and there's anything scheduled to move in between.  Okay. Can you say a little about how this mixing property of your proposal relates to the urban city which we heard about last lecture. So, so in that sector. I said, or goddess city was just a binary value. There was no  Value of the grid. The city right there. Go to city just said a chain is organic whether you it's both a periodic and reducible  And so that will mean that it will mix fine and  Didn't tell you how long it will take to to mix it. So I guess what you could see is, is that  Some properties which gives you a good city could bring you faster mixing. If you think about it like so, for example,  Where we had there in the gifts jumping all over them. I told you that I had a chain where the probability of moving between any two status positive in one step.  Well that's faster mixing. Then if I had a lot of steps to move between Tuesday. Right.  But both are organic. Right. So there's no difference in their city so so whether it changes or got it or not doesn't tell you whether it will  It will tell you whether it will mix, but it won't tell you what if it will mix fast the speed of mixing has to do with more fine properties of your marketing. Okay.  So is it Kelly is asking a good question about the Neil important sampling saying, well, if you reduce the temperature. So, let's say, basically the idea is I start with, with a proposal which is very wide and so that allows me to  Let's say I was here at all. I mean to to jump there and then I will start to have a picky proposal.  Well, then I'm stuck in this mode. So how do I go back to the demo. And that's true. This is just one way to get a good sample.  New sample from the distribution if you want to read. If you want to keep revisiting the modes, you need to also switch back again to high temperature value. So you need to. So, so you need to read to read these these anything scheduled  For multiple samples. Yeah.  Alright, so it is. Oh, did I forget. So it is a 334 we'll take a 10 minute break and after the break, we'll, we'll cover virtual methods.  Yes, so  Even asked a question about what I explained about  The annealed idea where you change the temperature. And so he asked  Does that mean that we need to repeat the Burnham, okay. So I just want to clarify.  Two different things when we talk about chain. First thing is, are you sampling sampling from. Are you getting a sample from the stationary distribution.  And that's where the burden is important, the burden is to make sure that you're you're you're getting one sample from this district distribution.  Because before that the simple you have in some sense is related to the initial condition and you have no idea if it's something coming from this district distribution.  And so and so. Using this annual method you are making sure you know that you will mix and that you're you're forgetting the initial condition and you sample from the correct distribution.  Now if I want to have another sample.  And now that. And the thing is, once you're in, especially distribution you stay in this district distribution. Right.  The problem though is you don't actually have. So the first let's say you you you get you get a sample from central distribution.  Well the next simple you get marginally is also from the central distribution. The problem though is that it's extremely crowded to the previous one, you have  And so if you want a new independent sample. Well, you're kind of stuck. And that's where you would need again.  To get an independent sample. You'll need again to forget the the condition of where you were and thus you will need to, again, wait, define a burden to mix.  Right. And so that's kind of yet. So to get another independent sample. You will need to use the burning time  But if you don't care about having a depend simple all you care is just making sure that the samples are from the correct distribution.  Well then you could still use all the samples in the long average it wouldn't be the problem is that, indeed, like the the  Effective number of samples you have is is very tiny because they're all correlated. And so if you have a slow mixing chain. You will need to need to repeat this kind of idea of a kneeling to get more than one simple  Good. Does that clarify either the effect of burden. So basically you can think of burnin indeed as just how long it takes to mix and so it can be useful for both for getting the actual condition as well as getting a new independent sample.  And if you want to depend sample you cannot be stuck in one mode.  Good. Alright, so now let's talk about various methods, which I love because you know I like to position and so rational methods is basically the idea of using approximate empty optimization to do approximate approximation in poverty theory.  very rational methods.  So you might have heard versatile em versatile and France. What does it mean, well, I'll give you the  The general idea first. So the journal idea.  Oh, let me there's a question.  So he was saying. So in the case where we want a simple from a second mode would reduce the kneeling if we don't care. We can just continue. Correct.  Okay. So the general idea for virtual methods is  Suppose we want to  Approximate  Sorry, the approximate some quantity  That are called theta star.  And the idea is you first express this quantity as a solution to an optimization problem.  To optimization  Problem.  And so for example will say, okay, theta star is the arg min  Over some constraints space of some objective function.  And we'll call this up for the optimization problem and then the main idea of devotional method is you will approximate the quantity you care about by approximating the optimization problem so approximate data star.  Via an approximation to opt  In. Let me give you a concrete example of that to make it a bit more  Less abstract. So let's do linear algebra.  And let's say we want to get an approximate  An approximation to the inverse  Of a matrix, right. So say you want solution to x equals b, right.  And so that means that x star. The thing we care about is just let's suppose that A's and vertical. It's a minus one times b.  So I want to solve it in your system. And I want to approximate the solution to a linear system.  Right now, there's no optimization here. It's just I want to solve a system. I want to approximate that. Well, one way to get an approximation.  Is to express x star as a solution of optimization and then approximately solve this optimization. So I could do. I can I actually have that in this case x star is the Ark men over x of the norm between x and  It turns out that  It's pretty clear.  That's the case.  And so and so, for example, one way to get extra would be to run gradient descent on this optimization problem.  And that's by the way, it's a very different way to think then standard  Say linear algebra techniques that people use for solving in our systems which are all usually exact techniques, by the way, which you know that after enough computation you get the exact solution.  Though if I want an approximation and, in particular, I also want to stop my algorithm early  I want to also have not too bad of an approximation. So the good thing with these optimization algorithm is that you know you get closer and closer to solution.  And so whenever you stop you know that you're not too far from the solution and then if you keep computing, you'll just get closer, but you're never like completely nowhere. Right. Whereas if you just run. I don't know, like, three rows of three, three steps of  Of  Howdy call that the elimination algorithm. I guess the cleaning nation.  But then the standard are going to be used to solve it in our system after three duration with a dimension is a million. You have nothing as an approximation, you don't have much guarantees Goshen any munition. Sorry. Thank you.  You want to reduce in row reduce Islam form or something.  So yeah, so that's the general idea.  And and and that's actually a philosophy, right. So that's that. So in some sense, in very general. The International method is just the idea of us some kind and it's called variation, all because  The idea of calculus of variations in analysis is basically optimization, how to solve optimization problem.  And then the cool thing here is if you approximate parts of the optimization problem, then you get an also an approximation to the quantity you care about.  So let's now give a  divisional em or rhythm.  And why am I talking about variation of em. It's also a motivation for the kind of optimization objective that we will use  For doing approximate in France.  And so  Let's recall the EM trick.  And this is in the context of a latent variable model.  I have p of x and z and z is unobserved.  I guess I have some parameter. I'll use  Data and then the EM tricks says that the luck of the marginal probability of X given data. So when z isn't figure it out, can be lower bounded by the expectation over queue of the complete log likelihood  P x z given data divided by q AMP Z right  And this can be seen as the auxiliary function which depends on both Q and data.  Alright so that was the lower bound. And then we had that the difference between what we want to compute  And our lower bound.  Which depends on both the q over z and the Pandora theta was actually the kale divergence between two  And the conditional of z given x  And data.  Right. And so in the standard he step.  What you're doing is you're maximizing the lower bound respect to q  So you did. Argh, max over q  Of the auxiliary function which depends on cute and we were fixing data to prove his value.  But we buy this transformation. This is equivalent to minimizing the kale.  Between Q  And the austere user avatar   Data user avatar   Right. user avatar   So in that guys thinking in terms of rational and France and approximate in France use interchangeably know because approximate in France is any way to do approximate influence which includes sampling and other techniques which are not version all methods.  SO VERSATILE inference is a special case of doing approximate in France.  So that's the relationship but I presented, for instance, not necessarily vibrational inference.  Okay, so that's the step it's minimizing the kale between Q in the posterior or z.  And then  A key aspect here when we we use this algorithm was that there was no restriction on cute. So basically we were looking at all the distributions.  From z. user avatar   Right. user avatar   And so one way to approximate the step because we might not be able to compute the posterior exactly and etc, etc, etc, or perhaps this procedure is too complicated. So we're not able to work with it is to approximate this this this constraint set with a simple set so  In the virginal so a variation. All it means the goal anyone. It's pretty standard here. So a visual approximation.  For the step.  Is through do  You'll get an approximate  Version of the update  T plus one by doing the aardman of the kale.  Over a simple set. So I'll call it q simple  Okay, so I have the kale.  between q and the posterior  And so because I'm not considering all distributions. This is a source of approximation.  And this is to approximate  So in some sense, normally the solution to this problem when there's no constraints is just the truth hysteria.  But now I will, I will get something which is not a true to posterior because it might not be in queue simple so I'll get just something which is closing kale to the truth exterior and that will be my approximation. So this will be used to approximate P AMP z, given X data.  And in particular, if I want to compute the conditional of Zed given x. That will give me an approximation of it.  And so now if I do the approximate M step in him.  I would do theta t plus one.  Is the arg max.  Over my parameters set and then I will use my approximate distribution instead of the correct exterior  Of the complete likelihood  And so you can think of it as now getting an approximation.  To the true EM algorithm.  But you're still maximizing a lower bound. Okay, so this is still this is still  A lower bound.  On the marginal like tiered  But  Unlike in standard em.  You will lose the Montana city guarantee  Mono Tony city.  Guarantee  On the Barcelona likes you.  As a function of tea.  So before we knew that when we look at the log likelihood at our different parameter that we have during this EM algorithm. We know that the log like you would always go up.  Because at every step we made the lower bound, and the thing we care about tight. But now, because we do an approximate is step we don't make things state.  And so it could be that the while we update theta, the likelihood go down so it doesn't actually go up always  But because we're pushing a lower bound were hoping that overall, we will increase things. It's just that we're not guaranteeing it's increase in between every iterations on like a Saturday. Yeah, it's still a meaningful.  So that would be the devotional EMR room.  And this kind of motivate using the kale as an up as a, as you know, an objective to approximate distribution. So, more generally,  You could use  Argument.  Of Q in some set queue of the kale between Q AMP p  To approximate p  So this is called a very rational approach.  To approximate  P and. And here there's two sources that approximation. First of all,  You might not be able to minimize exactly the KL so you know when you run these optimization over them, you know, you get smaller and smaller, but you never have executive minimize unless you have like closed form solution.  And because you're not minimizing overall distribution. This is also a source of approximation.  Okay and this is by the way this is call I projection for information projection  So the I mentioned it in some notes that I linked  When I talked about the Pythagorean Theorem. When we talk about exponential Femi, that's something I skipped in this class for time can look it back if you're curious squat information projection and then the idea is if q is simple.  Then computing the expectation respect to cue might be simple.  So I will have loved P over Q. That's what I need, when I do the computation. And so, you know, often you will be able to compute this expectation  Even though.  You will not be able to compete expression respect to pee pee may be super complicated so completing integral or some over P is difficult, but cue the Gaussian, for example. Well, practice, it's simple to compute  So Jacob is asking, Where is the last someone's monitor CT coming. So as I explained, is that in standard em, you're making the bound tight at every step but and so because you're making the bound tight.  You know that there was this proof, where I showed you that this quarter. I sent all with them on the auxiliary function also push the true objective up at every iteration of theta.  But now, because in the step we don't make the bounce tight.  We are not able to do that anymore.  And in particular, even though the auxiliary function is always going up because it's not tight with the thing you care about you could have that ability function is going up, while the true thing goes down because there's space to move in between.  Is a kale is saying.  The difference between em and virginal em in the GMM paradigm you whether the underlying that is a mixture of gushing or not.  I mean the version OEM depends what what is the virginal approximation, you use for the East step right so the standard is step in the GMM will give that  The distribution over z.  I mean, Z was just an indicator variable. So they were fairly they were there was almost no assumption about z. So,  I think I don't think in the GMM it would make much sense to do version OEM because the posterior is is very simple, because it's on the it's just re computing. What is the probability of the latent variable, given the observation. Right.  And it's just re normalizing some Goshen.  So if you have a more interesting z. So we'll get I think it let's say for example, Z was a latent variable with a continuous latent variable rather than a discouragement latent variable.  And normally, the and let's say the hysteria over z would be a mixture of. Gotcha. Let's say, for example, like just, just to be abstract  And when we talk about the version of factor analysis next week. All this will become clear because that's what what what will happen.  Well, then the trooper sticker normally would be a mixture of caution. Let's say that now you restrict yourself to only single gashes  And so then you'll try to find the single Gaussian in the East step which approximate the best the mixture of the true mixture of gushing you would get for the troposphere. user avatar ezekiel williams  But I guess, or if they just me elaborate on that. I guess kind of what I was wondering is if if the underlying data.  From that we're trying to model with the Gaussian Mixture Model and we're performing em on if the underlying data like this I variable was generated by  Or if it was distributed according to some sort of more complicated distribution than just this this  This like  Multi know meal, I guess, in that case, wouldn't it be variation of em now because we're using. Oh, no. user avatar   No, because  The vibrational aspect is the approximation in your model.  How the data is generated doesn't matter at all in this user avatar   Framework. user avatar   Like I'm allowed to do and I'll to use maximum next to the Gaussian Mixture Model for data, which has nothing to do with a Gaussian mixture.  The parameter, I will get will give me some mixture of Goshen. If the data is not at all. It makes sure. Gosh, and will it will have a very bad likelihood in general.  And it will have also even worse held out like you won't be a good fit, but that's what makes someone like you to do so here the vibrational aspect is the approximation in your model.  Rather than there's no link with the data. Let's put it this way.  Alright, so the projection interesting projection. So the idea here is, let's say,  Is we'll be able to compute the expectation respect to simple distribution and and when I talk about mean feel in a few minutes, it will become clear that we can do that sometimes.  There's another direction of the kale, you can use which I would mention do I won't go into details. So there's an alternative  Which is called the M projection  Where instead of minimizing the kale between Q AMP p you will do the kale between p  And cute, so the other direction.  So this is called an end projection for moment matching  Because this. This is the same direction when we didn't maximum entropy. By the way, this is like the same direction.  Is the same direction.  Let's see.  OK, so the direction I was using was the the information direction is where we get the exposure family. Correct.  Okay.  But it's called moment projection because, indeed, try to force to match the, the moment matching the map them. So basically,  There's this plot in the bishops book. Let's say this is my true distribution p  So when you do I projection because of the property of the kale, you're actually trying to fit one of the mode. Okay, so this is basically  The solution, you will get by doing it I prediction. Let's say with a human. Human model approximation. So basically you're trying to find the mode in some sense.  Whereas if you do em projection. You tried to find this unit model distribution that we try to use a different color.  So let's say said green  So this would be what you would get when you do an M projection  And basically, you will try to find distribution, such that when you look at this moment. So it's mean it's covariance, it matches the meaning the covariance or example of the true distribution.  And so this is kind of like the M is basically moment matching  And so this plot and the explanation. You can see figure in Bishop  And this direction of the kill that motivates  An algorithm call expectation propagation  EP is for expectation propagation  Which is trying to do approximate moment matching  And I put motivation in quote because actually the EP doesn't even do exact doesn't even really do moment matching, but it's kind of motivated from that.  And then, you know, depending on whether you care about having fitting the mode or fitting the moments you get different approximate algorithm and different behavior, right. So it depends what kind of the properties of the distribution. You need to match better, which one will do better.  Alright so let me talk about the I prediction direction and let's talk about the main field approximation. And this is what you will implement in your assignment for the Isaac model.  So the main field.  Approximation. Oops.  And I  As a reference. You can look at section 10.1 from Bishop  And the way I talk about it, by the way, is in a non parametric way which is kind of nice. Even though often when you see mean field and papers, people do parametric assumption of continuous distribution. But what I'm going to describe kind of highlight that you didn't need this assumption.  So it will become clear soon. So let's suppose  That the distribution you are trying to approximate P isn't the essential Femi  And as you recall any this you understood graphical model on discrete data can be put in a special family as long as you don't have zero probability. So that's so Isaac model is the example.  And so let's say we have  Actually, I will use a z, y, z, I guess, because that's kind of what you did in the east.  Alright, so  I have let's say p variable. So, I will have said one to that P and so because I'm an exponential family of P AMP z is basically exp of at transpose T of z. That's my sufficient statistics minus log partition function as a function of an eta is my clinical parameter right  And so the main field approximation.  Is to try to minimize the KL between Q AMP p  Where we use a  Simpler distributions.  Which factor eyes over every notes. So we'll have that Q mean field will be the set of distribution queue.  Which are fully independent. So, it will be a product over. I have some Q I have said, I  Yeah, so it's basically the set of fully factorization.  Distribution. So that's our simple set of this vision.  For where P normally doesn't lie in it. So the kale won't be zero, but we'll try to find now the closest for the factories distribution in kale to  Pee.  And so now let's look at the kale between Q AMP p  When Q has this form. Well, it's the expectation, by definition of log of p divided by q  But now because le appears in the special family when they take log of it, it will have a very nice form. So, this becomes  Papa, Papa.  Why do I have in mind is there.  Oh, yeah. So it's actually minus log Q 12 it  Really  Yeah, I always get in the wrong direction.  I always forget which one is so it's love Q tip action.  Right, so I get minus expectation respect to queue of log p. So I get minus x transpose expectation respect to queue of T AMP z.  Plus the log partition function. This doesn't depend on z. So when I think it's fiction. It's not there. And then I have also the expectation of qoq that's just negative with the entropy. But let's write it explicitly. So this is some overs Q AMP Z lug Q AMP z.  But this Q AMP z here is product over Jay q-j  Of that Jay  And then I will have summation over j lug of q-j of the j. So, that's so by by assuming that the log is a product sorry that the queue is a product, it simplifies a lot the computation of the entropy  In particular, I can use this to be activity, as usual, because I have an essential some over all busy, right. So that's huge. But now I can push it inside. And what I get is,  By using the fact that you will get the marginals, in some sense, to get summation over i. So, this whole thing becomes submission, or I some issue over Zed i q i have said i and then just log Q i have the day  And so in order to compute the entropy of fully independent distribution. I just need to compute the entropy of the marginal and some them over all the notes, that's much easier because it's a very tiny some  And so, so that's already one thing because competing the entropy of P. By the way, could have been super and tractable, but. Q. Because it's simple. It's a product, then it becomes simple and alright so now that's my kale. And so as I told you before I could compute its expectation efficiently.  And the idea of the main fuel algorithm is to actually do coordinate descent coordinate descent on the marginals, so you know current descent.  On cue eyes. Right, so you'll fit one of the queue eyes at a time.  So basically you fix  q-j for Jay not equal to i.  And then what you do is you minimize the objective  With respect to q i have the kale.  Then I have q i and then tonight I, in some sense,  That's the my revamped tradition of Q  With p  And this was minus expectation, with respect to q i  Have  A  Transpose  Expectation she let me write it down more in different color. So basically I have the expectation respect to q i can spit it as First Aid expedition respect to everything else except UI.  And this will depend on that I analytics and respect to QA because that's the variable I will optimize and so I have here expectation respect to Q not I have T AMP z.  So I marginalize out all the other variables for my fixed marginals  And then I get a function which only depends on Zed I alright so this whole thing here. I'll call this  If I have said I it's a function of, said I, then I think the expectation respect to q i have this function of as a day and QA is a variable that I will optimize over  Then I have my luck partition function, which I don't care because there's no QA in there. So I'll just say it's a constant.  Plus a constant.  And then I have the summation over all I but the only one which depends on QA is this one. So that's the one that keeps everything else is a constant. So, I will have my entropy term for QA  And so shank chow do is saying the queue. We we assumed inputs are not structured dependent. Yeah. So we're basically saying that all over variable or independent, but the marginals are trying to match the, the product of marginal is trying to match the the correct  Full joint and so we are free to change the marginals, but indeed, we don't have any correlation between or a variable.  Mean it's pretty rough approximation, but it's kind of the simplest and then next class, we will see structured mean field where we will generalize that to allowing some correlation, which are simple enough that we can handle.  Alright, but so why am I talking about that because I want to show you that the coordinate update on the scale have closed form solution. So I want to minimize the scale. I want to, I want to find the marginal and q i which minimize this kill  That's why only express  The objective in terms of QA. So it's linear, and QA here and there's the entropy Park, which is so this is negative entropy  But so it's convex because the entropy is concave. So it's a nice convex musician problem with Q i  And I will just use the Lagrange multiplier method. So let's do that. So that's something we actually did when we did the maximum entropy in the exponential family that was the same direction.  So this is like  The max and  Derivation, by the way, because we were minimizing the KL respect to do uniform distribution in this case.  But in this direction. So if we add the Lagrange multiplier.  For the some to one constraint.  Then we add in the objective lambda one minus some overs that i q i have said, I  And so now I can take the derivative of the Lagrangian respect to cue to get this tertiary point, but because it's convex. It will be the global men. So I want the derivative of that to be equal zero.  So let's complete derivative. So the first term here is just like linear and QA. So it's summation over q i have something so when I think the rate of I'm just left with the something right so I'm left with minus F I have said, I  And then I think the derivative of the entropy. So I get  Log. So when I think derivative of July, I get just the identity. So I get the lug of Q I have said, I think there is of the login get one of our QA which cancels with the QA, so I get plus one.  So that's the standard we think different of the entropy will always get logged press one. And then I think the derivative of the leg ranging piece where I had this this lambda here. So I have a minus this some over q is I'll just get minus one.  Actually might Islam. Sorry.  So this is my gradient and I want this to be equal to zero.  Okay, and so you solve for QA, you get that q i star.  As a function of the day is proportional to exp of f i have z.  Today, user avatar   Right. user avatar   And so it's an explanation of me and you can read normalize it to see what it should be.  And so  The point here is that if you do this quiet ascent descent approach.  You get that the journal means field.  Update.  When the target p  Is an expression of me.  Is  For the  Marginal that you update you just set it proportional to  Exp of at transpose  The expectation, with respect to Q not I at iteration t  F t AMP Z right so that's where the other candidates come in, you will come in in the expectation and then you take the expectation respect is sufficient, that this takes that gives you something. And that gives you the new margin all and QA  And so  The the  Let me give you the concrete example with Isaac model so that we can conclude. And also, you'll have everything for the assignment.  And then I'll say a few more words quick were words about the mean field and then you can ask question as well. So if I go to the icing model example. So P is an exponential family.  So what was our sufficient statistics in the minimal representation. Well, I had that  I had for every node I looked at the value of that I  And so that I hear is zero or one. Right. And I also look at the product of that I NS Ajay for my edges.  So those are messages that this sticks and so now I need to compute the expectation  Over all my productive marginals  Have just said. Jay, the indicator variable. So this is just Q j of weather is that Jay  Is equal to one.  Well, let's say I use a time t.  And so will represent this marginal because it's a brand new with one number, right, and we'll call this Muji  GJ new J.  And it depends on T.  And so that's one piece of the sufficient statistics and then I also need to compute  kuna I have the product of, said I, ends at J.  Well, that I hear is not random because Q not is not putting a decision on that is so this will appear later. So this is just that I and then the expectation was that jays already computed. So it's huge.  Alright, so now if I compute the  Edit transpose expectation of Q not I at iteration p of t AMP z.  What I get is, I will get into i said i because there's there's no distribution on, said I, when I looked at Q, not I.  I'll get  Stuff, which doesn't depend on that I  But so, for example, some over j not equal to i have a big expectation que, not I.  Have zero J.  And that's just new JT  But there is no Sendai in this thing. So it's a constant when I will compute the  The update here. Here I only care about what depends on, said I, right, because it's a decision of Rosetta  And then I have plus summation over my neighbors.  Have it i j  Expectation Q not, I have to have that I said, Jay.  And so as I said above this is just  Said I knew j of tea.  And then I have the rest plus rest.  Where there's knows that I in it. There's only new j and  Or actually, it would be the the marginal on on j and l, but it doesn't matter.  Okay, so that means that if I now plug this back in  I plug this in here to find what's the new marginal for that I  I get that.  The result of my main fuel update  Is that Q i t plus one.  Of, said I.  Is proportional to the exp of at the I  Said I plus that I summation over j in my neighbors have I at J. And then I have a new  New j of tea.  Which is just a representation of my cue, Jake. Right.  This is the parameter for my  Can think of it as a parameter  For Q, Jake. user avatar   Of tea. user avatar   So the main feel update is for some notes I that you want to update you get it's you. It's new parameters is the sigma and because you need to normalize that of Ed i plus summation over j in the neighbor of I  At AJ and then you have  You j of tea in the neighbors right so user avatar   You keep updating that user avatar   So that's the main fuel update  For Q AMP z.  I  With parameters new I  And so you can compare this  With the good sampling of date.  Just very similar, but it's a sampling instead of a  Sub value were in give sampling. You said Zed i t plus one equals one with probability  sigmoid of at the i plus summation over j in my neighbors.  Have at i j and then instead of using the parameter you actually use the sample value.  So you can think of.  Give sampling as a hard version of me feel right mean field, you have these the soft count like new represent the quality of something to be one.  And then you, you will update you will propagate to the neighbors. So basically at iteration t you will pick one of these nodes that you will do a quick update and you said the new value of the parameter by propagating  Using the value of the parameter of the neighbors and then using these these parameter coming into your distribution if you're trying to match.  Whereas in good sampling, you would actually have 01 values which are samples which in average or actually news, but that's kind of a different thing. But the structure is very similar.  Okay, so that was a bit over time.  Is there any question about the main feel update for the icing model.  Know, so. So basically, one thing I wanted to explain why I derived the main field model also in general. Here is also ties back to the maximum entropy style of derivation. But the point here is  It say now a Zed was a this is actually true. Also, if that is a continuous  Random variable you just now get read an exponential family. And the point is if I minimize the kale respect to all distribution.  All continuous distribution on z, the one which minimize it will be an exponential family with these this kind of like as the form for the sufficient statistics.  And if you read old paper, like for example vibrational in France in the later additional education model for Topic Models.  They will usually say suppose you approximate q i with an explanation of me with some parameter and then you're trying to do mean feel update  So they will make a parametric assumption on the distribution, which sounds like you're making more an approximation and you're  Okay. But it turns out that even if you didn't make any Patrick assumption the minimization of the kale will give you  The same one that they actually chose, which was like, usually they actually usually choose an extension of meds with you haven't actually the correct form of the country version.  Somebody asked a. Why does it that has one index and sometimes two very good question. So basically the sufficient statistics for the Isaac model has both nodes.  Features, as well as edge features. And so here this is it will have at that I, the clinical parameter associated with a node feature and it will have at that i j for the parameter associated with the edge feature. It's just like that.  So Jacob is asking  Where does the this double some go when I computed the  Product over Jay. Is that your question, Jacob. user avatar Jacob Louis Hoover  I was just, yeah. When you had the product over Jay and the line above that. Yeah, where did that go when you when you went to this where you drive this double some user avatar   Yeah. So basically what you can do is you can write this as  So what you do is you factor out because sometimes just do it explicitly so what what you have here is this only depends on q-j so I can  Anything here, which doesn't have to. Jay, can be  Pushed out of the sun. Right, so I could let me write it down. So this is, this is the same thing as user avatar   That mirror it is down here. user avatar   So this is summation over i.  Have summation over z.  Of product over  Jay, that's great. Like this not equal to i q-j of that, Jay.  Which actually I'll call this Q, not I. Let's write it this way.  So this is  Q. Not I, said, not I.  And then I will have user avatar   A user avatar   Q.  I have said I lug of  Q I have today.  OK, so now I still have my big some overhead. So all I've done is just I push this some outside  Okay, and now the whole thing is that  Here I have q i said i here I have knows that I so when I some over all variables. This is just a constant when I don't have I. And so I'll just get here the marginal right so this will just basically give you  Basically one time summation overs that I  Which is what I have  Because when I some on said not i q not I have said not I just get one right this is distribution and all the other variables when I sum of all possible values. It's one  Does it clarify. user avatar Jacob Louis Hoover  Think so. user avatar   So, and if it's not fully clear just  Try it, perhaps more key me with putting all the steps, but that's kind of the idea  And the other question.  Alright so let me stop the recording and i think is a kill. I'll take your question, but people can leave.  Yeah, so next class I will cover.  Bayesian methods and model selection.  So that's the plan. And then the next lecture will be on factor analysis model gushing network and the last lecture next week will be by Jose on Goshen processes and non Patrick Beijing methods. So that's the menu for the remaining of the lectures. user avatar   Let me stop the recording.
  Okay, so  What are we doing today. Today, today.  So we'll finish, I will make some comments about virtual instance  To complement what I propose last  Last lecture so finish.  virginal and I will cover Bayesian  The Bayesian approach, as well as model selection. And I think I'll say a few words about causality. user avatar   Okay. user avatar   And so let me continue on the main field.  That I presented last class so mean field was a way to get approximation to to get approximate to do approximate in France. So to get an approximate distribution and, in particular, you can use it to compute the approximate marginals in icing model right and  So the, the problem you're basically solving is men over queue in the queue mean seal approximation. So, cumin field was the set of distributions, which factor which are fully factories over your nodes.  And you compute the you minimize the kale between Q AMP t but I showed you what you could do a coordinate descent approach and that where you optimize one  Marginal over a node keeping everything else constants and these updates could be done in close form way in exponential family in general. Right.  And so one thing to note is that the kale is convex. So it's a complex function.  And so you could think this is a convex optimization problem.  But it turns out that the set of distribution which are fully independent is actually a non convex  Constraints it  And then it's a question of, okay, well, which said, Am I talking about, think of a parameter ization usually this parameter ization one be conflicts.  Okay. And one reason is, like, one way to prioritize your distribution is through the meantime position.  So in the in the exponential family I talked about the chemical composition. But it turns out that often there's a mapping between clinical parameter and mean which is once once we could also just use the mean pasteurization and, in particular, for the Isaac model.  You have the pump position which you needed was what's the mean on the edge because the sufficient statistics were both the the edge.  Assignment. So the weather two nodes are equal to one and  The node assignment. So whether or not is equal to one. So when they take the expectation of these submissions and this x, I get the probability  Of the to note to be equal to one. So that's a marginal. So that's new i j  And when the distribution is for the factories, you have the constraint that new AJ is equal to new i times Muji so it's a product constraints. And it turns out that this relationship.  Is actually non convex. So, this is this is non comics. This kind of constraints. It's not linear. It's actually a nonlinear equality constraints. So it's not comics.  And that's why the set of distributions is not complex  And Jacob, yes. The assumption in mean feel is that this set here was that, you know,  This was the set where q was such that q of x was the product over, I have to, I have x, right. So it was fully factorization.  And so something I covered in some years in the past, but I don't have time this year is to talk about something called the marginal Polito  Okay. And so if you're curious about that. I encourage you to look at lecture.  Where I mentioned the marginal potato, which is basically the set of moments which are achievable for specific sufficient statistics.  And you, and as I said you could paraphrase your distribution by their moments and the marginal poet top is always a conflict set actually  Could be seen as a context combination of the assignment on the corners, but it might be a very complicated Polito which has an exponential number of linear inequalities.  To specify which is why, even though it's a complex set, it might not be tractable to  Minimize over and the main feel approximation actually looks like that. So all the corners are the same, but it actually is kind of like curve in our approximation.  So this is the the the basically the the marginal the this the moment pasteurization for all the the midfield distributions and and these curved kind of like boundary comes from the the the quadratic inequality. Right. So you get this quadratic at the box. user avatar   Okay. user avatar   And so as I mentioned in general the the approach for for virtual methods is you can approximate the convex  Optimization problems. So you can approximate the constraints sets or you can approximate  The objective. So this is an example of approximating the complex and there are other ways to approximate the convict set in particular, there's  A famous method by Marty grain right and all which instead of doing an inner approximation makes an outer approximation of the marginal potato.  And a simple outer approximation. So, so it's actually easy to optimize over it. Actually, it's called the local consistency boy dope. It's similar to this notion I talked about when I  Talked about junction trees and stuff like that. Like you, you have that all the marginal or luckily consistent and then you can optimize over that and that gives you because they have a bigger constructs that they will give you an upper bound on the actual true kill  Okay, but please see this lecture. If you want to know more about it.  Alright, so  And so the point is, because it's a non convex constraint set that implies that you can get stuck. It's an unconference problems, you can get stuck in local minimum.  And so like in when you do em me NEMA like you do em.  Diverse the main field approximation depends a lot on your initial conditions. So normally, in practice, what you would do is you would try different random initialization and pick the one which has the best kale.  So that's often will give you much better solution and if you just only tried once. So it's a bit like doing em or also doing k means right so but be aware that you can get stuck in local media.  But even though you can get stuck in local media. One of the big advantage of diversionary approach in particular country compared to the sensing approaches that it's really easy to debug because you can monitor progress right  monitor progress by tracking. What's the kale between your current approximation and p  As a function of number three. So you could have like iterations and because your method is doing corner descent. It's actually usually decreasing. So, you will have a strictly decreasing function, then you can see. Oh.  You know, I've I converge to a local minima or how things are doing right and so  I'll put a constant here because as you will see in the assignment.  For the Isaac model. You cannot compute the kale. Exactly. Because to compute the kale. It will depend on the constant which is the Lord partition function.  Of the tuition fee that you can have compute but Dillard partition function of p doesn't depend on cute. So when you minimize respect to q is just a constant you don't care about this constant okay and so  So for example, you could just subtract login said p and then you will be able to compute the rest. Okay, so that's what you attract  During the assignment. And then, in particular, if you're doing  Mean field and your kale does something like this, you have a problem you have a bug in your code because it's supposed to be a strictly it's supposed to always go down because you're doing exact current minimization at the retreat.  So, so that's kind of an advantage of the virtual method is that it, it's easy to see whether you have book when you do sampling, you just observe a bunch of random samples. It's hard to know. Like, is this what I'm supposed to observe or is this a bug right so  So if I summarize the pros and the cons of  Virtual methods.  In very  High level terms. I mean, there's a lot of research in both and and it's not completely true all these things that were mentioned but  At me so basically if I compare virtual methods versus sampling to do approximate in France.  So one Pro is that because it's optimization days.  It's much easier to debug. As I mentioned, you can actually track progress is really much more easy than looking at like diagnostic for chains.  And it turns also that it is often faster to run than simply, it's not always true.  But it's sometimes much faster.  Particular  You will see in the lecture by will say on non parametric methods that virtual methods actually are very effective for these non parametric models, there's a bit of a notion of a concentration  Like a central limit theorem which kicks in. It's like in high dimension, you have a lot of dimension and you kind of have this concentration  Phenomenon, which appears that even though you're making an approximation. It's a pretty good approximation because of this kind of central limit concentration  And in contrast, you know, for for sampling because it's kind of like random it's  Because it's very noisy, it makes it harder to debug.  And the mixing problem of the chain is very problematic, right. So in theory, when you wait an infinite number of time you will sample from the correct distribution within practice usually your mics don't change that will in high dimension. And so it's more like a quite touristic  So that's this and then another aspect, though, is that the optimization method it the are biased.  In the meaning that  Even if you run your optimization method forever. So if I looked at where I converge and even if I did the global minimum my kale, you would have that the expectation with SPECT function respect to your approximation is not equal to the true expectation  Because for example in human feel you're restricted to a distribution which is fully fact rise but your true this vision was not fully effect right so you don't have a correct distribution. So when you will approximate  Expectation of a function, you won't get the correct answer.  Whereas for sampling  You do get on by a system, it  IE. If you take the expectation over all the possible sampling independent multicolored chain that you would run. And so what I will write is, I will write Q infinity as being the the  Sample you get after reading your chain forever, which means it has mixed. Okay. And if I take the expectation. Now, with respect to the to the samples, I would get, because if you have only one simple. You know, it's not the right thing. You need multiple sample to approximately the integration.  And so if you take the expectation respect to the random samples. And then you take the expectation of your function of that. Well, this will actually be the true expectation. So you will do the right thing in average  So here there's still the caveat that you know you needed to run your chain forever. Never happens in practice. So when you don't do that. You want this to give the right thing but  It's possible, even if you haven't, if you wait long enough, it's possible to get the right answer was for these virtual methods it, you cannot even get the right answer. Even if you have infinite computation.  Alright, Dora is asking mean feel is when I approach. What are some other approach. Well, as I mentioned, the general principle is to make an approximation.  Of your optimization problem. So make an optimization of the key objective make an approximation of the constraints. And so there's other ways. And there's many I will mention another one now.  But you know you you can look at  This book by rain Martin Jordan graphical model exponential families and  Forgive us the rest of the of the title, but the talk about much. It's actually a pretty big book, but it has a lot of other version of methods, but let me talk about structure of midfield super fast. Just to give you the, the idea  As another example, so structured mean field.  And so it's a generalization of midfield where instead of supposing that my distribution factor is fully I will say that my distribution factor eyes.  As a product over clicks. So I will have a product over k  Basically  Said, see, Jay.  Where these set of nodes will be chosen in such a way as you can do tractable inference. Okay, so see one, two CK is a partition.  Of V  And the Q, geez.  Are tractable.  Distributions.  So tractable disability missing this decision that you can easily do n friends with you can compute there entropy, etc, etc. And so an example of that, if they would be trees.  For example, it could be tree Eugene. And so the main field example is just every node, it's, it's on clicks. It's for the factories.  But structured midfield generates that to instead of assuming that everything is fully factories us you assume some correlation, but the patterns is in a way that things are still tractable. And so, in particular, let's, let's look at the Isaac model again.  So here's an Isaac model I will make a grid, blue, blue, blue user avatar   Alright, so we finished my grid. user avatar   Took user avatar   Alright, so I got a grid.  And so an example of splitting, you could do is actually in two sets. So you could have this set of nodes. Oops. How do I wonder user avatar   Bought user avatar   So you could have  A this set of nodes.  As one click.  And this set of nodes.  I mean it's, it's something really quick. It's just a set of notes. Okay, so see when Institue in this case are just nodes, because they're not fully connected in the original graph. And the point now is  When I looked in the edges appearing from the graph between these nodes like  I get a tree.  So you don't care about nodes appearing edges appearing between the two set because these are cut in some sense, but since. So here it's like if I have a tree.  On this part and I have a tree on this part. So I'm making an approximation of my my joint as the product of two independent  Marginal where each one has a tree structure. So I have this tree and which means I allow some correlation between these notes, it's just I now don't have any more correlation between this node and this stuff.  So it gives you a much better approximation and because the inner thing is are still trees. It turns out you can still compute the, the, the, the coordinate update in Tucson.  Ezekiel is asking about much more or less variants and sampling methods. Well, so the problem is is  How do you define variance. Right. So, so  Like if I use a deterministic initialization to my meaningful approximation. I always get the same solution. So there's no variance. Right.  Now you say, Okay, I could do random initialization and then they get different local minima. So there's a bit of variance, but even sometimes, you could have that always convert the same point. So there's no variance and so  And so it's it's because it's a deterministic optimization approach violence is not really the right concept what you have instead is just the quality of the approximation, like how far you are from the true solution. It's just, it cannot be explained as a bias perseverance.  And then there's also you can also have randomness promotional methods like if you do a stochastic approach stochastic optimization approach to it.  But so I would say that there's much variance universal method because just it's not it's not random. It's not a noisy random approach, but it has a bias, whereas the sampling approach usually is either unbiased or has low bias, but it has high variance  And to reduce the variance, you need to, you know, simple a lot user avatar   Okay. user avatar   All right, is there any question about virginal methods. user avatar jacob louis hoover  brief question to the sampling. Yeah, the stuff that we've been just saying about sampling. Good. All right. Like there are  Different ways to sample. Right. So like you could you could have a bad way of sampling that doesn't produce bias or I'm not sure what in what generality, are we speaking user avatar   Well, so, so, for example, all the exact centering techniques that I mentioned, like rejection sampling important sampling all these methods didn't have bias.  So, so an expectation over your rent your independent samples, you will get the I mean an expectation, you just have the right answer.  Now the Monte Carlo Mc, Mc approach has a bias in the sense that it depends on the initialization. And so it will be unbiased when you can ensure that the chain has next. And so that's where the the this infinity here was important. So we need infinite computational long enough competition.  And so, but the point here. I was making is that you can make this bias small by waiting long enough was here, even if I wait an infinite amount of time. I'm still bias. So that's kind of a qualitative difference  Okay. And indeed, you can also have a stupid Markov chain which  takes much longer to converge. And so in this case it will be it will be, you know, have higher bias.  And you could add a mark of chain that this doesn't have the correct session is division.  Either because you forgot about the convergence theorem and you didn't have the you made that change was not organic. For example, and then it doesn't conversion seated right thing and then of course it's biased because it doesn't even converse to the right thing.  Okay. user avatar jacob louis hoover  Yeah, thank you. user avatar   So there. I'll come back about a more time in France in the next lecture when we talk about the vibrational encoder, which is where it was introduced  And yeah, okay. So start back is asking a good question about convicts optimization. They're saying that are. I'm a bit confused about how you get local minimum. When you come you minimize a complex function, even though it's an uncommon accept. So the thing is  When you do constrained optimization. The stationary point is when the negative gradient is perpendicular to the boundary. Okay. And so  So, for example, like if it was a convict set. This would be a  Local men because they think it is pushing you away from the boundary in a particular fashion.  And and then where the complexity comes in. Is that the way the gradient changes in your set is such a way as you only have a convict set of possibilities, you can have multiple solutions in  In in context and position is just that all the set of solutions will have the same objective.  Now, when you have these these weird kind of like non convicts thing is it's pretty easy to kind of get stuck like you could have the  Year I could have the greatest in this direction. And here I could have gone in this direction. And so both of them or local men, which is they're not global mean  I guess it's a bit harder to see without planning the function. But, uh,  I mean, I guess the easiest way let's let's just do an example. Here's a convex function and then here's a non convex set. Right. So there's two pieces.  And so then I have a local men here. And I have a local men there I eat if I make local move because of the constraints. I'm not allowed to get out. So I had two different local guess that's or  Highlighting things  Is it. Okay, perfect. user avatar   Good. user avatar   Alright, so let's talk about Bayesian methods.  And so, unfortunately, I want to be able to go in as much details.  But I will still give you the important to limit.  So I already talked about Beijing methods in  In the first few lectures. When we talk about decision theory to trap. And just as a recap and basically now what I'll do is I'll be able to go a bit more detail because you have much more  Terminology and background. And so as a recap. You can think of property of going from a model to samples so that can give you data right so you can use property theory to go from  The model and then  You can answer a question about was a property of X given data and you could simple from it and then you get observations.  And then statistics is the inverse process of from observations inferring what could be the prostate model which generate it  And unfortunately, as I mentioned, this is an inverse problem which is ill defined is no golden solution and then  That the two philosophy. I mean, the frequent this philosophy is actually not a fix philosophy. It's basically everything else, which is not Beijing in some sense, but the approach is to have a bag of tools. So, you will have  Different ways to estimate which I already mentioned in this class. So there's maximum likelihood estimator. There's regularize maximum length you there's maximum entropy as another principle.  There's movement matching  And there is empirical risk minimization introduction address all these gives you different ways to estimate your parameter  Which might have different solution and then you also add an analysis of these methods to know, okay, what are the assumptions and there which they will work well or not and what are their properties at your direction.  And that's how you get a sense of, which one should be better than which are there is buddy depend on these assumptions. And so then, you know, in practice, which ones you use in practice will depends which one you think matches the assumptions you made. Okay.  So that's the frequent his approach if you're a subjective Bayesian  Just basically the purists Bayesian perspective.  Because you have the pragmatic Bayesian which  Which divert from that. And the idea of a subjective vision is you use probability everywhere.  To  Everywhere there is uncertainty. So it's to quantify your belief about the uncertainty. Right. And so then  The focus is  Getting the posterior so you have a posterior given the data.  And that will be proportional to some like cleared.  Times a prior  So this is the prior  This is the likelihood  And this is an austere and the prior present your prior belief before seeing the data about the thing that you're uncertain of  So anything you don't know you need to put the distribution. You don't know the parameters of your prior well then you put the distribution of your prior to this call I proprietor.  And then you don't know the the value of the parameter of the hyper prior well then you put another distribution center insert right, you need to encode your beliefs.  And then  The caricature that I mentioned.  Before is that the Bayesian is an optimist.  Where the optimist. Well, because in some sense they believe their model.  So,  You can get good so basically the idea is you will get  So the thank  You can get  Good models.  In quotes  And so because they think they can get good models you just obtain a method by doing influence in this model. user avatar   And France. user avatar   In the middle user avatar   Okay, so. So there's a question and well where does your prior come where does your likelihood come. Well, that's your model and  A subjective Bayesian kind of beliefs that you have a good model. So then the question is just to do and France, and then you get a method and because they believe in their model, they'll need to analyze the properties of their method because it's a good method by their optimistic efficient.  Whereas the frequencies in some senses more pessimistic and that's a caricature, right. So, but that's, I think one way to kind of like clarify the difference  And so because they're pessimists. They don't believe the models. I mean, you know, maximum likelihood. That's just a principle that doesn't need. And so they need to use analysis.  Tools to quantify the properties of the method and to make sure that it doesn't do crazy stuff.  And to understand what are the conditions that will do crazy stuff.  So Dora is asking if you have multiple hyper parameters.  Which might not necessarily be independent, do you perform a separate Beijing update on each or would you estimate them an all in one model. So the short answer is  You always do joint update. Right. That means that like the proper way to be a Bayesian is that there's something you you're working with.  You need to characterize all the beliefs you have about this all the possible interactions and then you do an update. In this model, which means usually that things will be jointly updated.  And practice. Usually what will happen is that, so this is intractable, then you need to make approximation and one approximation might be that you will treat things as independent, even though you assumed they were not independent.  A bit like a meaningful approach. You could have a mean Phillips.  Alright so that was the philosophy. And if I go back to our example that we had mentioned in lecture five or six, I forgot.  There was a bias coin.  And so we want to model.  The output of a coin, which might be biased and so we'll see if I knew the bias of my coin so excited given data would be brand new ye  Theta.  And then we might think, well, I will believe that my coin flips are independent. That's also an assumption.  You know, in general, you could put some dependence, if you wanted to.  And so you say, Okay, I have n coin flips of my xi, but all these results depends on my parameter and then you will have to put a prior over this parameter. Right, so you'll have a prior  over theta. And then this prior could depends on the hyper parameters seems bull alpha zero, beta zero. And so, and in the graphical model terminology we often use.to present things which are fixed. So these are hyper parameters.  For the prior  And so an example I had mentioned was already. I could have that theta is uniform  On 01 and uniform and 01 is actually the is actually the beta distribution with parameter one one, right.  And so, more generally, I could have as my prior over theta that it is a beta on theta with hyper parameter alpha zero and beta zero. And what's the  Yeah, so, so basically the if you remember beta. It's just distribution and 01 and to diminish. There was a distribution and the Protestant ethics.  Then Jacob is asking what was method here like a decision rule obtain a method by doing in France. This is to estimate. Yeah. So, good question. So method in the sense of statistical decision theory.  So when you do your decision theory, you will need to make some action to solve a problem.  And so if your problem is to estimate the quantity will then you will be you'll get a method to estimate a quantity, if your action is to make a prediction or to  Say, what should be the, the, the decision in a in a in a in their musical like should we say take a treatment or don't take a treatment. Well, then again you will use the posterior as a Bayesian to make this decision.  But yeah, so so method here is basically in a general sense of what you get. It's a decision ruling sensical decision theory.  Right. So that's our prior a betta and then or likelihood. Well, we already said what what it was, it was that it's a renewal fee. So this is Bernie's so that means I think theta.  I raised it to xi in one minus data, one minus x is that the brewery you like to do with power theta. And so now if I compute my posterior  Which was P of Theta, given my observation.  This is proportional to the like to turn the prior  And so I get the product of all my observations of p of x i given data that's my likelihood is they're independent times my prior  And so I get a theta raise to the summation over xi.  And one minus theta one minus summation over. Oh, sorry. And because I have any outputs.  And minus summation of her I have xi.  And the prior  Is basically theta raised to the alpha 01 minus data race to the beta zero minus one.  So that was the the the beta prior and you have to make sure that you're in 01  And so you have an indicator on zoom. So that's the prior  That would be the better prior  And so what I get any of the day is that my posterior so you can see here that I can group.  This with this. And so I basically get a beta posterior  And so it's called this  Let's define this as in one which is number of time and I seen the observation one. And so I get that the posterior  Is basically a beta on theta.  And then I take my prior accounts and I add in one  And I have my prior accounts for the other guess was beta zero  And then I add n minus one. So the number of time I've observed zero  So that was a new hysteria.  And so when the posterior isn't the same family as the prior  You call the prior to be conjugate. So this is called a conjugate prior  To the new you like you'd model.  So conjugate see depends both on the likelihood, but all it says is that when I use a prior when I have a prior in the specific family with this like you'd I will get the poster isn't the same family.  Just kind of neat because then you don't need to change the distribution. So, more generally,  When we talk about conjugate prior  If you consider  A family.  F.  Of distribution.  I'll call it like  Should say  F is  Distribution on theta. It has hyper parameter alpha and an offer belongs to some  parameter space alpha A typically Guess we'll use capital A.  And let's use a fancy  It's just for alpha, then we will say,  That F is a  Counter get  Family  To observation model.  P of X given data.  Theta.  given x and alpha belongs to the family F for any  X.  distributed according to X given thing.  Okay, so that means  That there exists in alpha prime  Which depends on x and theta.  Such that the posterior  Having seen the observation X is has the same distribution as just a PFA that given this hyper powder alpha brain.  And so alpha prime in the context of the flip coin model is basically adding number of times I've seen  X equals one and  X equals zero to the alpha zero and beta zero Patrick's  Okay. And so I'm  So that's a conjugate family and  As I had mentioned about good sampling. So if you use  conjure get  Prior  In a director graphical model.  Then give sampling  Can be easy, it's not always easy, but it can be easy.  Because it usually means that the these integral to compute the, the, the, the conditioning could be tractable. And so, for example, this was used  In the latest allocation model, this is the case.  In the LD model.  Which was a topic model for document.  Where  Basically  If you remember in homework one.  We had that if we use additional prior  It was conjugate  To the multi multi normal like you would model right  Because you computed the posterior the parameters of the posterior of the deer chalet when you had a multinational observation.  So basically the the LD model, what you have is you have  I mean, in one version.  Yeah, which one do I want. Okay, I'll do the traditional version. So you would have  Words.  In your document.  And  Each word would come from a topic which is not observed  And then  You would have  You know a bunch of words which depends on the length of a document. So, d index the document I index the word in a document.  And then what you would say is that is the latent variable which is which topic. Did the word come from.  So this is on key topics, there's a possibility. So that's the discrete variable that you will use. That's the IT WILL BE FROM HIM multinational model. And so you will have in this case.  It's given by theta, which gives me my distribution over possible topics. So this has to belong to the property simplex over k. So, these are called the topic proportion  So these are the topics.  And what happened is you need a prior over the topic proportion and so because it's multi normal observation, you can use a conjugate prior so here you would have that the  Theta d will be coming from a dairy share with character alpha  And then what happens is in the LD topic model, you have another plate outside which says how many documents I asked. So these are the documents.  And so now  I haven't observed  I haven't observed Z Evans over state. So I want to infer these from  From data. And so I can actually use give something for them. And actually there's even another parameter. If you're Bayesian because this is still a pretty Bayesian I would have here the  Beta. These are the  Parameters for your topic distribution.  We're also in the property simplex for each topic. Every distribution over words. So it's also a vector in the property simplex. This is k basically v by k where visa size of the vocabulary.  And so you would also need to sample it if you're Asian to get an approximate them with the austere and you could also use gives him so give something was one of the standard inference method in the LD topic model.  But if you were not able to follow that it's not crucial. This was just to illustrate the example use of conjugate family and in the world. So conjugate see is kind of a computational  Convenience. It's also kind of philosophically useful because you say, well, in some sense, you could think of your prior is coming from previous observation and because when you observe something your posterior is still the same family.  It's all consistent right so you could. And so now you kind of staying in the same families kind of philosophically appealing.  Though often these configured relationships are not that great to match what's happening in practice. So, you know, you're not restricted to use contiguous, especially when you have good approximate inference techniques coming from Markov Chain Monte Carlo or virtual methods.  So is there any question about this.  Time for a break.  So after the break, I am going to talk about model selection.  And causality is that the content of today and in particular because Beijing methods or pretty elegant to do model selection, because we have uncertainty on their models, then we can just put a distribution over them.  But is there any other question about what we've seen in the first half.  Nope. So there's no question. It is 233 so we have a break. Until 243 user avatar   Let's put some music. user avatar   Okay, so  Let's talk about model selection.  model selection.  Alright, so let's say I want to choose between two  graphical model.  Some people forgot to give their coffee and during the music.  Sorry about that. Alright. So say we want to choose.  Between two possible directive graphical model.  So m one here represent model one and and to represent model to son here. By the way, I apologize in advance. I'm a bit rough on the notation.  But hopefully it will still be clear. So let's say I have x one.  I mean issues with my random variable.  X one.  And then I have x two and I have extreme  And now I want to consider a different directory graphical model on those three variables. And so let's consider to choice. Either I have  This  Independent effect, I guess.  Progress with a name for this structure. And so I would have a parameter on this conditional a parameter on this conditional. And let's say a parameter on the on the marginal and next one. So that's my first  The GM, then the second one is  Actually a more complicated the GM, in some sense, it has more possibilities. I will still have this distribution distribution. So, I still have beta zero, beta one, but now I also have  This V structure here, so I will have this edge two x three. So now extra depends on both X one, X two directly. And so for this I will use a different pressurization I'll call it up to tell them  Why, because of the tilt, because it's not the same space. Right. And so in this graphical model.  The representation for theta theta one will parameters the conditional on X true given x one.  Apologies.  Need to be able to mute fast but you know  Master zoom yet fast enough to do that. Let's see if I can do it here for you all. user avatar   Know, and it doesn't work at all. user avatar   Okay. user avatar   Yeah. So basically in this one say that one would be prioritizing the conditional of extra given x one and then this graphical model, I would have a conditional on extreme which would depend on both X one, X two and it would use the parameter theta to tilde.  Right. And I guess here to make it clear, I will use the same  Lecture use this one because then the difference between the two one is more clear. So data to hear only portrays x two given extra given x two x one.  Wasn't the other graphical model. And this one actually. I would also have a dependence on next one.  Next week, sorry.  And so  Note that one is a special case of the other so  If you think of em one as being the set of. Oops. So note.  Here.  That  Everyone  is included in me too.  I put quotes because I didn't see that one was a set, I just said in one is basically just when we can present one model versus the other.  But if you think about the set of distribution which could be modeled by one. It's a subset of this of this mission, which could be modeled by him.  Okay.  And so  As a frequent is  You could estimate the parameters in each of these model.  To get different estimate of your distribution. So I could have theta m one  Maximum likelihood, which would be the art max overstayed at 110 a zero state of one antenna to  Have the log of the probability of my data given theta zero, theta one antenna two and model is m one  Okay, and that's where I'm being, you know, this is where my notation is weird. So what do I mean by that. This is just  Saying that I have to multiple choice for two different graphical structure. And so when I say model equals one. I just mean. And I'm saying I you know I am in this graphical structure.  So that would be the estimation in model one and then I can also define a parameter, which would be the submission and model to  Whereas I still do the max.  And then I would have stayed at zero instead of one doesn't change. But then, instead of theta to I am wouldn't have  Theta to  Tilde instead of to to note that this has a different space than 10 have to write  In particular, it's much it's a bigger  It's a bigger dimensional object because it's, it's an interesting a conditional which depends on more  Inputs, then I would still have to lug P of the data.  Say that zero, theta one and I have this data to till instead  And I need to kind of still put the context of my parameters. So I'm saying now I am in the model into  Okay.  And so then I could have you know two estimates on my parameters and each different model. Then the question is which model is the correct one. How do I choose the model. So if my job is to estimate the correct model. How do I choose it. Right. So, how to choose.  Between  Models.  And so one thing that you would know in any standard machine learning class is you could not just compare  The log likelihood for the maximum Nyquil parameter on model one versus model to because sometimes there's a bit of overfitting because you use the data to compute the parameter. And so you can evaluate your model by using the same data. So if you look at login of p of the data.  Given  The parameter estimated by maximum likelihood for model one and I say, I look at the model being equal to a model one  And if I compare that to the log like you'd have the data.  For model to estimated again on the data, saying, m is equal to empty.  Well, because the model. One is included in model to the right. The second one will always be bigger, you're maximizing over bigger space, you can always do just better and it looked like you. So you would always have  The left hand side, smaller than the right hand side since model one was included mode. So you will never choose the less complicated model that bigger computer model will always have better log likelihood and the training set when you use the training set to estimate the panelists.  Okay, so you would always choose the bigger model, which is, in some sense, already I lighting that you would be overfitting would always user avatar   Choose user avatar   The bigger model if you did that.  And so that's why in machine learning. You never choose hyper parameters or models on by using the training set, you always use  Something else like a held out test set evaluation set or something like that, right. So that's the idea is you use different data to evaluate your model than the one you used to train it  That's how you can get a good. That's how you can choose between different models. Okay, that's what you could do, also in the assignment. By the way, to choose the number of states in hmm model. So as a frequent is  You could use  So basically one standard approach would be to use cross validation  And so cross validation is you would split the training set in multiple pieces you would train them on on one piece and test on the other piece and then you would average over muscle multiple of these spirits.  For us a fixed model. And that's how you evaluate your model and then you choose the one which is the best, but the simplest version is to spit just in ever held out test set. So the simplest would be to have log  Of p of test data.  Of comparing for the maximum like your parameter, on which depends on the train data and then you just compare this, you look at the log latitude in a test data for the different models and then you pick one which has the best held out like next year.  Um, so yeah, so start Zach is asking a good question. So could we not possible that the bigger model create more local minima that are worse. And those are the smaller model. Yes, indeed. So this is kind of more computational question.  So this analysis here to simplify is indeed supposing we could complete the arcs. The will organize  It turns out that there's a bit of magic in the deep learning is that when you have more parameters. Usually the optimization is better behavior, you actually have less local minima.  So it turns out that by having more parameters you have always more space to move into so you're never stuck on like you have a neural network with a small number of parameters.  Even though it matches the structure that you use during the data. It's much harder to fit these parameters and if you use a bigger neural network.  Well, yeah, but so that the general story. You know that's something which is super important. Machine learning is if you want to choose the hyper parameter or the mall, you need to use different data in the training data.  But so now let's see what Beijing right so let's talk about this is frequently. So as infrequent is do use cross validation or held out has set that works fine. But if you're Bayesian you are uncertain about the model. So then would you do well. You put the distribution over models. Right.  Alright, so the Beijing alternatives.  Is  So if you're a true vision.  True in the purest sense, you will actually some over models, right, you will you don't know which model is the correct one. So you put a distribution of our models.  And so then, because you have not observed, what's the correct model when you will do prediction. You will marginalize out over models you will integrate them right. You will integrate out the uncertainty.  Out the uncertainty about em.  And so you need to have it prior over models. So you introduce a prior  Whatever models.  And that's called P of capital M.  And so now  If you want to make predictions about the world. So let's say you want to do, what should be the priority on some new observation given D. Where does my data.  And so as a good Bayesian you integrate out the uncertainty, you just use the laws are properties. So, this will be the summation over your model the property over X new  Given the data and my model and then probably T of the model, given my data is this is basically you have a posterior over model right  Now, there was this weird  Probably to over the data because knowing the model is not sufficient. I was willing to the parameter is in my model.  And so this is x new here.  Is the integral over your parameters which depends the set of parameters depend actually on the model, then you would have a new model with parameters will tell you, okay, what's the distribution over X given feta, and M.  And then I would have a posterior over a theta given my data in my model. The of theta.  And they still have was the protein over model, given the data.  OK, so now there's a bit of a unwrapping this this equation.  Oh, user avatar   There's a bunch of question. user avatar   People now discussing this, this surprising phenomenon in the burning were having more parameters help  So I think the best way to think about it why having more parameters health and deep learning is thinking about  That the way we train so using stochastic gradient descent, which is not all iPhone, the global minimum of my error.  Tree. There's unlimited number of global minimum. Anyway, but the way you train influence what you get. Okay. And it turns out that having more parameters.  The dynamics of the STD algorithm becomes much nicer. And so it has nicer properties. So in some sense, there's a implicit regularization, which happens from the training technique. And this has better properties when you have more parameters.  The same way when I mentioned that when you do non print trickle Bayesian it didn't really explain why. But I said there was a bit when you do virtual method.  There was a bit of a concentration phenomena which appears. And it turns out actually usually virtual method.  Do very well in high dimension and you do even better than in low dimension. So you had a new dimension, the virtual method could have actually high approximation error.  But when you have a lot of dimensions in some sense things kind of average out and you get a consideration phenomenon. It's some weird sense and you get a better approximation. Okay.  And so there's a. It's not the exact same phenomenon that is also a phenomenon that having more parameters, give you a nicer regurgitation aspect in the party.  But this is still like an active area of research.  Okay, but if I go back to the actual model selection. So this part here is standard Bayesian prediction.  This is standard Bayesian  Predictive  Distribution.  So this thing here is the standard Bayesian prediction predictive distribution for one model.  Right. So if you're Asian and the model was fixed you would integrate over the exterior, the quality of the observation. And that's how you get the property of a of a new point. And so this  Thing here is the standard posterior  Theta, given  Both the data.  But now you also fix the model.  That's the difference with and that's what we did in the past when we had a fixed model. Now the difference with before, is we will also marginalize  You will also have basically the posterior of our models.  So this is basically something you can think of it as doing model leveraging right  Because the predictive distribution will be the average predicted distribution over all the models.  And that would be the correct way to be a Bayesian you don't know about something you just somewhere. Okay.  But now, somebody could say, Well, I don't want you to, I want to tell me also what the good model. Okay, so it's a bit like when you  We could say if the problem is called model selection.  So in model selection we are actually forced to pick a model.  Okay. So the problem here is not. Oh, what is the best predictive  What's is the best predicted distribution over the data over the new observation.  Which would if that would be the task. If I wanted to, you know, predict new stuff and then the best way to do that is to actually combine  The predictions of all the models. But if I say if the task now is to estimate the model. I want to know what is the quote correct graphical model which captures data.  The kind of thing we would pick by cross validation. And so then we need to pick the model.  Which will maximize the quality of the model right so you pick the model.  that maximizes  The your belief on the model. So, which is the posterior over models, given the data right and so that this is proportional to the quality of the data given model times the prior over model.  And P of data.  Over given the model.  Of the data given the model. This is called the marginal like you  Feel today are giving them, also called marginal attitude. Because to get this, I need to marginalize out the parameters. Right. So I have that this thing is the integral over theta.  Of the probability of my data given data and my model times my prior which also depends on my model, the data.  So this is, this is the likelihood  Like the hood. And so I need to marginalize out the likelihood over the posterior over that over theta. And so that's why it's called the marginal IQ.  And if you suppose a uniform prior order models you just pick the model which maximize the margin IQ. So it could be a criteria and seduction. It was also a standard criteria to choose models.  And particularly can use choose hyper parameters in your in your prospect model. It's a threat to the key chain hmm model, you could actually put a prior over k and then compute the marginal likelihood, then you would get you could actually choose key.  But so Mar in in more  Specific terms.  To compare two models, we actually want to look at the ratio of their posterior so I could, I look at the posterior that m is equal to em one, given the data.  And I want to compare it with the posterior that and is equal to em to given data and you wouldn't pick the one which is maximum. So, if this is  Much bigger than what it means you favorite one model more than the other. And this ratio combining two terms. So it's, there's the difference between  The like the marginal likelihood for model one divided by the marginal likelihood for model two times the prior for model one divided by the prior from model.  And so the ratio of the marginal likelihood. This is called the base factor.  Because it tells you how much one  Model is  favored over another model, irrespective over your prior models. So this is really telling you the evidence from the data and then this ratio here is called the prior ratio. So, this depends on your belief about models.  And so often invasion paper, they would tell you what's the the base factor which tells you how much is the evidence support model one versus model.  And particularly if the ratio is really, really big, then even, even if you had a prior that one mo was 71 then the evidence is so strong. It was swept swanky prior. So it's kind of a week to report things which don't depend. The prayer.  And as I mentioned, if you pick a uniform prior  Over models.  That means that you will pick  The among key models.  Am One who, to empty.  The one which maximize the marginal IKEA like you  are proud to have the data given  Me.  The marginal attitude. Okay.  And so when you maximize when you pick a model. According to the marginal like you would not talking a prior. This is actually called empirical base is this called empirical base.  Or it's also called type to  Maximum likelihood user avatar   Okay. user avatar   Because instead of you instead of maximizing the likelihood as a function of parameter. Now you marginalize out the parameter. And now you get  A function of the model and you just do the same thing as you did to choose parameters you just find them all, which maximize the probability of the data.  And  When the number of models small  Of models.  Is quote small  Then this approach is fine.  And what I mean by fine. Is that didn't want overfit  Because here. There's no held out test data right so  You're still using the training set to fit the model. So to fit your hyper matter. So that's dangerous, but the marginal likelihood is less sensitive  To overfitting because there's not that many models for example.  And then there's basically a cartoon by one of the big Bayesian as we've been Germany, I would draw. So if you watch this tutorial. He likes to do this little cartoon.  And so in this case if you suppose that I have modeled one included model to include in the Model three  And then I compare the kind of  Marginal likelihood that can get  So, d are the possible data set.  And because model. One is small. It's putting mass on the small number of data points, some sense. It does support on a smaller number of data points in the cartoon way. And so then if we have say a uniform distribution of all of these  I can put a higher mass on every of the small number of data points. So this would be p of the given them one  But now Model T is bigger because it has to integrate over one, I need to spread it out more. So for example, this would be p of the given them to and then I could have said that the biggest model which get even the smaller  Margin electric  That's what the point now is, let's see, I observe other that point here.  Well then you won't you won't pick the biggest model because  The marginal likelihood for the biggest model will be smaller because it had to put mass on more example.  More possible observations. And so you would pick this one because this one actually even the smallest. It actually doesn't explain that all the data.  And so that's basically the argument that the marginal likelihood. It doesn't really overfit because it has to some two, one. Okay.  And so basically  What Zubin was pointing out is that when I use d given m this is normalized over d  As it  Being given them is normalized  Over  Possible observation. So that's why you cannot put super high probability over the possible observations versus if I use the likelihood of the maximum like your parameter, which was the thing I mentioned at the beginning, maximum record parameter which depends on the  This actually is not normalized because the parameter depends on the data set. And so this can actually overfit 3D back user avatar   Okay. user avatar   So in some sense, if the number of models smile marginal like who is fine, but you know, it's important to keep in mind that because it's still some kind of a  Non provisional approach the type to maximum likelihood can still overfit if the number of model is big.  Over fit.  If  Have many models. user avatar   Right. user avatar   And so an example of crazy amount of model is I could decide that the probably I defined weird model such that the quality of a specific observation, given the model is just a clinic or a delta on this.  Observation.  Okay, so basically what happens there is, I have my data and then I have a spike for model one spike from a little to spike for Model three etc. So basically, then choosing the the model, according to this criteria will to the overfit to the training set, because there's just too many models.  Alright, so I'm not sure I understand your question, Jacob.  But  Because the frequent is naturally estimate models by maximum Nike doesn't make any sense. So the frequencies would use held our test set or cross validation, just to make models.  And so  And this doesn't work. Innovation framework because the Beijing only user avatar jacob louis hoover  Look, user avatar   At the data that you have. It doesn't look at other data. user avatar jacob louis hoover  Yes. I mean this setup looks like. Sorry. Sorry, I guess. I mean, the setup looks like doing the frequency based approach only without the holdout validation, which looks kind of scary.  And. But maybe that's maybe I'm just thinking about it wrong. user avatar   Well, so the thing is  So, so this is the same thing as doing maximum like you  Right. It's just that instead of having an actual functionality marginal likelihood function. And so when you're a frequent this you estimate the parameter by maximizing the likelihood. And when there's all the parameters you can still over  Here, it's the same thing. If I have a lot of models I could still overfit. But usually what happens is you don't have that many models you have say like K models.  Were canes, a small number, which could be different graphs and then what you look is Western martial likelihood for each of those and  You MAXIMIZE OVER THESE AND because there's a small number, you're fine.  So Dora is asking, is this a theoretical insight or is this actually use practice. How can we ever marshmallows over to Paris. Yeah. So you can use sampling or rational methods to marginalize over parameters. Sometimes you can do it in close form.  And so a good example that you see in the last lecture is for gas should processes. So, gosh, and processes. There are basically fancy  infinite dimensional gal shins and they will depend on parameters for Colonel function and the Colonel function is basically a parameter ization of the covariance matrix and your dash. And so you need to estimate the parameters of your covariance function and  One way to do that is to marginalize the like you. And so this case you will marginalize over all possible parameters.  You get something which only depend on the hyper parameters of your kernel function and then you just pick the colonel function which maximize that. And the marginal execute can be computed semi I mean coast form. I mean you to inverse of matrices, but that can be computed user avatar   He user avatar   Model doesn't apply. Well, when you have too many models because in this case, first of all, that you know obviously have just this simple  So, yeah. So Jacob is asking you how does the cartoon break down when I have too many models. Well, so first of all, you're not able to just have this simple  Inclusion relationship, you get a lot of models which are not related to each other in any way like these, they're not. There's no inclusion information. They're just different models. They're saying complexity and none is included in the other one. And there's just a lot of them.  It's a because so here that this versus that versus this other one was because we were all included in each other and so they had these kind of like, you cannot assign as much mass on one and so that's why you you kind of pick the correct one. user avatar   Was this year doesn't have this inclusion. user avatar   If that's kind of the intuition. I could give for them. user avatar   Okay, so, so let me just give a few things before I get over time. So as you're asked, How do you compute the marginal likelihood. So you need to do approximation, how to compute marginal likelihood  So you need to compute an integral over your parameter  And so usually this is intractable in this in a few special cases like the ocean. So you will use approximation.  So you could do virtual in France.  Or sampling  So you basically complete your integral by doing multicolor integration.  And there's also some simple approximation which has been proposed, which are much simpler than these rational method.  But they have been analyzed. So one example is the Bayesian information criteria.  And you can think of it as making a tailor approximation of your, of your posterior over the parameter and then integrating it out because you get like a kind of a Gaussian and you can integrate  And in effect what happens is you count the number of parameters in your model, in some ways, but I don't have time to go into detail.  And so what I would like to finish today with is quickly talk about causality.  And so, in particular, there's this thing called structural causal model.  Which is a generalization of graphical model. So this basically takes graphical model and add something about intervention.  Is because because it becomes relevant when you can intervene on variable. So the idea is, I will say all my probability  Of observing some possible value will be a standard the GM. So I would have a property over x i given x by an end data I  And then the idea is that say my observation probably teeth. But then I can intervene on some node in my graphical model. So let's say for example, I have a causes be  And so the semantic here. And so here I would have  This would give me a  X be given X A, and then I would have a marginal over exit. Right.  And so what happened there is, if I intervene on the cards.  If I make an intervention on a  The conditional one change. So the idea there is that the only the marginal and a will change under this intervention, because of the causal structure.  So if I intervene on a the conditional doesn't change, which means then that depending on if possible. If I said the value x say to some specific that you will then be will depends  On the value x say using this conditional from their observation and if I intervene and be because the arrows in this direction. Well, then that means that I set be to a specific marginal  Which depends on my intervention. And so this basically conditional is disregarded in this model because I'm intervening on the I'm replacing the conditional of x be given x a buy a new one coming from intervention.  And so the integration models is depending on where the arrows are you will actually have different effect.  So when you intervene in a cause the conditional don't change. So then there's propagation of effect on what you intervene was if you're doing an effect. Well,  The cars didn't have any conditional in the other direction. Right. So, then they're not affected along here that the distribution of Rec. See here. I mean, depends on the margin on Linux. user avatar   Okay. user avatar   So that's the semantic our structural cause model, which is very different than graphical model graphical model.  Like if I look at these two possible direction. Let's say I had the arrow. The other direction, the set of distribution which I could characterize was the same in both directions. So there was no way to select the correct direction, right, there's no difference between them.  So if I add intervention, then I could distinguish these tips because the effect of the intervention will be very different, whether A is a cause or whether aids and effect. Okay, so you can identify so you can identify  Call calls all direction.  There's two main ways to do it either via intervention.  So, you, you, you, you, you can interact with your phenomenon and you can set some variable to specific values or to a specific distribution, not in influencing the other variables or via  Parametric assumptions.  So if  The set of distribution which or  portrays in a specific way is not the same as the other direction. So for example, this is coming from Bernard Shaw.  Book, but let's say I have x and y as my variable. So there's a question is why the cause of X or is x, the cause of why and let's say I make a Patrick assumption that why given x is actually a linear relationship with gas should noise which are independent.  Okay.  Well, if I flip the model around and I looked at x as a function of why I will still get a linear relationship but not independent. NOISE The noise will have some dependence, because when I will rotate this thing you'll see that the noise.  Mixes things around. Okay. And so if I looked for this parametric type of assumptions that oh, I have independent noise as a function of x, which is Gaussian  The set of distribution model in one direction is different than the one in the other direction. And so then I can identify the correct direction by just doing maximum likelihood and finding the correct distribution, because they don't they don't model, the same set of distributions. user avatar   Okay. user avatar   But the problem is, okay, well, you need to make assumptions. So if the assumptions are wrong. So for example, if the phenomenon is not linear within a bit gushing then the causal without intervention, you won't be able to identify the causal directions.  Okay. And so why cause our models are useful well because then we can first. You can also help estimate the parameters better when you make these  Parametric assumptions to threats drive you have the correct assumptions.  But the other ways, especially like if you want to know, how does a system behave under intervention, you need the causal model because the observation.  Distribution won't tell you how it how it how it behaves under intervention in particular in the two variable case.  If I intervene on a if I am in this situation, it's very different. And I'm in this situation. So in this situation when I intervene on a be doesn't do it doesn't change at all was in this situation, if I intervene on A, then B will be influenced by the value of a and I didn't use Tipperary  Is there any question about that.  The answer to your question is yes.  He is confused about this section what an integration model is will we be seeing more and because it are doing a review it next week actually. I just wanted to give you the high level idea. So let me try to clarify things here. So in a structural chasm, all, all I'm saying is that  Let's say I  Intervene  On node that's called j as it  So the semantic  Of intervening  On node.  It's called a j  Means that the distribution of X given  Intervention  On Jay  Will be product for i not equal to Jay  Of the original  Model. So the parameter the same everything is the same. So nothing has changed as the observation model, but then I will have p on x j  Which will depend on my intervention.  So it doesn't do so. I mean, I'm I can set it to arbitrary value depending on my intervention, I could put x g to a specific value for example. And now, what happened is that all the other factors stays the same.  Does this clarify if the model of intervention.  And so in the two node center setting. I have a marginal an eight and a conditional of beginning  So if I intervene on a and, in particular, I set the value of eight. So basically, my marginal and a will be a spike on the specific about  This is valid, then from the structural causal model. I know that the conditional have a big have any doesn't change. And so then basically my distribution of be will be x the p of x be given the value so and so, in some sense, it will change its distribution compared to before.  Marginal  Can witness temporal relationship in the data in a principal way to determine causal direction.  Yeah, I mean if you if you think that the time you know kind of determine like  That you know if you intervene in the past 30 few intervene in the future. It shouldn't influence the past so indeed, there's a bit of causality implicitly in the inner in a temporal model.  Yeah.  Well then I missed my  Camera trick assumption.  Okay, but I don't want to go too much over time because we got feedback from the students that we prefer not  Respecting enough the schedule. Now that we teach online.  So in my, in my side is because I see why it's recorded so even if they have to go to another class, they can always just watch the recording later but stuff.  Alright so let me stop the recording.
 Alright so today. The plan is to talk about Gaussian  Distributions.  So we'll talk about Gaussian  Networks.  And we'll talk about factor analysis.  And PCA  Which are basically can be seen as one of the simplest representation learning our rhythm and then we will briefly extend that to the vibrational auto encoding.  Okay. And, and then I'll go through a quick review of all the topics we seen through using the schedule, but I, this will probably be over time. Sorry, because I you want to go through all the material.  And I think the conclusion for the quick overview of the class, you'll see that, you know, you've seen a bunch of tools for multi dimensional  data modeling. A lot of these you've seen them quite superficially, because you know there's a lot of different topics you could go in much more depth. But the idea is to give you a adjust of these things. And now I'm seeing I'm having issues with my eraser that's so annoying. user avatar   Are you serious, user avatar   Get close clothes, my thing. user avatar   Alright, you can ask me a question while I reboot. My one note.  Any questions.  Any special format for the final project.  Yes, so you need to  Use the ICL format like is describing the project webpage. So the template is there. So you should use that to Natick  But their content itself like the sections. There's no special format. But, you know,  Oh, yeah, yeah. One note is having trouble restarting  Okay we kill one note.  Any other question.  Okay, it's working after killing it.  No software has been hurt in this process.  See if I can right okay can I raise  What's the problem with this knockout probably need to reboot. I'll do that over the break, perhaps  Yes, I haven't rebooted in a long time.  You serious  Okay.  So it's not working. It's frozen. I will reboot.  Oh, somebody's asking about alpha fold. Alright, so you you're getting, unfortunately, we'll do a three minutes pause while I reboot my laptop. So let me send I'll make user avatar   Jacob as Co host as usual. user avatar   And you can perhaps user avatar   Answer. user avatar   niggas niggas question right, sorry about that. Let me pause this recording user avatar   My apologies for these little technical difficulties.  So I was saying that, today, we're going to see the gash in distribution in more blurry details and this will be useful for both talking about virtual and don't quarter.  Factor analysis and also and Friday. You'll see gash in processes, which is the generalization of the Goshen for infinite dimensional vectors. Okay.  And but it will give us also the chance to see yet again more cool math. Alright. So. Suppose that X is a multivariate gal shins with me new and governance sigma. Okay.  And so let's say we're in dimension p. So you is a vector in RP and like variance will be a matrix PvP matrix and it has to be strictly positive definite, to have a well defined density  And so now what I'll first do is put the gash network in the exponential family that's something I told you we would do. So if I have my density with mean containerization you and sigma  Or I guess moment pasteurization so you have the normalization factor which is two pi race to the P and then the determinant of sigma, then I have x  Of minus one half.  X minus new transpose sigma inverse x minus p. So, that's the density  And if you remember we did this trick where I replace this x transpose sigma inverse x while so vector transpose matrix.  Vector. That's a scanner, so I can see the same thing as the trace of the one by one matrix. And then I use the circuit and property of the of the trace to move the vector around. So I can write it as trace of sigma inverse x minus mew x minus new transpose  And the reason I do that is to also highlight how it can be linear in the matrix sigma inverse. So it's a duck product between matrix thing my inverse and some kind of  X minus new time x minus b transpose and so to get exposure family. I want to identify what are the clinical parameters which will be in front of the sufficient statistics.  And so to identify the sufficient statistics, I will expand this quadratic form. So, this is x x transpose and then I have the crust term New X transpose  Minus x view transpose and then I have the new new transports. I just expanded the quadratic term.  And so now  I can rewrite the quadratic piece.  As that product between matrix inverse and minus x x transpose divide by two. Alright, so this so this piece here when I do trace of something  And then x x transpose. This is like a product and then I have the minus one half factor that I've included. So that's, that's where this comes from this came from.  And so that's part of the canon called pasteurization right so that's why in  I said that the clinical parameters Zeeshan for the gash and it's actually the inverse of the covariance that to you. So it's called the precision matrix. So, so this is basically the, the precision matrix which is the inverse of the covariance is one piece of the clinical parameter  If your tricks.  And then the other piece where x enters will be I have minus one half times too.  So basically I have trace of sigma inverse  So they have trace of sigma inverse here and I have  Basically knew right then. And so this is actually get if I write it in linear form, I think, to transpose of the first part. So this is sigma and verse mew times x.  In linear form. So that's a duck product between vectors. And so this tells us with the other clinical parameter and actually the notation we use often is just eta for this piece for the Gaston.  And then the rest is just not dependent on x. So there's no sufficient statistics piece is, it will be part of the log partition function. So, I will have new transpose sigma inverse  And so there's sufficient statistics.  I mean, you can always reschedule the sufficient statistics. Right. So there's not really a unique way. But one standard one for the  Spanish for the Goshen will be both x and minus x x transpose divide by two. Okay.  And so the Kennedy called parameter  Associated with this sufficient statistics.  Will be the, the, the precision matrix and the  The ETA.  Clinical parameter. And then you can relate  So you can see as a function of the moments. What are the Kennedy called parameter  So if I want to see what's my clinical parameter  As a function of  Of my  Woman parameter  So I could use  So as a function of Mew and covariance. So I would call it an ETA and then precision matrix.  And the relationship will be that at that is equal to sigma inverse you and the precision is just say my inverse  And so if I want to flip it around. If I know the clinical parameters and want to get the moment parameter. Well, I can get you by just taking sigma times at  Our precision inverse time at that. That's how I can get back to the mean when I have both the precision and the Kennedy call will do a piece of the cash and chemical parameters.  Okay.  OK, so I guess here. So the, the high level idea was to put in exponential family.  And so if I looked at the pasteurization in the exponential family.  In the Canada called pasteurization the density will be so it depends on it. And the precision matrix, it would be x of x transpose x. That's the first piece of disruption statistics, then I would have precision matrix that product with minus x x transpose it by two.  And then I would get the log partition function which is everything else. And so it turns out it would be one half at transpose sing.  lambda inverse Etta plus  Which is basically, by the way, this, this is just from this term right I've just re express musicians sigma in  In Canada called characterizations  And then I have the these terms which when I take put in the exponent. I think the log. So I get p divided by two log of two pi.  And then I have minus one half lug of the determinant of the precision. Okay. And so this  And I guess I need to close my parents Asus, and so this piece here is basically the log partition function. So, A as a function of it and precision.  For a gash material.  So this is the density. So this is my explanation of me for military reaction.  And if you remember there was this question of what is the valid set of Canada called parameters. So it's all possible kind of call parameter for which I can normalize my gushing so attack and actually be anything  Any vector. It's fine. It's really into the mean but sigma lambda theory has to be a positive definite matrix because it's basically the inverse of a positive different matrix and it has to be symmetric. So these are the constraints on these kind of competitors.  Okay, and  If you remember the properties of the explanation of me is that if I take the gradient of the log partition function I should get back the expected sufficient statistics. So if I take the green respect to  Have a have at a  Lambda I should get the expectation of X, because that's the first piece of decisions that this sticks, which is just view, which is a  lambda inverse at that right  And indeed,  You know, if I look at that. That's the only place where at top here. So when I think deserted respect to this goes away. I'm just left with lambda inverse enter the one half became the  Just one. And so indeed, this is what I got.  Again, and by this relationship here, you see that it's new.  So we're kind of like  verifying that everything works out.  But it's kind of a useful check and then the same way if I think the gradient with respect to lambda have a have a lambda. And if remember when I define  differential calculus for matrices. There are more general vectors I explained to you, like how to do it.  And so here you would get that this is the expectation, you should get that it's expectation of the sufficient statistics which is x x transpose it by two.  And this is related to the covariance in a specific way. I won't go into details, but yet you could check it as an exercise and Alex Jones question has been answered by Ishmael. Yes, the precision matrix. When you use squiggly  Bigger than zero, it means tricky positive definitive  Alright, so  So why do I do that. Well, first of all, it's also important for it's it's convenient to talk about what the exponential family.  For mitigation and then also we highlight the important clinical parameter for an exponential family formative Gaston which is the precision matrix. So, the, the fundamental quantities, the precision matrix, not the covariance matrix.  And if you remember I told you that you can always put a  Undirected graphical model on discrete data as an uncertain graphical model. Sorry. You can always take make take under the graphical model on discrete data with strictly positive  Distribution as an extension of family. It turns out it also works for the the Gaussian distribution, even though it's not discreet. So if I want to think about my gosh and distribution as on different graphical model on p nodes.  Well, I can just now think of it as I have p of x at  Lambda  This is basically x of y minus minus one half that product between the precision matrix, I will just rewrite it explicitly with some over all i and j up to p of lambda AJ and then I have x i exchange.  Then I have plus  My node potential. So that's it i X i and then I just have my look partition function.  And so here you see  Where the edges for the potential becomes right. So it turns out that this distribution, because it belongs to a undeterred graphical model where the edges.  are determined by which entries of the precision matrix or none, zero. So it's for all i, j, such that the precision is not equal to zero. user avatar   Okay. user avatar   And so it turns out that for a gash in the zeros in the precision matrix, not the covariance matrix.  Determine the conditional independence property.  Implies conditional independence.  Properties.  So this is just from the new GM perspective.  Because you could think of here Pex,  As being the product over my edge.  Of these pairwise potential sigh J x i xj. So for every pair of variables where the precision is non zero, I do have a potential which depends on these two  And so from this perspective, you could put all these nodes you could put edge between these nodes. So that's why you you get a gash in network. So it's a  It's a undeterred graphical model where the joint is a gash in and there's still some conditional independence assumption. So there is some factorization coming from this the zeros of the precision matrix.  Is there any question about this.  Alright, so somebody is asking if I can repeat the sentence from earlier about discrete random variable you Jim and positivity. So, so when I talked about the exponential family. I said that when you have  undeterred graphical model over discrete random variable which has positive PMS on all joint assignments, you can just take lug of your joint and then take the expert that because it's always zero. There's no problems with the log and that's why you can put in the next bunch of me.  And then the, the, the structure of your sufficient statistics explain how the edge will appear. And so an example of that was the icing model where you had these  These x i xj terms. So it's basically similar to the gash in network, but instead of having a continuous value for exciting things, J. You would get basically 01 values. Okay.  And now I'm saying, Okay, well we can do the same thing also for some continuous distribution in particular for joint Gaussian. You can also look at what's the gut under some difficult model perspective of it.  So the, so the the other tip Africa model will have many more distribution and just a Gaussian one  But the the gash in with a specific sparsity structure and the covariance will be in in specific under the graphical model that's kind of the, what I've just described here. So the edge set is determined by the sparsity structure of your decision matrix.  And so this brings me to talk about how to condition in a Gaussian  Because if we want to talk about conditional dependent statements we want to talk about how to compute conditional for a gallon and this will be useful, both for factor analysis and for  Gushing processes. And so I will do it. It told this aggression now on how to compute the conditional in a Gaussian and for this I will do a bit of linear algebra.  Review or  teach you about the shirt compliment. So the shirt compliment is a very useful concept on how to invert.  Black matrices.  Any two people appear in a lot of linear algebra manipulations.  And I can use it in order to prove what is the the conditional in the gaps and the effect of conditioning and aggression.  Alright, so suppose that may covariance is split in block.  In in four blocks. Okay, so basically one and two will represent to group of variable. So I think my vector and spit in two pieces. And so then I get four pieces for the conference.  So we get 1112221 and then two, two.  And so it turns out that if enough of these pieces are convertible. I can actually express the inverse of this matrix in terms of the block in versus as follow. So if I think the inverse of this. It turns out that you get a matrix with this structure. So, I will have  The inverse of the first block plus  Inverse of first block times  One two block and then I have this matrix M inverse which is actually the shirt compliment then sigma two one and then sigma one one inverse. That's the first piece. The second piece is  One one in verse one, two, and then inverse of the shirt compliment  Then this third piece here is this simplest, it's just the inverse of the shirt compliment. And then this one is basically in the other direction.  So I have minus inverse of sure compliment and then 2111  Alright, so  So if the inverse of the first block exist.  So if this inverse  Oops.  If this inverse exists and this inverse exist and the inverse of the whole thing exists and you can write it this way.  Okay, so what's this magic matrix em. So the matrix em here is I'll use invitation, the whole matrix. And then I take the shirt compliment respect to the first block.  So that's why I put a slash 16 my one one and by definition the shirt compliment is taking the second block. I mean, the bottom right block of my matrix. And then I do 1211 inverse and then  I don't have the right direction. Normally, the, the induction max. So it's 2111 and verse and then one, two, okay.  So there's a bit of like like a bit of a memo technique trick is that this has to do this should be size of the same size as one one and this should be the same size as to two  And so this is a shirt compliment respect to one one and so because that's to be the size of to do it will have block to to here and then  I kind of need to translate the effect of one one to the block to so that's why there's like this to 111 and then one, two, and then I really have something of the size  Sigma to do. OK.  So the notate the notation here is this call the shirt compliment user avatar   Oops. This visible link. user avatar   Sure compliment  Of the big matrix with respect to  The first block sigma one  And then there is also  Another short compliment you could use  Where instead of taking the district company respect to the first black, you can take the circle moment respect the second luck.  And then I would just do the same thing but replacing to with one. So, this will be one one minus 1222 inverse and then to one.  And so, and then you could do the exact same any position here, but instead of having the shirt compliment respect to the first block inverse here, you would have the shirt compliment respect to the second look here.  Okay, and then some kind of version of like this would be here, but with you know you spit with one and two. user avatar   Okay. user avatar   And so I could use either Shrek ultimate respect to to circle back to one, then I have to expression for the the inverse of my matrix. And so I would have this something like a complicated expression here, but on this luck.  And it should be equal to the inverse of em one. So you have this short compliment. And so this is actually a way to prove the  Something which is called the woodshed with Barry Sherman Morrison inversion from them. So you can use this hoops.  So I if you Chris about this. I encourage you to look at the Wikipedia article on this so you can use this to derive  The  would bury  Sherman  That's just a bunch of names bunch of mathematicians name Morrison inversion.  Formula.  Which tells you in some sense.  If I want to compute the inverse of this kind of combination of matrices.  I think an inverse here. Then, which is basically this m of minus one, then it's the same thing as using these block in versus with  The other short compliment inverse. Okay.  And why is this useful. Well, it could be that depending on how I have split my matrix, it could be that, for example,  That it could be that to the sigma to to is a scholar. So if they must if sigma to two is a scanner then computing is inverse is very simple. Whereas sigma one one might be a big matrix. And so, said, I think, to compute the inverse of a big matrix, I could now have to only compute  Simpler matrices next killer inverse  And so somebody is asking is this division. No, this is not division but it act, a bit like division, because it appear in like competing this kind of block inverse  But it and, in particular, okay, so more suggestion.  More property which explained them to notation. So it turns out that you have that the determinant of the big matrix is the product of the determinant of one block times the determinant of the shirt complement respect to this block.  And you have also the same thing with the other shirt COMPLIMENTS YOU CAN DO TO to if you want, and then at the determinant of the compliment respect to the second look  And so  And so you can see like, oh, you know, it kind of worked like a bit of an inverse, in some sense, right, but like you could think, oh, you know, like, this cancels out that but you know that's not really this is notation.  Okay.  So I if you're curious about this and to go much more in details about these properties. I encourage you to look at the Wikipedia article for basically how to compute the inverse of a of a block matrix.  But why are we talking about that. Well, let's talk about the joint of a Gaussian and what we'll do is we will factor is the joint into pieces.  And one piece is actually the marginal will identify as the marginal utility piece will be the conditional of one block on the block.  And that's and it will introduce these sure continent. And so if I have a joint Goshen, and I have x one x two and so x one will have dimension.  P one and external haven't mentioned p two, right. So that's what I meant by you split your vector in two pieces. And I want to compute. What's the conditional of extra given. Excellent.  Well, it turns out using the properties above, in particular, like I can  I can fact rise the determinant as this product of determinant. Right. So in the density of aggression, I have this determinant appearing here, right. So now I will just factors as a product and I will use this factorization now. So let's do it.  So I will have one square root of two pi race with the p one and then determinant of the first luck.  Okay. And so then for the other piece which is missing. I will have to pee two pilots are raised up to and then the determinant of the shirt compliment  Sure compliment with respect to the first book.  So now I've just factor eyes. The, the normalization of a question in two pieces.  And then I will have  X.  Of minus x one minus new one.  First block inverse x one minus you one and there's a transpose here.  And that's it.  Right, so  It turns out that you know when I want to have my my joint Goshen, I would need to compute the inverse of micro-grants right and then I would have this inverse of the first block and then I will have a bunch of other stuff. It turns out that things can cancel out into factorization.  And so what happened is, without going in all the details, but you can verify it at home is that you can write the missing piece as minus X two.  Minus new two  And then there's enough set of the mean coming from a conditioning, which I'll call a function of X one. So this would be function of X one, I'll call the next one.  Then I have transpose and then I have my my shirt compliment. So this is sigma  Sure compliments respect to sigma one one and verse  And then I have just the same thing here, x two minus new two and then minus the offset.  And that's it. That's actually  The joint. So, okay, I still didn't tell you what, be aware, be of X one.  Is defined as sigma two one sigma one one inverse x one minus new one. Okay, so basically  The analogy here like the intuition is you have these  You know sigma one one inverse sigma to one pieces. And so when I will take you know x minus mew here as big vector I can do a block multiplication and then gather things around. And it turns out that that's what I'll get you  Okay. And so now the big point is you can stare at this and and identify this as the marginal on the first the first group P of X one and then because this is the marginal. This will become the conditional right because you know that the  Joint on X one, X two is the product between the marginal in the condition. Okay.  And so  The first thing is  Why is this really a conditional. Well, when x one is fixed. This is a constant. And this is a constant. So you can just think of it as this being a new mean for your conditional of extra given x one.  And then I have some matrix inverse and then I have the correct determinant of this matrix. So this really is a Goshen on the variable x to  Just with some  Weird mean and weird convenience for any fix x one, this is a question. So when I integrate that respect to x two, I'll get one. So it's probably normalized for any one this is probably normalized so 3d has  It really has the property of a conditional. Okay. And this is definitely just a gash in on on X one, it has its covariance here. I mean, and the same programs here. So again, this also integrate to one when they interviewed respect. Excellent.  And so that's one way to identify what should be the covariance for the Gaussian conditional of x two given x one. Okay, so  By just this joint factorization. You can read us, what are the different parameter. Okay. So if I look at the mean parameter ization  Of the marginals into conditional  You have that the marginal are super simple. So the mean  unblock one  Sorry, the mean of the marginal so I'll use the annotation em for marginal. So the mean for the marginal unblock one is just new one. Nothing happened and the covariance for the marginal unblock one is just the first block grants. So it's super simple.  So when you lose the moment pasteurization marginalization in a garden is trivial. You just ignore the other pieces of your. Gotcha.  So this is  This is the  Power Meter.  Of the marginal  On next one.  So I guess I didn't stay at the beginning. But, you know, it turns out that for joining Gaussian marginal are still gushing conditional this discussion. That's one of the nice property.  Cannot find the mute button before sneezing. Yeah.  So Jacob is asking, how does the block the composition of the currency to this form of conditional. It's not direct. So what I tell you is is is from this kind of expression.  So when I have this expression and then I just put like x minus mew here and I do the block multiplication and I regroup all the terms.  It turns out that you get a bunch of cancellation and you could factor eyes. This term like this and this term Magnus.  And then you'll have just the this piece, which has the the appearing here. Okay. But there's a few steps to do that, then I don't have the time to go through all the details. It's just a bit painful regrouping  Think I might have done it in previous years. Notes  Okay, so, so, so, so these are the marginal parameters, but the conditional are more complicated, right, because the mean as this shift.  And then the covariance is a sheer. Sheer compliment. So if I want to know now. Okay. I want to compute the mean parameter for the conditional have to given one  And so I'll use now conditional Sam competing a conditional. Well, you take the original mean unblock two and then you add this function of X one, which has to do with the with  This definition here.  And then the covariance of the conditional  Is actually this shirt compliment. So I take the as it  So the covariance of the conditional  Is this shirt compliment with respect to the black one.  And so this is sigma to to minus sigma to one signal one one inverse signal into. Okay, so this is the parameter for the conditional  X two given x one were to here's a block.  It's not just one node.  Okay.  So that's just from this nice factorization. And now what about if I use the clinical characterization  And so when you actually use the clinical parameter instead of the meantime amateurs marginal becomes complicated but conditioning becomes simpler and that's also why  If. Remember I told you that the conditional independence could be read from the precision matrix which is actually coming from the chemical composition. So then if I want to look at the precision matrix of the conditional  To given one. Well, then there's just  Precision to to right because  The you know if if I take  Instead, the inverse of sigma is just this precision matrix, right, and then it will have block.  One one and this would be block to two  Right, and so the end. So, and we have that  This is the precision of  Of  The conditional. So it's actually just if I reorganize the inverse of this matrix in four blocks, it will just be the, the two to block.  Okay, so it's very simple. When you have a precision matrix to get conditional and connect the mean is also not too complicated when you look at the conditional. It's just at two and I don't need to inverse anything. I just take sigma to one and then  Have devalue. Excellent. Okay, so it's actually quite simple.  On the other hand, if I looked at the marginal then things are more complicated. So it turns out that the marginal  The candidate competitor for the mean for the first piece for the ETA for the marginal I take the original at one and then I need to inverse. The so I have a lambda one to lambda to inverse and then it  To  Yeah. And same thing when I look at the precision matrix unblock one for the marginal. I actually need to do a short compliments. So I need to do.  Precision one one minus 1222 inverse to one, which actually turns out it's the shirt compliment of the precision matrix with respect to the block to okay so it's more complicated.  So depending of if you care about marginals, are you care about conditional some characterization and the gushing are more useful than others. So moment it's useful for marginalization.  Kind of precision and clinical transition is useful for conditioning.  And in particular, if the block. I do.  So, for example,  If I use as my block.  The first group will have two nodes that say I NJ.  Let's just call this capital I, and I want to condition on all the other variables. So let's say x one is just exciting xj and x two, sorry. Yeah, the thing I'm conditioning on  Actually I'm condition to the next one. In this case, so x to the thing I  Am not conditioning on will be two nodes. So excited exchange and the thing I'm conditioning on will be all the other variables.  And so  If I want to compute the covariance of X i given X rest.  So basically, I want to compute  This conference because I'm conditioning, so I need to compute the short compliment  Oh, did I forget the No. Okay. And so this will be  The covariance of  Of  Conditioning on I give it a rest.  And so this is the precision, I  Inverse  Right, because  Whoops. So if I look at its precision.  If I looked at its precision privatization.  It's just a second block of the precision matrix.  And so  bomb bomb bomb. So why do I talk about that.  Oops, I'm having issues with my red well because if I write my block matrix over the variable in index that I, this would be  The precision entry i i j  J. I.  And then JJ and I need to take the inverse of that. Okay. And so now if I have that the crust term of the precision or equal to zero.  I get that the covariance matrix which is the inverse of that.  Is just diagonal. So it's just I minus 100 and then JJ minus one.  So when the covariance matrix is diagonal, you know that the variables are  Uncorrelated  But for a Gaussian. If you're uncorrelated. You also independent okay so that implies that this conditional because it's a Gaussian. It has zero on the, on the, when they looked at the end, the GPS.  I have zero on the diagonal terms means that x is independent xj. But this was in a conditional the submission. So it means that x is conditional of xj given X wrist, right, which is the same thing. I had already derived from the  So it's also true by the mark of property of the GM and the fact that already have a new gym but here it's another application of these  Computation of the parameters for conditional  Okay.  All right, so if you didn't follow all this in details. It's not too problematic. I mean, this is quite a lot of technical aspects.  But the the main point is  You know, here are the formulas to derive the parameters of the conditional in a Gaussian or the marginals, and one position is easier than the other. And when you have a Gaussian zeroes in the precision matrix will tell you, conditional independence property. user avatar   Okay. user avatar   So,  I will not take a break. Sorry for the delay. I think technical difficulties and stuff. And after the break. I'll start about factor analysis.  And talk about PC. But is there any question about the gushing before that.  Recording. Okay, so let's talk about this simplest model of representation learning  Called factor analysis.  And that will also uses or little properties that we just arrived for the gushing distribution.  And so the factor analysis.  Model is actually a model for a latent variable model so specific type of latent variable model.  And so the idea is we will have observed some x. So I'll shade to node.  In Rd  And we will explain X with a continuous z.  In our K were usually case, much smaller than the  Goal in modern time. We also have different Layton representation which could be high dimensional but at first.  The idea was to learn a reduced representation of our data in the latest world. And so we call this the latent representation  And some application is  In reducing the dimensionality of the day. So it could also do dimensionally reduction.  Where the worst case, much more than D. And so you could think that the data is explained by the main signal which is z.  And then  Plus some noise which is in higher dimension.  So, before going to this, the general model for the factor analysis model in working with that we will talk about  A deterministic model which is called PC.  And we will make the link with factor analysis, very soon.  With something called probabilistic PCs. So this is dimensionality.  Reduction. So it's for principal component analysis. One of the most basic standard  Data mining or data analysis tool.  And I'm pretty sure I'll expand in a way that you have not really seen it.  Perhaps even though you might have learned in the basic machine learning class. So the first way I will take the the synthetic view. So there's at least two different use for PCA  And so the first one is synthetic view.  And the idea is you want to find  Key  You want to find a subspace span by key vectors. So you want to find Kate or so normal vectors.  In Rd  So I will have w one, blah, blah, blah. To wk. So each of these vector is an RD. There are no such that when I project the data on this space, I get a good approximation to it, right. So, such that  The projection  Of x on the span.  Of W one two wk  Is a good  Approximation of x.  And so just looking at the subspace. I already have the principal factor a variation of my data in the US.  I i don't i i can be very close to my data point. And so a plot is let's say my data is in 3D.  And let's say  I have I'll do you know it's like a nice linear data. And so I could just use one  Line.  To explain my data. There's a bit of noise around it. And so I could use this one vector and already to good approximation. user avatar   It's convenient. user avatar   And synthetic because I can reset this size my data, such that with just this subspace. And it's a good approximation.  Alright, so now let's do a bit of linear algebra for that.  So that's called the matrix. W. The matrix which represent my basis. Okay, so it will have k row.  So sorry cave columns. So it's a D by k matrix for each column, as I mentioned, D and F key column. So what happened is, if I take the dot product between each of these column I there.  I get the identity because there are certain. All right, so I have  W transpose w is the identity and I will use k for it's a case by case metrics. In this case, this is by Arthur normality. user avatar   Okay. user avatar   Now, what about WW transport. So, W transpose W's their identity but WW tramples. What is it, it's actually not the identity because it's a huge d by d matrix.  But it's a projection matrix. Okay, so  So what is WW transpose  And so  What's right now w w transpose. So, it's not equal to the identity D.  And this equals the  End. So let's call the matrix pw to be WW transpose, you have the property that P w square  Is w w transpose w w transpose. This is the identity and k. So it's equal to pee. W.  So you have a matrix that when you square it, you get back to say matrix. This is what it's called. In the algebra, a projection matrix.  In general, it doesn't have to be our surrogate all projection, but in this case it is an orthogonal prediction. So this matrix is basically the, the operator, which gives you the expansion of your point in the basis given by w. This is the orthogonal projection  Matrix.  On the span of W one blah blah blah wk which is basically the column space of the matrix that way.  And so now if I apply this matrix to x.  Okay, what do I get well i get WW transpose x, and it can be right that if I use black form of my matrix W was W one blah blah blah wk each. These were my columns.  And then when I think W transpose x actually have now my column as rose x is a vector. So, roll time vector. It's a scanner product.  So you can think of that we transpose x as the vector where I have my first entries this killer product between W one and x and then my last centuries wk an x.  Okay. So, what I get is summation over k  wk  And this the scanner in front of my vector is basically W k x  And so you can think of wk x when I this is actually the projection of x onto the the wk element of the basis and that gives you the coefficient  In the expansion in this basis. Right. And so you get that this is actually so sorry w is not a basis because it doesn't span R amp D. They're independent vectors. And there are throwing them all. So it's actually a basis on the subspace span by it.  And so this you could think of it as the case coordinate of my new representation which I'll call z. And so this is basically w z.  And z is obtained by projecting x  On  Under subspace, which can be done by doing W transpose x  Okay.  And so this gives us a lower dimensional representation. So this is basically  A lower  Dimensional  Representation for x, because it only I only need key numbers to specify it I lose some information because X was in Rd so I cannot get everything. So there's a bit of approximation representation. But if my spaces will chosen. I can get things. So for example, let's say I'm in 2D.  Let's see my data.  Is kind of like this kind of distribution.  And so a good if I only want to use one vector to characterize I will look actually at the biggest extent. So we'll see that very soon. So basically this space here will be arc to the cake.  Which is your case equals one. So that would be the span of W one  And if I have a vector here. Let's call it x i. And so I think the projection on this subspace to get the representation in Z right so this would be  The vector here. So the, this would be she the  Zed. user avatar   One basically user avatar   But I mean, it's not a vector in this case because that is just one entry.  Oops. Cancel.  How do we do that, let's console can so  There we go. I will have that this length would be said one  Okay.  And so the PCA  Problem.  When I look at  The synthetic perspective is I want to find the subspace. So it's represented by this matrix W which is in D by key.  And it has a property that w transpose the value is the identity K  Such that the Reconstruction Era of my data point is smallest right so what I do is I some over my training point the norm between x i n. It's reconstruction, which will be WW transpose x  And so  W transpose excited. This could be seen as that I  And the subspace represented by W which is the column space of w is what we call the principles of space.  And so the PC problem in this case would be to find the best space, such that when I protect my data and the space. The L to norm of the errors is smart.  And know that w is  W is not unique.  So the subspace is unique, but its representation as a matrix is not unique because you can always rotate the basis  Okay, so only the subspace is unique. So the column space.  So for example, I could get a new basis by just multiplying w by a rotation matrix. So rotation matrix is a matrix such that it's orthogonal  Or transpose are in both direction is the identity, right, because then if I looked at the projection and this space. So if I do W tilde W to transpose. This is the same thing as W AR AR transpose w transpose and this is the identity K  And so I just get WW transports  Okay, so  So that's important to keep in mind when you do PCA, the actual vectors you get, let's say you decide to project in the subspace of dimension k, the actual basis you get for it, like the principal vector. They're not unique. You can always retain them.  So you shouldn't over interpret them. This is  OK, so now  Let's talk about an alternative viewpoint, because you might have seen PCA as maximize finding the direction which maximize the variance of the data.  So this is called the analysis viewpoint, and I'll make the link now using a bit of notation. So as usual, will have the data matrix and by D.  So I will have x one transpose as a row, blah, blah, blah. X transpose that my data matrix.  And so now there's some here can be written as the previous norm between my data matrix and w w transpose x transpose  So if you, you think of X transpose, I get each column is one data point.  And then basically  You know, this thing is applying the principles to one of the column.  So that's what happens when I do that and and and so I'm doing the Ultra normal each column. And when you do the Caribbean is normal of a matrix. It's just a some of the LTV normal each of the column of the matrix, but so  That's why it's just the same thing. But now, why do I write that. Well, okay. Let's now factor is out.  So this is basically the five factors that extra unspoken the right I have identity.  In dimension D.  Minus the projection matrix right WW transpose the projection matrix times x transpose  OK.  So now the for BTS norm on matrix is this just a standard L to Norman matrices. So it's the same thing as the trace of the product of the matrices. Right.  So this is the same thing as a trace of the transpose of a matrix which is x identity minus p w transpose than a identity minus p of w x transpose. Okay.  And so now it turns out that I didn't see my mouse pow is also a projection matrix. It's the or soccer. It's a projection on the orthogonal part of the space.  And so this thing here.  You can actually just expand it and you'll see it's pretty trivial. This is equal to identity minus p of the way  And so I get that this is  Trace of  Mix.  Identity minus p of W.  X transpose  Then I can use the circle and property of the trace. So this is the trace of  X transpose X identity in dimension d minus p AMP W.  And so x transpose x is a constant here. I'm only optimizing respect pow. And so, minimizing their recovery error.  Recovery.  Air also did I make a mistake somewhere.  Think you meant to write Eric transpose  Well, are. Yeah. So our is square. So it doesn't matter the order I wrote  So here, this thing it's it's  The same thing  Oh no you're right, it's just because I wrote the same thing on both sides.  Oops.  Thank you. Yes, that's what I want to write  I  Am inducing error by the answer of other people.  Is it. Oh, it's a square matrix, but that was not okay so i was saying that minimizing the recovery error is equivalent to maximising  As it so it is equivalent to maximize  The trace of X transpose X times P of W.  Which is WW transport says read this why WW transpose  Right, because this is a constant. So, I don't care. So now I want to minimize minus these things so minimizing minus something is the same thing as maximizing none minus. And so that's the stick.  And what is this. Well, let's write it more explicitly I can expand it as summation over key.  Of wk transpose x transpose x wk  And so you can think of it as I predictable you and my covariance matrix. So, X transpose X is basically like the empirical currents matrix. And then I want to maximize the variance in this projection. Okay.  So where is this coming from so  I have that one over n x transpose x  Is one over n summation over i have xi xi, transpose  And so if the mean of my data is zero, this is the empirical  Covariance  Of x when the summation over i have x is equal to zero, I either empirical mean equals zero.  So normally when you run PC, you make sure that  The data is centered first. So if it's if you don't have zero mean data, you subtract it means so that you get there being data, then you're on PC on the centered current matrix. So, x transpose x in this case will because you have to remain will be the right thing.  Somebody is asking you about where to transpose went, Well what happened is I can  Move this here. And then, sorry, I can move  This on this side and then when a compute this trace. You can see that this just as some over. Okay.  Okay, this sneaky invisible ink. Thank you, transposes back. Thank you for  Keeping an eye on this. Alright, so  This is what we call the analysis view of PCA  And so, which is that you've tried to find the direction, such that when you project in these direction you maximize the some of the empirical covariance empirical variance  In these new representation  So that's I also explain why.  When I do PCA here, the maximum variances in this direction, right. So, because that's where the. There's a lot of variation in my data sets.  And from this perspective. Now you can see how to find the value you basically look at the I get vectors of X transpose X good because you want to maximize the these quadratic form and so  The solution.  PCA is basically you get the top in visiting link.  Computation.  Of PCA  You basically get the top K eigenvectors of the empirical occurrence matrix or a scaled version of it of x transpose x  Okay.  And so an example of application is to the noise. The data because you try to  To find the principal direction of signal.  There's also so often you could like, take your data and then do PCA well first center it then do PC and then use the reduce Dimension Data because it will be cleaner. It has reduced the noise.  Or you can use dimension reduction. So early on. This was used, for example, like I have a bunch of faces of people and then you do you do PC on it and what you get are Eigen faces which are like kind of like kind of Nicole faces that you can express anybody's person face as a  Linear combination of these chemical face.  Some application. And then you could classify face using this representation which is less noisy in in the original one. It's an example of early application.  Okay, so now let's. Is there any question about this.  No question.  And so let's go back to the factor analysis model.  And so the factor analysis model is basically one of the simplest generative model you can have on continuous data as a latent variable.  So the idea is, I'll say, okay, my latent variable z, it's coming from  Kind of a uniform distribution. But, you know, having uniform distribution in in continuous space, right, because it blows up so  We'll actually use something similar, which is a Gaussian, but will use, you know, zero mean and identity covariance right so it's standard normal so  Basically, there's not much structure in the Latin speaks all the structure will be in the conditional of X given that, so we'll have that x is actually equal to W time z plus a mean and plus some noise.  And the noise. We will assume is independent on the latent variable and the noise will be basically normal with zero mean and  A diagonal covariance matrix. So that's one of the assumption of factor analysis. This is a db, db grant metrics, but it will be diagnosed. So there is no correlation in the noise all the correlation is coming from the linear transformation W. user avatar   Okay. user avatar   And so I have a visa. Gotcha. And the conditional x, given z is a gash in so the joint x and z is a Gaussian and the marginal. Next is also. Gotcha. So everything is a gotcha right if Z was a  discrete random variable with different gushing then when you marginalize out you get a mixture of caution, but when z is it is a continuous latent variable. Then you have the magic of that the closure of the. Gotcha.  But it's a very but the marginal x is a very specific gushing even though it's in high dimension D. It doesn't have full covariance matrix, it has a bit of structure. And so basically the model here. The idea is I have let's say I'm not super good at plotting stuff.  But let's see, I'm in 3D.  And then  Basically, you'll have you think that your data is kind of lying in a plane.  So this is basically the the W space suspend my the W space, but it's not crossing zero because I have a meeting. So let's put a mean somewhere. So that would be my main view. And then my distribution is actually around  This W space. And so the noise I have is basically some ellipses noise.  Like I have like some ellipse, or noise.  Around W and usually you think it's not big.  And so, you almost have a plane in this space, but you have a bit of gal should noise around it to make sure it's it fills the space.  And so now let's use our little properties of gases. And so I have that x, given z is a Gaussian with mean w z plus new  And then the covariance. I said was deep, which is diagonal  And so p of x. Now if I look at what's my marginal. I told you it's it's a garden.  And so we  And as before. We know that  We don't have to join specified. So let's just compute explicitly was the covariance. The, the means we have the expectation of X x by the tower property is the expectation of the conditional expectation of X given Z.  x, given z this is w z plus new  And so now when I think the expectation of WC plus new I get expedition of z which is zero plus new so I just get right so the marginal mean of my observation is just new to make sense. But the interesting part is the covariance to the covariance on x.  Is the covariance of W Zi plus view plus epsilon with itself.  But ze n  Epsilon from you is deterministic so doesn't do anything and then z and epsilon, they are independent. And so then the conference will add. So basically the cluster will vanish. So I'll get covariance of WC WC plus covariance of epsilon, epsilon  And so this conference here will basically give me a W covariance of z.  W transpose  And this will give me the right by definition of my noise. And so this is the identity. And so I get WW transpose plus d  Okay. And so by having this structured latent variable model we made equivalent Lee a model on X of aggression with a low rank covariance matrix. So, the equivalent model.  On X is that it comes from a gash in with me new but covariance WW transpose plus d  And so remember that w here is D by k. And so the rank of WW transpose is Atmos k. And so this is basically the  Lord low rank.  Covariance peace.  And because this is diagonal  If it was for then it's no assumption, but because this is diagonal. This only gives D degrees of freedom. Right, so  So it's much smaller. And if I had an arbitrary covariance, which would be the square  So it's kind of a low dimensional representation of my data.  Which is why you can think of it. Also as doing demonstrate reduction.  When you learn this journey model. user avatar   Okay. user avatar   So that's the factor analysis model and then oh, how do you learn the parameter W. The parameter d the power of you. Well, so you can do maximum likelihood. So you can estimate  W and D and new by maximum likelihood  And so because we have a latent variable model one standard way would be to do em.  Because we have a latent variable model.  So in the M I will need to compute  The posterior over the latent variable PFC given x  And so this is actually a Gaussian  With mean  You know, it's the conditional expectation of z given x  Which it turns out if you compute it. I won't go through the details. It's W transpose w w transpose plus diagonal inverse X minus the mean over x. The marginal me right then. You remember the marginal mean over x.  In this case could be computed from the the joint.  Well, I guess it just move right  Oh, sorry. I think New X here just mean the conditional me. Yeah, because the conditional the conditional mean this is the conditional me  I know this is just knew I think Oh man, I forgot these equations. user avatar   I think it's just user avatar   Well, let's keep the x and I'll have to verify that when I do this criminals.  Alright, so now the point is, I won't derive the EM updates.  Because we don't have time. But now I can make the link with polycystic PC. Okay, so probably stick PCA  Is now a probabilistic version of PCA  Because in PCA the Zed variable was deterministic given x right prostate PC. Now we have actually a generative model and then we can compute the distribution of z, which will actually be gotcha and prospect PCA is a special case.  Of the factor analysis model where I suppose the noises as a tropic  Factor analysis.  Where we suppose that the noise, instead of being an episode is spherical  And so d is just sigma square identity.  Okay, so, and probably stick PCA, it's a simpler version of the factor analysis where I have diagonal. So traffic noise.  And then you can still do em to to estimate the parameters. But why is it called plastic PCA well because if I think the limit of the noise sigma goes to zero, I get back to PC. So if I take the limit of sigma goes to zero.  For my conditional mean right so the conditional mean we could use  So basically we get that  The conditional z, given X is a Gaussian with mean conditional z, given X right and now when sigma goes to zero, they won't be any variants for this conditional  So the importance is what will be this mean and the conditional mean I told you was W transpose WW transpose plus sigma square identity in verse  Times x minus new  And so if I just look at this matrix, it turns out that when you take this limit as a man goes to zero. It turns out that this is equal to the pseudo inverse of W.  So that's one of the property of the universe, you get opinion from this limit of the inverse  Like this.  And the pseudo inverse  Trying to sneeze, but I can sleaze because I can mute myself anyway. So it turns out that if w  Transpose w is the identity, which is the case here for PC. It's the sort of versus just the transpose of W.  And so basically from this intuitive derivation. I didn't give you all the  The details, but you can show from that that PCA is basically the limit  Of prognostic PCA  When the noise goes to zero. user avatar   Okay. user avatar   So that's the link between factor analysis project PC and PCA. And as a side note, if you remember when I talked about the Bayesian approach I talked about the latency or certification model for text data.  Well, one way to basically derived LD model for text.  Is to have  That x given theta, the words counts. There are multi normal  With some transformation of theta.  So instead of having a Gaussian because you have data, you will use with normal. And so then instead of using and theta is the latent variable like z.  And then what you do is instead of using the Goshen, like in the factor analysis model, which is the conjugate prior of a Gaussian, you will use the conscious prior of multinational which is a division. So data is basically additionally alpha  And so, Ray buntine basically talked about it as  The LD model is being a discrete version of prognostic PC. user avatar   So that's when when perspective on on this. user avatar   Okay, so is there any question about prostate PCA factor analysis.  Simon is asking if the prospect approach fare better with outliers. Yes, because it has a bit of noise that you can use for for for  Handling  So that, so the problem stick PCA will have a bit more difficulty with noise because it's suppose it's very cold noise. So at least  With the factor analysis model, it will be a bit more robust if things are a bit farther from the the plane because you can actually in this case have ellipse. So in the notes. user avatar   But yeah, I do think it's a bit more robust user avatar   And it also in you've maintained on certainty about their presentation to us rather than in PCA where you just have it fixed representation  And so  Now if I take my factor analysis model and I extended over time, what I get is a common filter. Right.  The common filter.  You can think of it as  Taking your factor analysis.  Which I had my data point that I observation x i.  And then I moved to  A state space model.  Where there's some relationship between my latent variable.  Right. So you basically unroll in time.  Like hmm style.  So then you get  That 123 etc. Then you have observation.  And in the common filter model.  You suppose that the transition probably T on your latest state is a gotcha. So ZTE given z t minus one is just a normal with some linear transformation of the t minus one in the mean and a fixed currents in me.  And let me just blow my nose quickly.  Alright. Sorry about that. And so  With basically instead of discrete state. Now I have gash in state and then I can still use some product.  To compute the filtering probably T and the common filter.  And then when you compute the filtering distribution p AMP z, given X one, two t  What you get is called a common filtering algorithm.  And then the beauty of all these gushing is that  The posterior or Z always stay a Gaussian. In this case, so you can always represented with its mean and it's matrix.  OK, but so alright so it's 432 I think due to technical difficulties and  The explanation of the remaining of the class logistics. I got a bit late, I just want to quickly.  Give you the general just a virginal until quarter.  Because it relates to all of this.  And then I'll do a quick review of the class that will be very short just give you the high level ideas. And so I apologize to go over time.  It, it is recorded, so if you have to run. You can watch the recording later.  But yeah but sorry about that. So let me  Let me quickly go over rational and talk and colder.  Guess it's like the five minute version.  Which is the generalization of the factor analysis model to the nonlinear transformation, because in the factor analysis model we suppose that deleted the relationship between Z.  And x is just through an inner transformation and virtual to encoder. We use a neural network to transform the data. Right. And so the idea  Is to generalize.  So this is the generalization.  Of factor analysis.  Where instead of having or data patronized by some linear parameters will now have our data lying in some kind of like curve manifold like this.  And we want to try to prioritize this curve manifold.  And so the way  That people have done that is you start, you still have a latent variable which is basically the simplest distribution. So you just think it's a it's a Gaussian with no structure. So traffic.  And then all the structure comes from the the warping of your, of your transformation between Z annex and so you'll see that x, given z.  Is also a Gaussian, but it will have mean which depends on  Z.  And variance, which depends on  Z using a neural network where basically new w of z is the output of a neural network.  With parameter W.  Again so and and in the terminal, God will call it a decoder because given the code. Basically the latent representation  Of my data in the space. I can get its representation by just  Taking my neural network and getting with our domain and the variants of each of the dimension. So these are vector like mute mute WC is is a vector and actually usually the noise in high dimension is still diagnose right so it's factor analysis model. So this is basically a  So this is kind of an abuse of notation to say that the vector x as each of his entry.  With independent noise given by sigma double you said  sigma squared. It will use it. Okay, so basically this diagonal  Noise negative factor analysis model.  But instead of having a linear transformation of z, I have this nonlinear through neural network.  Okay, so then the question is, well, how do I estimate. Well, as usual, you do maximum likelihood estimation. So that means you need to use em because it's a latent variable model.  But now, unlike in factor analysis computing the posterior over z is not just a simple gashes because of these neural network. So this is intractable.  And so you need to use an approximation and in the virtual integrator use zoom use a virtual approach.  Jacob is asking if the gash in prior assumption is part of the VA model always will. There are modification of V where you use different other type of prayers, but the standard one. Yes. You start with a with a gotcha.  And we'll see where the prior comes into play when you learn  All right, so we need to approximate or posterior and so we will approximate  The posterior with  Some distribution. Q.  Which is our virtual approximation. And in this case we will LB trays or decision to again with neural network. And we also use cash it so we'll suppose that z, given X is actually also a normal  Independent normal with mean which depends on fi and depend on x and variance, which depends on fi and depends on next.  And so these are also output of a neural network.  And so these are called the encoder.  Because, given the observation X, they can give you, what's the distribution in the z space, in particular, you could use the mean to kind of like as a good representation. And so the mean will be the output of the neural nets have a different neural network.  OK, so now I need to learn the parameter five for devotional approximation and in the parameter double W for the journey model. So I do em. And so in em, if you remember there was this  Auxiliary functions. So you have that the log of p of x. The marginal is upper is lower bounded by the expected complete log likelihood  log of p of x, z.  And then plus the entropy  And it turns out that this expression if you massage it can be written as the expectation over queue but queue here as man is patronized by a neural network. So I'll just call this like this.  So I have the luxury of P w x, given z. So now I don't have the joint layer. So it's a different  Manipulation. And you've seen, and then I have the scale between my virginal approximation and the prior  So this is the normal series like okay so if you change the prior then you also change this legalization.  Of devotional approximation here, right. So, so you're trying to find. So basically when you do virtual encoder. You try to find  A very gentle. Well, first of all, you tried to find this observation model which maximize the quality of the observation, because you'll have the observation.  And you try to find a version approximation which maximize that. But is not too far from the prior. Okay, so that's kind of these two terms because you you want to, you want to maximize this which means you want to minimize the scale.  And so they are  So that's the gist of it, by the way.  And they are, they were very important innovation inversion and Toyota so so you start by saying, Okay, here's a specific model from observation and here's a very specific model, which is very natural for my version approximation.  But the nice thing is once we start to use these I can actually do something which is called the this type of virtual approximation allow something which is called the reprivatisation trick.  Re parametric ization  Trick  What's the representation trick is you can  Basically propagate the gradient through your through your distribution.  Simply. So if I look at my conditional said given x i need to have, I can actually generate discussion by just using my means.  And then I have my variance, which depends on the parameter an X times epsilon, where epsilon is just a symbol standard  Normal. And so the nice thing now is there's no parameter entering in my random variable, all the parameters are just in these scale or in front of it.  So it turns out that this will simplify a lot the update to maximize the quantity because then when I take the gradient, I'll, I'll be able to just compute the gradient through this parameter  Because this doesn't have any parameter in it. So the kind of like the randomness will be factored out. And so then you can back propagate through old. No, no. And that's work when you do the EMR data. Okay.  And so this strict Ashley is super powerful because it allows to efficiently run the M in these model by just doing pack propagation in your network.  And the other innovation within these, I will write down by the way in the scribbles the other new innovation was to penetrate devotional approximation using a neural network like normally international method for each data point. You could have a different version approximation.  With their own parameters. And so if you have like a billion observation or a million observation, you have a lot of parameters.  Here you condense all this information using only one neural network. So this is called a more ties Marshall inference because you share the information across all the data points for your version approximation.  It's not as powerful in some sense as having one parameter for each because you could have a better approximation, if you had one neural network for every data point, but it does save it as a computation. And there's also a bit of statistical gaining from  Sorry fact is asking, What if we were to use score function for green instead of repatriation. I'm not sure I understand your question, but  You might be referring to another trick which is different than reprints trick which is to use a score function, but I'm not familiar with this one.  Do you want to elaborate on your question or we can take it offline. user avatar Sarthak Mittal  We can take it offline. At last great user avatar   Is there. So that's actually the high level overview of v.  Was very fast. I'll put pointers. But interesting thing is, you can see all the all the pieces to this class, kind of like fit into place like devotional methods em factor analysis, etc, etc.  And so these both like factor analysis model and then ve  Y Z could be used to fit the very powerful observation model, but also Dr part of something called representation learning because you kind of like have a reduced representation of your data.  Right. In this case, you can think of like encoding your data from this latent variable then passing it through this complicated neural network. user avatar   And so user avatar   And so z could be a more convenient representation than just a complicated high dimensional vector x  Okay, so if there's no pressing question about V, let me just quickly give you a  Wrap Up of what we've seen in this class. So I'll switch to user avatar   The website share user avatar   See user avatar   What is this post user avatar   Oops. user avatar   Share again.  Okay, what am I sharing  This is what I share. Okay, so that's the website.  So what are the topics we've seen so  So basically the first few lectures were the fundamental of statistics and decision theory and property theory. Right. So a bit particular particular we saw maximum likelihood, and then  Statistical decision theory which which formalize how we analyze different statistical procedures.  And and and then we apply these ideas on simple to node graphical model to just start. So we just add x and y. And so in the continuous domain we had linear regression  And then in the discrete world we had logistic regression, if I want to do classification right  And already there. I motivated. This methods from a general model, right. So if you do need a regression. You can think of it as doing maximum likelihood when I suppose that my wife given x is as gash in notes, and it's a way of giving x as a linear transformation was gushing looks  Which is a bit like factor analysis, by the way.  And logistic regression was also fairly generic if I suppose I have class conditional and the exponential family.  Then you could see that you always get the logistic regression model with different features, right.  So that's what's basically the two variable model, which was also one way to get started in you know how to do numerical optimization because maximum likelihood usually is intractable. So for the logistic regression model we needed to use  Either great and methods as Judy or a threaded really really squares.  And I also talked about generative versus dismissive classification. So generative make more assumptions. This community only look at the Y given X, either as a function or as a conditional so logistic regression was this community of conditional was  Fisher display analysis was generative right and then you try that in your assignment you could compare different general assumptions, like if you do credit check the screen analysis, which is linear dismayed analysis.  Then you had different data set. And you could see how when you have the right assumption, the German model does really well when you don't have the right assumptions. For example, you suppose that the conference for the same, but it was not  Then leaner discussion analysis was doing worse than logistic regression. So there's so the conditional model was more  Was more robust  And then we started to talk about latent variable model.  Which brought us to one of the simplest one which is fashion extra model. And that was our introduction to the EM algorithm to do maximum likelihood in the  In the latent variable model. And then if you do a hard version of gushing mixture model, you get the standard gaming clustering algorithm.  And so all these was basically kind of warm up in two variable graphical model which very simple  And then the next of the class was now we go to full graphical model like so multiple notes right  And so I started so I basically talk about their active graphical model under graphical model and their properties in terms of how to represent distribution. Their conditional independence assumption you make  And  Then I describe how we can do in France in these model. So that's where you can see the advantage of having a graphical model is that it allows you to efficiently compute quantities like marginals or partition log partition function for under a graphical model using these  In general, it was a the graph 30 minute our rhythm, which is NP hard to run in general but for trees, we had the sun protocol rhythm, which is basically a dynamic program.  For the implementation of the graffiti minutes over them on the clever order for trees.  And then I told you that. Well, you can generalize these structures to mex product structure and you get the max product algorithm, which is basically a way to compute the max. So that's a B2B for the hmm  And the junction tree algorithm is basically the digitization of some product to general click trees.  And then there's a question of how you compute junction tree and I told you there. There's a lot of  Very deep relationship between a triangular that graph and the gravity mean it all over them and you know maximum spanning tree to threats with right so that's also like a lot of concept in this class was to relate graph theory with these plastic models.  And yeah, so hmm was basically the simplest latent variable model where now I I unroll it over time.  And so again we need to do, em, so I that's basically the bomb Welch or them. It's just the EM algorithm for hmm  And so that was a good application of how do we run the some product algorithm on this and you know we got the forward, backward recursion for that. Just, just an implementation of some products for each and  Then I did a bit of a side.  We on information theory.  So introduce kale divergence mutual information, these kind of concept.  Can be motivated from coding theory and and that was also a way to get to the exponential families and to maximum entropy. Right. So, maximum entropy is another principle to estimate parameters in your distribution which has different properties, then maximum likelihood. In general, but  We had this super powerful result that for the exponential family maximum likelihood was equivalent to maximum entropy with moment constraints. And that was all related through leveraging duality. So that was a also an excuse to show you. Interesting optimization techniques.  And then we went into more details of the exponential families.  Both because this is a way to kind of like talk about a lot of standard distributions in a unified manner. So, you know, like betta the replay gamma. All these are essential for me.  And also because the air is very naturally right we saw that when you do maximum entropy with woman constraints, you get an excellent show family. So the question for me is a very clinical  Type of distribution you get and, in particular, also understood graphical model. I told you, you can always put them in exponential family style for discrete data. So there was already some links there. And that's where I talk about icing models as well for that.  So all these was about  Basically representation and in France, there was a bit of maximum naked with the EM  But I didn't go in more details of how to do maximum likelihood in general graphical model. And that's where I did in this lecture, very quickly.  And in particular, when you have a directive graphical model with separable pattern ization it's very simple to do maximum and acute everything separates but when you have an underground model.  It doesn't separate. So that's why you need to do a gradient method instead of like perhaps close form updates.  And then we went into how to do approximate in France right so often you don't have closed form solution.  For a solution to do to compute the marginal and the partition function. And so then I talked both about sampling and virtual methods to do approximate in France.  And so to do sampling. I had to go through into theory of Markov chains which tells us how these Markov Chain Monte Carlo method converge is actually very elegant and one of the powerful ways to do approximate sampling  And one of the most kind of Nicole Markov Chain Monte Carlo method was good sampling, which I showed you how to apply it for Isaac model. In particular, that's what you do in assignment five  And then we talked about virtual methods virtual methods. The main idea is to express the quantity of interest that you want to approximate as an optimization problem.  And then you approximate the optimization problem. So there's a lot of different ways to do that. The simplest with with a kale divergence  And so then I talked about mean field where you suppose that your approximate distribution fully factories over your, your dimension of the objects that you want.  And then I also showed how to do mean field on the icing model. And it really looks like give sampling, but like a deterministic version of Gibson and both of them you will implemented on work five  Finally, talked about also Beijing methods again.  And  It's a nice way to talk about model selection.  Because there was this idea from Zubin that often.  Fitting your hyper parameters using Beijing method is less likely to overfit as long as you don't have too many of these hyper parameters or too many models, but to do model selection. Either you can just do, how does it look like you would with cross validation or just  One held out or you can use some Bayesian criteria like division information criteria and are you could put a prior very models and then you put here in France, right. So that would be the proper vision.  But just give you a gist of how to select models using these payment methods.  Didn't mention when very quickly on causality, just give you the just because it is basically a way to deal with data when you intervene so you need to have a model of what happens when you make an intervention on your and your data, like what would change in your model.  And you can use basically directed graph model, where does a bit more semantic coming from this intervention.  And fire today. We went to the simplest models for continuous data which is Gaussian and already looking at some of the properties and it enabled us also to talk about latent variable model for  Continuous data. So that's factor analysis and there's a deterministic version which is PCA  And if you unroll in time the latent variable model with Goshen, you get the common filter, which is like the HMM. BUT WITH Gaussian latent variable.  And finally, if you want to move away from a linear representation, you can go to these neural network transformation and fresco devotional its own calendar.  And so all this so far was finite penetration. Like I always add like exponential me with a  finite dimensional sufficient statistics. So that's why it's called parametric models because you have a finite number of parameters.  There's this thing called non parametric statistics, where you go to person models which can have an infinite number of parameters in particular that when you have more and more data, the complexity of your model can grow.  Indefinitely because you have infinite number of parameters. Unlike if you fix your neural network size when you have  More and more data at some point the capacity is limited. And so that's the part on non parametric models.  And that's what would be covered by will see on Friday with the simplest one which is Goshen discussion processes and dealership process is basically a Bayesian generalization of a Bayesian Gaussian Mixture Model of a mixture model.  Doesn't have to be Cashin, but it could be anything. And, but now you want to have an infinite number of cluster or an infinite number of mixtures and that's done using the their ship process.  Okay, so, um, that's the gist of it like I like there were some unifying theme like representation, how to how to represent your family, how to prioritize your family. How to do influence in your family. So that's computation.  But you know, it's also like a bag of tools basically high dimensional data.  Any remaining question.  I started this crash course on everything.  It's time to for a big break to absorb all that  Well, thanks a lot. That was quite fun actually to teach you, it was even though it's a it's a weird format, but actually quite enjoyed it, that you were a fun class.  And so Friday Jose will be teaching the last lecture, and I will see you next at the poster presentation in two weeks in, get it out there will be quite a lot of fun. So enjoy the rest of the class and see you later. Bye.