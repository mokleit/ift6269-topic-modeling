Lacoste-Julien Simon
00:00:05
Right, this is recording, so today.
00:00:11
We're going to see the EMR rhythm. So basically last class I started to talk about unsupervised learning. We want to fit.
00:00:21
Distribution to data where it has nothing to the labels. So that's kind of like the the very hand-waving or high level definition of unsupervised learning.
00:00:30
And then I presented the key mean algorithm, which was a clustering algorithm. So it's a very simple way to do unsupervised learning. It's not learning a distribution is just learning groups.
00:00:43
And this was kind of in preparation of learning about both
00:00:49
The EM algorithm and gospel mixture model where we will apply the immigrant. So that's the plan of today so em.
00:00:57
All with them.
00:00:59
And its application to maximum likelihood in a Gaussian Mixture Model.
00:01:05
And the link with k means is I told you that if you take the variance in a Gaussian Mixture Model and you let it goes to zero and you run em, you basically get teens. Okay.
00:01:16
Alright, so
00:01:20
Let's talk about expectation maximisation so this is this algorithm is basically to do maximum likelihood
00:01:32
In latent variable model like hood
00:01:37
In little verbal
00:02:12
Okay, so
00:02:13
Basically to recall the setup.
00:02:24
Trump with my eraser. So setup. So in or a latent variable model. One of the simplest is will say, Okay, we have some observation xi.
00:02:34
And these observation can be explained from some latent variable is that i which is not observed. So the exterior observed, but not as i said i and we could say that we have an ID copies of this model.
00:02:51
And
00:02:54
So x will be the observable.
00:03:00
And in this case, x could be the concatenation of all the excite together right so it's and z will be the latent variable, something we have not observed
00:03:14
And let's say would like to do maximum likelihood in this model. Well, so the log likelihood
00:03:24
What does it look like it would be the log of p of x one up to n, which depends on data.
00:03:34
So,
00:03:36
That is not observed. So why don't we do some method, we look at the margin all an x, right, and in last, last I mentioned, you know, that's the nice thing, having the structured object as the structure distribution.
00:03:47
Is you can easily make like these complicated multi modal distribution with simple pieces.
00:03:53
And and so the log of the luckier here. Okay, so as before. This is the log and because the model is independent, it will be product over my observations of the marginal on X i.
00:04:09
And then there's data.
00:04:12
And so that's nice because then there's just become a some right so that's the old thing that's also why we like these independent model. And that's also why we use the log like you instead of just the likelihood
00:04:22
Because the log will hit this the product and it becomes a nice some and so then what you have devices and some of you know functions.
00:04:31
But now the problem is that the marginal is not a simple
00:04:38
For example, exponential family. The marginal is written as the as the marginalization, right. So let's write this down. So this becomes summation over i.
00:04:49
Log
00:04:52
And then I have summation over my latent variable that's how I define my marginals of p of x i said i
user avatar
Unknown Speaker
00:05:03
Data
user avatar
Unknown Speaker
00:05:05
Okay.
user avatar
Lacoste-Julien Simon
00:05:06
And and again they put in a notation. So when I summon I put a variable there. What it means is I some over all the possible values of this variable, right. So here, all the possible values that said, I can fix that. That's the semantic
00:05:21
And now the big problem, compared to before, is that
00:05:26
Because of the summation. I'm not able to push the
00:05:33
The log insight. Right. So, so basically log of product becomes some of lugs but
00:05:40
The log of Assam is not the some of the lungs or some other versions. So this creates a problem. Okay.
00:05:47
And
00:05:52
And indeed, what happens is this actually what this some SEER with it gives it gives a multimodal distribution.
00:06:05
I guess we'll say, this gives a multi modal optimization problem.
00:06:13
And usually it's non comics.
00:06:18
So you will have, like, you know, little bumps. I'm trying to maximize that. And it's not a nice like a concave shape as before. Okay. So, for example, in the case of
00:06:29
A Gaussian or something like that, you know, this would be x of something. And if there was no some then the lug hit the X the X cancels out and I'm just left with like quadratic, for example. And that was very nice and compete.
00:06:42
But now, because of this annoying. Some in the middle, which is coming from the mixture aspect which is coming from the latest expect I get a non concave or non convex problem. Okay.
00:06:54
And
00:06:56
An e m is one trick to handle this this this annoyance. So, but let's talk about the options.
00:07:07
If you want to do maximum likelihood in a latent variable model.
00:07:17
So one option would be to do gradient ascent.
00:07:26
On a non concave objective.
00:07:36
And by the way, this is totally fine. So as an interesting historical note, I think.
00:07:46
More than 10 years ago, people who are much less comfortable
00:07:52
Working with non convicts Organon concave objective and so the fact that it was you know multimodal and on concave was seen as a big problem, which is kind of money that he had because he would actually work with convex if I version of the problem. So that's kind of neat.
00:08:11
But I guess with the deep learning craze where everything is on convex and things still works. I think people now are much less
00:08:19
Much less of a problem to just work run methods which don't have guarantees, because you don't have guarantees, because the thing is on concave so you can know whether you'll get to a global max.
user avatar
Unknown Speaker
00:08:30
But
user avatar
Lacoste-Julien Simon
00:08:32
But yeah, so. And why am I still teaching yen is both because actually it does really well in practice, it could work better than the grand method on the non cave concave objective and also it has a very nice interpretation and it gives a bit of insights on how to solve the problem.
00:08:51
Okay, somebody is asking what's the difference between a non concave objective and a convex objective or do the same thing. So, so
00:09:00
f is convex if in on the F minus f is concave. Okay, so by just taking a negative sign of your function, you can flip convex
00:09:12
Concave right so a convex function basically look like a ball shape function and a concave. Well, it's a ball in the other direction. If you just flip the sign.
00:09:21
And when I say non concave that just mean it's it's not concave, which means if I take a negative. It's not convex so
00:09:28
And I use convex concave interchangeably. Because okay if you want to maximize the function, you want it to become cave going to minimize the function. You want it to be convex. When you talk about convex optimization as a field you also include
00:09:43
Concave maximization in it because any way you can just always flip a sign and then maximizes and minimize so it's not the difference between concave, convex is not a big deal from a medical perspective.
user avatar
Unknown Speaker
00:09:55
Okay.
user avatar
Lacoste-Julien Simon
00:09:58
Alright, so first approach is do grid accent. And I'll talk a bit more about it later to compare with em, but the, what's the this EM algorithm.
00:10:09
The EMR rhythm. It's another blood quit method. So, so, you know, k means was a block coordinate optimization approach and the M is another one. It's a block cornet ascent method.
00:10:26
On a auxiliary function.
00:10:34
Which
00:10:35
lower bounds.
00:10:42
The yield.
00:10:49
So we will describe a function which is a more convenient to work with lower bound of the log likelihood, and we will push this lower bound up by maximizing the
00:11:02
Over my parameters and because it's a lower bound that I'm pushing up. Well, it will hopefully also push up the lights. The lights. So that's kind of motivation.
00:11:12
And it has a nice interpretation.
00:11:18
Interpretation.
00:11:22
In terms of filling in the missing data.
00:11:30
Okay, so, because he said is not observed and you can think of em as missing in the unobserved variable and then things becomes kind of like nice and convex because everything is observed. So basically, the two steps at the intuitive level for now. So the East step.
00:11:53
Can be sin. Sin seen as filling the Zed variable with soft values.
00:12:05
Why soft because we haven't distribution of disease. So we can just set said to one or zero, we need to have some problems with that. And the easy step is to compute these and then the M step.
00:12:17
Is to Max, the
00:12:22
Due to solve the maximization problem with respect to data for the fully observed model.
00:12:44
And so when you have, for example, in your assignment you did Fisher Leonard, it's coming analysis where you observe both Y and X right so you observe
00:12:56
You say X is a Gaussian, which depends on the labels, but you also observe the label. And so when you do maximum next year there, everything is nice. It's convex or concave. In this case, and you get a closed form solution.
00:13:07
So when you when you have the information about the class membership, things are nice and so the EMR with them can be seen as fill in the missing value with the step then
00:13:18
Do a standard maximum likelihood, which is nice and then repeat fill in the new again missing values and then fill in solar maximum actual argument. So now if you compare to came in. Let's go back to came in right so
00:13:32
Remember I mentioned the East step and the M step and you're like, Well, why is it D and m. So, it was exactly the same thing. So in the East that
00:13:40
We actually chose the cluster membership. So we were getting the value of the latent variable. In this case, which is a cluster membership and then in in the M step we were basically
00:13:52
Almost solving a maximal magnitude problem because it's like, what's the solution when you have us through McGough should model and you're trying to estimate the mean was the empirical mean, right, which is what you basically do
00:14:18
Yeah, so somebody asked what is blackcurrant ascent again so block grant ascent is
00:14:24
maximization of a function where you will fix the value of one block and maximize respect to the other variables.
00:14:34
And then you will fix the value of the other variables and then maximize respect to the the unfixed variable. So you can alternate you don't know how to solve exactly the maximization of are all variables. So you fix them. And then you maximize respect to the other ones.
00:14:50
Just like a coordinate that approach. And when you have more than one coordinate together, as you call it the block point approach because instead of being scale or it could be like vectors.
00:15:03
Alright, so let's look at a trick for em.
00:15:08
It's actually a fairly common trick.
00:15:12
That appears in the lot of
00:15:15
You know,
00:15:17
Places in probability
00:15:21
Theory. It's to use the
00:15:25
The Jensen's inequality. Okay, so what's the Jensen's inequality. So I will phrase the Jensen's inequality.
00:15:33
For concave function.
00:15:36
It's actually normally for a convex function, but you just put a negative sign and then it becomes for concave function.
00:15:48
In to die. I think I'll answer your question at the end. Okay, though I can answer quickly would GANs be an example of a block right method.
00:15:58
I don't think so. So, again, is actually a min max problems. It's a different approach to
00:16:02
Block wet optimization approach will be that if you optimize respect to one variable and not the other variable. At the same time, which sometimes people can do for GANs, but again formulation itself. It's not this to the a block MIT.
00:16:16
And for the rest of the class. If you don't know what I'm talking about. It's totally fine, because I have not said anything about what again is
00:16:23
Alright so Jensen's inequality.
00:16:26
So the nice thing is the expectation, with respect to a distribution of a function f.
00:16:35
is upper bounded by the function of the expectation. So you can flip the expectation and the function when with an inequality sign when the
00:16:49
The when f is concave
00:16:56
Okay. So, Q here is just some fixed distribution that's true for any Q
00:17:03
And
00:17:06
And if instead you would have FS convex, you would flip the inequality, right, because you can multiply both sides by minus one and then inequality flips and then F becomes in this case concave or minus f is concave and then you can use the narrative on the expectations.
00:17:24
But so I never remember this inequality, by the way, how is never remember which direction in this. So my recommendation is to just do a plot. Okay, so this is a concave function concave ball like this convicts ball up. Okay.
00:17:41
And if you don't remember which size the bowl should be well convicts us from unionization so at least. Remember that
00:17:48
And then if you try to minimize a function like this, you will have the optimum at the boundary which is not very nice. And so it's like this you want to use for having trouble with my
00:17:59
With my
00:18:01
Framing. Yeah, so this is comics. So now this is a concave function and you can think of taking the expectation of the function as. So this is, you know, this is f of that one. And this would be f of the, oops, sorry.
00:18:23
This would be Zeke, this would be said. One is that, too. And here would have efforts that too. And this would be f of zero.
00:18:31
Okay, so now I am taking an expectation of the function and basically taking a convex combination of points on the on the curve. And so what happened is that the was employee. If I take a convex combination between f of that one and two, I will lie on this curve here. Okay.
00:18:52
And so
00:18:54
Basically the expectation if I only have two points we expect to Q of f of z will basically lie.
00:19:10
Of on this line, so it will live below the curve. Whereas if I take now the convex combination of my points. So, so this point here would be the context combination of my disease. And if I evaluate the function at this point, I get this point here. So that would be f of expectation of the
00:19:32
And here I took only two points. You can think of when you take a an expectation as taking a summation over positive numbers times you're the thing you're taking the summation of and these positive numbers. Some one. So that's why it's called a convex combination
00:19:51
I mean, by definition, a convex combination is a summation a weighted summation of terms, where the weights or positive and some to one. So the weights basically represent the distribution
00:20:05
And so here you see that the function evaluated at the expectation is above the expectation of the function day which is what this inequality saying
00:20:17
Okay, so, so the trick to remember which one goes where is to do this little drawing
00:20:23
All right, so why am I talking about the Jetsons and the quality was because
00:20:30
We had this annoying.
00:20:32
And it's because we had this annoying lug and some right so there's some can be seen as an expectation, the slug is a concave function. So I will
00:20:41
Switch those two things by using Jensen's inequality and then I will have the lug of my joint
00:20:49
Which will now can if the joint has like is a garden. For example, I will have an X log will have the expo and everything will become simple again.
00:20:56
That's the beauty of the trick. So let me write this trick in more details. So I want to compute the marginal the lug of the marginal distribution. So this is summation of z of p of x and
00:21:13
Okay. So right now I forget the dependence on data.
00:21:18
And now, what I do is first of all I will do what you always do in some trick is you multiply by one, right, so I will multiply by Q AMP z and divide by Q AMP z.
00:21:34
So Q will add as my weighted combination to use a Jensen's inequality and like I still keep my joint p of x, z.
00:21:45
And so now what I have is the log of an expectation respect to Q of the function which is p of x, z divided by q AMP z.
00:22:04
And so by Jensen's inequality, I can now flip the expectation and the lug. So this is bigger or equal by Jensen's inequality.
00:22:22
That's the big trick to expectation respect to Q of lug of p of x, z divided by QC
00:22:37
And so perhaps we can now expand this a bit more. What is this, this is summation over Z QC
00:22:45
lug of
00:22:49
P of x, z. And this depends on data.
00:22:54
minus summation over Z lug of QC sorry there was accused in front of it. So that was a numerator. So the log of Peter right by Q becomes log of people plus minus log of q. So that's what I've done here. So of lug of QC
00:23:19
And so
00:23:21
This thing is the auxiliary function that I mentioned, which lower bounds, because by this inequality here we have that this
00:23:33
Quantity here is a lower bound on the thing that we care about, which is the log of the property of x ray guess I could have written here. This is log of property of x is equal to that.
00:23:44
And so by definition.
00:23:47
This funny objective will call this L and it depends on two things. It depends on the waiting that I use, which are the know by q and it depends on the parameters data.
00:24:03
And so by definition, what is this
00:24:06
So, the first term is the expectation respect to queue of the complete log likelihood. What is we call the complete like lucky because we have observed both x and z.
00:24:18
And I guess the expectation here respect to z perhaps output Z big just to make it clear.
00:24:24
And it depends on data.
00:24:27
And then the second term. It's actually called the entropy of Q
00:24:34
Is the entropy of Q. When we go when we talk about information theory we're going much more details on these specific quantity. So entropy
user avatar
Unknown Speaker
00:24:44
Of cute.
user avatar
Unknown Speaker
00:24:48
Okay.
user avatar
Lacoste-Julien Simon
00:24:54
And so this is the
00:24:57
Expected
00:25:00
Complete
00:25:02
lug likelihood
00:25:06
Like you're able to great
user avatar
Unknown Speaker
00:25:12
Thank you.
user avatar
Lacoste-Julien Simon
00:25:17
Okay, so
00:25:19
So to recap what we have is we have that
00:25:24
The lug of p of x, which depends on data.
00:25:29
Is lower bounded by this auxiliary function which depends on both Q AMP data for all distribution. Q And theta. Okay, so there's valid for all the solution Q AMP later.
00:25:42
And so then
00:25:59
EM algorithm is basically to do quite a cent on this auxiliary function which depends on q AMP data. OK. So the algorithm.
00:26:13
In the East step.
00:26:16
You will actually maximize respect to cute, so you'll set the new distribution cutie plus one.
00:26:23
Which is the arg max.
00:26:27
over q
00:26:33
And let's say what our cues recuse our distribution over z. Right. So all possible distribution over z.
00:26:45
Of the objective, I will optimize respect to Q, but I will fix theta at its previous value, right. So that's the cornet coordinate approach. So I will fix at its previous value data t
00:27:03
And we will see very soon that when you maximize over q this objective, there's a closed form formula which is super simple. You just need to set the distribution over z to be the hysteria.
00:27:19
Of z, given X, when I had the parameter of sanity.
00:27:26
OK, so the, so basically
00:27:28
Maximizing this auxiliary function with respect to Q. You just need to compute the conditional of z given x or the current parameter value.
00:27:37
And then m step.
00:27:41
You set your new parameter
00:27:44
To be the arc max.
00:27:48
Over
00:27:51
The opposite very you where I have fixed cutie plus one.
00:28:01
Security plus one is now fixed
00:28:04
And it's fade out I'm optimizing
00:28:14
And so
00:28:16
As I mentioned, this is the art max.
00:28:20
over theta of expectation where cutie plus one is fixed.
00:28:30
I'll put the Z dependence here just to be clear that this is a dependence on z of the complete look like yet. So a log p of x, z, which depends on data.
00:29:00
And so
00:29:05
So this
00:29:12
So this is
00:29:15
Another maximum likelihood problem.
00:29:21
But we're now the information is complete for complete information because we have fill in the missing value of the z by taking the expectation respect to cute.
00:29:38
And so in particular will see when we do the GMM thing that
00:29:43
When we had like a mixture of Gaussian model, then this maximum likelihood as an A plus form formula, which is kind of very neat. That's one of the adventures of em is often em will have closed form formula updates.
00:29:58
And often, not always, though, but for example when z is a binary variable so often.
00:30:09
You replace
00:30:12
Z with the expectation respect to q of z.
00:30:19
In
00:30:22
This expression.
00:30:26
Okay, so, and we'll see that in the case of
00:30:32
Of the
00:30:35
Gaussian Mixture Model how this
00:30:39
This
00:30:42
Manifests itself.
00:30:51
Okay, so then now there's already a few questions. When I say all possible decision of Rosie, are we not making any assumptions on the form of Q1 em instead of em, you don't. So, Q is really all possible decision of receipt
00:31:03
Will see later when we talk about various Chanel urine, because it's perhaps too complicated to work with all this mission of her Q we oversee sorry we will make a simplified some we will only optimize the verse, simply for centrify distributions. For example, it could be only gushing
00:31:23
We need some assumptions on the support of Q AMP Z A
00:31:28
I mean,
00:31:30
This is a bit of an abstract derivation
00:31:35
So right now you need that. So you're not allowed to have Q AMP z.
00:31:46
I guess hear the
00:31:49
Question. So, so you're not normally you don't like. So when you do these kind of things.
00:31:55
Whoops.
00:31:57
So if q AMP z equals zero. You know, you will have zero time divide by zero. So multiply by zero divide by zero is not a very nice thing.
00:32:06
So the convention normally here would be that
00:32:12
you exclude disease for which Q is equal to zero in this duration. So I think it's fine.
00:32:18
Thank you. Good. Have any Q in this case.
00:32:25
I will soon repeat the point about the posterior because I'll explain how the East step.
00:32:31
Gives you basically that the updated Q is the posterior
00:32:37
Assigning cutie plus one means redefining the distribution cute. Yes. So, so to t plus one just means that, because when I talk about the blackcurrant Ascent algorithm. I'm saying I will update these variable.
00:32:53
Incrementally like interests. Right. So I start with Q zero and Qaeda zero
00:32:58
Then I will find what's the next value for Q and I would call it Q1, for example. So this is kind of like you can think of it like when you do gradient descent or these kind of tradition techniques you keep updating the variables as you optimize
00:33:14
Okay, so now let's go back to
00:33:17
This fact here that I mentioned, which is that the solution for when I maximize the auxiliary function with respect to q
00:33:29
You get
00:33:32
That it's the posterior and actually let me see if I have somewhere.
00:33:42
Yeah, okay. So basically,
00:33:48
There's a property of the Jetsons and equality.
00:34:02
Alright, so this is unclean at me correct this.
00:34:08
Is cutie plus one, indeed.
00:34:29
My notes. Let's go back to where with
00:34:33
Accurate. Yes.
00:34:38
So,
00:34:40
We had
00:34:43
The log of p of x theta is an upper bound on the auxiliary function Q theta and this is from to Jensen's inequality.
00:34:58
It turns out that in the Jensen's inequality.
00:35:02
It says that you, you can actually get a quality. So in Jensen's
00:35:08
In equality.
00:35:11
We get equality.
00:35:18
Only if the function that we're taking an expectation
00:35:25
Is
00:35:34
Waiting
00:35:38
Okay, so let's be clear.
00:35:42
So,
00:35:49
lug. So basically I use that the lug of expectation of Q. Let's call this G because there's too many F now.
00:36:02
Because I use the inequality with so f is actually loved the lung function right
00:36:13
Okay, so I guess.
00:36:15
By the way, you can generalize this by
00:36:22
I guess I should have put here. You can also put a GMC
00:36:27
And a GMC they're
00:36:31
Allowed to also just take a
00:36:34
Function of random variables, right. So,
00:36:37
So as long as they keep it inside instead of z.
00:36:44
So why am I saying this because the Jensen's the equality here tells me that this is bigger equal then expectation respect to queue of lug of g of z. Good.
00:37:00
All right.
00:37:02
Now, what's happening is that
00:37:08
Because lag is a strictly concave function.
00:37:11
The Jensen's inequality wouldn't normally always be strict equality inequality.
00:37:18
Unless the random variable is digital and it is degenerate. Okay, so, so basically, and just an equality, you get
00:37:30
Strict
00:37:33
Inequality.
00:37:35
Unless okay and when
00:37:39
F is strictly concave
00:37:47
Which is the case for the log function is what I'm talking about that. So you get a strict inequality unless
00:37:54
The
00:37:57
Distribution.
00:37:59
Is the generate
00:38:04
The J naret. What does it mean to have a generic distribution, it means that the random variable only take one possible value.
00:38:13
And so then the expectation. There's only one point. And so the the thing is trivially equal
00:38:20
Because there's no sun.
00:38:24
I he takes only one value.
00:38:33
Uniform is not the same thing. Nope. So somebody is asking, well,
00:38:38
By degenerate. Do we mean uniform know we mean takes only one value so and so, in this case, ie when g of z is equal to constant
00:38:54
So if the thing I'm thinking, an expectation respect to q. So either Q is only putting mass on one variable or I have us as Josie a function such that
00:39:07
The output is only always the same value. So now when I think an expectation is always give a constant. So it doesn't do anything.
00:39:17
So I'm saying you get stricken of inequality, unless the distribution is degenerate. So, I mean, the distribution of G AMP z.
00:39:33
So g AMP z z is a random variable. So GMC is another random variable.
00:39:39
By just transforming its distribution.
00:39:43
So either Q would put all the mass on one value or G AMP z is a constant and actually now become much more clear here because
00:39:54
It will make things a bit more concrete. What was the G THAT WE USE IT WAS p of x, z divided by QC that was a function, we were taking the expectation of respect to
user avatar
Unknown Speaker
00:40:08
Respect to cute here.
user avatar
Lacoste-Julien Simon
00:40:18
And so
00:40:22
So g of z equals constant
00:40:29
Above so above. I mean,
00:40:34
In my em derivation trick.
00:40:39
This implies that p of x, z divided by q AMP z is equal to constant. That's what we're saying.
00:40:49
For all the right. So, x, x is fixed. So the only thing which is very is z. So, now if we're saying that the function g is constant, we mean that the ratio of px device P AMP z divided by Qc is a constant for Ozzy which implies that in this case.
00:41:08
QC
00:41:11
Is proportional to the joint.
00:41:15
Right, because their direct ship is on their constant
00:41:20
IE.
00:41:22
QC
00:41:26
And he was the star is the the posterior of z given x
00:41:34
And I'll put the theta back
00:41:38
Okay, because what happened is, is
00:41:43
This here is a strict inequality. So, this quantity is always smaller and this is the executive function. It's always strictly smaller than what we care about. And it will be equal when we set Q to be
00:41:59
The poster of z given x. Okay, so that's why I'm saying when I maximize
00:42:05
Respect to cue the rosary function, I get that. This is the maximum and I forgot to use temporary
user avatar
Unknown Speaker
00:42:15
So let me can so
user avatar
Unknown Speaker
00:42:33
Okay.
user avatar
Lacoste-Julien Simon
00:42:39
IE, the art max.
00:42:42
Over que
00:42:44
El que data t is equal to p z, given X at data t and
00:42:54
We have, in this case that the auxiliary function is equal to the thing we care about we get that L.
00:43:04
Of Q t plus one, some sense data t is equal to the log of x activity.
00:43:32
Alright so that is a few question.
00:43:38
Is in a very, very special case that God is a constant. Well, in this case, in some sense, it is but
00:43:44
The, the whole reason I was talking about His argument was to find in an easy way. The, the, in a close form. What was the
00:43:52
Solution of this maximization problem, right. So when I'm trying to maximize the spectral Q this auxiliary function well I said cue to be the posterior at different parameter and then it makes the absurdity function equal actually to the upper bound. So that's the trick.
00:44:09
And somebody saying well couldn't just GMC be uniform. No, because if this is uniform the lug of this will give will actually be the expectation outside the lug will change something so
00:44:22
The whole trick is, if this is a constant, then this expectation is only one value. There's no some over a bunch of stuff. And so there's no expectation. There's no expectation, then of course they're equal by trivial relationship. So the only reason that
00:44:39
Yeah, but because the login access to take on K function as soon as you take convex combination of multiple points you get strictly below which was like the curve I showed you before.
00:44:49
Here at any point on this line, except the corners, which are one point.
00:44:56
Or a strictly below the the concave function. So that's what's happening.
00:45:14
I'm not sure I get your question be no
00:45:19
So if you still have it perhaps
00:45:24
rephrase or
00:45:26
Give me more information.
00:45:37
Yes. So basically, let me repeat this part. So some people are asking why, why is there. Q star to star right
00:45:52
So what I'm saying here is that if I use this value of two
00:45:56
Makes
00:45:59
Equality.
00:46:02
El que theta is equal to log of p of x data. Okay.
00:46:11
So, so, so if I choose this distribution queue here I get equality between my auxiliary function and my upper bound
00:46:22
For any other value of the auxiliary of Q It's strictly lower. So if I maximize overall queue, because you can never go above the upper bound, then if I make it equal. This is my maximize
00:46:34
This will be greater or equal than all other values of l because of. Yeah. So basically, what you have here is that
00:46:45
L.
00:46:49
Of Qt plus one where cutie plus one is defined as I've just done theta t is an upper is equal
00:47:01
By the property. I just gave to the lug of p of x at data t. And we know this is a low an upper bound for the auxiliary function for all cute.
00:47:15
At the same data right
00:47:18
And does
00:47:21
This implies that cutie plus one.
00:47:25
Maximizes
00:47:29
L of Q data t with respect to keep all right.
00:47:49
Started stack is asking is Q, the poster. The only possible for
00:47:55
Yes. Okay, so basically the question here is, I get that Q is proportional to the joint and then I conclude that Q is equal to the posterior
00:48:05
Because Q is a distribution. So it has to be normalized one. And so, indeed.
00:48:11
You know, the only distribution which is proportional to join as a conditional
00:48:21
Okay.
00:48:25
Let's talk about some properties of the EM algorithm.
00:48:30
To make it a bit more concrete. So let's look at the properties
00:48:36
So,
00:48:39
The first property is that the log likelihood is actually non decreasing. So I have the dialogue. Let's do the Vex at theta t plus one is actually bigger than the log likelihood
00:48:54
At data t
00:48:57
And so I am. I mean, it could be actually equal. So there's nothing see a streak increase, but
00:49:04
Since I'm since I'm trying to push up the likelihood, and indeed this method does
00:49:09
And so here's the proof.
00:49:23
So the leg likelihood
00:49:26
Of x at data t plus one.
00:49:32
Is an upper bound on the auxiliary function evaluated cutie plus one and data t plus one.
00:49:44
And because when I did maximum like to respect to theta i was maximizing the auxiliary function respect to theta. So this is bigger than the log like you cutie plus evaluate at Qt plus one and data t because data t plus one was the maximum Iser
00:50:02
So this space fix and data t plus one is defined as the maximize or of dysfunction respect to data right so that's just by definition of data t plus one.
00:50:12
And now the magic is, I know that when I evaluate the function at to t plus one and data to like here, I get back the log length you
00:50:22
Because the, the, the auxiliary function in this case is tight. So this is equal to lug of
00:50:31
P of X activity.
00:50:35
And so that shows that
00:50:39
When a volatility plus one versus data T I just get higher, right.
00:50:49
So he's asking why is the only distribution that is proportional to the joint. The conditional
00:50:56
So this is you should review just says standard definition of conditional probabilities. This also explains why, by the way, I
00:51:06
I never talk about conditional. Like I always just say proportional to i don't care about the normalization constant because the normalization constant is is to make it some to one. And so by definition here, I will have that was the nomination constant of p of x, z, right. So,
00:51:27
The normalization constant of PM exists summation of z p of x, z. And this is just the marginal of x, right. So if I divide. If I divide p of x, z divided by px, I just get the conditional
00:51:43
So the magic that everything works.
00:51:49
OK, so now let me do a little plot of what's happening when we do em.
00:51:56
Okay, so what I'm going to blood here is just as a function of theta.
00:52:02
And I have my though you could think of the auxiliary function as being both a function of theta and parameters for q. So you could have, like, another dimension, but I will try to do a 3D plot because it's a bit difficult.
00:52:16
And so
00:52:18
I have my leg leg field which is probably non concave like this.
00:52:24
And what happens is
00:52:30
What do I want he
00:52:36
Let's say I started data t
00:52:41
And then I look at the auxiliary function.
00:52:47
As a function of feta.
00:52:50
Evaluated cute cheapest one right. We know that that cutie plus one.
00:52:55
And data to the objective function is actually equal to the upper body.
00:53:02
And so now if I looked at the very function. So this would be, for example, the auxiliary function.
00:53:10
And evaluated to t plus one. And now as a function of theta. Okay, so this is below the
00:53:19
The upper bound, and it is tight at delta t. Okay, this is tight here.
00:53:29
And again with a plot in blue is the log of p of x as a function of sailor. So I put a little dot for saying that's the function data. Now, what happened is I will find the theta which maximize this function. So, I will, for example, get
00:53:47
The new theta, which is here. So, this will be 30 plus one.
user avatar
Unknown Speaker
00:53:51
Okay.
user avatar
Lacoste-Julien Simon
00:53:53
So this is my max of my exit function. And now I will update my exterior. So I will update queue.
00:54:02
And then they will get
00:54:04
That the Q value will make the observer function tight at the value added plus one. So now you could have say whoops console. Let's put it in green.
00:54:16
So now this would be my new executive function.
00:54:21
This will be L of cute t plus two as a function of data. And that's the one. Now that I will
00:54:29
Optimize so it's it's tight at the point in, you know, I'll get by maximizing this one I'll get
00:54:38
See a new theta t plus two. And you can see that while I do that, indeed, the value
00:54:46
Of the likelihood at each of these points is increasing. So I am indeed while I'm doing this process. I am increasing my my my log likelihood. I will probably converge to this local max.
00:54:59
Because unfortunately this is non concave. We won't get a free lunch. We won't be able to do global max. We just converted to a local max.
00:55:07
And the nice thing is that as a function of feta these auxiliary function or nice and concave. Usually, it doesn't have to, but they're often nice and come, Kate. So that's why there are much
00:55:17
Nicer to work with because this was not concave so so that's why instead of working with the original non count a function. We work with these concave lower bound. So that's the the process of em.
00:55:40
And let me mention another property and then I'll stop for questions and we'll take a break. So basically one property of em Is that data T in EM
00:55:51
Converges
00:55:54
To a stationary point
00:56:01
Of the leg leg.
00:56:04
Leg of p of x as a function of data.
00:56:08
And what we mean is that the gradient respect to theta of lug of p of x data is equal to zero.
00:56:21
Supposing that it was
00:56:25
It was different.
00:56:28
And so like k means
00:56:35
Initialization is crucial.
00:56:40
Right, so depending on where initialize you get two different local max.
00:56:48
And so usually what you do is you'll do multiple Random Restart
00:56:56
And pick the
00:57:00
The solution among all these random restarts which has the best objective, because you can evaluate the objective when the bond is tight.
00:57:08
And in the case of GMM
00:57:13
Which will cover very soon. You can actually use came in, plus, plus.
00:57:22
To initialize the parameter mean this is a good way to finish he share lies.
00:57:32
The means of your assets.
00:57:50
Is this example, it looks clear we're not going to get the global max. Right. Yes.
00:57:57
But if I initialized. For example, here this is data zero, then there's a big chance that I would converse, the global next. So that kind of shows the importance of the socialization.
00:58:12
We couldn't have just taking the grand directly. It is asking
00:58:16
Yes. So that's the method one to do maximum likelihood is instead of doing em, you could do just gradient descent on the log like human
00:58:25
And the advantage here is actually each of these step was very powerful because when you do a grand step you just make a little step in this small direction.
00:58:33
Was in this case, you're doing global maximization of the lower bound at each step. So these are very powerful step like you can make super big step using em.
00:58:44
And the reason why it was efficient is because often these
00:58:49
Competition or close form so you can make a global max over q, you just compute the posterior, then you do global max over theta. It's a maximum I could problem, but it could have also closed form solution. So that goes much faster than if you just make small great instead
00:59:05
What was what's the after the break that sometimes it will be better to do agree that transition method than him.
00:59:16
So Marin is asking whether there's a relationship between him and a second order optimization method.
00:59:24
So it's not really a second order because there's no history and information in this case. So, so the EM is called, it's called a major realization or
00:59:34
I mean, if we if we flip the sign.
00:59:37
And we would minimize maximize then it would be called a major realization minimization algorithm. So modular ization means you will take an upper bound of your function, then you will minimize the upper bound
00:59:47
Here, what we're doing is guess mineralization maximization algorithm, which means we make a lower bound of the thing that we care about. And we maximize that.
00:59:55
And this is actually a generic approach to do optimization. So they're all optimistic method, which does that. But because there's no husson or gradient information here that it has nothing to do with first or second order approximation.
01:00:15
I'll get back to the rate of convergence after the break.
01:00:19
And if I forget. Remind me Karthik
01:00:22
Gaming plus plus I defined in the last lecture. Please look back at the notes from the last lecture, and when should we use GD instead of em, I will answer that after the break.
01:00:35
Because I want to talk a bit more about these properties. Cool. So let's take a 10 minute break. It is three to 34. Let's go back at
01:00:45
244
user avatar
Unknown Speaker
01:00:48
Let's see.
user avatar
Lacoste-Julien Simon
01:00:51
For
Logo
IFT6269 - Friday lectures- Shared screen with speaker view

00:00:06/00:48:45

Speed


Audio Transcript
Chat Messages
Search transcript
user avatar
Lacoste-Julien Simon
00:02
So let's finish the EM part
00:06
Oh, and the funny part of these steaks face of Spotify is my wife. So this is a lot of it is from my wife musical taste. I don't have that much credit
00:19
Anyhow, so
00:25
One last property of em and then I'll answer a bunch of questions. So
00:30
I want to now show you what's the difference between the the thing that we care about and the lower bound. Right. So recall that the lower bound the auxiliary function.
00:42
Is the by definition the expectation respect to queue of the joint like you had p of x, z. And then there's a failure divided by q. Right. That's the definition of the rooms every function. And so if I compare the lug of the marginal like to the thing that we care about.
01:03
And I subtract the lower bound.
01:12
I will now have minus expectation respect to q. So, so I so this here. I'll just put it to their
01:23
Oh, did I forget to use invisible ink.
01:30
And so yeah, so minus expectation Q like of p of x, z theta. So, haven't done much and then I divide by Q AMP Z. Oops.
01:43
Log of this divide the QC and what happened is the luggage.
01:48
Luggage px, oops.
01:52
The log of px
01:54
It's the same thing as
01:58
Now I can put it in the numerator here, right. So this would be now my p of x data.
02:10
Right, because if I take minus of lug of one over px
02:18
Then the minus cancels out. I'm just left with minus the other pics
02:28
And
02:31
That's rule my cell phone because I don't want to receive suspected spam.
02:36
I hate these spam saying
02:39
Like your
02:41
Social your social security number or whatever it's called in Canada has been
02:48
Figured out by the FBI, you need to call this number, or you'll get in trouble. I get these messages. I guess the 10s of these phone calls. This is so annoying.
03:01
And. All right. So why did I do that well. So the first thing is that then we noticed that the joint divided by the marginal is actually the conditional
03:14
So this thing here is P of z given x theta.
user avatar
Unknown Speaker
03:21
Okay.
user avatar
Lacoste-Julien Simon
03:23
And so I can
03:25
This is just expectation respect to Q and I flip the numerator and denominator, because I have the login so this is log of Q AMP z divided by p of z x data.
03:42
And by the way, this is by definition.
03:46
This is the
03:49
To
03:52
divergence between Q
03:55
And the poster.
03:58
Given texts of data.
04:01
This is called a kale divergence
04:05
And we'll revisit this again. So we talked about entropy. Today we talked about kill divergence. We'll revisit both of these quantities later on when we talk about information theory.
04:21
But it but you can see that the difference between your upper bound and the lower bound is the kale divergence between the queue and the posterior
04:31
Okay, and it killed everyone has a property that it's always tricky bigger than zero on as the two distribution or the same so you can see why when we try to maximize the
04:42
The lower bound respect to Q. We can we, we can make the difference between the lower bound and upper bound zero by sitting Q to be the posterior right so now it's clear from this perspective.
04:56
And so in some sense you could think of. I have log of p of x data and then I have my auxiliary function el que data here and this difference is the kill between q and the posterior
05:18
And we can make this different zero by just setting to to the puzzle.
05:30
Piece in purple is indeed p of x data per episode can rewrite it.
05:36
P of x theta.
05:51
I'm so I need you to. So it's saying empirically, don't we cannot really have access to the posterior to assign it to Q Right. Yes. Because we observe x
06:06
So who get. By the way, this is not a posterior on the parameter, right. This is a procedure in the sense of the conditional of z given x. So,
06:16
Beijing could think of it as a, it's a bit abuse of terminology we talked about the kind of a beige and so
06:22
Sort of thing of the sphere of z given X, just think of it as the conditional of z given x, then it's clear and we have access to x, we know which data we're considering right now in the EMR with them so we can compute the first the property of said given X for this parameter theta.
06:43
So the lug applies to everything to the right. So you can think of the slug here and the slug like this and
06:53
The P disappear by the p of like I had, I had this p and this be, then I replaced the ratio by the conditional. And then I I have the minus i just flip them right so that's why I have Q and then p AMP z here.
07:19
OK. So now why oh no I again didn't use the invisible link I
07:28
Let me rewind.
07:31
Control C Control V. Okay. And so we will revisit
07:36
This scale divergence. When we will talk about virtual conference.
07:45
So we will both revisit when we talk about information theory and also when we talk about
07:51
To do virtual inference.
07:59
And in particular, when we do virtual in France, we could use a set of distribution which are simpler than all distributions.
08:17
And so
08:21
So when we do the East step in standard em.
08:25
We actually set
08:27
cue to be the stereo.
08:31
This year, but if our model, sometimes in our model, we won't be able to compute this theory. Exactly. We will need to do approximation to complete this posterior
08:45
And. And so one way to approximate this quantity is to minimize the kale between some simple distribution and this quantity
08:55
And then the queue that we obtained by minimizing the kale will be kind of the approximation. So for example, we could have something here, which is a complicated multi
09:03
Mixture of gosh, and I will try to approximate with some gosh, and I will try to find the Goshen, which is the closest in Kiel distance
09:11
To this mixture and that's what we use instead of the mixture.
09:15
And and we see this as a special case of a rational approach and, in particular, you could
09:21
In the East step of em, you could replace the, the men respect to all this sufficient to only with a subset of distribution. And then what you get is called virginal ear, it will get back to that later.
09:35
And Dora is asking if we said Q AMP z to be the posterior will get Caleb zero. How does this relate to your earlier point of TZ being a constant.
09:46
Yeah. So what I said earlier, is that the in the Jensen's inequality, where we did
09:53
That go back. So here I flipped the log and the expectation by using Jensen inequality. And that's why we have an inequality norms and saying is that this will actually be equal if this thing here that I'm thinking the expectation. Oops.
10:13
If this thing I'm thinking, the expectation is a constant.
10:16
So it's not QC, which is a concept. It's the ratio of p the joint divided by QC and if this is a constant that means that Qc is actually proportional
10:24
To p of x givens, and z, which means that Qc is the posterior so that was the argument. The thing about the constant was to be able to prove that the to make the kill zero or to have actually equality, which we can see it also from making the halo equals zero. We said cutud posterior
10:50
So Hattie is asking that we are free ourselves to choose what's the distribution or form for
10:58
What we don't choose what's p AMP z, given X. Normally, because in our case, we have p of x, given z and P AMP z, which implicitly defined P AMP z, given X. And so we, she said, Well, can we just choose one is easy to compute
11:12
Yes, and parts good or we'll see when we talk about conjugate priors later are in a class that that's a way to make sure that it puts theory is easy to compute
11:22
But it doesn't mean that it's a good model of the actual data. So it's not because something is easy to compute that it's a good model of the data. So there are constraints in terms of like
11:32
Ease of in France, ease of computation, but perhaps it's not capturing the complexity of the data with a more complex model, but then you can compute. Exactly. And you need to be approximation. And we'll see a lot of examples of that.
11:50
Okay. So, last point about em.
11:54
Is that I'll let me see. There was some question in the pack global
12:01
rate of convergence. Yeah. Okay, so that's both the question about GD versus em and rate of convergence. So basically,
12:12
Em is a blood cornet method. Okay, so let me show you an example of level set of a function where the blood current method is not fast. So this is cute. And this is data. And what happened is
12:29
When you do exact date you only move parallel to the axis. So, this case you'll go there.
12:37
You go there, there, there, there, there, there, and you'll, you'll just make very small steps until you know you get close to the optimum.
12:45
So when you have these kind of elongated ellipse like this, which are kind of like 45 degrees with the access because you can only move pal to the access in the corner method.
12:55
It might be faster to fold the gradient or perhaps a second order method. In this case, like if you use you turns on this objective, it might, you know, follow the new bike move instead
13:08
You know, I guess. Here the gradient wouldn't be too bad.
13:13
Whereas my temporary. Yeah, so the gradient would probably do something like this. So it's still a bit slow but faster than the current methods. And then if you use like a Newton method, you might be able to move like
13:24
In this direction so much faster.
13:27
And so now there are convergence rates for em and for grading methods. It's actually a fairly novel feel because em kind of comes from statistics and statistics didn't really do that much convergence rate for monetization perspective think there's a nice paper by Mark Schmidt.
13:46
Analyzed em.
13:48
Recently on and it's convergence rate.
13:52
And so some so so here the difference will be how many steps you need for to get close to a century point and then there's a difference in terms of
14:02
How expensive is each iteration. Right. So the nice thing is the M is often the updates are closed form. So it's very cheap to compute, whereas
14:12
The the the grid method or hesitant needs to perhaps to compute their large matrix.
14:20
And so, so perhaps it would take less iterations of Newton's or question Newton method for example, but each iteration might be more expensive because the EM update or close form which are very fast. And so it's not super it's not simple to compare the two.
14:37
So it's more like an empirical question of like, well try both and see which one works best or see what other people have tried before. But for exponential family model. Often the M works nicely.
14:49
And the Gaussian Mixture Model is there is an example of that.
14:54
Is there any other question about em. So now the plan is to apply this on the Gaussian mixture models, it will make things much more concrete.
15:08
Dora asked. Oh, why do most people use GD then well
15:12
Defined most and people who are you talking about, right.
15:16
So if you're in statistics. Most people use em. If you're in deep learning. Most people don't know anything about statistics and they just use gradient descent because they don't know better. So
15:27
It depends on the community, you are, and also depends what you work with
15:31
Right. So if you're working with her no network. There's not even a journal isn't even a plastic model behind that really like the, the, the, the last layer of your network being a
15:41
cross entropy loss is just a way to get the kind of the nice function to minimize it's not really problematic model.
15:52
No, this was still a good question. Right. So I think it's, it's very valid question. There's a question, though, of where do you get your, your, your, your impressions. But it is true. So what I could say is that 10 or 15 years ago.
16:10
People would use a lot less green descent for these kind of methods because there was these nice, kind of like close form updates with em. And also, I would say that
16:22
One of the big
16:24
Change from deep learning was also just having a lot of libraries available. So, you know, competing the gradient through
16:33
You know, a competition graph you can automate those that very easily. So it's easy to just like oh I write down my model and then put
16:41
The grades competed exactly automatically for me, whereas computing the, the, the, er, the end step in the arbitrary model will you will need either to use a library, which does that which existed probably stick programming.
16:53
But they're not as easy as standard. And so that's why I think a lot of people now also just using GD because they have libraries to do that.
17:02
So I think that's the kind of a big advantage.
user avatar
Unknown Speaker
17:05
Of the computer science approach to Mission ready
user avatar
Lacoste-Julien Simon
17:11
Okay, so let's start the GMM model.
17:17
For
17:19
Let's apply em on the GMM
17:22
The guests are the Gaussian mixture.
17:26
Gaussian Mixture Model, I guess, Jimmy model is a bit of a
17:31
Logical that when you repeat things
17:35
To and as
17:37
I think
17:41
Alright, so
17:43
The first thing is
17:45
We have a latent variable model. So as I mentioned, we have
17:50
Our observation which will be the X i and then we have the data variables and I and we have and observations and so the model for that for the GMM was that, said I.
18:05
Was a move to New he with parameter pay
18:09
And x
18:15
X i givens that I
18:19
Equals Jake
18:21
Was a Gaussian with me new Jay and covariance sigma j
18:30
Dan, and when I say is that I equals, Jay. This is a shorthand notation.
18:39
To say that said i j equals one, right, because remember that I have encoded. The key choices of my latent variable zero dy.
18:52
With one hot encoding right so it's it's actually a vector and dimension k. Now, when that says that i equals jam just mean that the, the one hot is in dimension G.
19:02
Which means I chose the cake, the JS component which is why, then, my observation will use the mean and the covariance of my faith component
19:12
So the, the parameter in this model. There's a lot of parameters. Right. So data is actually pi. It has the means for all the component, and it has the conference's
19:27
For all the components.
19:31
And also I will use
19:36
For em. You remember there was this generic x and z. And so the notation here will be that x is actually the concatenation of all my observation and z the same thing as a concatenation of all my little variable.
20:05
Now I would like
20:07
To
20:10
Highlight the simple fact of when you have independent distribution.
20:16
So basically,
20:20
From this model, you can easily show that the whole posterior z given x
20:29
Is actually equal to the product over I have p have said i given x, which is equal to the product over I have the conditional have said, I give an excellent. So in some sense,
20:46
Because I had this independence between my my my my different eyes when I compute the the the conditional in the other direction, nothing changes. I also have independent conditional
user avatar
Unknown Speaker
20:59
Okay.
user avatar
Lacoste-Julien Simon
21:01
And and this and this part. And by the way,
21:08
So this thing here is equal actually to this thing.
21:13
Which means that x i is so, said I, is
21:19
Condition independent of all the X jays given xi, which makes sense because there's no link here between, said I, and the other exciting. So this is kind of a simple
21:35
Consequence of our graphical model or independence assumption. So it's, it's pretty easy to show that
21:44
So I'll just leave it as an exercise.
21:49
So exercise to the reader.
21:52
So how would you prove that. Well, you would just first so
21:57
You could compute how to prove that I suggest that you compute this marginals, so p of that I given X. What it means is I take my whole conditional and I sum of all, does that JS or Jane of equal to I
22:11
Can I get the marginal and then you'll see that the marginal will basically just be this thing here.
22:18
And so then
22:24
He know you will see that the whole joint separate as this product of these marginals
22:34
So yeah alright so now that's computer computer left field and I'll get back to this fact because it will come back very soon, which is why I'm mentioning it now, but that's compute the complete look like us, because that's what we'll need in EM
22:51
Like the
22:56
Alright, so the lug
22:58
Of p of x, z theta. So this is my whole joint over all the x's and all disease by the independence. This will just be some over i.
23:10
Wanted to end of log of p of x i givens that I say data.
23:21
And then plus
23:24
lug of P have said i given data.
23:31
Well with Bamber theater.
23:37
And so now in our model we have that these are options.
23:43
And these are multi new you.
user avatar
Unknown Speaker
23:48
Mean the new
user avatar
Lacoste-Julien Simon
23:53
And so let's write it down explicitly
23:56
I STILL GET MY some over all my training examples.
24:01
And then I can have. I could choose the correct component by summing over my little variables. So Jay one up to k have said i j
24:13
And then I have loved
24:15
Have p
24:18
Sorry, the logo of the Gaussian. It's right it explicitly. So this is a normal on the variable x i and then the parameter or new j and signature.
24:34
So only one of those that i j is equal to one. So that selects the correct component. According to said because we were conditioning mindset. In this case, right. So that's why when we know z we can pick the correct component. And same thing for the
24:52
The distribution over z i have summation over k here that i j and then lug of pie. G. So, by we're giving my distribution for the middle
25:07
OK, so now you see
25:11
The, the Zed appearance in the company like you. And so when we compute in the East step it's already in the M step we compute the expected complete like like you
25:24
Expectation respect to queue up the log like you
25:30
Then bye linearity over the expectation, what I get is just the same. Some want up to in some Jake was one of two key and then I get expectation respective Q of Zed i j
25:49
And then I get my lug of normal x i you Jay sigma j plus log of pages.
26:05
And so when I mentioned earlier that often during the M. A. What did I say that, blah, blah, blah.
26:16
Yeah, so in the M step.
26:21
I say you compute the expected completely likelihood. And then you maximize to find the next data. And I said, often when you do the expected computer act like you, you will replace z with the expectation of Q AMP z which is what we had here is it. Oh, did I yeah
26:40
Right so so disease that we didn't know for the marginal aspect we replaced them with the expectation expect to cure that i j that i j is a variable which is 01 so what I get here.
26:55
When you take an expectation of binary variable, all you get is the probability that the variable is equal to one, right. So expectation of respect to Q have said, Jay. This is just equal to the product team that Q gives that said i j is equal to one.
27:14
And Q isn't sufficient over all the Zed sites, not just one. So when we write Q of that I Jake was one. This is a marginal
27:22
So this is a marginal distribution.
27:32
But they buy the property that I mentioned here.
27:38
That the joint is just a product of these conditions. So it's clear to see that when I will some to compute the marginal I'll get at the marginal is just this probably t here. So the marginal
27:55
Oh, sorry. So this is not well first of all I need to use
28:00
invisible ink.
28:03
And
28:06
During em.
28:10
So during em.
28:14
Will have that Q
28:18
T plus one.
28:20
Of z is equal to the probability
28:26
over z given x
28:30
At theta t
user avatar
Unknown Speaker
28:32
Okay.
user avatar
Lacoste-Julien Simon
28:36
And and so that's the part where
28:40
I will if I want to compute the marginal
28:43
Of this queue. It was the marginal of the conditional PFC given x which is simply the
28:51
The conditional and said, I
28:54
And so
28:56
I will use the annotation here this Q
29:00
During the EMR rhythm. I'll call them weights. I use annotation Tao I G or the index for my variable and it depends on the iterations are called tie Tao Te and by definition this would be
29:17
The probability that said i j equals one given X i activity and this is equal to two t plus one said i j equals one.
user avatar
Unknown Speaker
29:33
Right, which is
user avatar
Lacoste-Julien Simon
29:36
This thing I mentioned here, this is the marginal and the marginal
29:42
In em. I said it to the not the joint acid into the conditional, but then when I marginalized this joint by the property. I mentioned earlier, I can just look at the conditional on on the individual variable experience and
29:55
That's because of the independence between my data point, if there's some time dependence or relationship between my data points. I couldn't just read this and you'll see that when we talk about em in the aid hidden Markov model later it will be a bit more complicated. These, these updates.
30:13
Okay. And so to compute the, the, the expected companies like, like you said, I need to compute these weights Tao I GT to compute these weights, I need to compute these conditions. And so the East step is basic computing this conditional. Right. And so the first step.
30:32
So the first step.
30:35
In em is computing these conditional
30:46
Qt cutie plus one.
30:55
Well, okay, so you could compute the whole joint. As I mentioned before,
31:00
Z given x at data t
31:06
But by the property. I mentioned before, this is just the product over I have p of that I given x i data.
31:19
And so the marginal
31:23
On, said I.
31:25
Is just the conditional that I given xi, which is proportional to the joint on excitement that I so this is a product of x i givens that I
31:38
stayed a tee times probably the opposite. I given data t
31:45
This tells you
31:49
This comes from the
31:53
The previous value of pie that you use.
31:56
This is a Gaussian
32:01
And so, what you get is that the the weight.
32:07
That you use in EM i j t
32:12
Is equal to cutie plus one is that i j equals one. And so this is just a normalization of these products of pie with a Gaussian. And so it's basically by Jay
32:29
T
32:31
Normal evaluated x i with parameter you Jay at iteration t and sigma j at iteration t
32:41
RI normalized right so you son. So let's call this summation over l.
32:47
over k p by El tee captions X I evaluated with mean you L of tea and sigma L.
33:02
And so this thing here is the joint of x i and then said i j being close to one.
33:11
Given my parameter set it.
33:14
And this is my marginals p of x i given parameter theta.
33:19
So when I divide by each other. I get the
33:26
The conditions.
33:30
Okay, so that's the step you just basically compute
33:35
For each possible values of Z's.
33:40
Was the conditional of this value of z given x, which is basically proportional to a product of the prior of this component times the Goshen evaluate an ex I
33:52
With the mean of this component
34:03
And so
34:06
To recap, so the East step.
34:11
For GMM
34:14
You compute
34:18
Tao i j. Oops, I'm having issues with my writing.
34:24
So i j
34:27
T
34:29
For all the training examples for I was one up to n.
34:34
Using the previous value of the parameter. Thanks. So using data to
34:40
And then in the M step.
34:45
You will maximize with respect to the parameters. So you J.
34:52
sigma j and j
34:55
The expected completely like you, which I wrote above it was submission over I summation of OJ of these soft wait staff counts, so i j
Lacoste-Julien Simon
35:11
And then I had the log
35:14
Of p of x given
35:19
Parameter mute mute mute j and sigma j
35:25
plus log of Baidu
35:29
So, by the way, this is very similar to the luckier you get in Fisher in earnest come in and assets. When you do the maximum like you would approach and so
35:42
It's an exercise because you already did in some sense to you in your previous assignment, but it's an exercise that then the parameters are have very simple clothes for updates. So the maximum likelihood parameter for pie.
35:59
At iteration t plus one.
36:02
Is just the proportion of mass that you've assigned to cluster G. Right. So it's the summation over i have these weights, I, J. P.
36:15
Divided by n. Right. So in some sense, these Tao are like the soft count. So normally when you do the
36:23
The perimeter of date and Fisher and leaner and semen analysis, what you have is that the update pie with the proportion of time you've observed
36:31
A specific class. So it's zero and one. Right, but now instead of reviving counts. We have actually the, the probability of these the variable being equal to one. So it's like a soft version of the cast
36:43
Just why I'm saying when you do em in negotiation model. It's like filling in the missing value with soft count value. Where does the softness comes from the quality of z given x
36:55
Well, these are stuff counts.
37:00
And similarly, now the new means for a company components will be the empirical mean of the vector where you use a weight. So it's the sum of all the training example and then you use a weighted sum.
37:19
Of each data point where the way that you use is actually the quality of that I
37:24
And so some over excited and then you divide by the mass that is this clusters. It's summation over i want to end of tau i g
37:37
And finally we have the covariance sigma j
37:43
Hat t plus one.
37:46
And then I get the empirical covariance where I relate again. Each of the entry. So we'll have a towel i j t here, then I have my standard covariance x minus new j hat t plus one.
38:05
And then x minus view Jay had t plus one.
user avatar
Unknown Speaker
38:12
Principles.
user avatar
Lacoste-Julien Simon
38:13
And I read normalize by the mass in this thing, which is summation of her I
38:19
Have to i, j
38:23
So these are the M step updates of a Gaussian Mixture Model in the East step I completed the Tao, and the M step I just update my parameter, according to these Tao, and the data.
38:37
So this is the M step.
38:42
For em.
38:46
For a Gaussian Mixture Model. And in the third assignment you will be basically implementing this algorithm.
39:00
Alright, so first of all,
39:02
I'm almost done. So let me talk about how to initialize because you need to initialize these parameters to run em.
39:12
So a standard example to initiate to have good initialization is you will get them you j zero by running key mean on your data.
39:22
Where a key is the number of
39:25
Components using your mixture model or an incoming dust. Plus, so that will find a nice, kind of like mean to set to your data and then the sigma j zero, you will use a big spherical
39:43
Covariance
39:47
So that's helps you to not be stuck in very low and small local minimum. So, you will use sigma square identity, where a sigma square is big.
40:00
And then the pies.
40:03
You could initialize them with the proportions.
40:08
Of points which when you run came in class best which fail in one cluster.
40:22
And as I mentioned before, so if you do em step in a Gaussian Mixture Model.
40:32
With
40:34
A fixed covariance
40:39
Which is just sigma square identity.
40:42
So you don't actually model the currency, just use it for variances and you let the width, the variance goes to zero.
40:54
You get the Cayman algorithm.
41:01
That's, that's what I'm saying. Key mean can be seen as the limit of em in a Gaussian Mixture Model when the variance goes to zero. And so to see this. You can look back at the updates.
41:15
So,
41:18
If
41:22
If I use
41:24
Very
41:26
Very narrow variance, which is the same thing as having a very precise direction. So it means that
41:35
Unless I'm very, very close. I have almost zero quality and what will happen is that when you compare the when you will compute this counts, you will basically get zero for all components we unless the one which are the closes, I guess.
41:57
The way you could show that is you could factor eyes in this expression.
42:04
Because I guess. Okay, so we can actually write it down here. So this here, what you get if I divide up in the bottom with this I get one divided by one plus summation of l not equal to j.
42:20
Of pi l normal x i.
42:29
You L sigma L.
42:33
And the thing is, this thing is x
42:37
Minus norm of x i.
42:42
That's this one.
42:47
X minus new el transpose sigma x minus one x minus new now.
42:57
There's some constant
42:59
Divided by
43:01
Two right
43:04
Alright, so I'm not going into details, but I'm just trying to give you the intuition. Now the problem is. All right, so if if I use the identity covariance. All I get is the norm between xi and UL and then they will have a sigma minus two.
43:20
And so when sigma is very close to zero, this will really blow up to
43:30
To a high value.
43:34
Oh, I forgot. There was a ratio. Okay, so
43:40
I get here, I would have
43:44
divided by pi j and blah, blah, blah, with you, Jay. Sigma G. OK. So this is kind of a mess here. Now, but the idea is, I will have the ratio of the distance
44:02
Perhaps I should have not tried to do that here. But basically, I will have the distance to L versus the distance to Jay and the one which is the closest will dominate and so
44:15
The result of this is that this data is at when sigma goes to zero will be equal to one for the mean, which is the closest to your point, and zero for all the other means. Okay.
44:29
And so
44:32
When you do em.
44:35
For a Gaussian Mixture Model.
44:38
Where you use
44:40
A fixed covariance, which is sigma square I with a very tiny covariance, very tiny, tiny variance, you get that the Tao, or basically zero and one. So there are hard cluster assignment like you doing Kimmy.
44:55
And then because these will just become zero. And once they mean update will just become the standard gaming update
user avatar
Unknown Speaker
45:03
Okay.
user avatar
Lacoste-Julien Simon
45:07
And as I mentioned during Kimmy.
45:09
Kimmy is like, assuming that all the cluster or spherical. So that's why it's the identity covariance
45:15
The nice advantage of gushing mixture model versus K mean is you will learn also the the cover and structure of the data. And so you will allow also to get cluster which are not spherical because you're learning this this conference.
45:29
So is there any question.
45:47
Yeah, so sorry attack is pointing out that if you do great descent in a Gaussian Mixture Model. You cannot just do green descent.
45:53
Because you need to make sure that your parameters are valid parameters. So, in particular the covariance matrix should stay positive.
46:01
strictly positive difference. So it's actually a constrained optimization problems you have to be careful if you do gradient updates was em here it's closed form updates, which or actually this case satisfied, a constraint. So it's not a problem.
46:16
So yeah, so doing grand descent on a Gaussian Mixture Model is non trivial. In this case,
46:22
You would need to do something which is called projected graded.
46:28
Any other question.
46:43
Oh, I guess I stopped. They stopped sharing. I didn't want to do it.
46:51
Sorry about that. I wanted to stop recording
46:57
So he's asking if came in, plus, plus.
47:01
Guarantee ensure conversions to a global max where the algorithm know
47:07
Gaming plus plus is just a way to get a good good initialization. But there's no theoretical convergence guarantees that you will get the global max for Kimmy Kimmy plus plus give you give you with high quality and approximation quality of the
47:22
Of the local max that you find it will be within luck K of the global max. But this doesn't really translate directly to the GMM
47:31
Global max. So this is more like a Eucharistic
47:40
So sorry fact mentioned that
47:43
Is that a problem that permutation of the parameters lead to the same likelihood
47:46
Well, it's kind of a problem. And it's also Why indeed like you get a non concave optimization, because you can just like
47:52
flipped the the the identity of by one and part two, and then new one in YouTube. And it's the exact same model is just I flipped the
48:02
The name of which cluster. I'm talking about. And so there are actually k factorial local max for sure in this problem. And so that's the problem.
48:12
Of latent variable model and it gives you kind of like optimization, kind of like issues, but we don't care because we don't care what's the identity of their cluster in this case.
48:23
I call this cluster one and this duster to have is versa doesn't change anything. It's the same model. So as you know, whichever yet to converge to is fine, but it will give kind of indeed optimization issues.
48:38
Okay, well I let me stop the recording.
user avatar
Unknown Speaker
48:42
And
user avatar
Lacoste-Julien Simon
48:44
Stop recording so